# @package _global_

defaults:
  - /data@data: lentils
  - /training@training: default
  - _self_

name: drcnn_adaclip

output_dir: ./outputs/${name}

# Data split IDs (from configs/data/lentils.yaml, shown here for visibility)
# train_ids: [0, 2, 3]  # Training frames
# val_ids: [1]       # Validation frames  
# test_ids: [1, 5]   # Test frames

# Debug settings - save intermediate tensors for inspection
debug:
  save_intermediates: true  # Set to false to disable debug saving
  save_dir: ${output_dir}/debug_intermediates

# Pipeline configuration (commented out - default values are used in drcnn_adaclip_gradient_training copy.py)
# This prevents YAML serialization issues when saving TrainRunConfig
# The script uses pipeline_cfg.get() with default values matching the commented values below
# pipeline:
#   data_node:
#     normal_class_ids: [0, 1]
#   
#   normalizer:
#     eps: 1e-6
#     use_running_stats: true
#   
#   mixer:
#     leaky_relu_negative_slope: 0.1  # Increased from 0.01 to be less aggressive
#     use_bias: true
#     use_activation: true
#     normalize_output: true  # Per-image, per-channel min-max normalization to [0, 1] for AdaClip compatibility
#     init_method: pca  # Options: "xavier", "kaiming", "pca", "zeros"
#     eps: 1e-6
#     reduction_scheme: [61, 16, 8, 3]  # Multi-layer gradual reduction (matches DRCNN paper style)
#   
#   adaclip:
#     weight_name: pretrained_all
#     backbone: ViT-L-14-336
#     prompt_text: "normal: lentils, anomaly: stones"
#     image_size: 518
#     prompting_depth: 4
#     prompting_length: 5
#     gaussian_sigma: 4.0
#     use_half_precision: false
#     enable_warmup: false
#     enable_gradients: true
#   
#   loss:
#     weight: 1.0
#     smooth: 1e-6
#     normalize_method: minmax  # Changed from "sigmoid" to "minmax" to preserve dynamic range
#   
#   decider:
#     quantile: 0.995
#   
#   viz:
#     mask_channel: 30
#     up_to: 5

training:
  seed: 42
  trainer:
    max_epochs: 20  # Reduced for testing
    accelerator: auto
    devices: 1
    default_root_dir: ${output_dir}
    precision: 32-true
    accumulate_grad_batches: 1
    enable_progress_bar: true
    enable_checkpointing: true
    log_every_n_steps: 10
    val_check_interval: 1.0
    check_val_every_n_epoch: 1
    gradient_clip_val: 1.0
    deterministic: false
    benchmark: false
  optimizer:
    name: adamw
    lr: 0.001  # Reduced from 0.01 to avoid instability after early progress
    weight_decay: 0.01
    betas: [0.9, 0.999]
  scheduler:
    name: reduce_on_plateau
    monitor: metrics_anomaly/iou
    mode: max
    factor: 0.5
    patience: 5
    threshold: 0.0001
    threshold_mode: rel
    cooldown: 0
    min_lr: 1e-6
    eps: 1e-8
    verbose: false

# Nodes to unfreeze for gradient training
unfreeze_nodes:
  - channel_mixer

# Loss and metric nodes
loss_nodes:
  - iou_loss

metric_nodes:
  - metrics_anomaly

freeze_nodes: []  # All other nodes remain frozen (AdaClip stays frozen)

