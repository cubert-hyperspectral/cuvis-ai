{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CUVIS.AI Documentation","text":"<p>CUVIS.AI is a modular, low-code/no-code framework for building reproducible machine-learning pipelines for hyperspectral data analysis. It provides a thin abstraction over PyTorch, PyTorch Lightning, and Hydra, with reusable nodes you can compose into graph-based HSI workflows.</p>"},{"location":"#what-you-can-do","title":"What you can do","text":"<ul> <li>Typed I/O System: Port-based connections with type safety and validation </li> <li>Statistical Initialization: Bootstrap models with non-parametric methods (RX detector, PCA) </li> <li>Gradient-Based Training: Fine-tune models with PyTorch Lightning </li> <li>Flexible Node Architecture: Composable preprocessing, feature extraction, and decision modules </li> <li>Comprehensive Monitoring: Integrated TensorBoard support and extendible to other frameworks</li> <li>Configuration Management: Hydra-based configuration with CLI overrides</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quickstart</li> <li>Core Concepts</li> <li>API Reference</li> <li>Tutorials</li> </ul> <p>Ready to get started?</p> <ul> <li>Start with the Installation Guide, then follow the Quickstart.</li> <li>Want to contribute? See the Contributing Guide.</li> <li>Found an issue? Report bugs / request features</li> </ul> <p>Apache License 2.0 \u2014 see LICENSE.</p>"},{"location":"api/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/#api-reference","title":"API Reference","text":"<p>Complete API documentation for CUVIS.AI modules and classes.</p>"},{"location":"api/#api-modules","title":"API Modules","text":"<ul> <li> <p> Pipeline &amp; Graph</p> <p>Pipeline and graph construction APIs</p> </li> <li> <p> Nodes</p> <p>Node base classes and node implementations</p> </li> <li> <p> Ports</p> <p>Port system and data contracts</p> </li> <li> <p> Training</p> <p>Training loops and utilities</p> </li> <li> <p> Data</p> <p>Data loading and dataset management</p> </li> <li> <p> Utilities</p> <p>Helper functions and utilities</p> </li> <li> <p> gRPC API</p> <p>Complete gRPC service documentation with all 46 RPC methods</p> </li> </ul> <p>Note: API documentation is auto-generated from code docstrings using mkdocstrings.</p>"},{"location":"api/data/","title":"Data","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/data/#data-api","title":"Data API","text":"<p>Data loading and preprocessing nodes.</p>"},{"location":"api/data/#overview","title":"Overview","text":"<p>Data loading functionality in CUVIS.AI is provided through data nodes. See the complete node documentation below.</p>"},{"location":"api/data/#data-nodes","title":"Data Nodes","text":""},{"location":"api/data/#cuvis_ai.node.data","title":"data","text":"<p>Data loading nodes for hyperspectral anomaly detection pipelines.</p> <p>This module provides specialized data nodes that convert multi-class segmentation datasets into binary anomaly detection tasks. Data nodes handle type conversions, label mapping, and format transformations required for pipeline processing.</p>"},{"location":"api/data/#cuvis_ai.node.data.LentilsAnomalyDataNode","title":"LentilsAnomalyDataNode","text":"<pre><code>LentilsAnomalyDataNode(\n    normal_class_ids, anomaly_class_ids=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Data node for Lentils anomaly detection dataset with binary label mapping.</p> <p>Converts multi-class Lentils segmentation data to binary anomaly detection format. Maps specified class IDs to normal (0) or anomaly (1) labels, and handles type conversions from uint16 to float32 for hyperspectral cubes.</p> <p>Parameters:</p> Name Type Description Default <code>normal_class_ids</code> <code>list[int]</code> <p>List of class IDs to treat as normal background (e.g., [0, 1] for unlabeled and black lentils)</p> required <code>anomaly_class_ids</code> <code>list[int] | None</code> <p>List of class IDs to treat as anomalies. If None, all classes not in normal_class_ids are treated as anomalies (default: None)</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to Node base class</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>_binary_mapper</code> <code>BinaryAnomalyLabelMapper</code> <p>Internal label mapper for converting multi-class to binary masks</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.node.data import LentilsAnomalyDataNode\n&gt;&gt;&gt; from cuvis_ai_core.data.datasets import SingleCu3sDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create datamodule for Lentils dataset\n&gt;&gt;&gt; datamodule = SingleCu3sDataModule(\n...     data_dir=\"data/lentils\",\n...     batch_size=4,\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data node with normal class specification\n&gt;&gt;&gt; data_node = LentilsAnomalyDataNode(\n...     normal_class_ids=[0, 1],  # Unlabeled and black lentils are normal\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use in pipeline\n&gt;&gt;&gt; pipeline.add_node(data_node)\n&gt;&gt;&gt; pipeline.connect(\n...     (data_node.cube, normalizer.data),\n...     (data_node.mask, metrics.targets),\n... )\n</code></pre> See Also <p>BinaryAnomalyLabelMapper : Label mapping utility used internally SingleCu3sDataModule : DataModule for loading CU3S hyperspectral data docs/tutorials/rx-statistical.md : Complete example with LentilsAnomalyDataNode</p> Notes <p>The node performs the following transformations: - Converts hyperspectral cube from uint16 to float32 - Maps multi-class mask [B, H, W] to binary mask [B, H, W, 1] - Extracts wavelengths from first batch element (assumes consistent wavelengths)</p> Source code in <code>cuvis_ai/node/data.py</code> <pre><code>def __init__(\n    self, normal_class_ids: list[int], anomaly_class_ids: list[int] | None = None, **kwargs\n) -&gt; None:\n    super().__init__(\n        normal_class_ids=normal_class_ids, anomaly_class_ids=anomaly_class_ids, **kwargs\n    )\n\n    self._binary_mapper = BinaryAnomalyLabelMapper(  # could have be used as a node as well\n        normal_class_ids=normal_class_ids,\n        anomaly_class_ids=anomaly_class_ids,\n    )\n</code></pre>"},{"location":"api/data/#cuvis_ai.node.data.LentilsAnomalyDataNode.forward","title":"forward","text":"<pre><code>forward(cube, mask=None, wavelengths=None, **_)\n</code></pre> <p>Process hyperspectral cube and convert labels to binary anomaly format.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Input hyperspectral cube, shape (B, H, W, C), dtype uint16</p> required <code>mask</code> <code>Tensor | None</code> <p>Multi-class segmentation mask, shape (B, H, W), dtype int32. If None, only cube is returned (default: None)</p> <code>None</code> <code>wavelengths</code> <code>Tensor | None</code> <p>Wavelengths for each channel, shape (B, C), dtype int32. If None, wavelengths are not included in output (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Tensor | ndarray]</code> <p>Dictionary containing: - \"cube\" : torch.Tensor     Converted hyperspectral cube, shape (B, H, W, C), dtype float32 - \"mask\" : torch.Tensor (optional)     Binary anomaly mask, shape (B, H, W, 1), dtype bool.     Only included if input mask is provided. - \"wavelengths\" : np.ndarray (optional)     Wavelength array, shape (C,), dtype int32.     Only included if input wavelengths are provided.</p> Source code in <code>cuvis_ai/node/data.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    wavelengths: torch.Tensor | None = None,\n    **_: Any,\n) -&gt; dict[str, torch.Tensor | np.ndarray]:\n    \"\"\"Process hyperspectral cube and convert labels to binary anomaly format.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Input hyperspectral cube, shape (B, H, W, C), dtype uint16\n    mask : torch.Tensor | None, optional\n        Multi-class segmentation mask, shape (B, H, W), dtype int32.\n        If None, only cube is returned (default: None)\n    wavelengths : torch.Tensor | None, optional\n        Wavelengths for each channel, shape (B, C), dtype int32.\n        If None, wavelengths are not included in output (default: None)\n\n    Returns\n    -------\n    dict[str, torch.Tensor | np.ndarray]\n        Dictionary containing:\n        - \"cube\" : torch.Tensor\n            Converted hyperspectral cube, shape (B, H, W, C), dtype float32\n        - \"mask\" : torch.Tensor (optional)\n            Binary anomaly mask, shape (B, H, W, 1), dtype bool.\n            Only included if input mask is provided.\n        - \"wavelengths\" : np.ndarray (optional)\n            Wavelength array, shape (C,), dtype int32.\n            Only included if input wavelengths are provided.\n    \"\"\"\n    result: dict[str, torch.Tensor | np.ndarray] = {\"cube\": cube.to(torch.float32)}\n\n    # wavelengths passthrough, could check that in all batch elements the same wavelengths are used\n    # input B x C -&gt; output C\n    if wavelengths is not None:\n        result[\"wavelengths\"] = wavelengths[0].cpu().numpy()\n\n    if mask is not None:\n        # Add channel dimension for mapper: BHW -&gt; BHWC\n        mask_4d = mask.unsqueeze(-1)\n\n        # Always apply binary mapper\n        mapped = self._binary_mapper.forward(\n            cube=cube,\n            mask=mask_4d,\n            **_,  # Pass through additional kwargs\n        )\n        result[\"mask\"] = mapped[\"mask\"]  # Already BHWC bool\n\n    return result\n</code></pre>"},{"location":"api/data/#related-pages","title":"Related Pages","text":"<ul> <li>Data Nodes Catalog</li> <li>Quickstart Guide</li> </ul>"},{"location":"api/nodes/","title":"Nodes","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/nodes/#nodes-api","title":"Nodes API","text":"<p>Complete API documentation for all node classes and implementations.</p>"},{"location":"api/nodes/#overview","title":"Overview","text":"<p>Nodes are the building blocks of CUVIS.AI pipelines. This page documents all available node implementations organized by functional category.</p>"},{"location":"api/nodes/#anomaly-detection-nodes","title":"Anomaly Detection Nodes","text":"<p>Statistical and deep learning methods for detecting anomalies in hyperspectral data.</p>"},{"location":"api/nodes/#rx-detector","title":"RX Detector","text":""},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector","title":"rx_detector","text":"<p>RX anomaly detection nodes for hyperspectral imaging.</p> <p>This module implements the Reed-Xiaoli (RX) anomaly detection algorithm, a widely used statistical method for detecting anomalies in hyperspectral images. The RX algorithm computes squared Mahalanobis distance from the background distribution, treating pixels with large distances as potential anomalies.</p> <p>The module provides two variants:</p> <ul> <li> <p>RXGlobal: Uses global statistics (mean, covariance) estimated from training data.   Supports two-phase training: statistical initialization followed by optional gradient-based   fine-tuning via unfreeze().</p> </li> <li> <p>RXPerBatch: Computes statistics independently for each batch on-the-fly without   requiring initialization. Useful for real-time processing or when training data is unavailable.</p> </li> </ul> <p>Reference:     Reed, I. S., &amp; Yu, X. (1990). \"Adaptive multiple-band CFAR detection of an optical     pattern with unknown spectral distribution.\" IEEE Transactions on Acoustics, Speech,     and Signal Processing, 38(10), 1760-1770.</p>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXBase","title":"RXBase","text":"<pre><code>RXBase(eps=1e-06, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Base class for RX anomaly detectors.</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def __init__(self, eps: float = 1e-6, **kwargs) -&gt; None:\n    self.eps = eps\n    super().__init__(eps=eps, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal","title":"RXGlobal","text":"<pre><code>RXGlobal(\n    num_channels, eps=1e-06, cache_inverse=True, **kwargs\n)\n</code></pre> <p>               Bases: <code>RXBase</code></p> <p>RX anomaly detector with global background statistics.</p> <p>Uses global mean (\u03bc) and covariance (\u03a3) estimated from training data to compute Mahalanobis distance scores. Supports two-phase training: statistical initialization followed by optional gradient-based fine-tuning.</p> <p>The detector computes anomaly scores as:</p> <pre><code>RX(x) = (x - \u03bc)\u1d40 \u03a3\u207b\u00b9 (x - \u03bc)\n</code></pre> <p>where x is a pixel spectrum, \u03bc is the background mean, and \u03a3 is the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>num_channels</code> <code>int</code> <p>Number of spectral channels in input data</p> required <code>eps</code> <code>float</code> <p>Small constant added to covariance diagonal for numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>cache_inverse</code> <code>bool</code> <p>If True, precompute and cache \u03a3\u207b\u00b9 for faster inference (default: True)</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to Node base class</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>Tensor or Parameter</code> <p>Background mean spectrum, shape (C,). Initially a buffer, becomes Parameter after unfreeze()</p> <code>cov</code> <code>Tensor or Parameter</code> <p>Background covariance matrix, shape (C, C)</p> <code>cov_inv</code> <code>Tensor or Parameter</code> <p>Cached pseudo-inverse of covariance (if cache_inverse=True)</p> <code>_statistically_initialized</code> <code>bool</code> <p>Flag indicating whether statistical_initialization() has been called</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.anomaly.rx_detector import RXGlobal\n&gt;&gt;&gt; from cuvis_ai_core.training import StatisticalTrainer\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create RX detector\n&gt;&gt;&gt; rx = RXGlobal(num_channels=61, eps=1.0e-6)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Phase 1: Statistical initialization\n&gt;&gt;&gt; stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n&gt;&gt;&gt; stat_trainer.fit()  # Computes \u03bc and \u03a3 from training data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Inference with frozen statistics\n&gt;&gt;&gt; output = rx.forward(data=hyperspectral_cube)\n&gt;&gt;&gt; scores = output[\"scores\"]  # [B, H, W, 1]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Phase 2: Optional gradient-based fine-tuning\n&gt;&gt;&gt; rx.unfreeze()  # Convert buffers to nn.Parameters\n&gt;&gt;&gt; # Now \u03bc and \u03a3 can be updated with gradient descent\n</code></pre> See Also <p>RXPerBatch : Per-batch RX variant without training MinMaxNormalizer : Recommended preprocessing before RX ScoreToLogit : Convert scores to logits for classification docs/tutorials/rx-statistical.md : Complete RX pipeline tutorial</p> Notes <p>After statistical_initialization(), mu and cov are stored as buffers (frozen by default). Call unfreeze() to convert them to trainable nn.Parameters for gradient-based optimization.</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def __init__(\n    self, num_channels: int, eps: float = 1e-6, cache_inverse: bool = True, **kwargs\n) -&gt; None:\n    self.num_channels = int(num_channels)\n    self.eps = eps\n    self.cache_inverse = cache_inverse\n    # Call Node.__init__ directly with all parameters for proper serialization\n    # We bypass RXBase.__init__ since it only accepts eps\n    # Node.__init__(self, num_channels=self.num_channels, eps=self.eps, cache_inverse=self.cache_inverse)\n\n    super().__init__(\n        num_channels=self.num_channels, eps=self.eps, cache_inverse=self.cache_inverse, **kwargs\n    )\n\n    # global stats - all stored as buffers initially\n    self.register_buffer(\"mu\", torch.zeros(self.num_channels, dtype=torch.float32))  # (C,)\n    self.register_buffer(\n        \"cov\", torch.zeros(self.num_channels, self.num_channels, dtype=torch.float32)\n    )  # (C,C)\n    self.register_buffer(\n        \"cov_inv\", torch.zeros(self.num_channels, self.num_channels, dtype=torch.float32)\n    )  # (C,C)\n    # Streaming accumulators (float64 for numerical stability)\n    self.register_buffer(\"_mean\", torch.zeros(self.num_channels, dtype=torch.float64))\n    self.register_buffer(\n        \"_M2\", torch.zeros(self.num_channels, self.num_channels, dtype=torch.float64)\n    )\n    self._n = 0\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize mu and Sigma from data iterator.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Iterator yielding dicts matching INPUT_SPECS (port-based format) Expected format: {\"data\": tensor} where tensor is BHWC</p> required Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize mu and Sigma from data iterator.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Iterator yielding dicts matching INPUT_SPECS (port-based format)\n        Expected format: {\"data\": tensor} where tensor is BHWC\n    \"\"\"\n    self.reset()\n    for batch_data in input_stream:\n        # Extract data from port-based dict\n        x = batch_data[\"data\"]\n        if x is not None:\n            self.update(x)\n\n    if self._n &gt; 0:\n        self.finalize()\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.unfreeze","title":"unfreeze","text":"<pre><code>unfreeze()\n</code></pre> <p>Convert mu and cov buffers to trainable nn.Parameters.</p> <p>Call this method after fit() to enable gradient-based optimization of the mean and covariance statistics. They will be converted from buffers to nn.Parameters, allowing gradient updates during training.</p> Example <p>rx.fit(input_stream)  # Statistical initialization rx.unfreeze()  # Enable gradient training</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Convert mu and cov buffers to trainable nn.Parameters.\n\n    Call this method after fit() to enable gradient-based optimization of\n    the mean and covariance statistics. They will be converted from buffers\n    to nn.Parameters, allowing gradient updates during training.\n\n    Example\n    -------\n    &gt;&gt;&gt; rx.fit(input_stream)  # Statistical initialization\n    &gt;&gt;&gt; rx.unfreeze()  # Enable gradient training\n    &gt;&gt;&gt; # Now RX statistics can be fine-tuned with gradient descent\n    \"\"\"\n    if self.mu.numel() &gt; 0 and self.cov.numel() &gt; 0:\n        # Convert buffers to parameters\n        self.mu = nn.Parameter(self.mu.clone(), requires_grad=True)\n        self.cov = nn.Parameter(self.cov.clone(), requires_grad=True)\n        if self.cov_inv.numel() &gt; 0:\n            self.cov_inv = nn.Parameter(self.cov_inv.clone(), requires_grad=True)\n    # Call parent to enable requires_grad\n    super().unfreeze()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.unfreeze--now-rx-statistics-can-be-fine-tuned-with-gradient-descent","title":"Now RX statistics can be fine-tuned with gradient descent","text":""},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.update","title":"update","text":"<pre><code>update(batch_bhwc)\n</code></pre> <p>Update streaming statistics with a new batch using Welford's algorithm.</p> <p>This method incrementally computes mean and covariance from batches of data using Welford's numerically stable online algorithm. Multiple batches can be processed sequentially before calling finalize() to compute final statistics.</p> <p>Parameters:</p> Name Type Description Default <code>batch_bhwc</code> <code>Tensor</code> <p>Input batch in BHWC format, shape (B, H, W, C)</p> required Notes <p>The algorithm maintains running accumulators (_mean, _M2, _n) in float64 for numerical stability. Batches with \u22641 samples are ignored. After update(), call finalize() to compute mu and cov from the accumulated statistics.</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>@torch.no_grad()\ndef update(self, batch_bhwc: torch.Tensor) -&gt; None:\n    \"\"\"Update streaming statistics with a new batch using Welford's algorithm.\n\n    This method incrementally computes mean and covariance from batches of data\n    using Welford's numerically stable online algorithm. Multiple batches can be\n    processed sequentially before calling finalize() to compute final statistics.\n\n    Parameters\n    ----------\n    batch_bhwc : torch.Tensor\n        Input batch in BHWC format, shape (B, H, W, C)\n\n    Notes\n    -----\n    The algorithm maintains running accumulators (_mean, _M2, _n) in float64\n    for numerical stability. Batches with \u22641 samples are ignored. After update(),\n    call finalize() to compute mu and cov from the accumulated statistics.\n    \"\"\"\n    X = _flatten_bhwc(batch_bhwc).reshape(-1, batch_bhwc.shape[-1])  # (M,C)\n    m = X.shape[0]\n    if m &lt;= 1:\n        return\n    mean_b = X.mean(0)\n    M2_b = (X - mean_b).T @ (X - mean_b)\n    if self._n == 0:\n        self._n, self._mean, self._M2 = m, mean_b, M2_b\n    else:\n        n, tot = self._n, self._n + m\n        delta = mean_b - self._mean\n        new_mean = self._mean + delta * (m / tot)\n        outer = torch.outer(delta, delta) * (n * m / tot)\n        self._n, self._mean, self._M2 = tot, new_mean, self._M2 + M2_b + outer\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Compute final mean and covariance from accumulated streaming statistics.</p> <p>This method converts the running accumulators (_mean, _M2) into the final mean (mu) and covariance (cov) matrices. The covariance is regularized with eps * I for numerical stability, and optionally caches the pseudo-inverse.</p> <p>Returns:</p> Type Description <code>RXGlobal</code> <p>Returns self for method chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fewer than 2 samples were accumulated (insufficient for covariance estimation)</p> Notes <p>After finalization, mu and cov are stored as buffers (frozen by default). Call unfreeze() to convert them to nn.Parameters for gradient-based training.</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>@torch.no_grad()\ndef finalize(self) -&gt; \"RXGlobal\":\n    \"\"\"Compute final mean and covariance from accumulated streaming statistics.\n\n    This method converts the running accumulators (_mean, _M2) into the final\n    mean (mu) and covariance (cov) matrices. The covariance is regularized with\n    eps * I for numerical stability, and optionally caches the pseudo-inverse.\n\n    Returns\n    -------\n    RXGlobal\n        Returns self for method chaining\n\n    Raises\n    ------\n    ValueError\n        If fewer than 2 samples were accumulated (insufficient for covariance estimation)\n\n    Notes\n    -----\n    After finalization, mu and cov are stored as buffers (frozen by default).\n    Call unfreeze() to convert them to nn.Parameters for gradient-based training.\n    \"\"\"\n    if self._n &lt;= 1:\n        raise ValueError(\"Not enough samples to finalize.\")\n    mu = self._mean.clone()\n    cov = self._M2 / (self._n - 1)\n    if self.eps &gt; 0:\n        cov = cov + self.eps * torch.eye(cov.shape[0], device=cov.device, dtype=cov.dtype)\n    # Always store as buffers initially (frozen by default)\n    self.mu = mu\n    self.cov = cov\n    if self.cache_inverse:\n        self.cov_inv = torch.linalg.pinv(cov)\n    else:\n        self.cov_inv = torch.empty(0, 0)\n    self._statistically_initialized = True\n    return self\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset all statistics and accumulators to empty state.</p> <p>Clears mu, cov, cov_inv, and all streaming accumulators (_mean, _M2, _n). After reset, the detector must be re-initialized via statistical_initialization() before it can be used for inference.</p> Notes <p>Use this method when you need to re-initialize the detector with different training data or when switching between different dataset distributions.</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all statistics and accumulators to empty state.\n\n    Clears mu, cov, cov_inv, and all streaming accumulators (_mean, _M2, _n).\n    After reset, the detector must be re-initialized via statistical_initialization()\n    before it can be used for inference.\n\n    Notes\n    -----\n    Use this method when you need to re-initialize the detector with different\n    training data or when switching between different dataset distributions.\n    \"\"\"\n    self.mu = torch.empty(0)\n    self.cov = torch.empty(0, 0)\n    self.cov_inv = torch.empty(0, 0)\n    self._n = 0\n    self._mean = torch.empty(0)\n    self._M2 = torch.empty(0, 0)\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXGlobal.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Forward pass computing anomaly scores.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor in BHWC format</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"scores\" key containing BHW1 anomaly scores</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Forward pass computing anomaly scores.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input tensor in BHWC format\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"scores\" key containing BHW1 anomaly scores\n    \"\"\"\n    if not self._statistically_initialized or self.mu.numel() == 0:\n        raise RuntimeError(\n            \"RXGlobal not initialized. Call statistical_initialization() before inference.\"\n        )\n    B, H, W, C = data.shape\n    N = H * W\n    X = data.view(B, N, C)\n    # Convert dtype if needed, but don't change device (assumes everything on same device)\n    Xc = X - self.mu.to(X.dtype)\n    if self.cov_inv.numel() &gt; 0:\n        cov_inv = self.cov_inv.to(X.dtype)\n        md2 = torch.einsum(\"bnc,cd,bnd-&gt;bn\", Xc, cov_inv, Xc)  # (B,N)\n    else:\n        md2 = self._quad_form_solve(Xc, self.cov.to(X.dtype))\n    scores = md2.view(B, H, W).unsqueeze(-1)  # Add channel dimension (B,H,W,1)\n    return {\"scores\": scores}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXPerBatch","title":"RXPerBatch","text":"<pre><code>RXPerBatch(eps=1e-06, **kwargs)\n</code></pre> <p>               Bases: <code>RXBase</code></p> <p>Computes \u03bc, \u03a3 per image in the batch on the fly; no fit/finalize.</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def __init__(self, eps: float = 1e-6, **kwargs) -&gt; None:\n    self.eps = eps\n    super().__init__(eps=eps, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.rx_detector.RXPerBatch.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Forward pass computing per-batch anomaly scores.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor in BHWC format</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"scores\" key containing BHW1 anomaly scores</p> Source code in <code>cuvis_ai/anomaly/rx_detector.py</code> <pre><code>def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Forward pass computing per-batch anomaly scores.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input tensor in BHWC format\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"scores\" key containing BHW1 anomaly scores\n    \"\"\"\n    B, H, W, C = data.shape\n    N = H * W\n    X_flat = _flatten_bhwc(data)  # (B,N,C)\n    mu = X_flat.mean(1, keepdim=True)  # (B,1,C)\n    Xc = X_flat - mu\n    cov = torch.matmul(Xc.transpose(1, 2), Xc) / max(N - 1, 1)  # (B,C,C)\n    eye = torch.eye(C, device=data.device, dtype=data.dtype).expand(B, C, C)\n    cov = cov + self.eps * eye\n    md2 = self._quad_form_solve(Xc, cov)  # (B,N)\n    scores = md2.view(B, H, W)\n    return {\"scores\": scores.unsqueeze(-1)}\n</code></pre>"},{"location":"api/nodes/#lad-detector","title":"LAD Detector","text":""},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector","title":"lad_detector","text":"<p>Laplacian Anomaly Detector (LAD) for hyperspectral anomaly detection.</p> <p>This module implements the Laplacian Anomaly Detector, a graph-based approach for detecting spectral anomalies in hyperspectral images. LAD constructs a spectral graph using Cauchy similarity weights and computes anomaly scores based on the graph Laplacian.</p> <p>The LAD algorithm identifies anomalies by measuring how unusual a pixel's spectral signature is within the spectral manifold learned from background data. Unlike RX detectors that assume Gaussian distributions, LAD captures nonlinear manifold structures through graph construction.</p> <p>Reference:     Gu, Y., Liu, Y., &amp; Zhang, Y. (2008). \"A selective KPCA algorithm based on high-order     statistics for anomaly detection in hyperspectral imagery.\" IEEE Geoscience and     Remote Sensing Letters, 5(1), 43-47.</p>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal","title":"LADGlobal","text":"<pre><code>LADGlobal(\n    num_channels,\n    eps=1e-08,\n    normalize_laplacian=True,\n    use_numpy_laplacian=True,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Laplacian Anomaly Detector (global), variant 'C' (Cauchy), port-based.</p> <p>This is the new cuvis.ai v3 implementation of the LAD detector. It follows the same mathematical definition as the legacy v2 <code>LADGlobal</code>, but exposes a port-based interface compatible with <code>CuvisPipeline</code>, <code>StatisticalTrainer</code>, and <code>GradientTrainer</code>.</p> Ports <p>INPUT_SPECS     <code>data</code> : float32, shape (-1, -1, -1, -1)         Input hyperspectral cube in BHWC format. OUTPUT_SPECS     <code>scores</code> : float32, shape (-1, -1, -1, 1)         Per pixel anomaly scores in BHW1 format.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>Small epsilon value for numerical stability in Laplacian construction.</p> <code>1e-8</code> <code>normalize_laplacian</code> <code>bool</code> <p>If True, applies symmetric normalization: L = D^{-\u00bd} (D - A) D^{-\u00bd}. If False, uses unnormalized Laplacian: L = D - A.</p> <code>True</code> <code>use_numpy_laplacian</code> <code>bool</code> <p>If True, constructs the Laplacian matrix using NumPy (float64, 1e-12 eps) for parity with reference implementations. If False, uses pure PyTorch.</p> <code>True</code> Training <p>After statistical initialization via <code>statistical_initialization()</code>, the node can be made trainable by calling <code>unfreeze()</code>. This converts the mean <code>M</code> and Laplacian <code>L</code> buffers to trainable <code>nn.Parameter</code> objects, enabling gradient-based fine-tuning.</p> Example <p>lad = LADGlobal(num_channels=61) stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule) stat_trainer.fit()  # Statistical initialization lad.unfreeze()  # Enable gradient training grad_trainer = GradientTrainer(pipeline=pipeline, datamodule=datamodule, ...) grad_trainer.fit()  # Gradient-based fine-tuning</p> Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>def __init__(\n    self,\n    num_channels: int,\n    eps: float = 1e-8,\n    normalize_laplacian: bool = True,\n    use_numpy_laplacian: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    self.num_channels = int(num_channels)\n    self.eps = float(eps)\n    self.normalize_laplacian = bool(normalize_laplacian)\n    self.use_numpy_laplacian = bool(use_numpy_laplacian)\n\n    super().__init__(\n        num_channels=self.num_channels,\n        eps=self.eps,\n        normalize_laplacian=self.normalize_laplacian,\n        use_numpy_laplacian=self.use_numpy_laplacian,\n        **kwargs,\n    )\n\n    # Streaming accumulators (float64 for numerical stability)\n    self.register_buffer(\n        \"_mean_run\", torch.zeros(self.num_channels, dtype=torch.float64)\n    )  # (C,)\n    self._count: int = 0\n    # Model buffers\n    self.register_buffer(\"M\", torch.zeros(self.num_channels, dtype=torch.float64))  # (C,)\n    self.register_buffer(\n        \"L\", torch.zeros(self.num_channels, self.num_channels, dtype=torch.float64)\n    )  # (C, C)\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Compute global mean M and Laplacian L from a port-based input stream.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Iterator yielding dicts matching INPUT_SPECS. Expected format: <code>{\"data\": tensor}</code> where tensor is BHWC.</p> required Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Compute global mean M and Laplacian L from a port-based input stream.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Iterator yielding dicts matching INPUT_SPECS.\n        Expected format: ``{\"data\": tensor}`` where tensor is BHWC.\n    \"\"\"\n    self.reset()\n\n    for batch_data in input_stream:\n        x = batch_data.get(\"data\")\n        if x is not None:\n            self.update(x)\n\n    if self._count &lt;= 0:\n        raise RuntimeError(\"No samples provided to LADGlobal.statistical_initialization()\")\n\n    self.finalize()\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal.update","title":"update","text":"<pre><code>update(batch_bhwc)\n</code></pre> <p>Update running mean statistics from a BHWC batch.</p> Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>@torch.no_grad()\ndef update(self, batch_bhwc: torch.Tensor) -&gt; None:\n    \"\"\"Update running mean statistics from a BHWC batch.\"\"\"\n    B, H, W, C = batch_bhwc.shape\n    X = batch_bhwc.reshape(B * H * W, C).to(dtype=torch.float64)\n    m = X.shape[0]\n    if m &lt;= 0:\n        return\n\n    mean_b = X.mean(dim=0)\n\n    if self._count == 0:\n        self._mean_run = mean_b\n        self._count = m\n    else:\n        tot = self._count + m\n        delta = mean_b - self._mean_run\n        self._mean_run = self._mean_run + delta * (m / tot)\n        self._count = tot\n\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize mean and Laplacian from accumulated statistics.</p> Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>@torch.no_grad()\ndef finalize(self) -&gt; None:\n    \"\"\"Finalize mean and Laplacian from accumulated statistics.\"\"\"\n    if self._count &lt;= 0 or self._mean_run.numel() == 0:\n        raise RuntimeError(\"No samples accumulated for LADGlobal.finalize()\")\n\n    M = self._mean_run.clone().to(dtype=torch.float64)\n    C = M.shape[0]\n    a = M.mean()\n\n    if self.use_numpy_laplacian:\n        # NumPy implementation for exact parity with legacy version\n        M_np = M.detach().cpu().numpy()\n        A_abs = np.abs(M_np[:, None] - M_np[None, :])\n        a_np = float(M_np.mean())\n        A_np = 1.0 / (1.0 + (A_abs / (a_np + 1e-12)) ** 2)\n        np.fill_diagonal(A_np, 0.0)\n        D_np = np.diag(A_np.sum(axis=1))\n        L_np = D_np - A_np\n\n        if self.normalize_laplacian:\n            d_np = np.diag(D_np)\n            d_inv_sqrt_np = np.where(d_np &gt; 0, 1.0 / (np.sqrt(d_np) + 1e-12), 0.0)\n            D_inv_sqrt_np = np.diag(d_inv_sqrt_np)\n            L_np = D_inv_sqrt_np @ L_np @ D_inv_sqrt_np\n\n        L = torch.from_numpy(L_np).to(dtype=torch.float64, device=M.device)\n    else:\n        Mi = M.view(C, 1)\n        Mj = M.view(1, C)\n        denom = a + torch.tensor(1e-12, dtype=torch.float64, device=M.device)\n        diff = torch.abs(Mi - Mj) / denom\n        A = 1.0 / (1.0 + diff.pow(2))\n        A.fill_diagonal_(0.0)\n\n        D = torch.diag(A.sum(dim=1))\n        L = D - A\n\n        if self.normalize_laplacian:\n            d = torch.diag(D)\n            d_inv_sqrt = torch.where(\n                d &gt; 0,\n                1.0 / torch.sqrt(d + torch.tensor(1e-12, dtype=torch.float64, device=M.device)),\n                torch.zeros_like(d),\n            )\n            D_inv_sqrt = torch.diag(d_inv_sqrt)\n            L = D_inv_sqrt @ L @ D_inv_sqrt\n\n    self.M = M\n    self.L = L\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset all statistics and model parameters to initial state.</p> <p>Clears the streaming mean accumulator (_mean_run), sample count (_count), global mean (M), and Laplacian matrix (L). After reset, the detector must be re-initialized via statistical_initialization() before inference.</p> Notes <p>Use this method to re-initialize the detector with different training data or when switching between different spectral distributions.</p> Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>@torch.no_grad()\ndef reset(self) -&gt; None:\n    \"\"\"Reset all statistics and model parameters to initial state.\n\n    Clears the streaming mean accumulator (_mean_run), sample count (_count),\n    global mean (M), and Laplacian matrix (L). After reset, the detector must\n    be re-initialized via statistical_initialization() before inference.\n\n    Notes\n    -----\n    Use this method to re-initialize the detector with different training data\n    or when switching between different spectral distributions.\n    \"\"\"\n    self.register_buffer(\n        \"_mean_run\", torch.zeros(self.num_channels, dtype=torch.float64)\n    )  # (C,)\n    self._count: int = 0\n    # Model buffers\n    self.register_buffer(\"M\", torch.zeros(self.num_channels, dtype=torch.float64))  # (C,)\n    self.register_buffer(\n        \"L\", torch.zeros(self.num_channels, self.num_channels, dtype=torch.float64)\n    )  # (C, C)\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal.unfreeze","title":"unfreeze","text":"<pre><code>unfreeze()\n</code></pre> <p>Convert M and L buffers to trainable nn.Parameters.</p> Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Convert M and L buffers to trainable nn.Parameters.\"\"\"\n    if self.M.numel() &gt; 0 and self.L.numel() &gt; 0:\n        device = self.M.device\n        # Store current values\n        M_data = self.M.clone()\n        L_data = self.L.clone()\n\n        # Remove buffer registrations\n        delattr(self, \"M\")\n        delattr(self, \"L\")\n\n        # Register as parameters\n        self.M = nn.Parameter(M_data, requires_grad=True)\n        self.L = nn.Parameter(L_data, requires_grad=True)\n        self.M.to(device=device)\n        self.L.to(device=device)\n\n    # Call parent to enable requires_grad\n    super().unfreeze()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.lad_detector.LADGlobal.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Compute LAD anomaly scores for a BHWC cube.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor in BHWC format.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with key <code>\"scores\"</code> containing BHW1 anomaly scores.</p> Source code in <code>cuvis_ai/anomaly/lad_detector.py</code> <pre><code>def forward(self, data: torch.Tensor, **_: Any) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Compute LAD anomaly scores for a BHWC cube.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input tensor in BHWC format.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with key ``\"scores\"`` containing BHW1 anomaly scores.\n    \"\"\"\n    if self.M.numel() == 0 or self.L.numel() == 0 or not self._statistically_initialized:\n        raise RuntimeError(\n            \"LADGlobal not finalized. Call statistical_initialization() before forward().\"\n        )\n\n    B, H, W, C = data.shape\n    N = H * W\n\n    X = data.view(B, N, C)\n\n    Xc = X - self.M.to(dtype=X.dtype)\n    L = self.L.to(dtype=X.dtype)\n\n    scores = torch.einsum(\"bnc,cd,bnd-&gt;bn\", Xc, L, Xc).view(B, H, W).unsqueeze(-1)\n    return {\"scores\": scores}\n</code></pre>"},{"location":"api/nodes/#deep-svdd","title":"Deep SVDD","text":""},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd","title":"deep_svdd","text":"<p>Deep SVDD encoder for the port-based cuvis.ai stack.</p>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.SpectralNet","title":"SpectralNet","text":"<pre><code>SpectralNet(in_dim, rep_dim=32, hidden=128)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple 2-layer MLP used by DeepSVDD to produce latent embeddings.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def __init__(self, in_dim: int, rep_dim: int = 32, hidden: int = 128) -&gt; None:\n    super().__init__()\n    self.fc1 = nn.Linear(in_dim, hidden, bias=True)\n    self.fc2 = nn.Linear(hidden, rep_dim, bias=False)\n\n    nn.init.kaiming_uniform_(self.fc1.weight, a=math.sqrt(5))\n    if self.fc1.bias is not None:\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.fc1.weight)\n        bound = 1 / math.sqrt(fan_in)\n        nn.init.uniform_(self.fc1.bias, -bound, bound)\n    nn.init.xavier_uniform_(self.fc2.weight)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.SpectralNet.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through two-layer spectral network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input features [B, C].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Projected features [B, rep_dim].</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through two-layer spectral network.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input features [B, C].\n\n    Returns\n    -------\n    torch.Tensor\n        Projected features [B, rep_dim].\n    \"\"\"\n    x = F.relu(self.fc1(x))\n    return self.fc2(x)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.RFFLayer","title":"RFFLayer","text":"<pre><code>RFFLayer(input_dim, n_features=2048, gamma=0.1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Random Fourier feature encoder for RBF kernels.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def __init__(self, input_dim: int, n_features: int = 2048, gamma: float = 0.1) -&gt; None:\n    super().__init__()\n    scale = math.sqrt(2.0 * float(gamma))\n    W = torch.randn(input_dim, n_features, dtype=torch.get_default_dtype()) * scale\n    b = torch.rand(n_features, dtype=torch.get_default_dtype()) * (2.0 * math.pi)\n    self.register_buffer(\"W\", W)\n    self.register_buffer(\"b\", b)\n    self.register_buffer(\n        \"z_scale\",\n        torch.tensor(math.sqrt(2.0 / float(n_features)), dtype=torch.get_default_dtype()),\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.RFFLayer.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Compute random Fourier features for RBF kernel approximation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input features [B, input_dim].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Random Fourier features [B, n_features].</p> Notes <p>Approximates RBF kernel via random Fourier features using: z(x) = sqrt(2/D) * cos(Wx + b) where W ~ N(0, 2*gamma*I).</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute random Fourier features for RBF kernel approximation.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input features [B, input_dim].\n\n    Returns\n    -------\n    torch.Tensor\n        Random Fourier features [B, n_features].\n\n    Notes\n    -----\n    Approximates RBF kernel via random Fourier features using:\n    z(x) = sqrt(2/D) * cos(Wx + b) where W ~ N(0, 2*gamma*I).\n    \"\"\"\n    W: torch.Tensor = self.W  # type: ignore[assignment]\n    b: torch.Tensor = self.b  # type: ignore[assignment]\n    z_scale: torch.Tensor = self.z_scale  # type: ignore[assignment]\n    proj = x @ W + b\n    return z_scale * torch.cos(proj)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDProjection","title":"DeepSVDDProjection","text":"<pre><code>DeepSVDDProjection(\n    *,\n    in_channels,\n    rep_dim=32,\n    hidden=128,\n    kernel=\"linear\",\n    n_rff=2048,\n    gamma=None,\n    mlp_forward_batch_size=65536,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Projection head that maps per-pixel features to Deep SVDD embeddings.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def __init__(\n    self,\n    *,\n    in_channels: int,\n    rep_dim: int = 32,\n    hidden: int = 128,\n    kernel: str = \"linear\",\n    n_rff: int = 2048,\n    gamma: float | None = None,\n    mlp_forward_batch_size: int = 65_536,\n    **kwargs: Any,\n) -&gt; None:\n    if in_channels &lt;= 0:\n        raise ValueError(f\"in_channels must be positive, got {in_channels}\")\n    self.in_channels = int(in_channels)\n    self.rep_dim = int(rep_dim)\n    self.hidden = int(hidden)\n    self.kernel = str(kernel)\n    self.n_rff = int(n_rff)\n    self.gamma = None if gamma is None else float(gamma)\n    self.mlp_forward_batch_size = max(1, int(mlp_forward_batch_size))\n\n    super().__init__(\n        in_channels=self.in_channels,\n        rep_dim=self.rep_dim,\n        hidden=self.hidden,\n        kernel=self.kernel,\n        n_rff=self.n_rff,\n        gamma=self.gamma,\n        mlp_forward_batch_size=self.mlp_forward_batch_size,\n        **kwargs,\n    )\n\n    # Build projection network eagerly with known in_channels\n    self._build_network()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDProjection.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Project BHWC features into a latent embedding space.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def forward(self, data: torch.Tensor, **_: Any) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Project BHWC features into a latent embedding space.\"\"\"\n    B, H, W, C = data.shape\n    if C != self.in_channels:\n        raise ValueError(f\"Expected {self.in_channels} channels, got {C}\")\n\n    flat = data.contiguous().reshape(B * H * W, C)\n\n    batch_size = self.mlp_forward_batch_size\n    embeddings = []\n    for start in range(0, flat.shape[0], batch_size):\n        chunk = flat[start : start + batch_size]\n        embeddings.append(self.net(chunk))\n    z = torch.cat(embeddings, dim=0).reshape(B, H, W, self.rep_dim)\n\n    return {\"embeddings\": z}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.ZScoreNormalizerGlobal","title":"ZScoreNormalizerGlobal","text":"<pre><code>ZScoreNormalizerGlobal(\n    *,\n    num_channels,\n    sample_n=500000,\n    seed=0,\n    eps=1e-08,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Port-based Deep SVDD z-score normalizer for BHWC cubes.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def __init__(\n    self,\n    *,\n    num_channels: int,\n    sample_n: int = 500_000,\n    seed: int = 0,\n    eps: float = 1e-8,\n    **kwargs: Any,\n) -&gt; None:\n    if num_channels &lt;= 0:\n        raise ValueError(f\"num_channels must be positive, got {num_channels}\")\n    self.num_channels = int(num_channels)\n    self.sample_n = int(sample_n)\n    self.seed = int(seed)\n    self.eps = float(eps)\n\n    super().__init__(\n        num_channels=self.num_channels,\n        sample_n=self.sample_n,\n        seed=self.seed,\n        eps=self.eps,\n        **kwargs,\n    )\n\n    # Pre-allocate buffers with known dimensions\n    self.register_buffer(\n        \"zscore_mean\", torch.zeros(num_channels, dtype=torch.get_default_dtype())\n    )\n    self.register_buffer(\n        \"zscore_std\", torch.ones(num_channels, dtype=torch.get_default_dtype())\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.ZScoreNormalizerGlobal.requires_initial_fit","title":"requires_initial_fit  <code>property</code>","text":"<pre><code>requires_initial_fit\n</code></pre> <p>Whether this node requires statistical initialization from training data.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always True for Z-score normalization.</p>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.ZScoreNormalizerGlobal.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Estimate per-band z-score statistics from the provided stream.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Estimate per-band z-score statistics from the provided stream.\"\"\"\n    pixels: list[np.ndarray] = []\n    for batch in input_stream:\n        data = batch.get(\"data\")\n        if data is None:\n            continue\n        data = data.contiguous()\n\n        B, H, W, C = data.shape\n        if C != self.num_channels:\n            raise ValueError(f\"Channel mismatch: expected {self.num_channels}, got {C}\")\n        # todo: https://cuvis.atlassian.net/browse/ALL-4971, later this will be in tensor\n        # to avoid converting to numpy and back with streaming stats\n        pixels.append(data.reshape(B * H * W, C).detach().cpu().numpy().astype(np.float32))\n\n    if not pixels:\n        raise RuntimeError(\n            \"DeepSVDDEncoder.statistical_initialization() did not receive any data\"\n        )\n\n    X_all = np.concatenate(pixels, axis=0)\n    n_stats = min(self.sample_n, X_all.shape[0])\n    if n_stats &lt; X_all.shape[0]:\n        rng = np.random.default_rng(self.seed)\n        idx = rng.choice(X_all.shape[0], size=n_stats, replace=False)\n        X_stats = X_all[idx]\n    else:\n        X_stats = X_all\n\n    mean, std = self._zscore_fit(X_stats, eps=self.eps)\n    self.zscore_mean.copy_(torch.from_numpy(mean).squeeze(0))\n    self.zscore_std.copy_(torch.from_numpy(std).squeeze(0))\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.ZScoreNormalizerGlobal.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Apply per-channel Z-score normalization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input feature tensor [B, H, W, C].</p> required <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"normalized\" key containing Z-score normalized data [B, H, W, C].</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If statistical_initialization() has not been called.</p> <code>ValueError</code> <p>If input channel count doesn't match initialized num_channels.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def forward(self, data: torch.Tensor, **_: Any) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Apply per-channel Z-score normalization.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input feature tensor [B, H, W, C].\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"normalized\" key containing Z-score normalized data [B, H, W, C].\n\n    Raises\n    ------\n    RuntimeError\n        If statistical_initialization() has not been called.\n    ValueError\n        If input channel count doesn't match initialized num_channels.\n    \"\"\"\n    if not self._statistically_initialized:\n        raise RuntimeError(\n            \"DeepSVDDEncoder requires statistical_initialization() before forward()\"\n        )\n\n    B, H, W, C = data.shape\n    if C != self.num_channels:\n        raise ValueError(f\"Channel mismatch: expected {self.num_channels}, got {C}\")\n\n    flat = data.contiguous().reshape(B * H * W, C)\n    mean = self.zscore_mean.to(dtype=flat.dtype, copy=False).unsqueeze(0)\n    std = self.zscore_std.to(dtype=flat.dtype, copy=False).unsqueeze(0)\n    flat = (flat - mean) / std\n\n    normalized = flat.reshape(B, H, W, C)\n    return {\"normalized\": normalized}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDScores","title":"DeepSVDDScores","text":"<p>               Bases: <code>Node</code></p> <p>Convert Deep SVDD embeddings + center vector into anomaly scores.</p>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDScores.forward","title":"forward","text":"<pre><code>forward(embeddings, center, **_)\n</code></pre> <p>Compute anomaly scores as squared distance from center.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Tensor</code> <p>Deep SVDD embeddings [B, H, W, D] from projection network.</p> required <code>center</code> <code>Tensor</code> <p>Center vector [D] from DeepSVDDCenterTracker.</p> required <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"scores\" key containing squared distances [B, H, W, 1].</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def forward(\n    self, embeddings: torch.Tensor, center: torch.Tensor, **_: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Compute anomaly scores as squared distance from center.\n\n    Parameters\n    ----------\n    embeddings : torch.Tensor\n        Deep SVDD embeddings [B, H, W, D] from projection network.\n    center : torch.Tensor\n        Center vector [D] from DeepSVDDCenterTracker.\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"scores\" key containing squared distances [B, H, W, 1].\n    \"\"\"\n    scores = ((embeddings - center.view(1, 1, 1, -1)) ** 2).sum(dim=-1, keepdim=True)\n    return {\"scores\": scores}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDCenterTracker","title":"DeepSVDDCenterTracker","text":"<pre><code>DeepSVDDCenterTracker(\n    *, rep_dim, alpha=0.1, update_in_eval=False, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Track and expose Deep SVDD center statistics with optional logging.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def __init__(\n    self, *, rep_dim: int, alpha: float = 0.1, update_in_eval: bool = False, **kwargs\n) -&gt; None:\n    if rep_dim &lt;= 0:\n        raise ValueError(f\"rep_dim must be positive, got {rep_dim}\")\n    if not (0.0 &lt; alpha &lt;= 1.0):\n        raise ValueError(\"alpha must be in (0, 1]\")\n    self.rep_dim = int(rep_dim)\n    self.alpha = float(alpha)\n    self.update_in_eval = bool(update_in_eval)\n\n    super().__init__(\n        rep_dim=self.rep_dim, alpha=self.alpha, update_in_eval=self.update_in_eval, **kwargs\n    )\n\n    # Pre-allocate buffer with known dimensions\n    self.register_buffer(\n        \"_tracked_center\", torch.zeros(rep_dim, dtype=torch.get_default_dtype())\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDCenterTracker.requires_initial_fit","title":"requires_initial_fit  <code>property</code>","text":"<pre><code>requires_initial_fit\n</code></pre> <p>Whether this node requires statistical initialization from training data.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always True for center tracking initialization.</p>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDCenterTracker.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize the Deep SVDD center from training embeddings.</p> <p>Computes the mean embedding across all training samples to initialize the hypersphere center.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Training data stream with embeddings [B, H, W, D].</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no embeddings are received from the input stream.</p> <code>ValueError</code> <p>If embedding dimensions don't match initialized rep_dim.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize the Deep SVDD center from training embeddings.\n\n    Computes the mean embedding across all training samples to initialize\n    the hypersphere center.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Training data stream with embeddings [B, H, W, D].\n\n    Raises\n    ------\n    RuntimeError\n        If no embeddings are received from the input stream.\n    ValueError\n        If embedding dimensions don't match initialized rep_dim.\n    \"\"\"\n    total = None\n    count = 0\n    for batch in input_stream:\n        embeddings = batch.get(\"embeddings\")\n        if embeddings is None:\n            embeddings = batch.get(\"data\")\n        if embeddings is None:\n            continue\n\n        # Validate dimensions\n        if embeddings.shape[-1] != self.rep_dim:\n            raise ValueError(\n                f\"Embedding dimension mismatch: expected {self.rep_dim}, got {embeddings.shape[-1]}\"\n            )\n\n        flat = embeddings.reshape(-1, embeddings.shape[-1])\n        batch_sum = flat.sum(dim=0)\n        total = batch_sum if total is None else total + batch_sum\n        count += flat.shape[0]\n\n    if total is None or count == 0:\n        raise RuntimeError(\n            \"DeepSVDDCenterTracker.statistical_initialization() received no embeddings\"\n        )\n\n    self._tracked_center.copy_((total / count).detach())\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.anomaly.deep_svdd.DeepSVDDCenterTracker.forward","title":"forward","text":"<pre><code>forward(embeddings, context=None, **_)\n</code></pre> <p>Track and output the Deep SVDD center with exponential moving average.</p> <p>Updates the center using EMA during training (and optionally during eval), then outputs the current center and center norm metric.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Tensor</code> <p>Deep SVDD embeddings [B, H, W, D].</p> required <code>context</code> <code>Context</code> <p>Execution context determining whether to update center.</p> <code>None</code> <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with: - \"center\" : torch.Tensor [D] - Current tracked center - \"metrics\" : list[Metric] - Center norm metric</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If statistical_initialization() has not been called.</p> <code>ValueError</code> <p>If embedding dimensions don't match initialized rep_dim.</p> Source code in <code>cuvis_ai/anomaly/deep_svdd.py</code> <pre><code>def forward(\n    self, embeddings: torch.Tensor, context: Context | None = None, **_: Any\n) -&gt; dict[str, Any]:\n    \"\"\"Track and output the Deep SVDD center with exponential moving average.\n\n    Updates the center using EMA during training (and optionally during eval),\n    then outputs the current center and center norm metric.\n\n    Parameters\n    ----------\n    embeddings : torch.Tensor\n        Deep SVDD embeddings [B, H, W, D].\n    context : Context, optional\n        Execution context determining whether to update center.\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with:\n        - \"center\" : torch.Tensor [D] - Current tracked center\n        - \"metrics\" : list[Metric] - Center norm metric\n\n    Raises\n    ------\n    RuntimeError\n        If statistical_initialization() has not been called.\n    ValueError\n        If embedding dimensions don't match initialized rep_dim.\n    \"\"\"\n    if not self._statistically_initialized:\n        raise RuntimeError(\n            \"DeepSVDDCenterTracker requires statistical_initialization() before forward()\"\n        )\n\n    # Validate dimensions\n    if embeddings.shape[-1] != self.rep_dim:\n        raise ValueError(\n            f\"Embedding dimension mismatch: expected {self.rep_dim}, got {embeddings.shape[-1]}\"\n        )\n\n    batch_mean = embeddings.mean(dim=(0, 1, 2)).detach()\n    should_update = context is None or context.stage is ExecutionStage.TRAIN\n    if not should_update and self.update_in_eval and context is not None:\n        should_update = context.stage in {ExecutionStage.VAL, ExecutionStage.TEST}\n\n    if should_update:\n        self._tracked_center.copy_(\n            (1.0 - self.alpha) * self._tracked_center + self.alpha * batch_mean\n        )\n\n    metrics = []\n    center_cpu = self._tracked_center.detach().cpu()\n    metrics.append(\n        Metric(\n            name=\"deepsvdd_center/norm\",\n            value=float(center_cpu.norm().item()),\n            stage=context.stage if context else ExecutionStage.INFERENCE,\n            epoch=context.epoch if context else 0,\n            batch_idx=context.batch_idx if context else 0,\n        )\n    )\n\n    center_value = self._tracked_center.detach().clone()\n    return {\"center\": center_value, \"metrics\": metrics}\n</code></pre>"},{"location":"api/nodes/#binary-decision-nodes","title":"Binary Decision Nodes","text":"<p>Nodes that convert anomaly scores into binary decisions (anomaly/normal).</p>"},{"location":"api/nodes/#binary-decider","title":"Binary Decider","text":""},{"location":"api/nodes/#cuvis_ai.deciders.binary_decider","title":"binary_decider","text":"<p>Binary decision nodes for thresholding anomaly scores and logits.</p> <p>This module provides threshold-based decision nodes that convert continuous anomaly scores or logits into binary decisions (anomaly/normal). Two strategies are available:</p> <ul> <li>BinaryDecider: Fixed threshold applied globally to sigmoid-transformed logits</li> <li>QuantileBinaryDecider: Adaptive per-batch thresholding using quantile statistics</li> </ul> <p>Decision nodes are typically placed at the end of anomaly detection pipelines to convert detector outputs into actionable binary masks for visualization or evaluation.</p>"},{"location":"api/nodes/#cuvis_ai.deciders.binary_decider.BinaryDecider","title":"BinaryDecider","text":"<pre><code>BinaryDecider(threshold=0.5, **kwargs)\n</code></pre> <p>               Bases: <code>BinaryDecider</code></p> <p>Simple decider node using a static threshold to classify data.</p> <p>Accepts logits as input, applies sigmoid transformation to convert to probabilities [0, 1], then applies threshold to produce binary decisions.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold to use for classification after sigmoid. Values &gt;= threshold are classified as anomalies (True). Default: 0.5</p> <code>0.5</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.deciders.binary_decider import BinaryDecider\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create decider with default threshold\n&gt;&gt;&gt; decider = BinaryDecider(threshold=0.5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to RX anomaly logits\n&gt;&gt;&gt; logits = torch.randn(4, 256, 256, 1)  # [B, H, W, C]\n&gt;&gt;&gt; output = decider.forward(logits=logits)\n&gt;&gt;&gt; decisions = output[\"decisions\"]  # [4, 256, 256, 1] boolean mask\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use in pipeline\n&gt;&gt;&gt; pipeline.connect(\n...     (logit_head.logits, decider.logits),\n...     (decider.decisions, visualizer.mask),\n... )\n</code></pre> See Also <p>QuantileBinaryDecider : Adaptive per-batch thresholding ScoreToLogit : Convert scores to logits before decisioning</p> Source code in <code>cuvis_ai/deciders/binary_decider.py</code> <pre><code>def __init__(self, threshold: float = 0.5, **kwargs) -&gt; None:\n    self.threshold = threshold\n    # Forward threshold to BaseDecider so Serializable captures it in hparams\n    super().__init__(threshold=threshold, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.deciders.binary_decider.BinaryDecider.forward","title":"forward","text":"<pre><code>forward(logits, **_)\n</code></pre> <p>Apply sigmoid and threshold-based decisioning on channels-last data.</p> <p>Args:     logits: Tensor shaped (B, H, W, C) containing logits.</p> <p>Returns:     Dictionary with \"decisions\" key containing (B, H, W, 1) decision mask.</p> Source code in <code>cuvis_ai/deciders/binary_decider.py</code> <pre><code>def forward(\n    self,\n    logits: Tensor,\n    **_: Any,\n) -&gt; dict[str, Tensor]:\n    \"\"\"Apply sigmoid and threshold-based decisioning on channels-last data.\n\n    Args:\n        logits: Tensor shaped (B, H, W, C) containing logits.\n\n    Returns:\n        Dictionary with \"decisions\" key containing (B, H, W, 1) decision mask.\n    \"\"\"\n\n    # Apply sigmoid if needed to convert logits to probabilities\n    tensor = torch.sigmoid(logits)\n\n    # Apply threshold to get binary decisions\n    decisions = tensor &gt;= self.threshold\n    return {\"decisions\": decisions}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.deciders.binary_decider.QuantileBinaryDecider","title":"QuantileBinaryDecider","text":"<pre><code>QuantileBinaryDecider(\n    quantile=0.995, reduce_dims=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BinaryDecider</code></p> <p>Quantile-based thresholding node operating on BHWC logits or scores.</p> <p>This decider computes a tensor-valued threshold per batch item using the requested quantile over one or more non-batch dimensions, then produces a binary mask where values greater than or equal to that threshold are marked as anomalies. Useful for adaptive thresholding when score distributions vary across batches.</p> <p>Parameters:</p> Name Type Description Default <code>quantile</code> <code>float</code> <p>Quantile in the closed interval [0, 1] used for the threshold computation (default: 0.995). Higher values (e.g., 0.99, 0.995) are typical for anomaly detection to capture rare events.</p> <code>0.995</code> <code>reduce_dims</code> <code>Sequence[int] | None</code> <p>Axes (relative to the input tensor) over which to compute the quantile. When <code>None</code> (default), all non-batch dimensions (H, W, C) are reduced. For per-channel thresholds, use reduce_dims=[1, 2] (reduce H, W only).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.deciders.binary_decider import QuantileBinaryDecider\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create quantile-based decider (99.5th percentile)\n&gt;&gt;&gt; decider = QuantileBinaryDecider(quantile=0.995)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to anomaly scores\n&gt;&gt;&gt; scores = torch.randn(4, 256, 256, 1)  # [B, H, W, C]\n&gt;&gt;&gt; output = decider.forward(logits=scores)\n&gt;&gt;&gt; decisions = output[\"decisions\"]  # [4, 256, 256, 1] boolean mask\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Per-channel thresholding (reduce H, W only)\n&gt;&gt;&gt; decider_perchannel = QuantileBinaryDecider(\n...     quantile=0.99,\n...     reduce_dims=[1, 2],  # Compute threshold per channel\n... )\n</code></pre> See Also <p>BinaryDecider : Fixed threshold decisioning</p> Source code in <code>cuvis_ai/deciders/binary_decider.py</code> <pre><code>def __init__(\n    self,\n    quantile: float = 0.995,\n    reduce_dims: Sequence[int] | None = None,\n    **kwargs,\n) -&gt; None:\n    self._validate_quantile(quantile)\n    self.quantile = float(quantile)\n    self.reduce_dims = (\n        tuple(int(dim) for dim in reduce_dims) if reduce_dims is not None else None\n    )\n    # Forward init params so Serializable records them for config serialization\n    super().__init__(quantile=self.quantile, reduce_dims=self.reduce_dims, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.deciders.binary_decider.QuantileBinaryDecider.forward","title":"forward","text":"<pre><code>forward(logits, **_)\n</code></pre> <p>Apply quantile-based thresholding to produce binary decisions.</p> <p>Computes per-batch thresholds using the specified quantile over reduce_dims, then classifies values &gt;= threshold as anomalies.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Input logits or anomaly scores, shape (B, H, W, C)</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary containing: - \"decisions\" : Tensor     Binary decision mask, shape (B, H, W, 1)</p> Source code in <code>cuvis_ai/deciders/binary_decider.py</code> <pre><code>def forward(self, logits: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Apply quantile-based thresholding to produce binary decisions.\n\n    Computes per-batch thresholds using the specified quantile over reduce_dims,\n    then classifies values &gt;= threshold as anomalies.\n\n    Parameters\n    ----------\n    logits : Tensor\n        Input logits or anomaly scores, shape (B, H, W, C)\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary containing:\n        - \"decisions\" : Tensor\n            Binary decision mask, shape (B, H, W, 1)\n    \"\"\"\n    tensor = logits\n    reduce_dims = self._resolve_reduce_dims(tensor.dim())\n\n    if len(reduce_dims) == 1:\n        threshold = torch.quantile(\n            tensor,\n            self.quantile,\n            dim=reduce_dims[0],\n            keepdim=True,\n        )\n    else:\n        tensor_ndim = tensor.dim()\n        dims_to_keep = tuple(i for i in range(tensor_ndim) if i not in reduce_dims)\n        new_order = (*dims_to_keep, *reduce_dims)\n        permuted = tensor.permute(new_order)\n        sizes_keep = [permuted.size(i) for i in range(len(dims_to_keep))]\n        flattened = permuted.reshape(*sizes_keep, -1)\n        threshold_flat = torch.quantile(\n            flattened,\n            self.quantile,\n            dim=len(dims_to_keep),\n            keepdim=True,\n        )\n        threshold_permuted = threshold_flat.reshape(\n            *sizes_keep,\n            *([1] * len(reduce_dims)),\n        )\n        inverse_order = [0] * tensor_ndim\n        for original_idx, permuted_idx in enumerate(new_order):\n            inverse_order[permuted_idx] = original_idx\n        threshold = threshold_permuted.permute(*inverse_order)\n\n    decisions = (tensor &gt;= threshold).to(torch.bool)\n    return {\"decisions\": decisions}\n</code></pre>"},{"location":"api/nodes/#two-stage-decider","title":"Two-Stage Decider","text":""},{"location":"api/nodes/#cuvis_ai.deciders.two_stage_decider","title":"two_stage_decider","text":"<p>Two-Stage Binary Decision Module.</p> <p>This module provides a two-stage binary decision node that first applies an image-level anomaly gate based on top-k statistics, then applies pixel-level quantile thresholding only for images that pass the gate.</p> <p>This approach reduces false positives by filtering out images with low overall anomaly scores before applying pixel-level decisions.</p> See Also <p>cuvis_ai.deciders.binary_decider : Simple threshold-based binary decisions</p>"},{"location":"api/nodes/#cuvis_ai.deciders.two_stage_decider.TwoStageBinaryDecider","title":"TwoStageBinaryDecider","text":"<pre><code>TwoStageBinaryDecider(\n    image_threshold=0.5,\n    top_k_fraction=0.001,\n    quantile=0.995,\n    reduce_dims=None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BinaryDecider</code></p> <p>Two-stage binary decider: image-level gate + pixel quantile mask.</p> Source code in <code>cuvis_ai/deciders/two_stage_decider.py</code> <pre><code>def __init__(\n    self,\n    image_threshold: float = 0.5,\n    top_k_fraction: float = 0.001,\n    quantile: float = 0.995,\n    reduce_dims: Sequence[int] | None = None,\n    **kwargs,\n) -&gt; None:\n    if not 0.0 &lt;= image_threshold &lt;= 1.0:\n        raise ValueError(\"image_threshold must be within [0, 1]\")\n    if not 0.0 &lt; top_k_fraction &lt;= 1.0:\n        raise ValueError(\"top_k_fraction must be in (0, 1]\")\n    if not 0.0 &lt;= quantile &lt;= 1.0:\n        raise ValueError(\"quantile must be within [0, 1]\")\n\n    self.image_threshold = float(image_threshold)\n    self.top_k_fraction = float(top_k_fraction)\n    self.quantile = float(quantile)\n    self.reduce_dims = (\n        tuple(int(dim) for dim in reduce_dims) if reduce_dims is not None else None\n    )\n    super().__init__(\n        image_threshold=self.image_threshold,\n        top_k_fraction=self.top_k_fraction,\n        quantile=self.quantile,\n        reduce_dims=self.reduce_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.deciders.two_stage_decider.TwoStageBinaryDecider.forward","title":"forward","text":"<pre><code>forward(logits, **_)\n</code></pre> <p>Apply two-stage binary decision: image-level gate + pixel quantile.</p> <p>Stage 1: Compute image-level anomaly score from top-k pixel scores. If below threshold, return blank mask (no anomalies).</p> <p>Stage 2: For images passing the gate, apply pixel-level quantile thresholding to create binary anomaly mask.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Anomaly scores [B, H, W, C] or [B, H, W, 1].</p> required <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"decisions\" key containing binary masks [B, H, W, 1].</p> Notes <p>The image-level score is computed as the mean of the top-k% highest pixel scores. For multi-channel inputs, the max across channels is used for each pixel.</p> Source code in <code>cuvis_ai/deciders/two_stage_decider.py</code> <pre><code>def forward(self, logits: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Apply two-stage binary decision: image-level gate + pixel quantile.\n\n    Stage 1: Compute image-level anomaly score from top-k pixel scores.\n    If below threshold, return blank mask (no anomalies).\n\n    Stage 2: For images passing the gate, apply pixel-level quantile\n    thresholding to create binary anomaly mask.\n\n    Parameters\n    ----------\n    logits : Tensor\n        Anomaly scores [B, H, W, C] or [B, H, W, 1].\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"decisions\" key containing binary masks [B, H, W, 1].\n\n    Notes\n    -----\n    The image-level score is computed as the mean of the top-k% highest\n    pixel scores. For multi-channel inputs, the max across channels is\n    used for each pixel.\n    \"\"\"\n    tensor = logits\n    bsz = tensor.shape[0]\n\n    # DEBUG: Log input tensor stats\n    logger.debug(\n        f\"TwoStageDecider input: shape={tensor.shape}, device={tensor.device}, \"\n        f\"dtype={tensor.dtype}, min={tensor.min().item():.6f}, \"\n        f\"max={tensor.max().item():.6f}, mean={tensor.mean().item():.6f}\"\n    )\n\n    decisions = []\n    for b in range(bsz):\n        scores = tensor[b]  # [H, W, C]\n        # Reduce to per-pixel max for image score\n        if scores.dim() == 3:\n            pixel_scores = scores.max(dim=-1)[0]\n        else:\n            pixel_scores = scores\n        flat = pixel_scores.reshape(-1)\n        k = max(\n            1,\n            int(\n                torch.ceil(\n                    torch.tensor(flat.numel() * self.top_k_fraction, dtype=torch.float32)\n                ).item()\n            ),\n        )\n        topk_vals, _ = torch.topk(flat, k)\n        image_score = topk_vals.mean().item()  # Convert to Python float for comparison\n\n        # DEBUG: Log intermediate computation values\n        logger.debug(\n            f\"TwoStageDecider[batch={b}]: k={k}, topk_min={topk_vals.min().item():.6f}, \"\n            f\"topk_max={topk_vals.max().item():.6f}, image_score={image_score:.6f}\"\n        )\n\n        # Stage 1: Image-level gate\n        if image_score &lt; self.image_threshold:\n            # Gate failed: return blank mask\n            logger.debug(\n                f\"TwoStageDecider: image_score={image_score:.6f} &lt; threshold={self.image_threshold:.6f}, \"\n                f\"returning blank mask\"\n            )\n            decisions.append(\n                torch.zeros((*pixel_scores.shape, 1), dtype=torch.bool, device=tensor.device)\n            )\n            continue\n\n        # Stage 2: Gate passed, apply pixel-level quantile thresholding\n        logger.debug(\n            f\"TwoStageDecider: image_score={image_score:.6f} &gt;= threshold={self.image_threshold:.6f}, \"\n            f\"applying quantile thresholding (q={self.quantile})\"\n        )\n        # Compute quantile threshold: reduce over all dimensions to get scalar per batch item\n        # This matches QuantileBinaryDecider behavior: for [B, H, W, C] it reduces over (H, W, C)\n        # For single batch item [H, W, C], we reduce over all dims (0, 1, 2)\n        threshold = torch.quantile(scores, self.quantile)\n\n        # Apply threshold: for multi-channel scores, take max across channels first\n        if scores.dim() == 3:  # [H, W, C]\n            # Take max across channels to get per-pixel score, then threshold\n            pixel_scores = scores.max(dim=-1, keepdim=False)[0]  # [H, W]\n            binary_map = (pixel_scores &gt;= threshold).unsqueeze(-1).to(torch.bool)  # [H, W, 1]\n        else:  # [H, W] - single channel\n            binary_map = (scores &gt;= threshold).unsqueeze(-1).to(torch.bool)  # [H, W, 1]\n\n        decisions.append(binary_map)\n\n    return {\"decisions\": torch.stack(decisions, dim=0)}\n</code></pre>"},{"location":"api/nodes/#data-preprocessing-nodes","title":"Data &amp; Preprocessing Nodes","text":"<p>Nodes for data loading, normalization, and preprocessing.</p>"},{"location":"api/nodes/#data-loader","title":"Data Loader","text":""},{"location":"api/nodes/#cuvis_ai.node.data","title":"data","text":"<p>Data loading nodes for hyperspectral anomaly detection pipelines.</p> <p>This module provides specialized data nodes that convert multi-class segmentation datasets into binary anomaly detection tasks. Data nodes handle type conversions, label mapping, and format transformations required for pipeline processing.</p>"},{"location":"api/nodes/#cuvis_ai.node.data.LentilsAnomalyDataNode","title":"LentilsAnomalyDataNode","text":"<pre><code>LentilsAnomalyDataNode(\n    normal_class_ids, anomaly_class_ids=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Data node for Lentils anomaly detection dataset with binary label mapping.</p> <p>Converts multi-class Lentils segmentation data to binary anomaly detection format. Maps specified class IDs to normal (0) or anomaly (1) labels, and handles type conversions from uint16 to float32 for hyperspectral cubes.</p> <p>Parameters:</p> Name Type Description Default <code>normal_class_ids</code> <code>list[int]</code> <p>List of class IDs to treat as normal background (e.g., [0, 1] for unlabeled and black lentils)</p> required <code>anomaly_class_ids</code> <code>list[int] | None</code> <p>List of class IDs to treat as anomalies. If None, all classes not in normal_class_ids are treated as anomalies (default: None)</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to Node base class</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>_binary_mapper</code> <code>BinaryAnomalyLabelMapper</code> <p>Internal label mapper for converting multi-class to binary masks</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.node.data import LentilsAnomalyDataNode\n&gt;&gt;&gt; from cuvis_ai_core.data.datasets import SingleCu3sDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create datamodule for Lentils dataset\n&gt;&gt;&gt; datamodule = SingleCu3sDataModule(\n...     data_dir=\"data/lentils\",\n...     batch_size=4,\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data node with normal class specification\n&gt;&gt;&gt; data_node = LentilsAnomalyDataNode(\n...     normal_class_ids=[0, 1],  # Unlabeled and black lentils are normal\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use in pipeline\n&gt;&gt;&gt; pipeline.add_node(data_node)\n&gt;&gt;&gt; pipeline.connect(\n...     (data_node.cube, normalizer.data),\n...     (data_node.mask, metrics.targets),\n... )\n</code></pre> See Also <p>BinaryAnomalyLabelMapper : Label mapping utility used internally SingleCu3sDataModule : DataModule for loading CU3S hyperspectral data docs/tutorials/rx-statistical.md : Complete example with LentilsAnomalyDataNode</p> Notes <p>The node performs the following transformations: - Converts hyperspectral cube from uint16 to float32 - Maps multi-class mask [B, H, W] to binary mask [B, H, W, 1] - Extracts wavelengths from first batch element (assumes consistent wavelengths)</p> Source code in <code>cuvis_ai/node/data.py</code> <pre><code>def __init__(\n    self, normal_class_ids: list[int], anomaly_class_ids: list[int] | None = None, **kwargs\n) -&gt; None:\n    super().__init__(\n        normal_class_ids=normal_class_ids, anomaly_class_ids=anomaly_class_ids, **kwargs\n    )\n\n    self._binary_mapper = BinaryAnomalyLabelMapper(  # could have be used as a node as well\n        normal_class_ids=normal_class_ids,\n        anomaly_class_ids=anomaly_class_ids,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.data.LentilsAnomalyDataNode.forward","title":"forward","text":"<pre><code>forward(cube, mask=None, wavelengths=None, **_)\n</code></pre> <p>Process hyperspectral cube and convert labels to binary anomaly format.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Input hyperspectral cube, shape (B, H, W, C), dtype uint16</p> required <code>mask</code> <code>Tensor | None</code> <p>Multi-class segmentation mask, shape (B, H, W), dtype int32. If None, only cube is returned (default: None)</p> <code>None</code> <code>wavelengths</code> <code>Tensor | None</code> <p>Wavelengths for each channel, shape (B, C), dtype int32. If None, wavelengths are not included in output (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Tensor | ndarray]</code> <p>Dictionary containing: - \"cube\" : torch.Tensor     Converted hyperspectral cube, shape (B, H, W, C), dtype float32 - \"mask\" : torch.Tensor (optional)     Binary anomaly mask, shape (B, H, W, 1), dtype bool.     Only included if input mask is provided. - \"wavelengths\" : np.ndarray (optional)     Wavelength array, shape (C,), dtype int32.     Only included if input wavelengths are provided.</p> Source code in <code>cuvis_ai/node/data.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    wavelengths: torch.Tensor | None = None,\n    **_: Any,\n) -&gt; dict[str, torch.Tensor | np.ndarray]:\n    \"\"\"Process hyperspectral cube and convert labels to binary anomaly format.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Input hyperspectral cube, shape (B, H, W, C), dtype uint16\n    mask : torch.Tensor | None, optional\n        Multi-class segmentation mask, shape (B, H, W), dtype int32.\n        If None, only cube is returned (default: None)\n    wavelengths : torch.Tensor | None, optional\n        Wavelengths for each channel, shape (B, C), dtype int32.\n        If None, wavelengths are not included in output (default: None)\n\n    Returns\n    -------\n    dict[str, torch.Tensor | np.ndarray]\n        Dictionary containing:\n        - \"cube\" : torch.Tensor\n            Converted hyperspectral cube, shape (B, H, W, C), dtype float32\n        - \"mask\" : torch.Tensor (optional)\n            Binary anomaly mask, shape (B, H, W, 1), dtype bool.\n            Only included if input mask is provided.\n        - \"wavelengths\" : np.ndarray (optional)\n            Wavelength array, shape (C,), dtype int32.\n            Only included if input wavelengths are provided.\n    \"\"\"\n    result: dict[str, torch.Tensor | np.ndarray] = {\"cube\": cube.to(torch.float32)}\n\n    # wavelengths passthrough, could check that in all batch elements the same wavelengths are used\n    # input B x C -&gt; output C\n    if wavelengths is not None:\n        result[\"wavelengths\"] = wavelengths[0].cpu().numpy()\n\n    if mask is not None:\n        # Add channel dimension for mapper: BHW -&gt; BHWC\n        mask_4d = mask.unsqueeze(-1)\n\n        # Always apply binary mapper\n        mapped = self._binary_mapper.forward(\n            cube=cube,\n            mask=mask_4d,\n            **_,  # Pass through additional kwargs\n        )\n        result[\"mask\"] = mapped[\"mask\"]  # Already BHWC bool\n\n    return result\n</code></pre>"},{"location":"api/nodes/#normalization","title":"Normalization","text":""},{"location":"api/nodes/#cuvis_ai.node.normalization","title":"normalization","text":"<p>Differentiable normalization nodes for BHWC hyperspectral data.</p> <p>This module provides a collection of normalization nodes designed for hyperspectral imaging pipelines. All normalizers operate on BHWC format ([batch, height, width, channels]) and maintain gradient flow for end-to-end training.</p> <p>Normalization strategies:</p> <ul> <li>MinMaxNormalizer: Scales data to [0, 1] range using min-max statistics</li> <li>ZScoreNormalizer: Standardizes data to zero mean and unit variance</li> <li>SigmoidNormalizer: Applies sigmoid transformation with median centering</li> <li>PerPixelUnitNorm: L2 normalization per pixel across channels</li> <li>IdentityNormalizer: No-op passthrough for testing or baseline comparisons</li> <li>SigmoidTransform: General-purpose sigmoid for logits\u2192probabilities</li> </ul> <p>Why Normalize?</p> <p>Normalization is critical for stable anomaly detection and deep learning:</p> <ol> <li>Stable covariance estimation: RX detectors require well-conditioned covariance matrices</li> <li>Gradient stability: Prevents exploding/vanishing gradients during training</li> <li>Comparable scales: Ensures different spectral ranges contribute equally</li> <li>Faster convergence: Accelerates gradient-based optimization</li> </ol> <p>BHWC Format Requirement</p> <p>All normalizers expect BHWC input format. For HWC tensors, add batch dimension:</p> <p>hwc_tensor = torch.randn(256, 256, 61)  # [H, W, C] bhwc_tensor = hwc_tensor.unsqueeze(0)   # [1, H, W, C]</p>"},{"location":"api/nodes/#cuvis_ai.node.normalization.IdentityNormalizer","title":"IdentityNormalizer","text":"<pre><code>IdentityNormalizer(**kwargs)\n</code></pre> <p>               Bases: <code>_ScoreNormalizerBase</code></p> <p>No-op normalizer; preserves incoming scores.</p> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.MinMaxNormalizer","title":"MinMaxNormalizer","text":"<pre><code>MinMaxNormalizer(\n    eps=1e-06, use_running_stats=True, **kwargs\n)\n</code></pre> <p>               Bases: <code>_ScoreNormalizerBase</code></p> <p>Min-max normalization per sample and channel (keeps gradients).</p> <p>Scales data to [0, 1] range using (x - min) / (max - min) transformation. Can operate in two modes:</p> <ol> <li>Per-sample normalization (use_running_stats=False): min/max computed per batch</li> <li>Global normalization (use_running_stats=True): uses running statistics from    statistical initialization</li> </ol> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>Small constant for numerical stability, prevents division by zero (default: 1e-6)</p> <code>1e-06</code> <code>use_running_stats</code> <code>bool</code> <p>If True, use global min/max from statistical_initialization(). If False, compute min/max per batch during forward pass (default: True)</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to Node base class</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>running_min</code> <code>Tensor</code> <p>Global minimum value computed during statistical initialization</p> <code>running_max</code> <code>Tensor</code> <p>Global maximum value computed during statistical initialization</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.node.normalization import MinMaxNormalizer\n&gt;&gt;&gt; from cuvis_ai_core.training import StatisticalTrainer\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Mode 1: Global normalization with statistical initialization\n&gt;&gt;&gt; normalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\n&gt;&gt;&gt; stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n&gt;&gt;&gt; stat_trainer.fit()  # Computes global min/max from training data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Inference uses global statistics\n&gt;&gt;&gt; output = normalizer.forward(data=hyperspectral_cube)\n&gt;&gt;&gt; normalized = output[\"normalized\"]  # [B, H, W, C], values in [0, 1]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Mode 2: Per-sample normalization (no initialization required)\n&gt;&gt;&gt; normalizer_local = MinMaxNormalizer(use_running_stats=False)\n&gt;&gt;&gt; output = normalizer_local.forward(data=hyperspectral_cube)\n&gt;&gt;&gt; # Each sample normalized independently using its own min/max\n</code></pre> See Also <p>ZScoreNormalizer : Z-score standardization SigmoidNormalizer : Sigmoid-based normalization docs/tutorials/rx-statistical.md : RX pipeline with MinMaxNormalizer</p> Notes <p>Global normalization (use_running_stats=True) is recommended for RX detectors to ensure consistent scaling between training and inference. Per-sample normalization can be useful for real-time processing when training data is unavailable.</p> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def __init__(self, eps: float = 1e-6, use_running_stats: bool = True, **kwargs) -&gt; None:\n    self.eps = float(eps)\n    self.use_running_stats = use_running_stats\n    super().__init__(eps=eps, use_running_stats=use_running_stats, **kwargs)\n\n    # Running statistics for global normalization\n    self.register_buffer(\"running_min\", torch.tensor(float(\"nan\")))\n    self.register_buffer(\"running_max\", torch.tensor(float(\"nan\")))\n\n    # Only require initialization when running stats are requested\n    self._requires_initial_fit_override = self.use_running_stats\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.MinMaxNormalizer.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Compute global min/max from data iterator.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Iterator yielding dicts matching INPUT_SPECS (port-based format) Expected format: {\"data\": tensor} where tensor is the scores/data</p> required Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def statistical_initialization(self, input_stream) -&gt; None:\n    \"\"\"Compute global min/max from data iterator.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Iterator yielding dicts matching INPUT_SPECS (port-based format)\n        Expected format: {\"data\": tensor} where tensor is the scores/data\n    \"\"\"\n    all_mins = []\n    all_maxs = []\n\n    for batch_data in input_stream:\n        # Extract data from port-based dict\n        x = batch_data.get(\"data\")\n        if x is not None:\n            # Flatten spatial dimensions\n            flat = x.reshape(x.shape[0], -1)\n            batch_min = flat.min()\n            batch_max = flat.max()\n            all_mins.append(batch_min)\n            all_maxs.append(batch_max)\n\n    if all_mins:\n        self.running_min = torch.stack(all_mins).min()\n        self.running_max = torch.stack(all_maxs).max()\n        self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.SigmoidNormalizer","title":"SigmoidNormalizer","text":"<pre><code>SigmoidNormalizer(std_floor=1e-06, **kwargs)\n</code></pre> <p>               Bases: <code>_ScoreNormalizerBase</code></p> <p>Median-centered sigmoid squashing per sample and channel.</p> <p>Applies sigmoid transformation centered at the median with standard deviation scaling:</p> <pre><code>sigmoid((x - median) / std)\n</code></pre> <p>Produces values in [0, 1] range with median mapped to 0.5.</p> <p>Parameters:</p> Name Type Description Default <code>std_floor</code> <code>float</code> <p>Minimum standard deviation threshold to prevent division by zero (default: 1e-6)</p> <code>1e-06</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to Node base class</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from cuvis_ai.node.normalization import SigmoidNormalizer\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create sigmoid normalizer\n&gt;&gt;&gt; normalizer = SigmoidNormalizer(std_floor=1.0e-6)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to hyperspectral data\n&gt;&gt;&gt; data = torch.randn(4, 256, 256, 61)  # [B, H, W, C]\n&gt;&gt;&gt; output = normalizer.forward(data=data)\n&gt;&gt;&gt; normalized = output[\"normalized\"]  # [4, 256, 256, 61], values in [0, 1]\n</code></pre> See Also <p>MinMaxNormalizer : Min-max scaling to [0, 1] ZScoreNormalizer : Z-score standardization</p> Notes <p>Sigmoid normalization is robust to outliers because extreme values are squashed asymptotically to 0 or 1. This makes it suitable for data with heavy-tailed distributions or sporadic anomalies.</p> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def __init__(self, std_floor: float = 1e-6, **kwargs) -&gt; None:\n    self.std_floor = float(std_floor)\n    super().__init__(std_floor=std_floor, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.ZScoreNormalizer","title":"ZScoreNormalizer","text":"<pre><code>ZScoreNormalizer(\n    dims=None, eps=1e-06, keepdim=True, **kwargs\n)\n</code></pre> <p>               Bases: <code>_ScoreNormalizerBase</code></p> <p>Z-score (standardization) normalization along specified dimensions.</p> <p>Computes: (x - mean) / (std + eps) along specified dims. Per-sample normalization with no statistical initialization required.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>list[int]</code> <p>Dimensions to compute statistics over (default: [1,2] for H,W in BHWC format)</p> <code>None</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>keepdim</code> <code>bool</code> <p>Whether to keep reduced dimensions (default: True)</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Normalize over spatial dimensions (H, W)\n&gt;&gt;&gt; zscore = ZScoreNormalizer(dims=[1, 2])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Normalize over all spatial and channel dimensions\n&gt;&gt;&gt; zscore_all = ZScoreNormalizer(dims=[1, 2, 3])\n</code></pre> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def __init__(\n    self, dims: list[int] | None = None, eps: float = 1e-6, keepdim: bool = True, **kwargs\n) -&gt; None:\n    self.dims = dims if dims is not None else [1, 2]\n    self.eps = float(eps)\n    self.keepdim = keepdim\n    super().__init__(dims=self.dims, eps=eps, keepdim=keepdim, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.SigmoidTransform","title":"SigmoidTransform","text":"<pre><code>SigmoidTransform(**kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Applies sigmoid transformation to convert logits to probabilities [0,1].</p> <p>General-purpose sigmoid node for converting raw scores/logits to probability space. Useful for visualization or downstream nodes that expect bounded [0,1] values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sigmoid = SigmoidTransform()\n&gt;&gt;&gt; # Route logits to both loss (raw) and visualization (sigmoid)\n&gt;&gt;&gt; graph.connect(\n...     (rx.scores, loss_node.predictions),  # Raw logits to loss\n...     (rx.scores, sigmoid.data),           # Logits to sigmoid\n...     (sigmoid.transformed, viz.scores),   # Probabilities to viz\n... )\n</code></pre> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.SigmoidTransform.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Apply sigmoid transformation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"transformed\" key containing sigmoid output</p> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def forward(self, data: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Apply sigmoid transformation.\n\n    Parameters\n    ----------\n    data : Tensor\n        Input tensor\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"transformed\" key containing sigmoid output\n    \"\"\"\n    return {\"transformed\": torch.sigmoid(data)}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.PerPixelUnitNorm","title":"PerPixelUnitNorm","text":"<pre><code>PerPixelUnitNorm(eps=1e-08, **kwargs)\n</code></pre> <p>               Bases: <code>_ScoreNormalizerBase</code></p> <p>Per-pixel mean-centering and L2 normalization across channels.</p> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def __init__(self, eps: float = 1e-8, **kwargs) -&gt; None:\n    self.eps = float(eps)\n    super().__init__(eps=self.eps, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.normalization.PerPixelUnitNorm.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Normalize BHWC tensors per pixel.</p> Source code in <code>cuvis_ai/node/normalization.py</code> <pre><code>def forward(self, data: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Normalize BHWC tensors per pixel.\"\"\"\n    normalized = self._normalize(data)\n    return {\"normalized\": normalized}\n</code></pre>"},{"location":"api/nodes/#preprocessors","title":"Preprocessors","text":""},{"location":"api/nodes/#cuvis_ai.node.preprocessors","title":"preprocessors","text":"<p>Preprocessing Nodes.</p> <p>This module provides nodes for preprocessing hyperspectral data, including wavelength-based band selection and filtering. These nodes help reduce dimensionality and focus analysis on specific spectral regions of interest.</p> See Also <p>cuvis_ai.node.band_selection : Advanced band selection methods cuvis_ai.node.normalization : Normalization and standardization nodes</p>"},{"location":"api/nodes/#cuvis_ai.node.preprocessors.BandpassByWavelength","title":"BandpassByWavelength","text":"<pre><code>BandpassByWavelength(\n    min_wavelength_nm, max_wavelength_nm=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Select channels by wavelength interval from BHWC tensors.</p> <p>This node filters hyperspectral data by keeping only channels within a specified wavelength range. Wavelengths must be provided via the input port.</p> <p>Parameters:</p> Name Type Description Default <code>min_wavelength_nm</code> <code>float</code> <p>Minimum wavelength (inclusive) to keep, in nanometers</p> required <code>max_wavelength_nm</code> <code>float | None</code> <p>Maximum wavelength (inclusive) to keep. If None, selects all wavelengths</p> <p>= min_wavelength_nm. Default: None</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create bandpass node\n&gt;&gt;&gt; bandpass = BandpassByWavelength(\n...     min_wavelength_nm=500.0,\n...     max_wavelength_nm=700.0,\n... )\n&gt;&gt;&gt; # Filter cube in BHWC format with wavelengths from input port\n&gt;&gt;&gt; wavelengths_tensor = torch.from_numpy(wavelengths).float()\n&gt;&gt;&gt; filtered = bandpass.forward(data=cube_bhwc, wavelengths=wavelengths_tensor)[\"filtered\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # For single HWC images, add a batch dimension first:\n&gt;&gt;&gt; # filtered = bandpass.forward(data=cube_hwc.unsqueeze(0), wavelengths=wavelengths_tensor)[\"filtered\"]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use with wavelengths from upstream node\n&gt;&gt;&gt; pipeline.connect(\n...     (data_node.outputs.cube, bandpass.data),\n...     (data_node.outputs.wavelengths, bandpass.wavelengths),\n... )\n</code></pre> Source code in <code>cuvis_ai/node/preprocessors.py</code> <pre><code>def __init__(\n    self,\n    min_wavelength_nm: float,\n    max_wavelength_nm: float | None = None,\n    **kwargs,\n) -&gt; None:\n    self.min_wavelength_nm = float(min_wavelength_nm)\n    self.max_wavelength_nm = float(max_wavelength_nm) if max_wavelength_nm is not None else None\n\n    super().__init__(\n        min_wavelength_nm=self.min_wavelength_nm,\n        max_wavelength_nm=self.max_wavelength_nm,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.preprocessors.BandpassByWavelength.forward","title":"forward","text":"<pre><code>forward(data, wavelengths, **kwargs)\n</code></pre> <p>Filter cube by wavelength range.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>Tensor</code> <p>Wavelengths tensor [C] in nanometers.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"filtered\" key containing filtered cube [B, H, W, C_filtered]</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no channels are selected by the provided wavelength range</p> Source code in <code>cuvis_ai/node/preprocessors.py</code> <pre><code>def forward(self, data: Tensor, wavelengths: Tensor, **kwargs: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Filter cube by wavelength range.\n\n    Parameters\n    ----------\n    data : Tensor\n        Input hyperspectral cube [B, H, W, C].\n    wavelengths : Tensor\n        Wavelengths tensor [C] in nanometers.\n    **kwargs : Any\n        Additional keyword arguments (unused).\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"filtered\" key containing filtered cube [B, H, W, C_filtered]\n\n    Raises\n    ------\n    ValueError\n        If no channels are selected by the provided wavelength range\n    \"\"\"\n\n    # Create mask for wavelength range\n    if self.max_wavelength_nm is None:\n        keep_mask = wavelengths &gt;= self.min_wavelength_nm\n    else:\n        keep_mask = (wavelengths &gt;= self.min_wavelength_nm) &amp; (\n            wavelengths &lt;= self.max_wavelength_nm\n        )\n\n    if keep_mask.sum().item() == 0:\n        raise ValueError(\"No channels selected by the provided wavelength range\")\n\n    # Filter cube\n    filtered = data[..., keep_mask]\n\n    return {\"filtered\": filtered}\n</code></pre>"},{"location":"api/nodes/#conversion","title":"Conversion","text":""},{"location":"api/nodes/#cuvis_ai.node.conversion","title":"conversion","text":"<p>RX Logit Head for Anomaly Detection</p> <p>This module provides a trainable head that converts RX anomaly scores into logits for binary anomaly classification. It can be trained end-to-end with binary cross-entropy loss.</p>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit","title":"ScoreToLogit","text":"<pre><code>ScoreToLogit(init_scale=1.0, init_bias=0.0, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Trainable head that converts RX scores to anomaly logits.</p> <p>This node takes RX anomaly scores (typically Mahalanobis distances) and applies a learned affine transformation to produce logits suitable for binary classification with BCEWithLogitsLoss.</p> <p>The transformation is: logit = scale * (score - bias)</p> <p>Parameters:</p> Name Type Description Default <code>init_scale</code> <code>float</code> <p>Initial value for the scale parameter</p> <code>1.0</code> <code>init_bias</code> <code>float</code> <p>Initial value for the bias parameter (threshold)</p> <code>0.0</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>Parameter or Tensor</code> <p>Scale factor applied to scores</p> <code>bias</code> <code>Parameter or Tensor</code> <p>Bias (threshold) subtracted from scores before scaling</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # After RX detector\n&gt;&gt;&gt; rx = RXGlobal(eps=1e-6)\n&gt;&gt;&gt; logit_head = ScoreToLogit(init_scale=1.0, init_bias=5.0)\n&gt;&gt;&gt; logit_head.unfreeze()  # Enable gradient training\n&gt;&gt;&gt; graph.connect(rx.scores, logit_head.scores)\n</code></pre> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def __init__(\n    self,\n    init_scale: float = 1.0,\n    init_bias: float = 0.0,\n    **kwargs,\n) -&gt; None:\n    self.init_scale = init_scale\n    self.init_bias = init_bias\n\n    super().__init__(\n        init_scale=init_scale,\n        init_bias=init_bias,\n        **kwargs,\n    )\n\n    # Initialize as buffers (frozen by default)\n    self.register_buffer(\"scale\", torch.tensor(init_scale, dtype=torch.float32))\n    self.register_buffer(\"bias\", torch.tensor(init_bias, dtype=torch.float32))\n\n    # Streaming accumulators for statistics (similar to RXGlobal)\n    self.register_buffer(\"_mean\", torch.tensor(float(\"nan\")))\n    self.register_buffer(\"_M2\", torch.tensor(float(\"nan\")))\n    self.register_buffer(\"_n\", torch.tensor(0, dtype=torch.long))\n    # Allow using the head with the provided init_scale/init_bias without forcing a fit()\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.unfreeze","title":"unfreeze","text":"<pre><code>unfreeze()\n</code></pre> <p>Convert scale and bias buffers to trainable nn.Parameters.</p> <p>Call this method to enable gradient-based optimization of the scale and bias parameters. They will be converted from buffers to nn.Parameters, allowing gradient updates during training.</p> Example <p>logit_head = ScoreToLogit(init_scale=1.0, init_bias=5.0) logit_head.unfreeze()  # Enable gradient training</p> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Convert scale and bias buffers to trainable nn.Parameters.\n\n    Call this method to enable gradient-based optimization of the\n    scale and bias parameters. They will be converted from buffers to\n    nn.Parameters, allowing gradient updates during training.\n\n    Example\n    -------\n    &gt;&gt;&gt; logit_head = ScoreToLogit(init_scale=1.0, init_bias=5.0)\n    &gt;&gt;&gt; logit_head.unfreeze()  # Enable gradient training\n    &gt;&gt;&gt; # Now scale and bias can be optimized\n    \"\"\"\n    if self.scale is not None and self.bias is not None:\n        # Convert buffers to parameters\n        self.scale = nn.Parameter(self.scale.clone(), requires_grad=True)\n        self.bias = nn.Parameter(self.bias.clone(), requires_grad=True)\n    # Call parent to enable requires_grad\n    super().unfreeze()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.unfreeze--now-scale-and-bias-can-be-optimized","title":"Now scale and bias can be optimized","text":""},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize bias from statistics of RX scores using streaming approach.</p> <p>Uses Welford's algorithm for numerically stable online computation of mean and standard deviation, similar to RXGlobal.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Iterator yielding dicts matching INPUT_SPECS (port-based format) Expected format: {\"scores\": tensor} where tensor is the RX scores</p> required Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def statistical_initialization(self, input_stream) -&gt; None:\n    \"\"\"Initialize bias from statistics of RX scores using streaming approach.\n\n    Uses Welford's algorithm for numerically stable online computation of\n    mean and standard deviation, similar to RXGlobal.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Iterator yielding dicts matching INPUT_SPECS (port-based format)\n        Expected format: {\"scores\": tensor} where tensor is the RX scores\n    \"\"\"\n    self.reset()\n    for batch_data in input_stream:\n        # Extract scores from port-based dict\n        scores = batch_data.get(\"scores\")\n        if scores is not None:\n            self.update(scores)\n\n    if self._n &gt; 0:\n        self.finalize()\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.update","title":"update","text":"<pre><code>update(scores)\n</code></pre> <p>Update running statistics with a batch of scores.</p> <p>Uses Welford's online algorithm for numerically stable computation.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>Batch of RX scores in BHWC format</p> required Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>@torch.no_grad()\ndef update(self, scores: torch.Tensor) -&gt; None:\n    \"\"\"Update running statistics with a batch of scores.\n\n    Uses Welford's online algorithm for numerically stable computation.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        Batch of RX scores in BHWC format\n    \"\"\"\n    # Flatten to 1D\n    X = scores.flatten()\n    m = X.shape[0]\n    if m &lt;= 1:\n        return\n\n    mean_b = X.mean()\n    M2_b = ((X - mean_b) ** 2).sum()\n\n    n = int(self._n.item())\n    if n == 0:\n        self._n = torch.tensor(m, dtype=torch.long, device=self.scale.device)\n        self._mean = mean_b\n        self._M2 = M2_b\n    else:\n        tot = n + m\n        delta = mean_b - self._mean\n        new_mean = self._mean + delta * (m / tot)\n        self._M2 = self._M2 + M2_b + (delta**2) * (n * m / tot)\n        self._n = torch.tensor(tot, dtype=torch.long, device=self.scale.device)\n        self._mean = new_mean\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize statistics and set bias to mean + 2*std.</p> <p>This threshold (mean + 2*std) is a common heuristic for anomaly detection, capturing ~95% of normal data under Gaussian assumption.</p> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>@torch.no_grad()\ndef finalize(self) -&gt; None:\n    \"\"\"Finalize statistics and set bias to mean + 2*std.\n\n    This threshold (mean + 2*std) is a common heuristic for anomaly detection,\n    capturing ~95% of normal data under Gaussian assumption.\n    \"\"\"\n    if int(self._n.item()) &lt;= 1:\n        raise ValueError(\"Not enough samples to finalize ScoreToLogit statistics.\")\n\n    mean = self._mean.clone()\n    variance = self._M2 / (self._n - 1)\n    std = torch.sqrt(variance)\n\n    # Set bias to mean + 2*std (threshold for anomalies)\n    self.bias = mean + 2.0 * std\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset all statistics and accumulators.</p> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all statistics and accumulators.\"\"\"\n    self._n = torch.tensor(0, dtype=torch.long, device=self.scale.device)\n    self._mean = torch.tensor(float(\"nan\"))\n    self._M2 = torch.tensor(float(\"nan\"))\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.forward","title":"forward","text":"<pre><code>forward(scores, **_)\n</code></pre> <p>Transform RX scores to logits.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>Input RX scores with shape (B, H, W, 1)</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"logits\" key containing transformed scores</p> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def forward(self, scores: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Transform RX scores to logits.\n\n    Parameters\n    ----------\n    scores : torch.Tensor\n        Input RX scores with shape (B, H, W, 1)\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"logits\" key containing transformed scores\n    \"\"\"\n\n    if not self._statistically_initialized:\n        raise RuntimeError(\n            \"ScoreToLogit not initialized. Call statistical_initialization() before forward().\"\n        )\n    # Apply affine transformation: logit = scale * (score - bias)\n    logits = self.scale * (scores - self.bias)\n\n    return {\"logits\": logits}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.get_threshold","title":"get_threshold","text":"<pre><code>get_threshold()\n</code></pre> <p>Get the current anomaly threshold (bias value).</p> <p>Returns:</p> Type Description <code>float</code> <p>Current threshold value</p> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def get_threshold(self) -&gt; float:\n    \"\"\"Get the current anomaly threshold (bias value).\n\n    Returns\n    -------\n    float\n        Current threshold value\n    \"\"\"\n    return self.bias.item()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.set_threshold","title":"set_threshold","text":"<pre><code>set_threshold(threshold)\n</code></pre> <p>Set the anomaly threshold (bias value).</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>New threshold value</p> required Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def set_threshold(self, threshold: float) -&gt; None:\n    \"\"\"Set the anomaly threshold (bias value).\n\n    Parameters\n    ----------\n    threshold : float\n        New threshold value\n    \"\"\"\n    with torch.no_grad():\n        self.bias.fill_(threshold)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.conversion.ScoreToLogit.predict_anomalies","title":"predict_anomalies","text":"<pre><code>predict_anomalies(logits)\n</code></pre> <p>Convert logits to binary anomaly predictions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits from forward pass, shape (B, H, W, 1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Binary predictions (0=normal, 1=anomaly), shape (B, H, W, 1)</p> Source code in <code>cuvis_ai/node/conversion.py</code> <pre><code>def predict_anomalies(self, logits: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert logits to binary anomaly predictions.\n\n    Parameters\n    ----------\n    logits : torch.Tensor\n        Logits from forward pass, shape (B, H, W, 1)\n\n    Returns\n    -------\n    torch.Tensor\n        Binary predictions (0=normal, 1=anomaly), shape (B, H, W, 1)\n    \"\"\"\n    return (logits &gt; 0).float()\n</code></pre>"},{"location":"api/nodes/#channel-band-selection-nodes","title":"Channel &amp; Band Selection Nodes","text":"<p>Nodes for selecting and transforming spectral channels.</p>"},{"location":"api/nodes/#band-selection","title":"Band Selection","text":""},{"location":"api/nodes/#cuvis_ai.node.band_selection","title":"band_selection","text":"<p>Band selection nodes for HSI to RGB conversion.</p> <p>This module provides port-based nodes for selecting spectral bands from hyperspectral cubes and composing RGB images for downstream processing (e.g., with AdaCLIP).</p>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.BandSelectorBase","title":"BandSelectorBase","text":"<p>               Bases: <code>Node</code></p> <p>Base class for hyperspectral band selection strategies.</p> <p>This base class defines the common input/output ports for band selection nodes. Subclasses should implement the <code>forward()</code> method to perform specific band selection strategies.</p> Ports <p>INPUT_SPECS     <code>cube</code> : float32, shape (-1, -1, -1, -1)         Hyperspectral cube in BHWC format.     <code>wavelengths</code> : float32, shape (-1,)         Wavelength array in nanometers. OUTPUT_SPECS     <code>rgb_image</code> : float32, shape (-1, -1, -1, 3)         Composed RGB image in BHWC format (0-1 range).     <code>band_info</code> : dict         Metadata about selected bands.</p>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.BaselineFalseRGBSelector","title":"BaselineFalseRGBSelector","text":"<pre><code>BaselineFalseRGBSelector(\n    target_wavelengths=(650.0, 550.0, 450.0), **kwargs\n)\n</code></pre> <p>               Bases: <code>BandSelectorBase</code></p> <p>Fixed wavelength band selection (e.g., 650, 550, 450 nm).</p> <p>Selects bands nearest to the specified target wavelengths for R, G, B channels. This is the simplest band selection strategy that produces \"true color-ish\" images.</p> <p>Parameters:</p> Name Type Description Default <code>target_wavelengths</code> <code>tuple[float, float, float]</code> <p>Target wavelengths for R, G, B channels in nanometers. Default: (650.0, 550.0, 450.0)</p> <code>(650.0, 550.0, 450.0)</code> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    target_wavelengths: tuple[float, float, float] = (650.0, 550.0, 450.0),\n    **kwargs,\n) -&gt; None:\n    super().__init__(target_wavelengths=target_wavelengths, **kwargs)\n    self.target_wavelengths = target_wavelengths\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.BaselineFalseRGBSelector.forward","title":"forward","text":"<pre><code>forward(cube, wavelengths, context=None, **_)\n</code></pre> <p>Select bands and compose RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>Tensor</code> <p>Wavelength array [C].</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"rgb_image\" and \"band_info\" keys.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    wavelengths: Any,\n    context: Context | None = None,  # noqa: ARG002\n    **_: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Select bands and compose RGB image.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Hyperspectral cube [B, H, W, C].\n    wavelengths : torch.Tensor\n        Wavelength array [C].\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"rgb_image\" and \"band_info\" keys.\n    \"\"\"\n    wavelengths_np = np.asarray(wavelengths, dtype=np.float32)\n\n    # Find nearest bands\n    indices = [self._nearest_band_index(wavelengths_np, nm) for nm in self.target_wavelengths]\n\n    # Compose RGB\n    rgb = self._compose_rgb(cube, indices)\n\n    band_info = {\n        \"strategy\": \"baseline_false_rgb\",\n        \"band_indices\": indices,\n        \"band_wavelengths_nm\": [float(wavelengths_np[i]) for i in indices],\n        \"target_wavelengths_nm\": list(self.target_wavelengths),\n    }\n\n    return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.HighContrastBandSelector","title":"HighContrastBandSelector","text":"<pre><code>HighContrastBandSelector(\n    windows=((440, 500), (500, 580), (610, 700)),\n    alpha=0.1,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BandSelectorBase</code></p> <p>Data-driven band selection using spatial variance + Laplacian energy.</p> <p>For each wavelength window, selects the band with the highest score based on: score = variance + alpha * Laplacian_energy</p> <p>This produces \"high contrast\" images that may work better for visual anomaly detection.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>Sequence[tuple[float, float]]</code> <p>Wavelength windows for Blue, Green, Red channels. Default: ((440, 500), (500, 580), (610, 700)) for visible spectrum.</p> <code>((440, 500), (500, 580), (610, 700))</code> <code>alpha</code> <code>float</code> <p>Weight for Laplacian energy term. Default: 0.1</p> <code>0.1</code> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    windows: Sequence[tuple[float, float]] = ((440, 500), (500, 580), (610, 700)),\n    alpha: float = 0.1,\n    **kwargs,\n) -&gt; None:\n    super().__init__(windows=windows, alpha=alpha, **kwargs)\n    self.windows = list(windows)\n    self.alpha = alpha\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.HighContrastBandSelector.forward","title":"forward","text":"<pre><code>forward(cube, wavelengths, context=None, **_)\n</code></pre> <p>Select high-contrast bands and compose RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>Tensor</code> <p>Wavelength array [C].</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"rgb_image\" and \"band_info\" keys.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    wavelengths: Any,\n    context: Context | None = None,  # noqa: ARG002\n    **_: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Select high-contrast bands and compose RGB image.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Hyperspectral cube [B, H, W, C].\n    wavelengths : torch.Tensor\n        Wavelength array [C].\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"rgb_image\" and \"band_info\" keys.\n    \"\"\"\n    wavelengths_np = np.asarray(wavelengths, dtype=np.float32)\n    # Use first batch item for band selection\n    cube_np = cube[0].cpu().numpy()\n\n    selected_indices = []\n    for start, end in self.windows:\n        mask = (wavelengths_np &gt;= start) &amp; (wavelengths_np &lt;= end)\n        window_indices = np.where(mask)[0]\n\n        if len(window_indices) == 0:\n            # Fallback to nearest single wavelength\n            nearest = self._nearest_band_index(wavelengths_np, (start + end) / 2.0)\n            selected_indices.append(int(nearest))\n            continue\n\n        scores = []\n        for idx in window_indices:\n            band = cube_np[..., idx]\n            variance = float(np.var(band))\n            lap_energy = float(np.mean(np.abs(laplace(band))))\n            scores.append(variance + self.alpha * lap_energy)\n\n        best_idx = int(window_indices[int(np.argmax(scores))])\n        selected_indices.append(best_idx)\n\n    rgb = self._compose_rgb(cube, selected_indices)\n\n    band_info = {\n        \"strategy\": \"high_contrast\",\n        \"band_indices\": selected_indices,\n        \"band_wavelengths_nm\": [float(wavelengths_np[i]) for i in selected_indices],\n        \"windows_nm\": [[float(s), float(e)] for s, e in self.windows],\n        \"alpha\": self.alpha,\n    }\n\n    return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.CIRFalseColorSelector","title":"CIRFalseColorSelector","text":"<pre><code>CIRFalseColorSelector(\n    nir_nm=860.0, red_nm=670.0, green_nm=560.0, **kwargs\n)\n</code></pre> <p>               Bases: <code>BandSelectorBase</code></p> <p>Color Infrared (CIR) false color composition.</p> <p>Maps NIR to Red, Red to Green, Green to Blue for false-color composites. This is useful for highlighting vegetation and certain anomalies.</p> <p>Parameters:</p> Name Type Description Default <code>nir_nm</code> <code>float</code> <p>Near-infrared wavelength in nm. Default: 860.0</p> <code>860.0</code> <code>red_nm</code> <code>float</code> <p>Red wavelength in nm. Default: 670.0</p> <code>670.0</code> <code>green_nm</code> <code>float</code> <p>Green wavelength in nm. Default: 560.0</p> <code>560.0</code> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    nir_nm: float = 860.0,\n    red_nm: float = 670.0,\n    green_nm: float = 560.0,\n    **kwargs,\n) -&gt; None:\n    super().__init__(nir_nm=nir_nm, red_nm=red_nm, green_nm=green_nm, **kwargs)\n    self.nir_nm = nir_nm\n    self.red_nm = red_nm\n    self.green_nm = green_nm\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.CIRFalseColorSelector.forward","title":"forward","text":"<pre><code>forward(cube, wavelengths, context=None, **_)\n</code></pre> <p>Select CIR bands and compose false-color image.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>Tensor</code> <p>Wavelength array [C].</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"rgb_image\" and \"band_info\" keys.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    wavelengths: Any,\n    context: Context | None = None,  # noqa: ARG002\n    **_: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Select CIR bands and compose false-color image.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Hyperspectral cube [B, H, W, C].\n    wavelengths : torch.Tensor\n        Wavelength array [C].\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"rgb_image\" and \"band_info\" keys.\n    \"\"\"\n    wavelengths_np = np.asarray(wavelengths, dtype=np.float32)\n\n    # CIR mapping: NIR -&gt; R, Red -&gt; G, Green -&gt; B\n    nir_idx = self._nearest_band_index(wavelengths_np, self.nir_nm)\n    red_idx = self._nearest_band_index(wavelengths_np, self.red_nm)\n    green_idx = self._nearest_band_index(wavelengths_np, self.green_nm)\n\n    indices = [nir_idx, red_idx, green_idx]\n    rgb = self._compose_rgb(cube, indices)\n\n    band_info = {\n        \"strategy\": \"cir_false_color\",\n        \"band_indices\": indices,\n        \"band_wavelengths_nm\": [float(wavelengths_np[i]) for i in indices],\n        \"target_wavelengths_nm\": [self.nir_nm, self.red_nm, self.green_nm],\n        \"channel_mapping\": {\"R\": \"NIR\", \"G\": \"Red\", \"B\": \"Green\"},\n    }\n\n    return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedBandSelectorBase","title":"SupervisedBandSelectorBase","text":"<pre><code>SupervisedBandSelectorBase(\n    num_spectral_bands,\n    score_weights=(1.0, 1.0, 1.0),\n    lambda_penalty=0.5,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>BandSelectorBase</code></p> <p>Base class for supervised band selection strategies.</p> <p>This class adds an optional <code>mask</code> input port and implements common logic for statistical initialization via :meth:<code>fit</code>.</p> <p>The mask is assumed to be binary (0/1), where 1 denotes the positive class (e.g. stone) and 0 denotes the negative class (e.g. lentil/background).</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    num_spectral_bands: int,\n    score_weights: tuple[float, float, float] = (1.0, 1.0, 1.0),\n    lambda_penalty: float = 0.5,\n    **kwargs: Any,\n) -&gt; None:\n    # Call super().__init__ FIRST so Serializable captures hparams correctly\n    super().__init__(\n        num_spectral_bands=num_spectral_bands,\n        score_weights=score_weights,\n        lambda_penalty=lambda_penalty,\n        **kwargs,\n    )\n    # Then set instance attributes\n    self.num_spectral_bands = num_spectral_bands\n    self.score_weights = score_weights\n    self.lambda_penalty = lambda_penalty\n    # Initialize buffers with correct shapes (not empty)\n    # selected_indices: always 3 for RGB\n    # score buffers: num_spectral_bands\n    self.register_buffer(\"selected_indices\", torch.zeros(3, dtype=torch.long), persistent=True)\n    self.register_buffer(\n        \"band_scores\", torch.zeros(num_spectral_bands, dtype=torch.float32), persistent=True\n    )\n    self.register_buffer(\n        \"fisher_scores\", torch.zeros(num_spectral_bands, dtype=torch.float32), persistent=True\n    )\n    self.register_buffer(\n        \"auc_scores\", torch.zeros(num_spectral_bands, dtype=torch.float32), persistent=True\n    )\n    self.register_buffer(\n        \"mi_scores\", torch.zeros(num_spectral_bands, dtype=torch.float32), persistent=True\n    )\n    # Use standard instance attribute for initialization tracking\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedBandSelectorBase.requires_initial_fit","title":"requires_initial_fit  <code>property</code>","text":"<pre><code>requires_initial_fit\n</code></pre> <p>Whether this node requires statistical initialization from training data.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always True for supervised band selectors.</p>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedCIRBandSelector","title":"SupervisedCIRBandSelector","text":"<pre><code>SupervisedCIRBandSelector(\n    windows=(\n        (840.0, 910.0),\n        (650.0, 720.0),\n        (500.0, 570.0),\n    ),\n    score_weights=(1.0, 1.0, 1.0),\n    lambda_penalty=0.5,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>SupervisedBandSelectorBase</code></p> <p>Supervised CIR/NIR band selection with window constraints.</p> <p>Windows are typically set to:     - NIR: 840-910 nm     - Red: 650-720 nm     - Green: 500-570 nm</p> <p>The selector chooses one band per window using a supervised score (Fisher + AUC + MI) with an mRMR-style redundancy penalty.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    windows: Sequence[tuple[float, float]] = ((840.0, 910.0), (650.0, 720.0), (500.0, 570.0)),\n    score_weights: tuple[float, float, float] = (1.0, 1.0, 1.0),\n    lambda_penalty: float = 0.5,\n    **kwargs: Any,\n) -&gt; None:\n    # Call super().__init__ FIRST so Serializable captures hparams correctly\n    super().__init__(\n        score_weights=score_weights,\n        lambda_penalty=lambda_penalty,\n        windows=list(windows),\n        **kwargs,\n    )\n    # Then set instance attributes\n    self.windows = list(windows)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedCIRBandSelector.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize band selection using supervised scoring with CIR windows.</p> <p>Computes Fisher, AUC, and MI scores for each band, applies mRMR selection within CIR-specific wavelength windows, and stores the 3 selected bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Training data stream with cube, mask, and wavelengths.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mRMR selection doesn't return exactly 3 bands.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize band selection using supervised scoring with CIR windows.\n\n    Computes Fisher, AUC, and MI scores for each band, applies mRMR selection\n    within CIR-specific wavelength windows, and stores the 3 selected bands.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Training data stream with cube, mask, and wavelengths.\n\n    Raises\n    ------\n    ValueError\n        If the mRMR selection doesn't return exactly 3 bands.\n    \"\"\"\n    cubes, masks, wavelengths = self._collect_training_data(input_stream)\n    band_scores, fisher_scores, auc_scores, mi_scores = _compute_band_scores_supervised(\n        cubes,\n        masks,\n        wavelengths,\n        self.score_weights,\n    )\n    corr_matrix = _compute_band_correlation_matrix(cubes, len(wavelengths))\n    selected_indices = _mrmr_band_selection(\n        band_scores,\n        wavelengths,\n        self.windows,\n        corr_matrix,\n        self.lambda_penalty,\n    )\n    if len(selected_indices) != 3:\n        raise ValueError(\n            f\"SupervisedCIRBandSelector expected 3 bands, got {len(selected_indices)}\"\n        )\n    self._store_scores_and_indices(\n        band_scores, fisher_scores, auc_scores, mi_scores, selected_indices\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedCIRBandSelector.forward","title":"forward","text":"<pre><code>forward(cube, wavelengths, mask=None, context=None, **_)\n</code></pre> <p>Generate false-color RGB from selected CIR bands.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>ndarray</code> <p>Wavelengths for each channel [C].</p> required <code>mask</code> <code>Tensor</code> <p>Ground truth mask (unused in forward, required for initialization).</p> <code>None</code> <code>context</code> <code>Context</code> <p>Pipeline execution context (unused).</p> <code>None</code> <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"rgb_image\" [B, H, W, 3] and \"band_info\" metadata.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the node has not been statistically initialized.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    wavelengths: np.ndarray,\n    mask: torch.Tensor | None = None,  # noqa: ARG002\n    context: Context | None = None,  # noqa: ARG002\n    **_: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Generate false-color RGB from selected CIR bands.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Hyperspectral cube [B, H, W, C].\n    wavelengths : np.ndarray\n        Wavelengths for each channel [C].\n    mask : torch.Tensor, optional\n        Ground truth mask (unused in forward, required for initialization).\n    context : Context, optional\n        Pipeline execution context (unused).\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"rgb_image\" [B, H, W, 3] and \"band_info\" metadata.\n\n    Raises\n    ------\n    RuntimeError\n        If the node has not been statistically initialized.\n    \"\"\"\n    if not self._statistically_initialized or self.selected_indices.numel() != 3:\n        raise RuntimeError(\"SupervisedCIRBandSelector not fitted\")\n\n    wavelengths_np = np.asarray(wavelengths, dtype=np.float32)\n    indices = self.selected_indices.tolist()\n    rgb = self._compose_rgb(cube, indices)\n\n    band_info = {\n        \"strategy\": \"supervised_cir\",\n        \"band_indices\": indices,\n        \"band_wavelengths_nm\": [float(wavelengths_np[i]) for i in indices],\n        \"windows_nm\": [[float(s), float(e)] for s, e in self.windows],\n        \"score_weights\": list(self.score_weights),\n        \"lambda_penalty\": float(self.lambda_penalty),\n    }\n    return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedWindowedFalseRGBSelector","title":"SupervisedWindowedFalseRGBSelector","text":"<pre><code>SupervisedWindowedFalseRGBSelector(\n    windows=(\n        (440.0, 500.0),\n        (500.0, 580.0),\n        (610.0, 700.0),\n    ),\n    score_weights=(1.0, 1.0, 1.0),\n    lambda_penalty=0.5,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>SupervisedBandSelectorBase</code></p> <p>Supervised band selection constrained to visible RGB windows.</p> <p>Similar to :class:<code>HighContrastBandSelector</code>, but uses label-driven scores. Default windows:     - Blue: 440\u2013500 nm     - Green: 500\u2013580 nm     - Red: 610\u2013700 nm</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    windows: Sequence[tuple[float, float]] = ((440.0, 500.0), (500.0, 580.0), (610.0, 700.0)),\n    score_weights: tuple[float, float, float] = (1.0, 1.0, 1.0),\n    lambda_penalty: float = 0.5,\n    **kwargs: Any,\n) -&gt; None:\n    # Call super().__init__ FIRST so Serializable captures hparams correctly\n    super().__init__(\n        score_weights=score_weights,\n        lambda_penalty=lambda_penalty,\n        windows=list(windows),\n        **kwargs,\n    )\n    # Then set instance attributes\n    self.windows = list(windows)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedWindowedFalseRGBSelector.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize band selection using supervised scoring with RGB windows.</p> <p>Computes Fisher, AUC, and MI scores for each band, applies mRMR selection within RGB wavelength windows (blue/green/red), and stores the 3 selected bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Training data stream with cube, mask, and wavelengths.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mRMR selection doesn't return exactly 3 bands.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize band selection using supervised scoring with RGB windows.\n\n    Computes Fisher, AUC, and MI scores for each band, applies mRMR selection\n    within RGB wavelength windows (blue/green/red), and stores the 3 selected bands.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Training data stream with cube, mask, and wavelengths.\n\n    Raises\n    ------\n    ValueError\n        If the mRMR selection doesn't return exactly 3 bands.\n    \"\"\"\n    cubes, masks, wavelengths = self._collect_training_data(input_stream)\n    band_scores, fisher_scores, auc_scores, mi_scores = _compute_band_scores_supervised(\n        cubes,\n        masks,\n        wavelengths,\n        self.score_weights,\n    )\n    corr_matrix = _compute_band_correlation_matrix(cubes, len(wavelengths))\n    selected_indices = _mrmr_band_selection(\n        band_scores,\n        wavelengths,\n        self.windows,\n        corr_matrix,\n        self.lambda_penalty,\n    )\n    if len(selected_indices) != 3:\n        raise ValueError(\n            f\"SupervisedWindowedFalseRGBSelector expected 3 bands, got {len(selected_indices)}\",\n        )\n    self._store_scores_and_indices(\n        band_scores, fisher_scores, auc_scores, mi_scores, selected_indices\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedWindowedFalseRGBSelector.forward","title":"forward","text":"<pre><code>forward(cube, wavelengths, mask=None, context=None, **_)\n</code></pre> <p>Generate false-color RGB from selected windowed bands.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>ndarray</code> <p>Wavelengths for each channel [C].</p> required <code>mask</code> <code>Tensor</code> <p>Ground truth mask (unused in forward, required for initialization).</p> <code>None</code> <code>context</code> <code>Context</code> <p>Pipeline execution context (unused).</p> <code>None</code> <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"rgb_image\" [B, H, W, 3] and \"band_info\" metadata.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the node has not been statistically initialized.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    wavelengths: np.ndarray,\n    mask: torch.Tensor | None = None,  # noqa: ARG002\n    context: Context | None = None,  # noqa: ARG002\n    **_: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Generate false-color RGB from selected windowed bands.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Hyperspectral cube [B, H, W, C].\n    wavelengths : np.ndarray\n        Wavelengths for each channel [C].\n    mask : torch.Tensor, optional\n        Ground truth mask (unused in forward, required for initialization).\n    context : Context, optional\n        Pipeline execution context (unused).\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"rgb_image\" [B, H, W, 3] and \"band_info\" metadata.\n\n    Raises\n    ------\n    RuntimeError\n        If the node has not been statistically initialized.\n    \"\"\"\n    if not self._statistically_initialized or self.selected_indices.numel() != 3:\n        raise RuntimeError(\"SupervisedWindowedFalseRGBSelector not fitted\")\n\n    wavelengths_np = np.asarray(wavelengths, dtype=np.float32)\n    indices = self.selected_indices.tolist()\n    rgb = self._compose_rgb(cube, indices)\n\n    band_info = {\n        \"strategy\": \"supervised_windowed_false_rgb\",\n        \"band_indices\": indices,\n        \"band_wavelengths_nm\": [float(wavelengths_np[i]) for i in indices],\n        \"windows_nm\": [[float(s), float(e)] for s, e in self.windows],\n        \"score_weights\": list(self.score_weights),\n        \"lambda_penalty\": float(self.lambda_penalty),\n    }\n    return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedFullSpectrumBandSelector","title":"SupervisedFullSpectrumBandSelector","text":"<pre><code>SupervisedFullSpectrumBandSelector(\n    score_weights=(1.0, 1.0, 1.0),\n    lambda_penalty=0.5,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>SupervisedBandSelectorBase</code></p> <p>Supervised selection without window constraints.</p> <p>Picks the top-3 discriminative bands globally with an mRMR-style redundancy penalty applied over the full spectrum.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def __init__(\n    self,\n    score_weights: tuple[float, float, float] = (1.0, 1.0, 1.0),\n    lambda_penalty: float = 0.5,\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(score_weights=score_weights, lambda_penalty=lambda_penalty, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedFullSpectrumBandSelector.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize band selection using supervised scoring across full spectrum.</p> <p>Computes Fisher, AUC, and MI scores for each band, applies mRMR selection globally without wavelength window constraints, and stores the 3 selected bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Training data stream with cube, mask, and wavelengths.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mRMR selection doesn't return exactly 3 bands.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize band selection using supervised scoring across full spectrum.\n\n    Computes Fisher, AUC, and MI scores for each band, applies mRMR selection\n    globally without wavelength window constraints, and stores the 3 selected bands.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Training data stream with cube, mask, and wavelengths.\n\n    Raises\n    ------\n    ValueError\n        If the mRMR selection doesn't return exactly 3 bands.\n    \"\"\"\n    cubes, masks, wavelengths = self._collect_training_data(input_stream)\n    band_scores, fisher_scores, auc_scores, mi_scores = _compute_band_scores_supervised(\n        cubes,\n        masks,\n        wavelengths,\n        self.score_weights,\n    )\n    corr_matrix = _compute_band_correlation_matrix(cubes, len(wavelengths))\n    selected_indices = _select_top_k_bands(\n        band_scores,\n        corr_matrix,\n        k=3,\n        lambda_penalty=self.lambda_penalty,\n    )\n    if len(selected_indices) != 3:\n        raise ValueError(\n            f\"SupervisedFullSpectrumBandSelector expected 3 bands, got {len(selected_indices)}\",\n        )\n    self._store_scores_and_indices(\n        band_scores, fisher_scores, auc_scores, mi_scores, selected_indices\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.band_selection.SupervisedFullSpectrumBandSelector.forward","title":"forward","text":"<pre><code>forward(cube, wavelengths, mask=None, context=None, **_)\n</code></pre> <p>Generate false-color RGB from globally selected bands.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>wavelengths</code> <code>ndarray</code> <p>Wavelengths for each channel [C].</p> required <code>mask</code> <code>Tensor</code> <p>Ground truth mask (unused in forward, required for initialization).</p> <code>None</code> <code>context</code> <code>Context</code> <p>Pipeline execution context (unused).</p> <code>None</code> <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"rgb_image\" [B, H, W, 3] and \"band_info\" metadata.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the node has not been statistically initialized.</p> Source code in <code>cuvis_ai/node/band_selection.py</code> <pre><code>def forward(\n    self,\n    cube: torch.Tensor,\n    wavelengths: np.ndarray,\n    mask: torch.Tensor | None = None,  # noqa: ARG002\n    context: Context | None = None,  # noqa: ARG002\n    **_: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Generate false-color RGB from globally selected bands.\n\n    Parameters\n    ----------\n    cube : torch.Tensor\n        Hyperspectral cube [B, H, W, C].\n    wavelengths : np.ndarray\n        Wavelengths for each channel [C].\n    mask : torch.Tensor, optional\n        Ground truth mask (unused in forward, required for initialization).\n    context : Context, optional\n        Pipeline execution context (unused).\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"rgb_image\" [B, H, W, 3] and \"band_info\" metadata.\n\n    Raises\n    ------\n    RuntimeError\n        If the node has not been statistically initialized.\n    \"\"\"\n    if not self._statistically_initialized or self.selected_indices.numel() != 3:\n        raise RuntimeError(\"SupervisedFullSpectrumBandSelector not fitted\")\n\n    wavelengths_np = np.asarray(wavelengths, dtype=np.float32)\n    indices = self.selected_indices.tolist()\n    rgb = self._compose_rgb(cube, indices)\n\n    band_info = {\n        \"strategy\": \"supervised_full_spectrum\",\n        \"band_indices\": indices,\n        \"band_wavelengths_nm\": [float(wavelengths_np[i]) for i in indices],\n        \"score_weights\": list(self.score_weights),\n        \"lambda_penalty\": float(self.lambda_penalty),\n    }\n    return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre>"},{"location":"api/nodes/#channel-mixer","title":"Channel Mixer","text":""},{"location":"api/nodes/#cuvis_ai.node.channel_mixer","title":"channel_mixer","text":"<p>Learnable channel mixer node for DRCNN-style spectral data reduction.</p> <p>This module implements a learnable channel mixer based on the Data Reduction CNN (DRCNN) approach from Zeegers et al. (2020). The mixer performs spectral pixel-wise 1x1 convolutions to reduce hyperspectral data to a smaller number of channels (e.g., 61 \u2192 3 for RGB compatibility).</p> <p>Reference:     Zeegers et al., \"Task-Driven Learned Hyperspectral Data Reduction Using End-to-End     Supervised Deep Learning,\" J. Imaging 6(12):132, 2020.</p>"},{"location":"api/nodes/#cuvis_ai.node.channel_mixer.LearnableChannelMixer","title":"LearnableChannelMixer","text":"<pre><code>LearnableChannelMixer(\n    input_channels,\n    output_channels,\n    leaky_relu_negative_slope=0.01,\n    use_bias=True,\n    use_activation=True,\n    normalize_output=True,\n    init_method=\"xavier\",\n    eps=1e-06,\n    reduction_scheme=None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Learnable channel mixer for hyperspectral data reduction (DRCNN-style).</p> <p>This node implements a learnable linear combination layer that reduces the number of spectral channels through spectral pixel-wise 1x1 convolutions. Based on the DRCNN approach, it uses: - 1x1 convolution (linear combination across spectral dimension) - Leaky ReLU activation (a=0.01) - Bias parameters - Optional PCA-based initialization</p> <p>The mixer is designed to be trained end-to-end with a downstream model (e.g., AdaClip) while keeping the downstream model frozen. This allows the mixer to learn optimal spectral combinations for the specific task.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of input spectral channels (e.g., 61 for hyperspectral cube)</p> required <code>output_channels</code> <code>int</code> <p>Number of output channels (e.g., 3 for RGB compatibility)</p> required <code>leaky_relu_negative_slope</code> <code>float</code> <p>Negative slope for Leaky ReLU activation (default: 0.01, as per DRCNN paper)</p> <code>0.01</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias parameters (default: True, as per DRCNN paper)</p> <code>True</code> <code>use_activation</code> <code>bool</code> <p>Whether to apply Leaky ReLU activation (default: True, as per DRCNN paper)</p> <code>True</code> <code>normalize_output</code> <code>bool</code> <p>Whether to apply per-channel min-max normalization to [0, 1] range (default: True). This matches the behavior of band selectors and ensures compatibility with AdaClip. When True, each output channel is normalized independently using per-batch statistics.</p> <code>True</code> <code>init_method</code> <code>('xavier', 'kaiming', 'pca', 'zeros')</code> <p>Weight initialization method (default: \"xavier\") - \"xavier\": Xavier/Glorot uniform initialization - \"kaiming\": Kaiming/He uniform initialization - \"pca\": Initialize from PCA components (requires statistical_initialization) - \"zeros\": Zero initialization (weights and bias start at zero)</p> <code>\"xavier\"</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>reduction_scheme</code> <code>list[int] | None</code> <p>Multi-layer reduction scheme for gradual channel reduction (default: None). If None, uses single-layer reduction (input_channels \u2192 output_channels). If provided, must start with input_channels and end with output_channels. Example: [61, 16, 8, 3] means: - Layer 1: 61 \u2192 16 channels - Layer 2: 16 \u2192 8 channels - Layer 3: 8 \u2192 3 channels This matches the DRCNN paper's multi-layer architecture for better optimization.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>conv</code> <code>Conv2d</code> <p>1x1 convolutional layer performing spectral mixing</p> <code>activation</code> <code>LeakyReLU or None</code> <p>Leaky ReLU activation function (if use_activation=True)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create mixer: 61 channels \u2192 3 channels (single-layer)\n&gt;&gt;&gt; mixer = LearnableChannelMixer(\n...     input_channels=61,\n...     output_channels=3,\n...     leaky_relu_negative_slope=0.01,\n...     init_method=\"xavier\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create mixer with multi-layer reduction (matches DRCNN paper)\n&gt;&gt;&gt; mixer = LearnableChannelMixer(\n...     input_channels=61,\n...     output_channels=3,\n...     reduction_scheme=[61, 16, 8, 3],  # Gradual reduction\n...     leaky_relu_negative_slope=0.01,\n...     init_method=\"xavier\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Optional: Initialize from PCA\n&gt;&gt;&gt; # mixer.statistical_initialization(input_stream)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Enable gradient training\n&gt;&gt;&gt; mixer.unfreeze()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Forward pass: [B, H, W, 61] \u2192 [B, H, W, 3]\n&gt;&gt;&gt; output = mixer.forward(data=hsi_cube)\n&gt;&gt;&gt; rgb_like = output[\"rgb\"]  # [B, H, W, 3]\n</code></pre> Source code in <code>cuvis_ai/node/channel_mixer.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    leaky_relu_negative_slope: float = 0.01,\n    use_bias: bool = True,\n    use_activation: bool = True,\n    normalize_output: bool = True,\n    init_method: Literal[\"xavier\", \"kaiming\", \"pca\", \"zeros\"] = \"xavier\",\n    eps: float = 1e-6,\n    reduction_scheme: list[int] | None = None,\n    **kwargs,\n) -&gt; None:\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.leaky_relu_negative_slope = leaky_relu_negative_slope\n    self.use_bias = use_bias\n    self.use_activation = use_activation\n    self.normalize_output = normalize_output\n    self.init_method = init_method\n    self.eps = eps\n\n    # Determine reduction scheme: if None, use single-layer (backward compatible)\n    # If provided, use multi-layer gradual reduction (e.g., [61, 16, 8, 3])\n    if reduction_scheme is None:\n        reduction_scheme = [input_channels, output_channels]\n    else:\n        # Validate reduction scheme\n        if reduction_scheme[0] != input_channels:\n            raise ValueError(\n                f\"First element of reduction_scheme must match input_channels: \"\n                f\"got {reduction_scheme[0]}, expected {input_channels}\"\n            )\n        if reduction_scheme[-1] != output_channels:\n            raise ValueError(\n                f\"Last element of reduction_scheme must match output_channels: \"\n                f\"got {reduction_scheme[-1]}, expected {output_channels}\"\n            )\n        if len(reduction_scheme) &lt; 2:\n            raise ValueError(\n                f\"reduction_scheme must have at least 2 elements, got {len(reduction_scheme)}\"\n            )\n\n    self.reduction_scheme = reduction_scheme\n    self.num_layers = len(reduction_scheme) - 1  # Number of reduction layers\n\n    super().__init__(\n        input_channels=input_channels,\n        output_channels=output_channels,\n        leaky_relu_negative_slope=leaky_relu_negative_slope,\n        use_bias=use_bias,\n        use_activation=use_activation,\n        normalize_output=normalize_output,\n        init_method=init_method,\n        eps=eps,\n        reduction_scheme=reduction_scheme,\n        **kwargs,\n    )\n\n    # Create multi-layer reduction architecture (as per DRCNN paper)\n    # Each layer performs: C_in \u2192 C_out reduction via 1x1 convolution\n    self.convs = nn.ModuleList()\n    for i in range(self.num_layers):\n        in_ch = reduction_scheme[i]\n        out_ch = reduction_scheme[i + 1]\n        conv = nn.Conv2d(\n            in_channels=in_ch,\n            out_channels=out_ch,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=use_bias,\n        )\n        self.convs.append(conv)\n\n    # Leaky ReLU activation (as per DRCNN paper)\n    # Note: Leaky ReLU with a=0.01 can be very aggressive, killing most negative values\n    # Consider using a higher value (e.g., 0.1) or removing activation if issues occur\n    if use_activation:\n        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n    else:\n        self.activation = None\n\n    # Initialize weights based on method\n    self._initialize_weights()\n\n    # Track initialization state\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.channel_mixer.LearnableChannelMixer.requires_initial_fit","title":"requires_initial_fit  <code>property</code>","text":"<pre><code>requires_initial_fit\n</code></pre> <p>Whether this node requires statistical initialization.</p>"},{"location":"api/nodes/#cuvis_ai.node.channel_mixer.LearnableChannelMixer.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize mixer weights from PCA components.</p> <p>This method computes PCA on the input data and initializes the mixer weights to the top principal components. This provides a good starting point for gradient-based optimization.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Iterator yielding dicts matching INPUT_SPECS (port-based format) Expected format: {\"data\": tensor} where tensor is [B, H, W, C_in]</p> required Notes <p>This method is only used when init_method=\"pca\". For other initialization methods, weights are set in init.</p> Source code in <code>cuvis_ai/node/channel_mixer.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize mixer weights from PCA components.\n\n    This method computes PCA on the input data and initializes the mixer weights\n    to the top principal components. This provides a good starting point for\n    gradient-based optimization.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Iterator yielding dicts matching INPUT_SPECS (port-based format)\n        Expected format: {\"data\": tensor} where tensor is [B, H, W, C_in]\n\n    Notes\n    -----\n    This method is only used when init_method=\"pca\". For other initialization\n    methods, weights are set in __init__.\n    \"\"\"\n    if self.init_method != \"pca\":\n        return  # No statistical initialization needed\n\n    # Collect all data for PCA\n    all_data = []\n    for batch_data in input_stream:\n        x = batch_data[\"data\"]\n        if x is not None:\n            # Flatten spatial dimensions: [B, H, W, C] -&gt; [B*H*W, C]\n            flat = x.reshape(-1, x.shape[-1])\n            all_data.append(flat)\n\n    if not all_data:\n        raise ValueError(\"No data provided for PCA initialization\")\n\n    # Concatenate all samples\n    X = torch.cat(all_data, dim=0)  # [N, C_in]\n\n    # Compute mean and center data\n    mean = X.mean(dim=0, keepdim=True)  # [1, C_in]\n    X_centered = X - mean  # [N, C_in]\n\n    # Compute SVD to get principal components\n    # X_centered = U @ S @ V.T where V contains principal components\n    U, S, Vt = torch.linalg.svd(X_centered, full_matrices=False)\n\n    # For multi-layer, initialize only the first layer with PCA\n    # Subsequent layers use xavier initialization (already done in _initialize_weights)\n    first_layer_out_channels = self.reduction_scheme[1]\n\n    # Extract top first_layer_out_channels components\n    # Vt is [min(N, C_in), C_in], we want first first_layer_out_channels rows\n    n_components = min(first_layer_out_channels, Vt.shape[0])\n    components = Vt[:n_components, :].clone()  # [n_components, C_in]\n\n    # If we need more output channels than components, pad with zeros\n    if n_components &lt; first_layer_out_channels:\n        padding = torch.zeros(\n            first_layer_out_channels - n_components,\n            self.input_channels,\n            device=components.device,\n            dtype=components.dtype,\n        )\n        components = torch.cat([components, padding], dim=0)\n\n    # Set weights for first layer: conv weight shape is [C_out, C_in, 1, 1]\n    # We need to transpose components: [C_out, C_in]\n    with torch.no_grad():\n        self.convs[0].weight.data = components.view(\n            first_layer_out_channels, self.input_channels, 1, 1\n        )\n\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.channel_mixer.LearnableChannelMixer.unfreeze","title":"unfreeze","text":"<pre><code>unfreeze()\n</code></pre> <p>Enable gradient-based training of mixer weights.</p> <p>Call this method to allow gradient updates during training. The mixer weights and biases will be optimized via backpropagation.</p> Example <p>mixer = LearnableChannelMixer(input_channels=61, output_channels=3) mixer.unfreeze()  # Enable gradient training</p> Source code in <code>cuvis_ai/node/channel_mixer.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Enable gradient-based training of mixer weights.\n\n    Call this method to allow gradient updates during training. The mixer\n    weights and biases will be optimized via backpropagation.\n\n    Example\n    -------\n    &gt;&gt;&gt; mixer = LearnableChannelMixer(input_channels=61, output_channels=3)\n    &gt;&gt;&gt; mixer.unfreeze()  # Enable gradient training\n    &gt;&gt;&gt; # Now mixer weights can be optimized with gradient descent\n    \"\"\"\n    # Ensure parameters require gradients for all layers\n    for conv in self.convs:\n        for param in conv.parameters():\n            param.requires_grad = True\n    # Call parent to enable requires_grad on the module\n    super().unfreeze()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.channel_mixer.LearnableChannelMixer.unfreeze--now-mixer-weights-can-be-optimized-with-gradient-descent","title":"Now mixer weights can be optimized with gradient descent","text":""},{"location":"api/nodes/#cuvis_ai.node.channel_mixer.LearnableChannelMixer.forward","title":"forward","text":"<pre><code>forward(data, context=None, **_)\n</code></pre> <p>Apply learnable channel mixing to input.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor [B, H, W, C_in] in BHWC format</p> required <code>context</code> <code>Context</code> <p>Execution context with epoch, batch_idx, stage info</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"rgb\" key containing reduced channels [B, H, W, C_out]</p> Source code in <code>cuvis_ai/node/channel_mixer.py</code> <pre><code>def forward(self, data: Tensor, context: Context | None = None, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Apply learnable channel mixing to input.\n\n    Parameters\n    ----------\n    data : Tensor\n        Input tensor [B, H, W, C_in] in BHWC format\n    context : Context, optional\n        Execution context with epoch, batch_idx, stage info\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"rgb\" key containing reduced channels [B, H, W, C_out]\n    \"\"\"\n    B, H, W, C_in = data.shape\n\n    # DEBUG: Print input info\n    if hasattr(self, \"_debug\") and self._debug:\n        print(\n            f\"[LearnableChannelMixer] Input: shape={data.shape}, \"\n            f\"min={data.min().item():.4f}, max={data.max().item():.4f}, \"\n            f\"mean={data.mean().item():.4f}, requires_grad={data.requires_grad}\"\n        )\n\n    # Validate input channels\n    if C_in != self.input_channels:\n        raise ValueError(\n            f\"Expected {self.input_channels} input channels, got {C_in}. \"\n            f\"Input shape: {data.shape}\"\n        )\n\n    # DEBUG disabled: previously saved input tensor here (_save_debug_tensor).\n    # for b in range(B):\n    #     self._save_debug_tensor(data[b], \"input\", context, frame_idx=b)\n\n    # Convert from BHWC to BCHW for Conv2d\n    data_bchw = data.permute(0, 3, 1, 2)  # [B, C_in, H, W]\n\n    # Apply multi-layer reduction (as per DRCNN paper)\n    # Each layer: 1x1 conv \u2192 Leaky ReLU (if enabled)\n    mixed = data_bchw\n    for i, conv in enumerate(self.convs):\n        # Apply 1x1 convolution (spectral mixing)\n        mixed = conv(mixed)  # [B, C_out_i, H, W]\n\n        # Apply Leaky ReLU activation if enabled (except after last layer if we normalize)\n        # For multi-layer, we apply activation after each layer except the last\n        # The last layer's output will be normalized, so we skip activation there if normalize_output=True\n        if self.activation is not None:\n            if i &lt; len(self.convs) - 1 or not self.normalize_output:\n                mixed = self.activation(mixed)\n\n    # Convert back from BCHW to BHWC\n    mixed_bhwc = mixed.permute(0, 2, 3, 1)  # [B, H, W, C_out]\n\n    # DEBUG disabled: previously saved output_before_norm tensor here (_save_debug_tensor).\n    # for b in range(B):\n    #     self._save_debug_tensor(mixed_bhwc[b], \"output_before_norm\", context, frame_idx=b)\n\n    # Apply per-channel normalization to [0, 1] range (matching band selector behavior)\n    # This ensures compatibility with AdaClip preprocessing\n    if self.normalize_output:\n        # Per-image, per-channel min/max normalization to [0, 1]\n        # This ensures each image is normalized independently for visual consistency\n        # Shape: [B, H, W, C_out]\n        B_norm, H_norm, W_norm, C_norm = mixed_bhwc.shape\n        # Reshape to [B, H*W, C] for easier per-image processing\n        mixed_flat = mixed_bhwc.view(B_norm, H_norm * W_norm, C_norm)\n        # Compute min/max per image, per channel: [B, 1, C]\n        rgb_min = mixed_flat.amin(dim=1, keepdim=True)  # [B, 1, C]\n        rgb_max = mixed_flat.amax(dim=1, keepdim=True)  # [B, 1, C]\n        denom = (rgb_max - rgb_min).clamp_min(self.eps)\n        # Normalize: [B, H*W, C]\n        mixed_normalized = (mixed_flat - rgb_min) / denom\n        # Reshape back and clamp\n        mixed_bhwc = mixed_normalized.view(B_norm, H_norm, W_norm, C_norm).clamp_(0.0, 1.0)\n\n    # DEBUG disabled: previously saved output_after_norm tensor here (_save_debug_tensor).\n    # for b in range(B):\n    #     self._save_debug_tensor(mixed_bhwc[b], \"output_after_norm\", context, frame_idx=b)\n\n    # DEBUG: Print output info\n    if hasattr(self, \"_debug\") and self._debug:\n        print(\n            f\"[LearnableChannelMixer] Output: shape={mixed_bhwc.shape}, \"\n            f\"min={mixed_bhwc.min().item():.4f}, max={mixed_bhwc.max().item():.4f}, \"\n            f\"mean={mixed_bhwc.mean().item():.4f}, requires_grad={mixed_bhwc.requires_grad}\"\n        )\n\n    return {\"rgb\": mixed_bhwc}\n</code></pre>"},{"location":"api/nodes/#concrete-selector","title":"Concrete Selector","text":""},{"location":"api/nodes/#cuvis_ai.node.concrete_selector","title":"concrete_selector","text":"<p>Concrete/Gumbel-Softmax band selector node for hyperspectral data.</p> <p>This module implements a learnable band selector using the Concrete / Gumbel-Softmax relaxation, suitable for end-to-end training with AdaClip.</p> <p>The selector learns <code>K</code> categorical distributions over <code>T</code> input bands, and during training uses the Gumbel-Softmax trick to produce differentiable approximate one-hot selection weights that become increasingly peaked as the temperature :math:<code>\\tau</code> is annealed.</p> <p>For each output channel :math:<code>c \\in {1, \\dots, K}</code>, we learn logits <code>L_c in R^T</code> and sample:</p> <p>.. math::</p> <pre><code>w_c = \\text{softmax}\\left( \\frac{L_c + g}{\\tau} \\right), \\quad\ng \\sim \\text{Gumbel}(0, 1)\n</code></pre> <p>The resulting weights are used to form K-channel RGB-like images:</p> <p>.. math::</p> <pre><code>Y[:, :, c] = \\sum_{t=1}^T w_c[t] \\cdot X[:, :, t]\n</code></pre> <p>where <code>X</code> is the input hyperspectral cube in <code>[0, 1]</code>.</p>"},{"location":"api/nodes/#cuvis_ai.node.concrete_selector.ConcreteBandSelector","title":"ConcreteBandSelector","text":"<pre><code>ConcreteBandSelector(\n    input_channels,\n    output_channels=3,\n    tau_start=10.0,\n    tau_end=0.1,\n    max_epochs=20,\n    use_hard_inference=True,\n    eps=1e-06,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Concrete/Gumbel-Softmax band selector for hyperspectral cubes.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of input spectral channels (e.g., 61 for hyperspectral cube).</p> required <code>output_channels</code> <code>int</code> <p>Number of output channels (default: 3 for RGB/AdaClip compatibility).</p> <code>3</code> <code>tau_start</code> <code>float</code> <p>Initial temperature for Gumbel-Softmax (default: 10.0).</p> <code>10.0</code> <code>tau_end</code> <code>float</code> <p>Final temperature for Gumbel-Softmax (default: 0.1).</p> <code>0.1</code> <code>max_epochs</code> <code>int</code> <p>Number of epochs over which to exponentially anneal :math:<code>\\tau</code> from <code>tau_start</code> to <code>tau_end</code> (default: 20).</p> <code>20</code> <code>use_hard_inference</code> <code>bool</code> <p>If True, uses hard argmax selection at inference/validation time (one-hot weights). If False, uses softmax over logits (default: True).</p> <code>True</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6).</p> <code>1e-06</code> Notes <ul> <li>During training (<code>context.stage == 'train'</code>), the node samples   Gumbel noise and uses the Concrete relaxation with the current   temperature :math:<code>\\tau(\\text{epoch})</code>.</li> <li>During validation/test/inference, it uses deterministic weights   without Gumbel noise.</li> <li>The node exposes <code>selection_weights</code> so that repulsion penalties   (e.g., DistinctnessLoss) can be attached in the pipeline.</li> </ul> Source code in <code>cuvis_ai/node/concrete_selector.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int = 3,\n    tau_start: float = 10.0,\n    tau_end: float = 0.1,\n    max_epochs: int = 20,\n    use_hard_inference: bool = True,\n    eps: float = 1e-6,\n    **kwargs: Any,\n) -&gt; None:\n    self.input_channels = int(input_channels)\n    self.output_channels = int(output_channels)\n    self.tau_start = float(tau_start)\n    self.tau_end = float(tau_end)\n    self.max_epochs = int(max_epochs)\n    self.use_hard_inference = bool(use_hard_inference)\n    self.eps = float(eps)\n\n    if self.output_channels &lt;= 0:\n        raise ValueError(f\"output_channels must be positive, got {output_channels}\")\n    if self.input_channels &lt;= 0:\n        raise ValueError(f\"input_channels must be positive, got {input_channels}\")\n    if self.tau_start &lt;= 0.0 or self.tau_end &lt;= 0.0:\n        raise ValueError(\"tau_start and tau_end must be positive.\")\n\n    super().__init__(\n        input_channels=self.input_channels,\n        output_channels=self.output_channels,\n        tau_start=self.tau_start,\n        tau_end=self.tau_end,\n        max_epochs=self.max_epochs,\n        use_hard_inference=self.use_hard_inference,\n        eps=self.eps,\n        **kwargs,\n    )\n\n    # Learnable logits for Categorical over input channels: [C_out, C_in]\n    self.logits = nn.Parameter(torch.zeros(self.output_channels, self.input_channels))\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.concrete_selector.ConcreteBandSelector.get_selection_weights","title":"get_selection_weights","text":"<pre><code>get_selection_weights(deterministic=True)\n</code></pre> <p>Return current selection weights without data dependency.</p> <p>Parameters:</p> Name Type Description Default <code>deterministic</code> <code>bool</code> <p>If True, uses softmax over logits (no Gumbel noise) at a \"midpoint\" temperature (geometric mean of start/end). If False, uses current logits with <code>tau_end</code>.</p> <code>True</code> Source code in <code>cuvis_ai/node/concrete_selector.py</code> <pre><code>def get_selection_weights(self, deterministic: bool = True) -&gt; Tensor:\n    \"\"\"Return current selection weights without data dependency.\n\n    Parameters\n    ----------\n    deterministic : bool, optional\n        If True, uses softmax over logits (no Gumbel noise) at a\n        \"midpoint\" temperature (geometric mean of start/end). If False,\n        uses current logits with ``tau_end``.\n    \"\"\"\n    if deterministic:\n        tau = math.sqrt(self.tau_start * self.tau_end)\n    else:\n        tau = self.tau_end\n\n    return F.softmax(self.logits / tau, dim=-1)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.concrete_selector.ConcreteBandSelector.get_selected_bands","title":"get_selected_bands","text":"<pre><code>get_selected_bands()\n</code></pre> <p>Return argmax band indices per output channel.</p> Source code in <code>cuvis_ai/node/concrete_selector.py</code> <pre><code>def get_selected_bands(self) -&gt; Tensor:\n    \"\"\"Return argmax band indices per output channel.\"\"\"\n    with torch.no_grad():\n        return torch.argmax(self.logits, dim=-1)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.concrete_selector.ConcreteBandSelector.forward","title":"forward","text":"<pre><code>forward(data, context=None, **_)\n</code></pre> <p>Apply Concrete/Gumbel-Softmax band selection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor [B, H, W, C_in] in BHWC format.</p> required <code>context</code> <code>Context</code> <p>Execution context with stage and epoch information.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with: - <code>\"rgb\"</code>: [B, H, W, C_out] RGB-like image. - <code>\"selection_weights\"</code>: [C_out, C_in] current weights.</p> Source code in <code>cuvis_ai/node/concrete_selector.py</code> <pre><code>def forward(\n    self,\n    data: Tensor,\n    context: Context | None = None,\n    **_: Any,\n) -&gt; dict[str, Tensor]:\n    \"\"\"Apply Concrete/Gumbel-Softmax band selection.\n\n    Parameters\n    ----------\n    data : Tensor\n        Input tensor [B, H, W, C_in] in BHWC format.\n    context : Context, optional\n        Execution context with stage and epoch information.\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with:\n        - ``\"rgb\"``: [B, H, W, C_out] RGB-like image.\n        - ``\"selection_weights\"``: [C_out, C_in] current weights.\n    \"\"\"\n    B, H, W, C_in = data.shape\n\n    tau = self._current_tau(context)\n    device = data.device\n\n    if self.training and context is not None and context.stage == ExecutionStage.TRAIN:\n        # Gumbel-Softmax sampling during training\n        g = _sample_gumbel(self.logits.shape, device=device, eps=self.eps)\n        weights = F.softmax((self.logits + g) / tau, dim=-1)  # [C_out, C_in]\n    else:\n        # Deterministic selection for val/test/inference\n        if self.use_hard_inference:\n            # Hard argmax \u2192 one-hot\n            indices = torch.argmax(self.logits, dim=-1)  # [C_out]\n            weights = torch.zeros_like(self.logits)\n            weights.scatter_(1, indices.unsqueeze(-1), 1.0)\n        else:\n            # Softmax over logits at low temperature\n            weights = F.softmax(self.logits / self.tau_end, dim=-1)\n\n    # Weighted sum over spectral dimension: [B, H, W, C_in] x [C_out, C_in] -&gt; [B, H, W, C_out]\n    rgb = torch.einsum(\"bhwc,kc-&gt;bhwk\", data, weights)\n\n    return {\n        \"rgb\": rgb,\n        \"selection_weights\": weights,\n    }\n</code></pre>"},{"location":"api/nodes/#channel-selector","title":"Channel Selector","text":""},{"location":"api/nodes/#cuvis_ai.node.selector","title":"selector","text":"<p>Soft channel selector node for learnable channel selection.</p>"},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector","title":"SoftChannelSelector","text":"<pre><code>SoftChannelSelector(\n    n_select,\n    input_channels,\n    init_method=\"uniform\",\n    temperature_init=5.0,\n    temperature_min=0.1,\n    temperature_decay=0.9,\n    hard=False,\n    eps=1e-06,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Soft channel selector with temperature-based Gumbel-Softmax selection.</p> <p>This node learns to select a subset of input channels using differentiable channel selection with temperature annealing. Supports: - Statistical initialization (uniform or importance-based) - Gradient-based optimization with temperature scheduling - Entropy and diversity regularization - Hard selection at inference time</p> <p>Parameters:</p> Name Type Description Default <code>n_select</code> <code>int</code> <p>Number of channels to select</p> required <code>input_channels</code> <code>int</code> <p>Number of input channels</p> required <code>init_method</code> <code>('uniform', 'variance')</code> <p>Initialization method for channel weights (default: \"uniform\")</p> <code>\"uniform\"</code> <code>temperature_init</code> <code>float</code> <p>Initial temperature for Gumbel-Softmax (default: 5.0)</p> <code>5.0</code> <code>temperature_min</code> <code>float</code> <p>Minimum temperature (default: 0.1)</p> <code>0.1</code> <code>temperature_decay</code> <code>float</code> <p>Temperature decay factor per epoch (default: 0.9)</p> <code>0.9</code> <code>hard</code> <code>bool</code> <p>If True, use hard selection at inference (default: False)</p> <code>False</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6)</p> <code>1e-06</code> <p>Attributes:</p> Name Type Description <code>channel_logits</code> <code>Parameter or Tensor</code> <p>Unnormalized channel importance scores [n_channels]</p> <code>temperature</code> <code>float</code> <p>Current temperature for Gumbel-Softmax</p> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def __init__(\n    self,\n    n_select: int,\n    input_channels: int,\n    init_method: Literal[\"uniform\", \"variance\"] = \"uniform\",\n    temperature_init: float = 5.0,\n    temperature_min: float = 0.1,\n    temperature_decay: float = 0.9,\n    hard: bool = False,\n    eps: float = 1e-6,\n    **kwargs,\n) -&gt; None:\n    self.n_select = n_select\n    self.input_channels = input_channels\n    self.init_method = init_method\n    self.temperature_init = temperature_init\n    self.temperature_min = temperature_min\n    self.temperature_decay = temperature_decay\n    self.hard = hard\n    self.eps = eps\n\n    super().__init__(\n        n_select=n_select,\n        input_channels=input_channels,\n        init_method=init_method,\n        temperature_init=temperature_init,\n        temperature_min=temperature_min,\n        temperature_decay=temperature_decay,\n        hard=hard,\n        eps=eps,\n        **kwargs,\n    )\n\n    # Temperature tracking (not a parameter, managed externally)\n    self.temperature = temperature_init\n    self._n_channels = input_channels\n\n    # Validate selection size\n    if self.n_select &gt; self._n_channels:\n        raise ValueError(\n            f\"Cannot select {self.n_select} channels from {self._n_channels} available channels\"  # nosec B608\n        )\n\n    # Initialize channel logits based on method - always as buffer\n    if self.init_method == \"uniform\":\n        # Uniform initialization\n        logits = torch.zeros(self._n_channels)\n    elif self.init_method == \"variance\":\n        # Random initialization - will be refined with fit if called\n        logits = torch.randn(self._n_channels) * 0.01\n    else:\n        raise ValueError(f\"Unknown init_method: {self.init_method}\")\n\n    # Store as buffer initially\n    self.register_buffer(\"channel_logits\", logits)\n\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize channel selection weights from data.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Iterator yielding dicts matching INPUT_SPECS (port-based format) Expected format: {\"data\": tensor} where tensor is BHWC</p> required Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize channel selection weights from data.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Iterator yielding dicts matching INPUT_SPECS (port-based format)\n        Expected format: {\"data\": tensor} where tensor is BHWC\n    \"\"\"\n    # Collect statistics from first batch to determine n_channels\n    first_batch = next(iter(input_stream))\n    x = first_batch[\"data\"]\n\n    if x is None:\n        raise ValueError(\"No data provided for selector initialization\")\n\n    self._n_channels = x.shape[-1]\n\n    if self.n_select &gt; self._n_channels:\n        raise ValueError(\n            f\"Cannot select {self.n_select} channels from {self._n_channels} available channels\"  # nosec B608\n        )\n\n    # Initialize channel logits based on method\n    if self.init_method == \"uniform\":\n        # Uniform initialization\n        logits = torch.zeros(self._n_channels)\n    elif self.init_method == \"variance\":\n        # Importance-based initialization using channel variance\n        all_data = []\n        all_data.append(x)\n\n        # Collect more data for better statistics\n        for batch_data in input_stream:\n            x_batch = batch_data[\"data\"]\n            if x_batch is not None:\n                all_data.append(x_batch)\n\n        # Concatenate and compute variance per channel\n        X = torch.cat(all_data, dim=0)  # [B, H, W, C]\n        X_flat = X.reshape(-1, X.shape[-1])  # [B*H*W, C]\n\n        # Compute variance for each channel\n        variance = X_flat.var(dim=0)  # [C]\n\n        # Use log variance as initial logits (high variance = high importance)\n        logits = torch.log(variance + self.eps)\n    else:\n        raise ValueError(f\"Unknown init_method: {self.init_method}\")\n\n    # Store as buffer\n    self.channel_logits.data[:] = logits.clone()\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector.unfreeze","title":"unfreeze","text":"<pre><code>unfreeze()\n</code></pre> <p>Convert channel logits buffer to trainable nn.Parameter.</p> <p>Call this method to enable gradient-based optimization of channel selection weights. The logits will be converted from a buffer to an nn.Parameter, allowing gradient updates during training.</p> Example <p>selector = SoftChannelSelector(n_select=10, input_channels=100) selector.unfreeze()  # Enable gradient training</p> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Convert channel logits buffer to trainable nn.Parameter.\n\n    Call this method to enable gradient-based optimization of channel\n    selection weights. The logits will be converted from a buffer to an\n    nn.Parameter, allowing gradient updates during training.\n\n    Example\n    -------\n    &gt;&gt;&gt; selector = SoftChannelSelector(n_select=10, input_channels=100)\n    &gt;&gt;&gt; selector.unfreeze()  # Enable gradient training\n    &gt;&gt;&gt; # Now channel selection weights can be optimized\n    \"\"\"\n    if self.channel_logits is not None:\n        # Convert buffer to parameter\n        self.channel_logits = nn.Parameter(self.channel_logits.clone())\n    # Call parent to enable requires_grad\n    super().unfreeze()\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector.unfreeze--now-channel-selection-weights-can-be-optimized","title":"Now channel selection weights can be optimized","text":""},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector.update_temperature","title":"update_temperature","text":"<pre><code>update_temperature(epoch=None, step=None)\n</code></pre> <p>Update temperature with decay schedule.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current epoch number (used for per-epoch decay)</p> <code>None</code> <code>step</code> <code>int</code> <p>Current training step (for more granular control)</p> <code>None</code> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def update_temperature(self, epoch: int | None = None, step: int | None = None) -&gt; None:\n    \"\"\"Update temperature with decay schedule.\n\n    Parameters\n    ----------\n    epoch : int, optional\n        Current epoch number (used for per-epoch decay)\n    step : int, optional\n        Current training step (for more granular control)\n    \"\"\"\n    if epoch is not None:\n        # Exponential decay per epoch\n        self.temperature = max(\n            self.temperature_min, self.temperature_init * (self.temperature_decay**epoch)\n        )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector.get_selection_weights","title":"get_selection_weights","text":"<pre><code>get_selection_weights(hard=None)\n</code></pre> <p>Get current channel selection weights.</p> <p>Parameters:</p> Name Type Description Default <code>hard</code> <code>bool</code> <p>If True, use hard selection (top-k). If None, uses self.hard.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Selection weights [n_channels] summing to n_select</p> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def get_selection_weights(self, hard: bool | None = None) -&gt; Tensor:\n    \"\"\"Get current channel selection weights.\n\n    Parameters\n    ----------\n    hard : bool, optional\n        If True, use hard selection (top-k). If None, uses self.hard.\n\n    Returns\n    -------\n    Tensor\n        Selection weights [n_channels] summing to n_select\n    \"\"\"\n    if hard is None:\n        hard = self.hard and not self.training\n\n    if hard:\n        # Hard selection: top-k channels\n        _, top_indices = torch.topk(self.channel_logits, self.n_select)\n        weights = torch.zeros_like(self.channel_logits)\n        weights[top_indices] = 1.0\n    else:\n        # Soft selection with Gumbel-Softmax\n        # First, compute selection probabilities\n        probs = F.softmax(self.channel_logits / self.temperature, dim=-1)\n\n        # Scale to sum to n_select instead of 1\n        weights = probs * self.n_select\n\n    return weights\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.SoftChannelSelector.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Apply soft channel selection to input.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor [B, H, W, C]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"selected\" key containing reweighted channels and optional \"weights\" key containing selection weights</p> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def forward(self, data: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Apply soft channel selection to input.\n\n    Parameters\n    ----------\n    data : Tensor\n        Input tensor [B, H, W, C]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"selected\" key containing reweighted channels\n        and optional \"weights\" key containing selection weights\n    \"\"\"\n    # Get selection weights\n    weights = self.get_selection_weights()\n\n    # Ensure weights are on same device\n    # weights = weights.to(data.device) # no need\n\n    # Apply channel-wise weighting: [B, H, W, C] * [C]\n    selected = data * weights.view(1, 1, 1, -1)\n\n    # Prepare output dictionary - weights always exposed for loss/metric nodes\n    outputs = {\"selected\": selected, \"weights\": weights}\n\n    return outputs\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.TopKIndices","title":"TopKIndices","text":"<pre><code>TopKIndices(k, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Utility node that surfaces the top-k channel indices from selector weights.</p> <p>This node extracts the indices of the top-k weighted channels from a selector's weight vector. Useful for introspection and reporting which channels were selected.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of top indices to return</p> required <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>Number of top indices to return</p> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def __init__(self, k: int, **kwargs: Any) -&gt; None:\n    self.k = int(k)\n\n    # Extract Node base parameters from kwargs to avoid duplication\n    name = kwargs.pop(\"name\", None)\n    execution_stages = kwargs.pop(\"execution_stages\", None)\n\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        k=self.k,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.selector.TopKIndices.forward","title":"forward","text":"<pre><code>forward(weights, **_)\n</code></pre> <p>Return the indices of the top-k weighted channels.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Tensor</code> <p>Channel selection weights [n_channels]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"indices\" key containing top-k indices</p> Source code in <code>cuvis_ai/node/selector.py</code> <pre><code>def forward(self, weights: torch.Tensor, **_: Any) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Return the indices of the top-k weighted channels.\n\n    Parameters\n    ----------\n    weights : torch.Tensor\n        Channel selection weights [n_channels]\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"indices\" key containing top-k indices\n    \"\"\"\n    top_k = min(self.k, weights.shape[-1]) if weights.numel() else 0\n    if top_k == 0:\n        return {\"indices\": torch.zeros(0, dtype=torch.int64, device=weights.device)}\n\n    _, indices = torch.topk(weights, top_k)\n    return {\"indices\": indices}\n</code></pre>"},{"location":"api/nodes/#deep-learning-nodes","title":"Deep Learning Nodes","text":"<p>Nodes implementing deep learning components.</p>"},{"location":"api/nodes/#adaclip","title":"AdaCLIP","text":""},{"location":"api/nodes/#cuvis_ai.node.adaclip","title":"adaclip","text":"<p>AdaCLIP Anomaly Detection Nodes.</p> <p>This module provides nodes for zero-shot anomaly detection using the AdaCLIP (Adaptive CLIP) model. Two implementations are available:</p> <ul> <li>AdaCLIPLocalNode: Loads and runs the CLIP vision model locally for inference</li> <li>AdaCLIPAPINode: Calls the AdaCLIP HuggingFace Space API for inference</li> </ul> <p>AdaCLIP uses CLIP's vision features to detect anomalies based on text prompts, enabling zero-shot anomaly detection without training data.</p> See Also <p>cuvis_ai_core.node.huggingface : Base classes for HuggingFace model nodes</p>"},{"location":"api/nodes/#cuvis_ai.node.adaclip.AdaCLIPLocalNode","title":"AdaCLIPLocalNode","text":"<pre><code>AdaCLIPLocalNode(\n    model_name=\"AdaCLIP\",\n    cache_dir=None,\n    text_prompt=\"normal: lentils, anomaly: stones\",\n    revision=None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>HuggingFaceLocalNode</code></p> <p>AdaCLIP anomaly detection with local HF loading.</p> Source code in <code>cuvis_ai/node/adaclip.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"AdaCLIP\",\n    cache_dir: str | None = None,\n    text_prompt: str = \"normal: lentils, anomaly: stones\",\n    revision: str | None = None,\n    **kwargs,\n) -&gt; None:\n    self.text_prompt = text_prompt\n    self.revision = revision\n\n    super().__init__(\n        model_name=model_name,\n        cache_dir=cache_dir,\n        text_prompt=text_prompt,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.adaclip.AdaCLIPLocalNode.forward","title":"forward","text":"<pre><code>forward(image, text_prompt=None, context=None, **kwargs)\n</code></pre> <p>Run AdaCLIP anomaly detection with local CLIP model.</p> <p>Processes images through CLIP vision encoder and generates anomaly scores based on feature norms. Supports gradient passthrough for training pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>RGB image [B, H, W, 3] in range [0, 1] or [0, 255].</p> required <code>text_prompt</code> <code>str</code> <p>Text description for anomaly detection. If None, uses self.text_prompt. Note: Current implementation uses feature norms; text prompts will be integrated in future versions.</p> <code>None</code> <code>context</code> <code>Any</code> <p>Pipeline execution context (unused, for compatibility).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary containing: - \"anomaly_mask\" : Tensor [B, 1, 1, 1] - Binary anomaly predictions - \"anomaly_scores\" : Tensor [B, 1, 1, 1] - Normalized anomaly scores [0, 1]</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If CLIP inference fails or model is not properly loaded.</p> Source code in <code>cuvis_ai/node/adaclip.py</code> <pre><code>def forward(\n    self,\n    image: Tensor,\n    text_prompt: str | None = None,\n    context: Any | None = None,  # context captured for pipeline compatibility\n    **kwargs: Any,\n) -&gt; dict[str, Tensor]:\n    \"\"\"Run AdaCLIP anomaly detection with local CLIP model.\n\n    Processes images through CLIP vision encoder and generates anomaly\n    scores based on feature norms. Supports gradient passthrough for\n    training pipelines.\n\n    Parameters\n    ----------\n    image : Tensor\n        RGB image [B, H, W, 3] in range [0, 1] or [0, 255].\n    text_prompt : str, optional\n        Text description for anomaly detection. If None, uses self.text_prompt.\n        Note: Current implementation uses feature norms; text prompts will be\n        integrated in future versions.\n    context : Any, optional\n        Pipeline execution context (unused, for compatibility).\n    **kwargs : Any\n        Additional keyword arguments (unused).\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary containing:\n        - \"anomaly_mask\" : Tensor [B, 1, 1, 1] - Binary anomaly predictions\n        - \"anomaly_scores\" : Tensor [B, 1, 1, 1] - Normalized anomaly scores [0, 1]\n\n    Raises\n    ------\n    RuntimeError\n        If CLIP inference fails or model is not properly loaded.\n    \"\"\"\n    # Use instance variable if text_prompt not provided\n    if text_prompt is None:\n        text_prompt = self.text_prompt\n\n    image_processed = self._preprocess_image(image)\n\n    # Ensure model is on the same device as input\n    # Get model's current device\n    model_device = next(self.model.parameters()).device\n    if image_processed.device != model_device:\n        # Move model to input device (more efficient than moving data back)\n        self.model.to(image_processed.device)\n\n    try:\n        # CLIP models expect pixel_values as keyword argument\n        # Keep gradients enabled for gradient passthrough\n        outputs = self.model(pixel_values=image_processed)\n\n        # CLIPVisionModel returns BaseModelOutputWithPooling with:\n        # - pooler_output: [B, 768] global image features\n        # - last_hidden_state: [B, 50, 768] patch-level features\n        scores = outputs.pooler_output  # [B, 768]\n\n        # Reshape global features to spatial format [B, 1, 1, 1]\n        # Use feature norm as anomaly score\n        batch_size = scores.shape[0]\n        scores = scores.norm(dim=-1, keepdim=True)  # [B, 1]\n        scores = scores.view(batch_size, 1, 1, 1)  # [B, 1, 1, 1]\n\n        # Normalize scores to [0, 1] range for interpretability\n        # Use min-max normalization to preserve gradients better\n        scores_min = scores.min()\n        scores_max = scores.max()\n        if scores_max &gt; scores_min:\n            scores = (scores - scores_min) / (scores_max - scores_min + 1e-8)\n        else:\n            # All scores are the same - normalize to 0.5\n            scores = torch.ones_like(scores) * 0.5\n\n        # Create binary anomaly mask (threshold at 0.5)\n        anomaly_mask = scores &gt; 0.5\n\n        return {\n            \"anomaly_mask\": anomaly_mask,\n            \"anomaly_scores\": scores,\n        }\n    except Exception as exc:  # pragma: no cover - defensive path\n        logger.error(f\"Local AdaCLIP inference failed: {exc}\")\n        raise RuntimeError(\n            f\"AdaCLIP local inference failed: {exc}\\n\"\n            f\"Model: {self.model_name}\\n\"\n            f\"Input shape: {image.shape}\\n\"\n            f\"Text prompt: {text_prompt}\"\n        ) from exc\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.adaclip.AdaCLIPAPINode","title":"AdaCLIPAPINode","text":"<pre><code>AdaCLIPAPINode(\n    space_url=\"Caoyunkang/AdaCLIP\",\n    dataset_option=\"All\",\n    text_prompt=\"normal: lentils, anomaly: stones\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>HuggingFaceAPINode</code></p> <p>AdaCLIP anomaly detection via HuggingFace Spaces API.</p> <p>This node calls the AdaCLIP Space for zero-shot anomaly detection. API backend is non-differentiable and suitable for inference only.</p> <p>Parameters:</p> Name Type Description Default <code>space_url</code> <code>str</code> <p>AdaCLIP Space URL (default: \"Caoyunkang/AdaCLIP\")</p> <code>'Caoyunkang/AdaCLIP'</code> <code>dataset_option</code> <code>str</code> <p>Dataset selection option (default: \"All\")</p> <code>'All'</code> <code>text_prompt</code> <code>str</code> <p>Text prompt for anomaly detection (default: \"normal: lentils, anomaly: stones\")</p> <code>'normal: lentils, anomaly: stones'</code> <code>**kwargs</code> <p>Additional arguments passed to HuggingFaceAPINode</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create node\n&gt;&gt;&gt; adaclip = AdaCLIPAPINode()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run inference\n&gt;&gt;&gt; rgb_image = torch.rand(1, 224, 224, 3)  # BHWC format\n&gt;&gt;&gt; result = adaclip.forward(image=rgb_image)\n&gt;&gt;&gt; anomaly_mask = result[\"anomaly_mask\"]  # [B, H, W, 1]\n</code></pre> Source code in <code>cuvis_ai/node/adaclip.py</code> <pre><code>def __init__(\n    self,\n    space_url: str = \"Caoyunkang/AdaCLIP\",\n    dataset_option: str = \"All\",\n    text_prompt: str = \"normal: lentils, anomaly: stones\",\n    **kwargs,\n) -&gt; None:\n    self.dataset_option = dataset_option\n    self.text_prompt = text_prompt\n\n    super().__init__(\n        space_url=space_url,\n        dataset_option=dataset_option,\n        text_prompt=text_prompt,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.adaclip.AdaCLIPAPINode.forward","title":"forward","text":"<pre><code>forward(\n    image, text_prompt=None, dataset_option=None, **kwargs\n)\n</code></pre> <p>Run AdaCLIP anomaly detection via API.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>RGB image [B, H, W, 3] in BHWC format</p> required <code>text_prompt</code> <code>str</code> <p>Text description of anomaly to detect. If None, uses self.text_prompt.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"anomaly_mask\" and optionally \"anomaly_scores\"</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If API call fails</p> <code>ValueError</code> <p>If image format is invalid</p> Source code in <code>cuvis_ai/node/adaclip.py</code> <pre><code>def forward(\n    self,\n    image: Tensor,\n    text_prompt: str | None = None,\n    dataset_option: str | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Tensor]:\n    \"\"\"Run AdaCLIP anomaly detection via API.\n\n    Parameters\n    ----------\n    image : Tensor\n        RGB image [B, H, W, 3] in BHWC format\n    text_prompt : str, optional\n        Text description of anomaly to detect. If None, uses self.text_prompt.\n    **kwargs\n        Additional arguments (unused)\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"anomaly_mask\" and optionally \"anomaly_scores\"\n\n    Raises\n    ------\n    RuntimeError\n        If API call fails\n    ValueError\n        If image format is invalid\n    \"\"\"\n\n    # Use instance variable if text_prompt not provided\n    if text_prompt is None:\n        text_prompt = self.text_prompt\n\n    # Process each image in batch\n    batch_size = image.shape[0]\n    masks = []\n\n    for i in range(batch_size):\n        img = image[i]  # [H, W, 3]\n\n        img_np = img.detach().cpu().numpy()\n\n        # Normalize to [0, 255] if in [0, 1]\n        if img_np.max() &lt;= 1.0:\n            img_np = (img_np * 255).astype(np.uint8)\n        else:\n            img_np = img_np.astype(np.uint8)\n\n        # Convert to PIL Image\n        pil_img = Image.fromarray(img_np)\n\n        try:\n            # Call API\n            logger.debug(f\"Calling AdaCLIP API for image {i + 1}/{batch_size}\")\n            with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp_img:\n                pil_img.save(tmp_img.name)\n                tmp_path = tmp_img.name\n\n            try:\n                result = self.client.predict(\n                    handle_file(tmp_path),\n                    text_prompt,\n                    dataset_option,\n                    api_name=\"/predict\",\n                )\n            finally:\n                try:\n                    os.remove(tmp_path)\n                except OSError:\n                    logger.warning(f\"Failed to remove temp image: {tmp_path}\")\n\n            # Parse result\n            # Note: Actual return format depends on AdaCLIP Space implementation\n            # The Space currently returns (output_image_path, anomaly_score_str)\n            if isinstance(result, np.ndarray):\n                mask_np = result\n            elif isinstance(result, (list, tuple)):\n                first = result[0]\n                if isinstance(first, np.ndarray):\n                    mask_np = first\n                elif isinstance(first, str):\n                    # Gradio returns a temporary file path \u2013 load and convert to array\n                    from PIL import Image as PILImage\n\n                    mask_np = np.array(PILImage.open(first))\n                else:\n                    raise ValueError(\n                        f\"Unexpected first element type in result tuple: {type(first)}\"\n                    )\n            else:\n                raise ValueError(f\"Unexpected API result type: {type(result)}\")\n\n            mask = torch.from_numpy(mask_np)\n\n            # Ensure correct shape [H, W, 1]\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(-1)\n            elif mask.dim() == 3 and mask.shape[-1] == 3:\n                # Convert RGB mask to single channel\n                mask = mask.float().mean(dim=-1, keepdim=True)\n\n            # Resize to original spatial resolution if needed\n            orig_h, orig_w = img.shape[0], img.shape[1]\n            if mask.shape[0] != orig_h or mask.shape[1] != orig_w:\n                # interpolate expects NCHW\n                mask = mask.permute(2, 0, 1).unsqueeze(0).float()\n                mask = torch.nn.functional.interpolate(\n                    mask,\n                    size=(orig_h, orig_w),\n                    mode=\"bilinear\",\n                    align_corners=False,\n                )\n                mask = mask.squeeze(0).permute(1, 2, 0)\n\n            # Convert to binary mask\n            if mask.dtype != torch.bool:\n                mask = mask &gt; 0\n\n            masks.append(mask)\n\n        except Exception as e:\n            logger.error(f\"API call failed for image {i + 1}/{batch_size}: {e}\")\n            raise RuntimeError(\n                f\"AdaCLIP API call failed: {e}\\n\"\n                f\"Space: {self.space_url}\\n\"\n                f\"Text prompt: {text_prompt}\"\n            ) from e\n\n    # Stack batch\n    anomaly_mask = torch.stack(masks, dim=0)  # [B, H, W, 1]\n\n    return {\n        \"anomaly_mask\": anomaly_mask,\n    }\n</code></pre>"},{"location":"api/nodes/#analysis-dimensionality-reduction","title":"Analysis &amp; Dimensionality Reduction","text":"<p>Nodes for dimensionality reduction and feature extraction.</p>"},{"location":"api/nodes/#pca","title":"PCA","text":""},{"location":"api/nodes/#cuvis_ai.node.pca","title":"pca","text":"<p>Trainable PCA node for dimensionality reduction with gradient-based optimization.</p>"},{"location":"api/nodes/#cuvis_ai.node.pca.TrainablePCA","title":"TrainablePCA","text":"<pre><code>TrainablePCA(\n    num_channels,\n    n_components,\n    whiten=False,\n    init_method=\"svd\",\n    eps=1e-06,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Trainable PCA node with orthogonality regularization.</p> <p>This node performs Principal Component Analysis (PCA) for dimensionality reduction and can be trained end-to-end with gradient descent. It supports: - Statistical initialization from data - Gradient-based fine-tuning with orthogonality constraints - Explained variance tracking</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of principal components to retain</p> required <code>whiten</code> <code>bool</code> <p>If True, scale components by explained variance (default: False)</p> <code>False</code> <code>init_method</code> <code>('svd', 'random')</code> <p>Initialization method for components (default: \"svd\")</p> <code>\"svd\"</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6)</p> <code>1e-06</code> <p>Attributes:</p> Name Type Description <code>components</code> <code>Parameter or Tensor</code> <p>Principal components matrix [n_components, n_features]</p> <code>mean</code> <code>Tensor</code> <p>Feature-wise mean [n_features]</p> <code>explained_variance</code> <code>Tensor</code> <p>Variance explained by each component [n_components]</p> Source code in <code>cuvis_ai/node/pca.py</code> <pre><code>def __init__(\n    self,\n    num_channels: int,\n    n_components: int,\n    whiten: bool = False,\n    init_method: Literal[\"svd\", \"random\"] = \"svd\",\n    eps: float = 1e-6,\n    **kwargs,\n) -&gt; None:\n    self.n_components = n_components\n    self.whiten = whiten\n    self.init_method = init_method\n    self.eps = eps\n\n    super().__init__(\n        num_channels=num_channels,\n        n_components=n_components,\n        whiten=whiten,\n        init_method=init_method,\n        eps=eps,\n        **kwargs,\n    )\n\n    # Buffers for statistical initialization (private to avoid conflicts with output ports)\n    self.register_buffer(\"_mean\", torch.empty(num_channels))\n    self.register_buffer(\"_explained_variance\", torch.empty(n_components))\n    self.register_buffer(\"_components\", torch.empty(n_components, num_channels))\n\n    self._statistically_initialized = False\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.pca.TrainablePCA.statistical_initialization","title":"statistical_initialization","text":"<pre><code>statistical_initialization(input_stream)\n</code></pre> <p>Initialize PCA components from data using SVD.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>InputStream</code> <p>Input stream yielding dicts matching INPUT_SPECS (port-based format) Expected format: {\"data\": tensor} where tensor is BHWC</p> required Source code in <code>cuvis_ai/node/pca.py</code> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Initialize PCA components from data using SVD.\n\n    Parameters\n    ----------\n    input_stream : InputStream\n        Input stream yielding dicts matching INPUT_SPECS (port-based format)\n        Expected format: {\"data\": tensor} where tensor is BHWC\n    \"\"\"\n    # Todo: this should not concatenate all data and then do SVD - this is not scalable.\n    # Iether do incremental PCA or use a subset of data.\n    # Collect all data\n    all_data = []\n    for batch_data in input_stream:\n        x = batch_data[\"data\"]\n        if x is not None:\n            # Flatten spatial dimensions: [B, H, W, C] -&gt; [B*H*W, C]\n            flat = x.reshape(-1, x.shape[-1])\n            all_data.append(flat)\n\n    if not all_data:\n        raise ValueError(\"No data provided for PCA initialization\")\n\n    # Concatenate all samples\n    X = torch.cat(all_data, dim=0)  # [N, C]\n\n    # Compute mean and center data\n    self._mean = X.mean(dim=0)  # [C]\n    X_centered = X - self._mean  # [N, C]\n\n    # Compute SVD\n    # X_centered = U @ S @ V.T where V contains principal components\n    U, S, Vt = torch.linalg.svd(X_centered, full_matrices=False)\n\n    # Extract top n_components and store as buffer\n    self._components = Vt[: self.n_components, :].clone()  # [n_components, C]\n\n    # Compute explained variance\n    # Variance = (S^2) / (N - 1)\n    variance = (S**2) / (X.shape[0] - 1)\n    self._explained_variance = variance[: self.n_components].clone()  # [n_components]\n\n    self._statistically_initialized = True\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.pca.TrainablePCA.unfreeze","title":"unfreeze","text":"<pre><code>unfreeze()\n</code></pre> <p>Convert components buffer to trainable nn.Parameter.</p> <p>Call this method after fit() to enable gradient-based training of the principal components. The components will be converted from a buffer to an nn.Parameter, allowing gradient updates during training.</p> Example <p>pca.statistical_initialization(input_stream)  # Statistical initialization pca.unfreeze()  # Enable gradient training</p> Source code in <code>cuvis_ai/node/pca.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Convert components buffer to trainable nn.Parameter.\n\n    Call this method after fit() to enable gradient-based training of the\n    principal components. The components will be converted from a buffer\n    to an nn.Parameter, allowing gradient updates during training.\n\n    Example\n    -------\n    &gt;&gt;&gt; pca.statistical_initialization(input_stream)  # Statistical initialization\n    &gt;&gt;&gt; pca.unfreeze()  # Enable gradient training\n    &gt;&gt;&gt; # Now PCA components can be fine-tuned with gradient descent\n    \"\"\"\n    if self._components.numel() &gt; 0:\n        # Convert buffer to parameter\n        self._components = nn.Parameter(self._components.clone())\n    # Call parent to enable requires_grad\n    super().unfreeze()  # could this have unintended side effects? like the graph be unfrozen?\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.pca.TrainablePCA.unfreeze--now-pca-components-can-be-fine-tuned-with-gradient-descent","title":"Now PCA components can be fine-tuned with gradient descent","text":""},{"location":"api/nodes/#cuvis_ai.node.pca.TrainablePCA.forward","title":"forward","text":"<pre><code>forward(data, **_)\n</code></pre> <p>Project data onto principal components.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor [B, H, W, C]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"projected\" key containing PCA-projected data</p> Source code in <code>cuvis_ai/node/pca.py</code> <pre><code>def forward(self, data: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Project data onto principal components.\n\n    Parameters\n    ----------\n    data : Tensor\n        Input tensor [B, H, W, C]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"projected\" key containing PCA-projected data\n    \"\"\"\n    if not self._statistically_initialized:\n        raise RuntimeError(\"PCA not initialized. Call statistical_initialization() first.\")\n\n    B, H, W, C = data.shape\n\n    # Flatten spatial dimensions\n    x_flat = data.reshape(-1, C)  # [B*H*W, C]\n\n    # Ensure mean is on the same device as input\n    mean = self._mean.to(data.device)\n\n    # Center data\n    x_centered = x_flat - mean  # [B*H*W, C]\n\n    # Ensure components are on the same device as input\n    components = (\n        self._components.to(data.device)\n        if isinstance(self._components, Tensor)\n        else self._components\n    )\n\n    # Project onto components: X_proj = X @ components.T\n    x_proj = x_centered @ components.T  # [B*H*W, n_components]\n\n    # Whiten if requested\n    if self.whiten:\n        # Ensure explained_variance is on the same device\n        explained_variance = self._explained_variance.to(data.device)\n        # Scale by 1/sqrt(explained_variance)\n        scale = 1.0 / torch.sqrt(explained_variance + self.eps)\n        x_proj = x_proj * scale\n\n    # Reshape back to spatial dimensions\n    x_proj = x_proj.reshape(B, H, W, self.n_components)\n\n    # Prepare output dictionary\n    outputs = {\"projected\": x_proj}\n\n    # Add optional outputs for loss/metric nodes\n    # Expose explained variance ratio\n    if self._explained_variance.numel() &gt; 0:\n        total_variance = self._explained_variance.sum()\n        variance_ratio = self._explained_variance / (total_variance + self.eps)\n        outputs[\"explained_variance_ratio\"] = variance_ratio.to(data.device)\n\n    # Expose components for loss/metric nodes\n    if self._components.numel() &gt; 0:\n        outputs[\"components\"] = self._components\n\n    return outputs\n</code></pre>"},{"location":"api/nodes/#visualization-nodes","title":"Visualization Nodes","text":"<p>Nodes for creating visualizations and TensorBoard logging.</p>"},{"location":"api/nodes/#visualizations","title":"Visualizations","text":""},{"location":"api/nodes/#cuvis_ai.node.visualizations","title":"visualizations","text":"<p>Visualization sink nodes for monitoring training progress (port-based architecture).</p>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.CubeRGBVisualizer","title":"CubeRGBVisualizer","text":"<pre><code>CubeRGBVisualizer(name=None, up_to=5)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Creates false-color RGB images from hyperspectral cube using channel weights.</p> <p>Selects 3 channels with highest weights for R, G, B channels and creates a false-color visualization with wavelength annotations.</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def __init__(self, name: str | None = None, up_to: int = 5) -&gt; None:\n    super().__init__(name=name, execution_stages={ExecutionStage.INFERENCE, ExecutionStage.VAL})\n    self.up_to = up_to\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.CubeRGBVisualizer.forward","title":"forward","text":"<pre><code>forward(cube, weights, wavelengths, context)\n</code></pre> <p>Generate false-color RGB visualizations from hyperspectral cube.</p> <p>Selects the 3 channels with highest weights and creates RGB images with wavelength annotations. Also generates a bar chart showing channel weights with the selected channels highlighted.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Hyperspectral cube [B, H, W, C].</p> required <code>weights</code> <code>Tensor</code> <p>Channel selection weights [C] indicating importance of each channel.</p> required <code>wavelengths</code> <code>Tensor</code> <p>Wavelengths for each channel [C] in nanometers.</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx information.</p> required <p>Returns:</p> Type Description <code>dict[str, list[Artifact]]</code> <p>Dictionary with \"artifacts\" key containing list of visualization artifacts.</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def forward(self, cube, weights, wavelengths, context) -&gt; dict[str, list[Artifact]]:\n    \"\"\"Generate false-color RGB visualizations from hyperspectral cube.\n\n    Selects the 3 channels with highest weights and creates RGB images\n    with wavelength annotations. Also generates a bar chart showing\n    channel weights with the selected channels highlighted.\n\n    Parameters\n    ----------\n    cube : Tensor\n        Hyperspectral cube [B, H, W, C].\n    weights : Tensor\n        Channel selection weights [C] indicating importance of each channel.\n    wavelengths : Tensor\n        Wavelengths for each channel [C] in nanometers.\n    context : Context\n        Execution context with stage, epoch, batch_idx information.\n\n    Returns\n    -------\n    dict[str, list[Artifact]]\n        Dictionary with \"artifacts\" key containing list of visualization artifacts.\n    \"\"\"\n    top3_indices = torch.topk(weights, k=3).indices.cpu().numpy()\n    top3_wavelengths = wavelengths[top3_indices]\n\n    batch_size = min(cube.shape[0], self.up_to)\n    artifacts = []\n\n    for b in range(batch_size):\n        rgb_channels = cube[b, :, :, top3_indices].cpu().numpy()\n\n        rgb_img = (rgb_channels - rgb_channels.min()) / (\n            rgb_channels.max() - rgb_channels.min() + 1e-8\n        )\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        ax1.imshow(rgb_img)\n        ax1.set_title(\n            f\"False RGB: R={top3_wavelengths[0]:.1f}nm, \"\n            f\"G={top3_wavelengths[1]:.1f}nm, B={top3_wavelengths[2]:.1f}nm\"\n        )\n        ax1.axis(\"off\")\n\n        ax2.bar(range(len(wavelengths)), weights.detach().cpu().numpy())\n        ax2.scatter(\n            top3_indices,\n            weights[top3_indices].detach().cpu().numpy(),\n            c=\"red\",\n            s=100,\n            zorder=3,\n        )\n        ax2.set_xlabel(\"Channel Index\")\n        ax2.set_ylabel(\"Weight\")\n        ax2.set_title(\"Channel Selection Weights\")\n        ax2.grid(True, alpha=0.3)\n\n        for idx in top3_indices:\n            ax2.annotate(\n                f\"{wavelengths[idx]:.0f}nm\",\n                xy=(idx, weights[idx].item()),\n                xytext=(0, 10),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                fontsize=8,\n            )\n\n        plt.tight_layout()\n\n        img_array = fig_to_array(fig, dpi=150)\n\n        artifact = Artifact(\n            name=f\"viz_rgb_sample_{b}\",\n            value=img_array,\n            el_id=b,\n            desc=f\"False RGB visualization for sample {b}\",\n            type=ArtifactType.IMAGE,\n        )\n        artifacts.append(artifact)\n        plt.close(fig)\n\n    return {\"artifacts\": artifacts}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.PCAVisualization","title":"PCAVisualization","text":"<pre><code>PCAVisualization(up_to=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Visualize PCA-projected data with scatter and image plots.</p> <p>Creates visualizations for each batch element showing: 1. Scatter plot of H*W points in 2D PC space (using first 2 PCs) 2. Image representation of the 2D projection reshaped to [H, W, 2]</p> <p>Points in scatter plot are colored by spatial position. Returns artifacts for monitoring systems.</p> <p>Executes only during validation stage.</p> <p>Parameters:</p> Name Type Description Default <code>up_to</code> <code>int</code> <p>Maximum number of batch elements to visualize. If None, visualizes all (default: None)</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pca_viz = PCAVisualization(up_to=10)\n&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; graph.connect(\n...     (pca.projected, pca_viz.data),\n...     (pca_viz.artifacts, tensorboard_node.artifacts),\n... )\n</code></pre> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def __init__(self, up_to: int | None = None, **kwargs) -&gt; None:\n    self.up_to = up_to\n\n    super().__init__(execution_stages={ExecutionStage.VAL}, up_to=up_to, **kwargs)\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.PCAVisualization.forward","title":"forward","text":"<pre><code>forward(data, context)\n</code></pre> <p>Create PCA projection visualizations as Artifact objects.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>PCA-projected data tensor [B, H, W, C] (uses first 2 components)</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with \"artifacts\" key containing list of Artifact objects</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def forward(self, data: torch.Tensor, context: Context) -&gt; dict:\n    \"\"\"Create PCA projection visualizations as Artifact objects.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        PCA-projected data tensor [B, H, W, C] (uses first 2 components)\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict\n        Dictionary with \"artifacts\" key containing list of Artifact objects\n    \"\"\"\n    # Convert to numpy\n    data_np = data.detach().cpu().numpy()\n\n    # Handle input shape: [B, H, W, C]\n    if data_np.ndim != 4:\n        raise ValueError(f\"Expected 4D input [B, H, W, C], got shape: {data_np.shape}\")\n\n    B, H, W, C = data_np.shape\n\n    if C &lt; 2:\n        raise ValueError(f\"Expected at least 2 components, got {C}\")\n\n    # Extract context information\n    stage = context.stage.value\n    epoch = context.epoch\n    batch_idx = context.batch_idx\n\n    # Determine how many images to visualize from this batch\n    up_to_batch = B if self.up_to is None else min(B, self.up_to)\n\n    # List to collect artifacts\n    artifacts = []\n\n    # Loop through each batch element\n    for i in range(up_to_batch):\n        # Get projection for this batch element: [H, W, C]\n        projection = data_np[i]\n\n        # Use only first 2 components\n        projection_2d = projection[:, :, :2]  # [H, W, 2]\n\n        # Flatten spatial dimensions for scatter plot\n        projection_flat = projection_2d.reshape(-1, 2)  # [H*W, 2]\n\n        # Create spatial position colors using 2D HSV encoding\n        # x-coordinate maps to Hue (0-1)\n        # y-coordinate maps to Saturation (0-1)\n        # Value is constant at 1.0 for brightness\n        y_coords, x_coords = np.meshgrid(np.arange(H), np.arange(W), indexing=\"ij\")\n\n        # Normalize coordinates to [0, 1]\n        x_norm = x_coords / (W - 1) if W &gt; 1 else np.zeros_like(x_coords)\n        y_norm = y_coords / (H - 1) if H &gt; 1 else np.zeros_like(y_coords)\n\n        # Create HSV colors: H from x, S from y, V constant\n        hsv_colors = np.stack(\n            [\n                x_norm.flatten(),  # Hue from x-coordinate\n                y_norm.flatten(),  # Saturation from y-coordinate\n                np.ones(H * W),  # Value constant at 1.0\n            ],\n            axis=-1,\n        )\n\n        # Convert HSV to RGB for matplotlib\n        from matplotlib.colors import hsv_to_rgb\n\n        rgb_colors = hsv_to_rgb(hsv_colors)\n\n        # Create figure with 3 subplots\n        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\n        # Subplot 1: Scatter plot colored by 2D spatial position\n        axes[0].scatter(\n            projection_flat[:, 0],\n            projection_flat[:, 1],\n            c=rgb_colors,\n            alpha=0.6,\n            s=20,\n        )\n        axes[0].set_xlabel(\"PC1 (1st component)\")\n        axes[0].set_ylabel(\"PC2 (2nd component)\")\n        axes[0].set_title(f\"PCA Scatter - {stage} E{epoch} B{batch_idx} Img{i}\")\n        axes[0].grid(True, alpha=0.3)\n\n        # Subplot 2: Spatial reference image\n        # Create reference image showing the spatial color coding\n        spatial_reference = hsv_to_rgb(\n            np.stack([x_norm, y_norm, np.ones_like(x_norm)], axis=-1)\n        )\n        axes[1].imshow(spatial_reference, aspect=\"auto\")\n        axes[1].set_xlabel(\"Width (\u2192 Hue)\")\n        axes[1].set_ylabel(\"Height (\u2192 Saturation)\")\n        axes[1].set_title(\"Spatial Color Reference\")\n\n        # Subplot 3: Image representation\n        # Normalize each channel to [0, 1] for visualization\n        pc1_norm = (projection_2d[:, :, 0] - projection_2d[:, :, 0].min()) / (\n            projection_2d[:, :, 0].max() - projection_2d[:, :, 0].min() + 1e-8\n        )\n        pc2_norm = (projection_2d[:, :, 1] - projection_2d[:, :, 1].min()) / (\n            projection_2d[:, :, 1].max() - projection_2d[:, :, 1].min() + 1e-8\n        )\n\n        # Create RGB image: PC1 in red channel, PC2 in green channel, zeros in blue\n        img_rgb = np.stack([pc1_norm, pc2_norm, np.zeros_like(pc1_norm)], axis=-1)\n\n        axes[2].imshow(img_rgb, aspect=\"auto\")\n        axes[2].set_xlabel(\"Width\")\n        axes[2].set_ylabel(\"Height\")\n        axes[2].set_title(\"PCA Image (R=PC1, G=PC2)\")\n\n        # Add statistics text\n        pc1_min = projection_2d[:, :, 0].min()\n        pc1_max = projection_2d[:, :, 0].max()\n        pc2_min = projection_2d[:, :, 1].min()\n        pc2_max = projection_2d[:, :, 1].max()\n        stats_text = (\n            f\"Shape: [{H}, {W}]\\n\"\n            f\"Points: {H * W}\\n\"\n            f\"PC1 range: [{pc1_min:.3f}, {pc1_max:.3f}]\\n\"\n            f\"PC2 range: [{pc2_min:.3f}, {pc2_max:.3f}]\"\n        )\n        fig.text(\n            0.98,\n            0.5,\n            stats_text,\n            ha=\"left\",\n            va=\"center\",\n            bbox={\n                \"boxstyle\": \"round\",\n                \"facecolor\": \"wheat\",\n                \"alpha\": 0.5,\n            },\n        )\n\n        plt.tight_layout()\n\n        # Convert figure to numpy array (RGB format)\n        img_array = fig_to_array(fig, dpi=150)\n\n        # Create Artifact object\n        artifact = Artifact(\n            name=f\"pca_projection_img{i:02d}\",\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n            value=img_array,\n            el_id=i,\n            desc=f\"PCA projection for {stage} epoch {epoch}, batch {batch_idx}, image {i}\",\n            type=ArtifactType.IMAGE,\n        )\n        artifacts.append(artifact)\n\n        progress_total = self.up_to if self.up_to else B\n        description = (\n            f\"Created PCA projection artifact ({i + 1}/{progress_total}): {artifact.name}\"\n        )\n        logger.info(description)\n\n        plt.close(fig)\n\n    # Return artifacts\n    return {\"artifacts\": artifacts}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.AnomalyMask","title":"AnomalyMask","text":"<pre><code>AnomalyMask(channel, up_to=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Visualize anomaly detection with GT and predicted masks.</p> <p>Creates side-by-side visualizations showing ground truth masks, predicted masks, and overlay comparisons on hyperspectral cube images. The overlay shows: - Green: True Positives (correct anomaly detection) - Red: False Positives (false alarms) - Yellow: False Negatives (missed anomalies)</p> <p>Also displays IoU and other metrics. Returns a list of Artifact objects for logging to monitoring systems.</p> <p>Executes during validation and inference stages.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>int</code> <p>Channel index to use for cube visualization (required)</p> required <code>up_to</code> <code>int</code> <p>Maximum number of images to visualize. If None, visualizes all (default: None)</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decider = BinaryDecider(threshold=0.2)\n&gt;&gt;&gt; viz_mask = AnomalyMask(channel=30, up_to=5)\n&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; graph.connect(\n...     (logit_head.logits, decider.data),\n...     (decider.decisions, viz_mask.decisions),\n...     (data_node.mask, viz_mask.mask),\n...     (data_node.cube, viz_mask.cube),\n...     (viz_mask.artifacts, tensorboard_node.artifacts),\n... )\n</code></pre> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def __init__(self, channel: int, up_to: int | None = None, **kwargs) -&gt; None:\n    self.channel = channel\n    self.up_to = up_to\n\n    super().__init__(\n        execution_stages={ExecutionStage.VAL, ExecutionStage.TEST, ExecutionStage.INFERENCE},\n        channel=channel,\n        up_to=up_to,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.AnomalyMask.forward","title":"forward","text":"<pre><code>forward(decisions, cube, context, mask=None, scores=None)\n</code></pre> <p>Create anomaly mask visualizations with GT/pred comparison.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>Tensor</code> <p>Binary anomaly decisions [B, H, W, 1]</p> required <code>mask</code> <code>Tensor | None</code> <p>Ground truth anomaly mask [B, H, W, 1] (optional)</p> <code>None</code> <code>cube</code> <code>Tensor</code> <p>Original cube [B, H, W, C] for visualization</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with \"artifacts\" key containing list of Artifact objects</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def forward(\n    self,\n    decisions: torch.Tensor,\n    cube: torch.Tensor,\n    context: Context,\n    mask: torch.Tensor | None = None,\n    scores: torch.Tensor | None = None,\n) -&gt; dict:\n    \"\"\"Create anomaly mask visualizations with GT/pred comparison.\n\n    Parameters\n    ----------\n    decisions : torch.Tensor\n        Binary anomaly decisions [B, H, W, 1]\n    mask : torch.Tensor | None\n        Ground truth anomaly mask [B, H, W, 1] (optional)\n    cube : torch.Tensor\n        Original cube [B, H, W, C] for visualization\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict\n        Dictionary with \"artifacts\" key containing list of Artifact objects\n    \"\"\"\n    # Extract context information\n    stage = context.stage.value\n    epoch = context.epoch\n    batch_idx = context.batch_idx\n\n    # Use decisions directly (already binary)\n    pred_mask = decisions.float()\n\n    # Convert to numpy and squeeze channel dimension\n    pred_mask_np = pred_mask.detach().cpu().numpy().squeeze(-1)  # [B, H, W]\n    cube_np = cube.detach().cpu().numpy()  # [B, H, W, C]\n\n    # Determine if we should use ground truth\n    # Skip GT comparison if: mask not provided, inference stage, or mask is all zeros\n    use_gt = (\n        mask is not None and context.stage != ExecutionStage.INFERENCE and mask.any().item()\n    )\n\n    # Process ground truth mask if available\n    gt_mask_np = None\n    batch_iou = None\n    if use_gt:\n        gt_mask_np = mask.detach().cpu().numpy().squeeze(-1)  # [B, H, W]\n\n        # Add binary mask assertion\n        unique_values = np.unique(gt_mask_np)\n        if not np.all(np.isin(unique_values, [0, 1, True, False])):\n            raise ValueError(\n                f\"AnomalyMask expects binary masks with only values {{0, 1}}. \"\n                f\"Found unique values: {unique_values}. \"\n                f\"Ensure LentilsAnomolyDataNode is configured with anomaly_class_ids \"\n                f\"to convert multi-class masks to binary.\"\n            )\n\n        # Compute batch-level IoU (matches AnomalyDetectionMetrics computation)\n        batch_gt = gt_mask_np &gt; 0.5  # [B, H, W] bool\n        batch_pred = pred_mask_np &gt; 0.5  # [B, H, W] bool\n        batch_tp = np.logical_and(batch_pred, batch_gt).sum()\n        batch_fp = np.logical_and(batch_pred, ~batch_gt).sum()\n        batch_fn = np.logical_and(~batch_pred, batch_gt).sum()\n        batch_iou = batch_tp / (batch_tp + batch_fp + batch_fn + 1e-8)\n\n    # Determine how many images to visualize from this batch\n    batch_size = pred_mask_np.shape[0]\n    up_to_batch = batch_size if self.up_to is None else min(batch_size, self.up_to)\n\n    # List to collect artifacts\n    artifacts = []\n\n    # Loop through each image in the batch up to the limit\n    for i in range(up_to_batch):\n        # Get predicted mask for this image\n        pred = pred_mask_np[i] &gt; 0.5  # [H, W] bool\n\n        # Get cube channel for visualization\n        cube_img = cube_np[i]  # [H, W, C]\n        cube_channel = cube_img[:, :, self.channel]\n\n        # Normalize cube channel to [0, 1] for display\n        cube_norm = (cube_channel - cube_channel.min()) / (\n            cube_channel.max() - cube_channel.min() + 1e-8\n        )\n\n        if use_gt:\n            # Mode A: Full comparison with ground truth\n            assert gt_mask_np is not None, \"gt_mask_np should not be None when use_gt is True\"\n            gt = gt_mask_np[i] &gt; 0.5  # [H, W] bool\n\n            # Compute confusion matrix\n            tp = np.logical_and(pred, gt)  # True Positives\n            fp = np.logical_and(pred, ~gt)  # False Positives\n            fn = np.logical_and(~pred, gt)  # False Negatives\n            # Compute metrics\n            tp_count = tp.sum()\n            fp_count = fp.sum()\n            fn_count = fn.sum()\n\n            precision = tp_count / (tp_count + fp_count + 1e-8)\n            recall = tp_count / (tp_count + fn_count + 1e-8)\n            iou = tp_count / (tp_count + fp_count + fn_count + 1e-8)\n\n            # Create figure with 3 subplots\n            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n            # Subplot 1: Ground truth mask\n            axes[0].imshow(gt, cmap=\"gray\", aspect=\"auto\")\n            axes[0].set_title(\"Ground Truth Mask\")\n            axes[0].set_xlabel(\"Width\")\n            axes[0].set_ylabel(\"Height\")\n\n            # Subplot 2: Cube with TP/FP/FN overlay\n            per_image_ap = None\n            if scores is not None:\n                raw_scores = scores[i, ..., 0]\n                probs = torch.sigmoid(raw_scores).flatten()\n                target_tensor = mask[i, ..., 0].flatten().to(dtype=torch.long)\n                if probs.numel() == target_tensor.numel():\n                    per_image_ap = binary_average_precision(probs, target_tensor).item()\n\n            axes[1].imshow(cube_norm, cmap=\"gray\", aspect=\"auto\")\n\n            # Create color overlay\n            overlay = np.zeros((*gt.shape, 4))\n            overlay[tp] = [0, 1, 0, 0.6]  # Green: True Positives\n            overlay[fp] = [1, 0, 0, 0.6]  # Red: False Positives\n            overlay[fn] = [1, 1, 0, 0.6]  # Yellow: False Negatives\n            # TN pixels remain transparent (no overlay)\n\n            overlay_title = f\"Overlay (Channel {self.channel}) - IoU: {iou:.3f}\"\n            if per_image_ap is not None:\n                overlay_title += f\" | AP: {per_image_ap:.3f}\"\n            overlay_title += \"\\nGreen=TP, Red=FP, Yellow=FN\"\n\n            axes[1].imshow(overlay, aspect=\"auto\")\n            axes[1].set_title(overlay_title)\n            axes[1].set_xlabel(\"Width\")\n            axes[1].set_ylabel(\"Height\")\n\n            # Subplot 3: Predicted mask with metrics in title\n            axes[2].imshow(pred, cmap=\"gray\", aspect=\"auto\")\n\n            # Add metrics as title (smaller font)\n            metrics_title = (\n                f\"Predicted Mask\\nIoU: {iou:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f}\"\n            )\n            if per_image_ap is not None:\n                metrics_title += f\" | AP: {per_image_ap:.4f}\"\n            metrics_title += f\"\\nBatch IoU: {batch_iou:.4f} (all {batch_size} imgs) | Ch: {self.channel}/{cube_img.shape[2]}\"\n            axes[2].set_title(metrics_title, fontsize=9)\n            axes[2].set_xlabel(\"Width\")\n            axes[2].set_ylabel(\"Height\")\n\n            log_msg = f\"Created anomaly mask artifact ({i + 1}/{up_to_batch}): IoU: {iou:.3f}\"\n        else:\n            # Mode B: Prediction-only visualization (no ground truth)\n            # Create figure with 2 subplots\n            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n            # Subplot 1: Cube with predicted overlay\n            axes[0].imshow(cube_norm, cmap=\"gray\", aspect=\"auto\")\n\n            # Create prediction overlay (cyan for predicted anomalies)\n            overlay = np.zeros((*pred.shape, 4))\n            overlay[pred] = [0, 1, 1, 0.6]  # Cyan: Predicted anomalies\n\n            axes[0].imshow(overlay, aspect=\"auto\")\n            axes[0].set_title(\n                f\"Prediction Overlay (Channel {self.channel})\\nCyan=Predicted Anomalies\"\n            )\n            axes[0].set_xlabel(\"Width\")\n            axes[0].set_ylabel(\"Height\")\n\n            # Subplot 2: Predicted mask\n            axes[1].imshow(pred, cmap=\"gray\", aspect=\"auto\")\n            axes[1].set_title(\"Predicted Mask\")\n            axes[1].set_xlabel(\"Width\")\n            axes[1].set_ylabel(\"Height\")\n\n            # Add statistics as text\n            pred_pixels = pred.sum()\n            total_pixels = pred.size\n            pred_ratio = pred_pixels / total_pixels\n\n            stats_text = (\n                f\"Prediction Stats:\\n\"\n                f\"Anomaly pixels: {pred_pixels}\\n\"\n                f\"Total pixels: {total_pixels}\\n\"\n                f\"Anomaly ratio: {pred_ratio:.4f}\\n\"\n                f\"\\n\"\n                f\"Channel: {self.channel}/{cube_img.shape[2]}\\n\"\n                f\"\\n\"\n                f\"Mode: Inference/No GT\"\n            )\n\n            fig.text(\n                0.98,\n                0.5,\n                stats_text,\n                ha=\"left\",\n                va=\"center\",\n                bbox={\n                    \"boxstyle\": \"round\",\n                    \"facecolor\": \"lightblue\",\n                    \"alpha\": 0.5,\n                },\n                fontfamily=\"monospace\",\n            )\n\n            log_msg = (\n                f\"Created anomaly mask artifact ({i + 1}/{up_to_batch}): prediction-only mode\"\n            )\n\n        # Add main title with epoch/batch info\n        fig.suptitle(\n            f\"Anomaly Mask Visualization - {stage} E{epoch} B{batch_idx} Img{i}\",\n            fontsize=14,\n            fontweight=\"bold\",\n        )\n\n        plt.tight_layout()\n\n        # Convert figure to numpy array (RGB format)\n        img_array = fig_to_array(fig, dpi=150)\n\n        # Create Artifact object\n        artifact = Artifact(\n            name=f\"anomaly_mask_img{i:02d}\",\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n            value=img_array,\n            el_id=i,\n            desc=f\"Anomaly mask for {stage} epoch {epoch}, batch {batch_idx}, image {i}\",\n            type=ArtifactType.IMAGE,\n        )\n        artifacts.append(artifact)\n\n        logger.info(log_msg)\n\n        plt.close(fig)\n\n    # Return artifacts\n    return {\"artifacts\": artifacts}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.ScoreHeatmapVisualizer","title":"ScoreHeatmapVisualizer","text":"<pre><code>ScoreHeatmapVisualizer(\n    normalize_scores=True, cmap=\"inferno\", up_to=5, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Log LAD/RX score heatmaps as TensorBoard artifacts.</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def __init__(\n    self,\n    normalize_scores: bool = True,\n    cmap: str = \"inferno\",\n    up_to: int | None = 5,\n    **kwargs,\n) -&gt; None:\n    self.normalize_scores = normalize_scores\n    self.cmap = cmap\n    self.up_to = up_to\n    super().__init__(\n        execution_stages={ExecutionStage.VAL, ExecutionStage.TEST, ExecutionStage.INFERENCE},\n        normalize_scores=normalize_scores,\n        cmap=cmap,\n        up_to=up_to,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.ScoreHeatmapVisualizer.forward","title":"forward","text":"<pre><code>forward(scores, context)\n</code></pre> <p>Generate heatmap visualizations of anomaly scores.</p> <p>Creates color-mapped heatmaps of anomaly scores for visualization in TensorBoard. Optionally normalizes scores to [0, 1] range for consistent visualization across batches.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>Anomaly scores [B, H, W, 1] from detection nodes (e.g., RX, LAD).</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx information.</p> required <p>Returns:</p> Type Description <code>dict[str, list[Artifact]]</code> <p>Dictionary with \"artifacts\" key containing list of heatmap artifacts.</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def forward(self, scores: torch.Tensor, context: Context) -&gt; dict[str, list[Artifact]]:\n    \"\"\"Generate heatmap visualizations of anomaly scores.\n\n    Creates color-mapped heatmaps of anomaly scores for visualization\n    in TensorBoard. Optionally normalizes scores to [0, 1] range for\n    consistent visualization across batches.\n\n    Parameters\n    ----------\n    scores : Tensor\n        Anomaly scores [B, H, W, 1] from detection nodes (e.g., RX, LAD).\n    context : Context\n        Execution context with stage, epoch, batch_idx information.\n\n    Returns\n    -------\n    dict[str, list[Artifact]]\n        Dictionary with \"artifacts\" key containing list of heatmap artifacts.\n    \"\"\"\n    artifacts: list[Artifact] = []\n    batch_limit = scores.shape[0] if self.up_to is None else min(scores.shape[0], self.up_to)\n\n    for idx in range(batch_limit):\n        score_map = scores[idx, ..., 0].detach().cpu().numpy()\n\n        if self.normalize_scores:\n            min_v = float(score_map.min())\n            max_v = float(score_map.max())\n            if max_v - min_v &gt; 1e-9:\n                score_map = (score_map - min_v) / (max_v - min_v)\n            else:\n                score_map = np.zeros_like(score_map)\n\n        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n        im = ax.imshow(score_map, cmap=self.cmap)\n        ax.set_title(f\"Score Heatmap #{idx}\")\n        ax.axis(\"off\")\n        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n        img_array = fig_to_array(fig, dpi=150)\n        plt.close(fig)\n\n        artifact = Artifact(\n            name=f\"score_heatmap_img{idx:02d}\",\n            value=img_array,\n            el_id=idx,\n            desc=\"Anomaly score heatmap\",\n            type=ArtifactType.IMAGE,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n        artifacts.append(artifact)\n\n    return {\"artifacts\": artifacts}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.RGBAnomalyMask","title":"RGBAnomalyMask","text":"<pre><code>RGBAnomalyMask(up_to=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Visualize anomaly detection with GT and predicted masks on RGB images.</p> <p>Similar to AnomalyMask but designed for RGB images (e.g., from band selectors). Creates side-by-side visualizations showing ground truth masks, predicted masks, and overlay comparisons on RGB images. The overlay shows: - Green: True Positives (correct anomaly detection) - Red: False Positives (false alarms) - Yellow: False Negatives (missed anomalies)</p> <p>Also displays IoU and other metrics. Returns a list of Artifact objects for logging to monitoring systems.</p> <p>Executes during validation and inference stages.</p> <p>Parameters:</p> Name Type Description Default <code>up_to</code> <code>int</code> <p>Maximum number of images to visualize. If None, visualizes all (default: None)</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decider = BinaryDecider(threshold=0.2)\n&gt;&gt;&gt; viz_mask = RGBAnomalyMask(up_to=5)\n&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; graph.connect(\n...     (decider.decisions, viz_mask.decisions),\n...     (data_node.mask, viz_mask.mask),\n...     (band_selector.rgb_image, viz_mask.rgb_image),\n...     (viz_mask.artifacts, tensorboard_node.artifacts),\n... )\n</code></pre> <p>Initialize RGBAnomalyMask visualizer.</p> <p>Parameters:</p> Name Type Description Default <code>up_to</code> <code>int | None</code> <p>Maximum number of images to visualize. If None, visualizes all (default: None)</p> <code>None</code> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def __init__(self, up_to: int | None = None, **kwargs) -&gt; None:\n    \"\"\"Initialize RGBAnomalyMask visualizer.\n\n    Parameters\n    ----------\n    up_to : int | None, optional\n        Maximum number of images to visualize. If None, visualizes all (default: None)\n    \"\"\"\n    self.up_to = up_to\n    super().__init__(\n        execution_stages={ExecutionStage.VAL, ExecutionStage.TEST, ExecutionStage.INFERENCE},\n        up_to=up_to,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.visualizations.RGBAnomalyMask.forward","title":"forward","text":"<pre><code>forward(\n    decisions,\n    rgb_image,\n    mask=None,\n    context=None,\n    scores=None,\n)\n</code></pre> <p>Create anomaly mask visualizations with GT/pred comparison on RGB images.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>Tensor</code> <p>Binary anomaly decisions [B, H, W, 1]</p> required <code>rgb_image</code> <code>Tensor</code> <p>RGB image [B, H, W, 3] for visualization</p> required <code>mask</code> <code>Tensor | None</code> <p>Ground truth anomaly mask [B, H, W, 1] (optional)</p> <code>None</code> <code>context</code> <code>Context | None</code> <p>Execution context with stage, epoch, batch_idx</p> <code>None</code> <code>scores</code> <code>Tensor | None</code> <p>Optional anomaly logits/scores [B, H, W, 1]</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with \"artifacts\" key containing list of Artifact objects</p> Source code in <code>cuvis_ai/node/visualizations.py</code> <pre><code>def forward(\n    self,\n    decisions: torch.Tensor,\n    rgb_image: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    context: Context | None = None,\n    scores: torch.Tensor | None = None,\n) -&gt; dict:\n    \"\"\"Create anomaly mask visualizations with GT/pred comparison on RGB images.\n\n    Parameters\n    ----------\n    decisions : torch.Tensor\n        Binary anomaly decisions [B, H, W, 1]\n    rgb_image : torch.Tensor\n        RGB image [B, H, W, 3] for visualization\n    mask : torch.Tensor | None\n        Ground truth anomaly mask [B, H, W, 1] (optional)\n    context : Context | None\n        Execution context with stage, epoch, batch_idx\n    scores : torch.Tensor | None\n        Optional anomaly logits/scores [B, H, W, 1]\n\n    Returns\n    -------\n    dict\n        Dictionary with \"artifacts\" key containing list of Artifact objects\n    \"\"\"\n    if context is None:\n        raise ValueError(\"RGBAnomalyMask.forward() requires a Context object\")\n\n    # Convert to numpy only at this point (keep on device until last moment)\n    pred_mask_np: np.ndarray = tensor_to_numpy(decisions.float().squeeze(-1))  # [B, H, W]\n    rgb_np: np.ndarray = tensor_to_numpy(rgb_image)  # [B, H, W, 3]\n\n    # Normalize RGB to [0, 1]\n    if rgb_np.max() &gt; 1.0:\n        rgb_np = rgb_np / 255.0\n    rgb_np = np.clip(rgb_np, 0.0, 1.0)\n\n    # Check if GT available and valid\n    use_gt = (\n        mask is not None and context.stage != ExecutionStage.INFERENCE and mask.any().item()\n    )\n\n    # Validate and convert GT if available\n    gt_mask_np: np.ndarray | None = None\n    batch_iou: float | None = None\n    if use_gt:\n        assert mask is not None\n        gt_mask_np = tensor_to_numpy(mask.squeeze(-1))  # [B, H, W]\n        unique_values = np.unique(gt_mask_np)\n        if not np.all(np.isin(unique_values, [0, 1, True, False])):\n            raise ValueError(f\"RGBAnomalyMask expects binary masks, found: {unique_values}\")\n        # Compute batch IoU\n        batch_pred = pred_mask_np &gt; 0.5\n        batch_gt = gt_mask_np &gt; 0.5\n        tp = np.logical_and(batch_pred, batch_gt).sum()\n        batch_iou = float(\n            tp\n            / (\n                tp\n                + np.logical_and(batch_pred, ~batch_gt).sum()\n                + np.logical_and(~batch_pred, batch_gt).sum()\n                + 1e-8\n            )\n        )\n\n    batch_size = pred_mask_np.shape[0]\n    up_to_batch = min(batch_size, self.up_to or batch_size)\n    artifacts = []\n\n    # Loop through images and visualize\n    for i in range(up_to_batch):\n        pred = pred_mask_np[i] &gt; 0.5\n        rgb_img = rgb_np[i]\n        gt = gt_mask_np[i] &gt; 0.5 if gt_mask_np is not None else None\n\n        # Compute metrics and AP if GT available\n        metrics: dict | None = None\n        per_image_ap: float | None = None\n        if gt is not None:\n            metrics = self._compute_metrics(pred, gt)\n            if scores is not None and mask is not None:\n                raw_scores = scores[i, ..., 0]\n                probs = torch.sigmoid(raw_scores).flatten()\n                target_tensor = mask[i, ..., 0].flatten().to(dtype=torch.long)\n                if probs.device != target_tensor.device:\n                    target_tensor = target_tensor.to(probs.device)\n                if probs.numel() == target_tensor.numel():\n                    per_image_ap = binary_average_precision(probs, target_tensor).item()\n\n        # Create figure and plot\n        ncols = 3 if gt is not None else 2\n        fig, axes = plt.subplots(1, ncols, figsize=(6 * ncols, 6))\n        if ncols == 1:\n            axes = [axes]\n\n        if gt is not None and metrics is not None and batch_iou is not None:\n            self._plot_with_gt(\n                axes, rgb_img, pred, gt, metrics, batch_iou, batch_size, per_image_ap\n            )\n            log_msg = (\n                f\"Created RGB anomaly mask ({i + 1}/{up_to_batch}): IoU={metrics['iou']:.3f}\"\n            )\n        else:\n            self._plot_no_gt(axes, rgb_img, pred)\n            log_msg = f\"Created RGB anomaly mask ({i + 1}/{up_to_batch}) (no GT)\"\n\n        plt.tight_layout()\n        img_array = fig_to_array(fig, dpi=150)\n        plt.close(fig)\n\n        artifact = Artifact(\n            name=f\"rgb_anomaly_mask_img{i:02d}\",\n            value=img_array,\n            el_id=i,\n            desc=log_msg,\n            type=ArtifactType.IMAGE,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n        artifacts.append(artifact)\n\n    return {\"artifacts\": artifacts}\n</code></pre>"},{"location":"api/nodes/#tensorboard-visualization","title":"TensorBoard Visualization","text":""},{"location":"api/nodes/#cuvis_ai.node.drcnn_tensorboard_viz","title":"drcnn_tensorboard_viz","text":"<p>TensorBoard visualization node for DRCNN-AdaClip training.</p> <p>This node creates image artifacts for TensorBoard logging to visualize: - Input HSI cube (false-color RGB visualization) - Mixer output (what AdaClip sees as input) - Ground truth anomaly masks - AdaClip anomaly scores (as heatmap)</p>"},{"location":"api/nodes/#cuvis_ai.node.drcnn_tensorboard_viz.DRCNNTensorBoardViz","title":"DRCNNTensorBoardViz","text":"<pre><code>DRCNNTensorBoardViz(\n    hsi_channels=None,\n    max_samples=4,\n    log_every_n_batches=1,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>TensorBoard visualization node for DRCNN-AdaClip pipeline.</p> <p>Creates image artifacts for logging to TensorBoard: - Input HSI cube visualization (false-color RGB from selected channels) - Mixer output (3-channel RGB-like image that AdaClip sees) - Ground truth anomaly mask - AdaClip anomaly scores (as heatmap)</p> <p>Parameters:</p> Name Type Description Default <code>hsi_channels</code> <code>list[int]</code> <p>Channel indices to use for false-color RGB visualization of HSI input (default: [0, 20, 40] for a simple false-color representation)</p> <code>None</code> <code>max_samples</code> <code>int</code> <p>Maximum number of samples to log per batch (default: 4)</p> <code>4</code> <code>log_every_n_batches</code> <code>int</code> <p>Log images every N batches to reduce TensorBoard size (default: 1, log every batch)</p> <code>1</code> Source code in <code>cuvis_ai/node/drcnn_tensorboard_viz.py</code> <pre><code>def __init__(\n    self,\n    hsi_channels: list[int] | None = None,\n    max_samples: int = 4,\n    log_every_n_batches: int = 1,\n    **kwargs,\n) -&gt; None:\n    if hsi_channels is None:\n        hsi_channels = [0, 20, 40]  # Default: use channels 0, 20, 40 for false-color RGB\n    self.hsi_channels = hsi_channels\n    self.max_samples = max_samples\n    self.log_every_n_batches = log_every_n_batches\n    self._batch_counter = 0\n\n    super().__init__(\n        execution_stages={ExecutionStage.TRAIN, ExecutionStage.VAL, ExecutionStage.TEST},\n        hsi_channels=hsi_channels,\n        max_samples=max_samples,\n        log_every_n_batches=log_every_n_batches,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.drcnn_tensorboard_viz.DRCNNTensorBoardViz.forward","title":"forward","text":"<pre><code>forward(\n    hsi_cube,\n    mixer_output,\n    ground_truth_mask,\n    adaclip_scores,\n    context=None,\n    **_,\n)\n</code></pre> <p>Create image artifacts for TensorBoard logging.</p> <p>Parameters:</p> Name Type Description Default <code>hsi_cube</code> <code>Tensor</code> <p>Input HSI cube [B, H, W, C]</p> required <code>mixer_output</code> <code>Tensor</code> <p>Mixer output (RGB-like) [B, H, W, 3]</p> required <code>ground_truth_mask</code> <code>Tensor</code> <p>Ground truth anomaly mask [B, H, W, 1]</p> required <code>adaclip_scores</code> <code>Tensor</code> <p>AdaClip anomaly scores [B, H, W, 1]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx info</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[Artifact]]</code> <p>Dictionary with \"artifacts\" key containing list of Artifact objects</p> Source code in <code>cuvis_ai/node/drcnn_tensorboard_viz.py</code> <pre><code>def forward(\n    self,\n    hsi_cube: Tensor,\n    mixer_output: Tensor,\n    ground_truth_mask: Tensor,\n    adaclip_scores: Tensor,\n    context: Context | None = None,\n    **_: Any,\n) -&gt; dict[str, list[Artifact]]:\n    \"\"\"Create image artifacts for TensorBoard logging.\n\n    Parameters\n    ----------\n    hsi_cube : Tensor\n        Input HSI cube [B, H, W, C]\n    mixer_output : Tensor\n        Mixer output (RGB-like) [B, H, W, 3]\n    ground_truth_mask : Tensor\n        Ground truth anomaly mask [B, H, W, 1]\n    adaclip_scores : Tensor\n        AdaClip anomaly scores [B, H, W, 1]\n    context : Context, optional\n        Execution context with stage, epoch, batch_idx info\n\n    Returns\n    -------\n    dict[str, list[Artifact]]\n        Dictionary with \"artifacts\" key containing list of Artifact objects\n    \"\"\"\n    if context is None:\n        context = Context()\n\n    # Skip logging if not the right batch interval\n    self._batch_counter += 1\n    if (self._batch_counter - 1) % self.log_every_n_batches != 0:\n        return {\"artifacts\": []}\n\n    artifacts = []\n    B = hsi_cube.shape[0]\n    num_samples = min(B, self.max_samples)\n\n    # Convert tensors to numpy for visualization\n    hsi_np = hsi_cube.detach().cpu().numpy()\n    mixer_np = mixer_output.detach().cpu().numpy()\n    mask_np = ground_truth_mask.detach().cpu().numpy()\n    scores_np = adaclip_scores.detach().cpu().numpy()\n\n    for b in range(num_samples):\n        # 1. HSI Input Visualization (false-color RGB)\n        hsi_img = self._create_hsi_visualization(hsi_np[b])\n        artifact = Artifact(\n            name=f\"hsi_input_sample_{b}\",\n            value=hsi_img,\n            el_id=b,\n            desc=f\"HSI input (false-color RGB) for sample {b}\",\n            type=ArtifactType.IMAGE,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n        artifacts.append(artifact)\n\n        # 2. Mixer Output (what AdaClip sees as input)\n        mixer_img = self._normalize_image(mixer_np[b])  # Already [H, W, 3]\n        artifact = Artifact(\n            name=f\"mixer_output_adaclip_input_sample_{b}\",\n            value=mixer_img,\n            el_id=b,\n            desc=f\"Mixer output (AdaClip input) for sample {b}\",\n            type=ArtifactType.IMAGE,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n        artifacts.append(artifact)\n\n        # 3. Ground Truth Mask\n        mask_img = self._create_mask_visualization(mask_np[b])  # [H, W, 1] -&gt; [H, W, 3]\n        artifact = Artifact(\n            name=f\"ground_truth_mask_sample_{b}\",\n            value=mask_img,\n            el_id=b,\n            desc=f\"Ground truth anomaly mask for sample {b}\",\n            type=ArtifactType.IMAGE,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n        artifacts.append(artifact)\n\n        # 4. AdaClip Scores (as heatmap)\n        scores_img = self._create_scores_heatmap(scores_np[b])  # [H, W, 1] -&gt; [H, W, 3]\n        artifact = Artifact(\n            name=f\"adaclip_scores_heatmap_sample_{b}\",\n            value=scores_img,\n            el_id=b,\n            desc=f\"AdaClip anomaly scores (heatmap) for sample {b}\",\n            type=ArtifactType.IMAGE,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n        artifacts.append(artifact)\n\n    return {\"artifacts\": artifacts}\n</code></pre>"},{"location":"api/nodes/#monitor","title":"Monitor","text":""},{"location":"api/nodes/#cuvis_ai.node.monitor","title":"monitor","text":"<p>TensorBoard Monitoring Nodes.</p> <p>This module provides nodes for logging artifacts and metrics to TensorBoard during pipeline execution. The monitoring nodes are sink nodes that accept artifacts (visualizations) and metrics from upstream nodes and write them to TensorBoard logs for visualization and analysis.</p> <p>The primary use case is logging training and validation metrics, along with visualizations like heatmaps, RGB renderings, and PCA plots during model training.</p> See Also <p>cuvis_ai.node.visualizations : Nodes that generate artifacts for monitoring</p>"},{"location":"api/nodes/#cuvis_ai.node.monitor.TensorBoardMonitorNode","title":"TensorBoardMonitorNode","text":"<pre><code>TensorBoardMonitorNode(\n    output_dir=\"./runs\",\n    run_name=None,\n    comment=\"\",\n    flush_secs=120,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>TensorBoard monitoring node for logging artifacts and metrics.</p> <p>This is a SINK node that logs visualizations (artifacts) and metrics to TensorBoard. Accepts optional inputs for artifacts and metrics, allowing predecessors to be filtered by execution_stage without causing errors.</p> <p>Executes during all stages (ALWAYS).</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory for TensorBoard logs (default: \"./runs\")</p> <code>'./runs'</code> <code>comment</code> <code>str</code> <p>Comment to append to log directory name (default: \"\")</p> <code>''</code> <code>flush_secs</code> <code>int</code> <p>How often to flush pending events to disk (default: 120)</p> <code>120</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; heatmap_viz = AnomalyHeatmap(cmap='hot', up_to=10)\n&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; graph.connect(\n...     (heatmap_viz.artifacts, tensorboard_node.artifacts),\n... )\n</code></pre> Source code in <code>cuvis_ai/node/monitor.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str = \"./runs\",\n    run_name: str | None = None,\n    comment: str = \"\",\n    flush_secs: int = 120,\n    **kwargs,\n) -&gt; None:\n    self.output_dir = Path(output_dir)\n    self.run_name = run_name\n    self.comment = comment\n    self.flush_secs = flush_secs\n    self._writer = None\n    self._tensorboard_available = False\n\n    super().__init__(\n        execution_stages={ExecutionStage.ALWAYS},\n        output_dir=str(output_dir),\n        run_name=run_name,\n        comment=comment,\n        flush_secs=flush_secs,\n        **kwargs,\n    )\n\n    # Check if tensorboard is available\n\n    self._SummaryWriter = SummaryWriter\n\n    # Determine the log directory with run name\n    self.log_dir = self._resolve_log_dir()\n\n    # Initialize TensorBoard writer\n    self.log_dir.mkdir(parents=True, exist_ok=True)\n    self._writer = self._SummaryWriter(\n        log_dir=str(self.log_dir),\n        comment=self.comment,\n        flush_secs=self.flush_secs,\n    )\n    logger.info(f\"TensorBoard writer initialized: {self.log_dir}\")\n    logger.info(f\"To view visualizations, run: uv run tensorboard --logdir={self.output_dir}\")\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.monitor.TensorBoardMonitorNode.forward","title":"forward","text":"<pre><code>forward(artifacts=None, metrics=None, context=None)\n</code></pre> <p>Log artifacts and metrics to TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx, global_step</p> <code>None</code> <code>artifacts</code> <code>list[Artifact]</code> <p>List of artifacts to log (default: None)</p> <code>None</code> <code>metrics</code> <code>list[Metric]</code> <p>List of metrics to log (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Empty dict (sink node has no outputs)</p> Source code in <code>cuvis_ai/node/monitor.py</code> <pre><code>def forward(\n    self,\n    artifacts: list[Artifact] | None = None,\n    metrics: list[Metric] | None = None,\n    context: Context | None = None,\n) -&gt; dict:\n    \"\"\"Log artifacts and metrics to TensorBoard.\n\n    Parameters\n    ----------\n    context : Context\n        Execution context with stage, epoch, batch_idx, global_step\n    artifacts : list[Artifact], optional\n        List of artifacts to log (default: None)\n    metrics : list[Metric], optional\n        List of metrics to log (default: None)\n\n    Returns\n    -------\n    dict\n        Empty dict (sink node has no outputs)\n    \"\"\"\n    if context is None:\n        context = Context()\n\n    stage = context.stage.value\n    step = context.global_step\n\n    # Flatten artifacts if it's a list of lists (variadic port)\n    if artifacts is not None:\n        if (\n            isinstance(artifacts, list)\n            and len(artifacts) &gt; 0\n            and isinstance(artifacts[0], list)\n        ):\n            artifacts = [item for sublist in artifacts for item in sublist]\n\n    # Log artifacts\n    if artifacts is not None:\n        for artifact in artifacts:\n            self._log_artifact(artifact, stage, step)\n        logger.debug(f\"Logged {len(artifacts)} artifacts to TensorBoard at step {step}\")\n\n    # Flatten metrics if variadic input provided\n    if (\n        metrics is not None\n        and isinstance(metrics, list)\n        and metrics\n        and isinstance(metrics[0], list)\n    ):\n        metrics = [item for sublist in metrics for item in sublist]\n\n    # Log metrics\n    if metrics is not None:\n        for metric in metrics:\n            self._log_metric(metric, stage, step)\n        logger.debug(f\"Logged {len(metrics)} metrics to TensorBoard at step {step}\")\n\n    return {}\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.monitor.TensorBoardMonitorNode.log","title":"log","text":"<pre><code>log(name, value, step)\n</code></pre> <p>Log a scalar value to TensorBoard.</p> <p>This method provides a simple interface for external trainers to log metrics directly, complementing the port-based logging. Used by GradientTrainer to log train/val losses to the same TensorBoard directory as graph metrics and artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name/tag for the scalar (e.g., \"train/loss\", \"val/accuracy\")</p> required <code>value</code> <code>float</code> <p>Scalar value to log</p> required <code>step</code> <code>int</code> <p>Global step number</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; # From external trainer\n&gt;&gt;&gt; tensorboard_node.log(\"train/loss\", 0.5, step=100)\n</code></pre> Source code in <code>cuvis_ai/node/monitor.py</code> <pre><code>def log(self, name: str, value: float, step: int) -&gt; None:\n    \"\"\"Log a scalar value to TensorBoard.\n\n    This method provides a simple interface for external trainers\n    to log metrics directly, complementing the port-based logging.\n    Used by GradientTrainer to log train/val losses to the same\n    TensorBoard directory as graph metrics and artifacts.\n\n    Parameters\n    ----------\n    name : str\n        Name/tag for the scalar (e.g., \"train/loss\", \"val/accuracy\")\n    value : float\n        Scalar value to log\n    step : int\n        Global step number\n\n    Examples\n    --------\n    &gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n    &gt;&gt;&gt; # From external trainer\n    &gt;&gt;&gt; tensorboard_node.log(\"train/loss\", 0.5, step=100)\n    \"\"\"\n    self._writer.add_scalar(name, value, step)\n</code></pre>"},{"location":"api/nodes/#label-processing","title":"Label Processing","text":"<p>Nodes for label conversion and manipulation.</p>"},{"location":"api/nodes/#labels","title":"Labels","text":""},{"location":"api/nodes/#cuvis_ai.node.labels","title":"labels","text":"<p>Label Mapping Nodes.</p> <p>This module provides nodes for converting multi-class segmentation masks to binary anomaly labels. These nodes are useful when training with datasets that have multi-class annotations but the task requires binary anomaly detection.</p> <p>The main node remaps class IDs to binary labels (0=normal, 1=anomaly) based on configurable normal and anomaly class ID lists.</p> See Also <p>cuvis_ai.deciders : Binary decision nodes for threshold-based classification</p>"},{"location":"api/nodes/#cuvis_ai.node.labels.BinaryAnomalyLabelMapper","title":"BinaryAnomalyLabelMapper","text":"<pre><code>BinaryAnomalyLabelMapper(\n    normal_class_ids, anomaly_class_ids=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Convert multi-class segmentation masks to binary anomaly targets.</p> <p>Masks are remapped to torch.long tensors with 0 representing normal pixels and 1 indicating anomalies.</p> <p>Parameters:</p> Name Type Description Default <code>normal_class_ids</code> <code>Iterable[int]</code> <p>Class IDs that should be considered normal (default: (0, 2)).</p> required <code>anomaly_class_ids</code> <code>Iterable[int] | None</code> <p>Explicit anomaly IDs. When <code>None</code> all IDs not in <code>normal_class_ids</code> are treated as anomalies. When provided, only these IDs are treated as anomalies and all others (including those not in normal_class_ids) are treated as normal.</p> <code>None</code> Source code in <code>cuvis_ai/node/labels.py</code> <pre><code>def __init__(\n    self,\n    normal_class_ids: Iterable[int],\n    anomaly_class_ids: Iterable[int] | None = None,\n    **kwargs,\n) -&gt; None:\n    self.normal_class_ids = tuple(int(c) for c in normal_class_ids)\n    self.anomaly_class_ids = (\n        tuple(int(c) for c in anomaly_class_ids) if anomaly_class_ids is not None else None\n    )\n\n    # Validate that there are no overlaps between normal and anomaly class IDs\n    if self.anomaly_class_ids is not None:\n        overlap = set(self.normal_class_ids) &amp; set(self.anomaly_class_ids)\n        if overlap:\n            raise ValueError(\n                f\"Overlap detected between normal_class_ids and anomaly_class_ids: {overlap}. \"\n                \"Class IDs cannot be both normal and anomaly.\"\n            )\n\n        # Check for gaps in coverage and issue warning\n        all_specified_ids = set(self.normal_class_ids) | set(self.anomaly_class_ids)\n        max_id = max(all_specified_ids) if all_specified_ids else 0\n\n        # Find gaps (missing class IDs)\n        expected_ids = set(range(max_id + 1))\n        gaps = expected_ids - all_specified_ids\n\n        if gaps:\n            warnings.warn(\n                f\"Gap detected in class ID coverage. The following class IDs are not specified \"\n                f\"in either normal_class_ids or anomaly_class_ids: {gaps}. \"\n                f\"These will be treated as normal classes. To specify all classes explicitly, \"\n                f\"include them in normal_class_ids or anomaly_class_ids.\",\n                UserWarning,\n                stacklevel=2,\n            )\n            # Add gaps to normal_class_ids as requested\n            self.normal_class_ids = tuple(sorted(set(self.normal_class_ids) | gaps))\n\n    self._target_dtype = torch.long\n\n    super().__init__(\n        normal_class_ids=self.normal_class_ids,\n        anomaly_class_ids=self.anomaly_class_ids,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/nodes/#cuvis_ai.node.labels.BinaryAnomalyLabelMapper.forward","title":"forward","text":"<pre><code>forward(cube, mask, **_)\n</code></pre> <p>Map multi-class labels to binary anomaly labels.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Tensor</code> <p>Features/scores to pass through [B, H, W, C]</p> required <code>mask</code> <code>Tensor</code> <p>Multi-class segmentation masks [B, H, W, 1]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"cube\" (pass-through) and \"mask\" (binary bool) keys</p> Source code in <code>cuvis_ai/node/labels.py</code> <pre><code>def forward(self, cube: Tensor, mask: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Map multi-class labels to binary anomaly labels.\n\n    Parameters\n    ----------\n    cube : Tensor\n        Features/scores to pass through [B, H, W, C]\n    mask : Tensor\n        Multi-class segmentation masks [B, H, W, 1]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"cube\" (pass-through) and \"mask\" (binary bool) keys\n    \"\"\"\n    if self.anomaly_class_ids is not None:\n        # Explicit anomaly class IDs: only these are anomalies, rest are normal\n        mask_anomaly = self._membership_mask(mask, self.anomaly_class_ids)\n    else:\n        # Original behavior: normal_class_ids are normal, everything else is anomaly\n        mask_normal = self._membership_mask(mask, self.normal_class_ids)\n        mask_anomaly = ~mask_normal\n\n    mapped = torch.zeros_like(mask, dtype=self._target_dtype, device=mask.device)\n    mapped = torch.where(mask_anomaly, torch.ones_like(mapped), mapped)\n\n    # Convert to bool for smaller tensor size\n    mapped = mapped.bool()\n\n    return {\"cube\": cube, \"mask\": mapped}\n</code></pre>"},{"location":"api/nodes/#related-pages","title":"Related Pages","text":"<ul> <li>Node System Deep Dive</li> <li>Node Catalog</li> <li>Add Built-in Node</li> </ul>"},{"location":"api/pipeline/","title":"Pipeline & Graph","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/pipeline/#pipeline-graph-api","title":"Pipeline &amp; Graph API","text":""},{"location":"api/pipeline/#overview","title":"Overview","text":"<p>Pipeline and graph construction in CUVIS.AI is handled through configuration files and the PyTorch Lightning framework. The cuvis_ai package provides nodes that can be composed into pipelines.</p> <p>For information on building and configuring pipelines, see the guides below.</p>"},{"location":"api/pipeline/#pipeline-construction","title":"Pipeline Construction","text":"<p>Pipeline construction is done through:</p> <ul> <li>YAML Configuration: Define pipelines declaratively using YAML config files</li> <li>Python API: Build pipelines programmatically using PyTorch Lightning modules</li> <li>Node Composition: Compose nodes from the cuvis_ai package into processing graphs</li> </ul>"},{"location":"api/pipeline/#related-pages","title":"Related Pages","text":"<ul> <li>Build Pipeline (Python) - Python API for pipeline construction</li> <li>Build Pipeline (YAML) - YAML configuration approach</li> <li>Pipeline Lifecycle - Understanding pipeline execution</li> <li>Node System Deep Dive - Node composition patterns</li> <li>Configuration Basics - Configuration system overview</li> </ul>"},{"location":"api/pipeline/#node-api-reference","title":"Node API Reference","text":"<p>For the complete API of available nodes that can be used in pipelines, see:</p> <ul> <li>Nodes API - All available node implementations</li> </ul>"},{"location":"api/ports/","title":"Ports","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/ports/#ports-api-reference","title":"Ports API Reference","text":"<p>Complete API reference for the Typed I/O port system in CUVIS.AI.</p>"},{"location":"api/ports/#overview","title":"Overview","text":"<p>The port system provides typed input/output interfaces for all nodes, enabling type-safe connections, runtime validation, and flexible pipeline construction. Each node defines its input and output ports using <code>PortSpec</code> objects.</p>"},{"location":"api/ports/#core-components","title":"Core Components","text":""},{"location":"api/ports/#portspec","title":"PortSpec","text":"<p>The <code>PortSpec</code> class defines the specification for a port, including its type, shape constraints, and metadata.</p> <p>Attributes: - <code>name</code>: Port identifier - <code>port_type</code>: \"input\" or \"output\" - <code>shape</code>: Expected tensor shape with dimension constraints - <code>dtype</code>: Expected data type (optional) - <code>description</code>: Human-readable description - <code>stage</code>: Execution stage (\"train\", \"eval\", \"both\")</p> <p>Example: <pre><code>from cuvis_ai_schemas.pipeline import PortSpec\n\n# Define an input port for hyperspectral data\ndata_port = PortSpec(\n    name=\"data\",\n    port_type=\"input\",\n    shape=(-1, -1, -1, -1),  # (batch, height, width, channels)\n    description=\"Raw hyperspectral cube input\"\n)\n\n# Define an output port for normalized data\nnormalized_port = PortSpec(\n    name=\"normalized\", \n    port_type=\"output\",\n    shape=(-1, -1, -1, -1),\n    description=\"Normalized hyperspectral cube\"\n)\n</code></pre></p>"},{"location":"api/ports/#inputport-outputport","title":"InputPort / OutputPort","text":"<p>Port instances that are attached to nodes and used for connections.</p> <p>Creating Ports: <pre><code>from cuvis_ai_schemas.pipeline import InputPort, OutputPort\n\n# Create port instances\ninput_port = InputPort(spec=data_port, node=normalizer)\noutput_port = OutputPort(spec=normalized_port, node=normalizer)\n</code></pre></p>"},{"location":"api/ports/#port-compatibility-rules","title":"Port Compatibility Rules","text":"<p>Ports can be connected if they satisfy compatibility rules:</p>"},{"location":"api/ports/#shape-compatibility","title":"Shape Compatibility","text":"<ul> <li>Fixed dimensions must match exactly</li> <li>Variable dimensions (<code>-1</code>) can match any size</li> <li>Batch dimensions are typically variable</li> </ul>"},{"location":"api/ports/#type-compatibility","title":"Type Compatibility","text":"<ul> <li>Input ports can only connect to output ports</li> <li>Ports must have compatible data types</li> <li>Stage constraints must be satisfied</li> </ul>"},{"location":"api/ports/#connection-validation","title":"Connection Validation","text":"<pre><code># Check if ports are compatible\nif input_port.is_compatible_with(output_port):\n    pipeline.connect(output_port, input_port)\nelse:\n    print(\"Ports are incompatible\")\n</code></pre>"},{"location":"api/ports/#node-port-declarations","title":"Node Port Declarations","text":"<p>Nodes declare their ports using <code>INPUT_SPECS</code> and <code>OUTPUT_SPECS</code> class attributes.</p>"},{"location":"api/ports/#example-node-implementation","title":"Example Node Implementation","text":"<pre><code>from cuvis_ai_core.node.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\n\nclass MinMaxNormalizer(Node):\n    \"\"\"Min-max normalization node.\"\"\"\n\n    # Input port specifications\n    INPUT_SPECS = [\n        PortSpec(\n            name=\"data\",\n            port_type=\"input\",\n            shape=(-1, -1, -1, -1),\n            description=\"Raw hyperspectral cube\"\n        )\n    ]\n\n    # Output port specifications  \n    OUTPUT_SPECS = [\n        PortSpec(\n            name=\"normalized\",\n            port_type=\"output\", \n            shape=(-1, -1, -1, -1),\n            description=\"Normalized cube [0, 1]\"\n        )\n    ]\n\n    def __init__(self, eps=1e-6, use_running_stats=True):\n        super().__init__()\n        self.eps = eps\n        self.use_running_stats = use_running_stats\n\n    def forward(self, **inputs):\n        data = inputs[\"data\"]\n        # Normalization logic here\n        normalized = (data - self.running_min) / (self.running_max - self.running_min + self.eps)\n        return {\"normalized\": normalized}\n</code></pre>"},{"location":"api/ports/#port-based-connections","title":"Port-Based Connections","text":""},{"location":"api/ports/#basic-connection","title":"Basic Connection","text":"<pre><code># Connect two nodes using their ports\npipeline.connect(normalizer.normalized, selector.data)\n</code></pre>"},{"location":"api/ports/#multiple-connections","title":"Multiple Connections","text":"<pre><code># Fan-in multiple outputs to a single input (e.g., monitoring artifacts)\npipeline.connect(\n    (viz_mask.artifacts, tensorboard_node.artifacts),\n    (viz_rgb.artifacts, tensorboard_node.artifacts),\n)\n</code></pre>"},{"location":"api/ports/#stage-aware-connections","title":"Stage-Aware Connections","text":"<pre><code># Connect nodes for specific execution stages\npipeline.connect(normalizer.normalized, selector.data, stage=\"train\")\npipeline.connect(selector.selected, pca.features, stage=\"both\")\n</code></pre>"},{"location":"api/ports/#loss-nodes-without-an-aggregator","title":"Loss Nodes Without an Aggregator","text":"<p><code>LossAggregator</code> has been removed\u2014the trainer now collects individual loss nodes directly. Register every loss/regularizer node with the <code>GradientTrainer</code> (or any custom trainer) and feed their inputs through standard port connections, as shown in <code>examples//03_channel_selector.py</code>.</p> <pre><code>pipeline.connect(\n    (logit_head.logits, bce_loss.predictions),\n    (data_node.mask, bce_loss.targets),\n    (selector.weights, entropy_loss.weights),\n    (selector.weights, diversity_loss.weights),\n)\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss, entropy_loss, diversity_loss],\n    metric_nodes=[metrics_anomaly],\n    trainer_config=training_cfg.trainer,\n    optimizer_config=training_cfg.optimizer,\n)\n</code></pre>"},{"location":"api/ports/#batch-distribution","title":"Batch Distribution","text":"<p>The port system enables explicit batch distribution to specific input ports.</p>"},{"location":"api/ports/#single-input","title":"Single Input","text":"<pre><code># Distribute batch to a specific input port\noutputs = pipeline.forward(batch={f\"{normalizer.id}.data\": input_data})\n</code></pre>"},{"location":"api/ports/#multiple-inputs","title":"Multiple Inputs","text":"<pre><code># Distribute different data to different input ports\noutputs = pipeline.forward(batch={\n    f\"{node1.id}.data1\": data1,\n    f\"{node2.id}.data2\": data2,\n    f\"{node3.id}.features\": features\n})\n</code></pre>"},{"location":"api/ports/#batch-key-format","title":"Batch Key Format","text":"<p>Batch keys follow the pattern: <code>{node_id}.{port_name}</code></p>"},{"location":"api/ports/#dimension-resolution","title":"Dimension Resolution","text":"<p>The port system automatically resolves variable dimensions during execution.</p>"},{"location":"api/ports/#dynamic-shape-resolution","title":"Dynamic Shape Resolution","text":"<pre><code># Port with variable dimensions\nport_spec = PortSpec(\n    name=\"features\",\n    port_type=\"input\", \n    shape=(-1, -1, -1, -1)  # All dimensions variable\n)\n\n# During execution, dimensions are resolved from input data\n# Input shape: (32, 64, 64, 100) \u2192 Output shape: (32, 64, 64, n_components)\n</code></pre>"},{"location":"api/ports/#constraint-validation","title":"Constraint Validation","text":"<pre><code># Port with fixed channel dimension\nport_spec = PortSpec(\n    name=\"features\",\n    port_type=\"input\",\n    shape=(-1, -1, -1, 100)  # Fixed channel dimension\n)\n\n# Connection will fail if channel dimension doesn't match\n</code></pre>"},{"location":"api/ports/#common-port-patterns","title":"Common Port Patterns","text":""},{"location":"api/ports/#normalization-nodes","title":"Normalization Nodes","text":"<p>Input Ports: - <code>data</code>: Raw hyperspectral cube</p> <p>Output Ports: - <code>normalized</code>: Normalized data</p>"},{"location":"api/ports/#feature-extraction","title":"Feature Extraction","text":"<p>Input Ports: - <code>features</code>: Input features for transformation</p> <p>Output Ports: - <code>projected</code>: Transformed features - <code>explained_variance</code>: Statistical metrics</p>"},{"location":"api/ports/#anomaly-detection","title":"Anomaly Detection","text":"<p>Input Ports: - <code>data</code>: Features for anomaly scoring</p> <p>Output Ports: - <code>scores</code>: Anomaly detection scores - <code>logits</code>: Logit-transformed scores</p>"},{"location":"api/ports/#loss-nodes","title":"Loss Nodes","text":"<p>Input Ports: - Variadic inputs for loss computation</p> <p>Output Ports: - <code>loss</code>: Computed loss value</p>"},{"location":"api/ports/#error-handling","title":"Error Handling","text":""},{"location":"api/ports/#port-not-found","title":"Port Not Found","text":"<pre><code>try:\n    pipeline.connect(normalizer.nonexistent, selector.data)\nexcept AttributeError as e:\n    print(f\"Port error: {e}\")\n    # Error: 'MinMaxNormalizer' object has no attribute 'nonexistent'\n</code></pre>"},{"location":"api/ports/#incompatible-ports","title":"Incompatible Ports","text":"<pre><code>try:\n    pipeline.connect(normalizer.normalized, pca.features)\nexcept ValueError as e:\n    print(f\"Compatibility error: {e}\")\n    # Error: Port shapes are incompatible: (-1, -1, -1, -1) vs (-1, -1, -1, 3)\n</code></pre>"},{"location":"api/ports/#missing-batch-distribution","title":"Missing Batch Distribution","text":"<pre><code>try:\n    outputs = pipeline.forward(batch=input_data)\nexcept KeyError as e:\n    print(f\"Batch error: {e}\")\n    # Error: Unable to find input port for batch key\n</code></pre>"},{"location":"api/ports/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/ports/#custom-port-specifications","title":"Custom Port Specifications","text":"<pre><code># Create custom port with specific constraints\ncustom_port = PortSpec(\n    name=\"embedding\",\n    port_type=\"output\",\n    shape=(-1, 512),  # Fixed embedding dimension\n    dtype=torch.float32,\n    description=\"Feature embeddings\"\n)\n</code></pre>"},{"location":"api/ports/#port-inspection","title":"Port Inspection","text":"<pre><code># Inspect port properties\nport = normalizer.normalized\nprint(f\"Port name: {port.name}\")\nprint(f\"Port type: {port.port_type}\")\nprint(f\"Expected shape: {port.shape}\")\nprint(f\"Description: {port.description}\")\n</code></pre>"},{"location":"api/ports/#connection-graph","title":"Connection Graph","text":"<pre><code># Get all connections in the pipeline\nconnections = pipeline.get_connections()\nfor source, target in connections:\n    print(f\"{source.node.name}.{source.name} \u2192 {target.node.name}.{target.name}\")\n</code></pre>"},{"location":"api/ports/#best-practices","title":"Best Practices","text":"<ol> <li>Use Descriptive Port Names: Choose names that clearly indicate the port's purpose</li> <li>Define Shape Constraints: Use fixed dimensions when possible for early error detection</li> <li>Document Ports: Provide clear descriptions for each port</li> <li>Test Port Compatibility: Validate connections during development</li> <li>Use Stage Filtering: Leverage stage-aware execution for performance</li> </ol>"},{"location":"api/ports/#api-reference","title":"API Reference","text":""},{"location":"api/ports/#cuvis_ai_schemas.pipeline","title":"pipeline","text":"<p>Pipeline structure schemas.</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.ConnectionConfig","title":"ConnectionConfig","text":"<p>               Bases: <code>_BaseConfig</code></p> <p>Connection between two nodes.</p> <p>Attributes:</p> Name Type Description <code>from_node</code> <code>str</code> <p>Source node ID</p> <code>from_port</code> <code>str</code> <p>Source port name</p> <code>to_node</code> <code>str</code> <p>Target node ID</p> <code>to_port</code> <code>str</code> <p>Target port name</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.NodeConfig","title":"NodeConfig","text":"<p>               Bases: <code>_BaseConfig</code></p> <p>Node configuration within a pipeline.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique node identifier</p> <code>class_name</code> <code>str</code> <p>Fully-qualified class name (e.g., 'my_package.MyNode') Alias: 'class' for backward compatibility</p> <code>params</code> <code>dict[str, Any]</code> <p>Node parameters/hyperparameters Alias: 'hparams' for backward compatibility</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig","title":"PipelineConfig","text":"<p>               Bases: <code>_BaseConfig</code></p> <p>Pipeline structure configuration.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Pipeline name</p> <code>nodes</code> <code>list[NodeConfig] | list[dict[str, Any]]</code> <p>Node definitions (can be NodeConfig or dict for flexibility)</p> <code>connections</code> <code>list[ConnectionConfig] | list[dict[str, Any]]</code> <p>Node connections (can be ConnectionConfig or dict for flexibility)</p> <code>frozen_nodes</code> <code>list[str]</code> <p>Node IDs to keep frozen during training</p> <code>metadata</code> <code>PipelineMetadata | None</code> <p>Optional pipeline metadata</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.to_proto","title":"to_proto","text":"<pre><code>to_proto()\n</code></pre> <p>Convert to proto message.</p> <p>Requires cuvis-ai-schemas[proto] to be installed.</p> <p>Returns:</p> Type Description <code>PipelineConfig</code> <p>Proto message representation</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>def to_proto(self) -&gt; cuvis_ai_pb2.PipelineConfig:\n    \"\"\"Convert to proto message.\n\n    Requires cuvis-ai-schemas[proto] to be installed.\n\n    Returns\n    -------\n    cuvis_ai_pb2.PipelineConfig\n        Proto message representation\n    \"\"\"\n    try:\n        from cuvis_ai_schemas.grpc.v1 import cuvis_ai_pb2\n    except ImportError as exc:\n        msg = \"Proto support not installed. Install with: pip install cuvis-ai-schemas[proto]\"\n        raise ImportError(msg) from exc\n\n    return cuvis_ai_pb2.PipelineConfig(config_bytes=self.model_dump_json().encode(\"utf-8\"))\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.from_proto","title":"from_proto  <code>classmethod</code>","text":"<pre><code>from_proto(proto_config)\n</code></pre> <p>Load from proto message.</p> <p>Parameters:</p> Name Type Description Default <code>proto_config</code> <code>PipelineConfig</code> <p>Proto message to deserialize</p> required <p>Returns:</p> Type Description <code>PipelineConfig</code> <p>Loaded configuration</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>@classmethod\ndef from_proto(cls, proto_config: cuvis_ai_pb2.PipelineConfig) -&gt; PipelineConfig:\n    \"\"\"Load from proto message.\n\n    Parameters\n    ----------\n    proto_config : cuvis_ai_pb2.PipelineConfig\n        Proto message to deserialize\n\n    Returns\n    -------\n    PipelineConfig\n        Loaded configuration\n    \"\"\"\n    return cls.model_validate_json(proto_config.config_bytes.decode(\"utf-8\"))\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.to_json","title":"to_json","text":"<pre><code>to_json()\n</code></pre> <p>Convert to JSON string.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Convert to JSON string.\"\"\"\n    return self.model_dump_json()\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(payload)\n</code></pre> <p>Load from JSON string.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>@classmethod\ndef from_json(cls, payload: str) -&gt; PipelineConfig:\n    \"\"\"Load from JSON string.\"\"\"\n    return cls.model_validate_json(payload)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data)\n</code></pre> <p>Load from dictionary.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; PipelineConfig:\n    \"\"\"Load from dictionary.\"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.save_to_file","title":"save_to_file","text":"<pre><code>save_to_file(path)\n</code></pre> <p>Save pipeline configuration to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Output file path</p> required Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>def save_to_file(self, path: str | Path) -&gt; None:\n    \"\"\"Save pipeline configuration to YAML file.\n\n    Parameters\n    ----------\n    path : str | Path\n        Output file path\n    \"\"\"\n    from pathlib import Path\n\n    output_path = Path(path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(self.model_dump(), f, sort_keys=False)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineConfig.load_from_file","title":"load_from_file  <code>classmethod</code>","text":"<pre><code>load_from_file(path)\n</code></pre> <p>Load pipeline configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Input file path</p> required <p>Returns:</p> Type Description <code>PipelineConfig</code> <p>Loaded configuration</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>@classmethod\ndef load_from_file(cls, path: str | Path) -&gt; PipelineConfig:\n    \"\"\"Load pipeline configuration from YAML file.\n\n    Parameters\n    ----------\n    path : str | Path\n        Input file path\n\n    Returns\n    -------\n    PipelineConfig\n        Loaded configuration\n    \"\"\"\n    from pathlib import Path\n\n    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f)\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineMetadata","title":"PipelineMetadata","text":"<p>               Bases: <code>_BaseConfig</code></p> <p>Pipeline metadata for documentation and discovery.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Pipeline name</p> <code>description</code> <code>str</code> <p>Human-readable description</p> <code>created</code> <code>str</code> <p>Creation timestamp (ISO format)</p> <code>tags</code> <code>list[str]</code> <p>Tags for categorization and search</p> <code>author</code> <code>str</code> <p>Author name or email</p> <code>cuvis_ai_version</code> <code>str</code> <p>Version of cuvis-ai-schemas used</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineMetadata.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineMetadata.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data)\n</code></pre> <p>Load from dictionary.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; PipelineMetadata:\n    \"\"\"Load from dictionary.\"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PipelineMetadata.to_proto","title":"to_proto","text":"<pre><code>to_proto()\n</code></pre> <p>Convert to proto message.</p> <p>Requires cuvis-ai-schemas[proto] to be installed.</p> <p>Returns:</p> Type Description <code>PipelineMetadata</code> <p>Proto message representation</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/config.py</code> <pre><code>def to_proto(self) -&gt; cuvis_ai_pb2.PipelineMetadata:\n    \"\"\"Convert to proto message.\n\n    Requires cuvis-ai-schemas[proto] to be installed.\n\n    Returns\n    -------\n    cuvis_ai_pb2.PipelineMetadata\n        Proto message representation\n    \"\"\"\n    try:\n        from cuvis_ai_schemas.grpc.v1 import cuvis_ai_pb2\n    except ImportError as exc:\n        msg = \"Proto support not installed. Install with: pip install cuvis-ai-schemas[proto]\"\n        raise ImportError(msg) from exc\n\n    return cuvis_ai_pb2.PipelineMetadata(\n        name=self.name,\n        description=self.description,\n        created=self.created,\n        tags=list(self.tags),\n        author=self.author,\n        cuvis_ai_version=self.cuvis_ai_version,\n    )\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PortCompatibilityError","title":"PortCompatibilityError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when attempting to connect incompatible ports.</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.DimensionResolver","title":"DimensionResolver","text":"<p>Utility class for resolving symbolic dimensions in port shapes.</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.DimensionResolver.resolve","title":"resolve  <code>staticmethod</code>","text":"<pre><code>resolve(shape, node)\n</code></pre> <p>Resolve symbolic dimensions to concrete values.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int | str, ...]</code> <p>Shape specification with flexible (-1), fixed (int), or symbolic (str) dims.</p> required <code>node</code> <code>Any | None</code> <p>Node instance to resolve symbolic dimensions from.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Resolved shape with concrete integer values.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If symbolic dimension references non-existent node attribute.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/ports.py</code> <pre><code>@staticmethod\ndef resolve(\n    shape: tuple[int | str, ...],\n    node: Any | None,\n) -&gt; tuple[int, ...]:\n    \"\"\"Resolve symbolic dimensions to concrete values.\n\n    Parameters\n    ----------\n    shape : tuple[int | str, ...]\n        Shape specification with flexible (-1), fixed (int), or symbolic (str) dims.\n    node : Any | None\n        Node instance to resolve symbolic dimensions from.\n\n    Returns\n    -------\n    tuple[int, ...]\n        Resolved shape with concrete integer values.\n\n    Raises\n    ------\n    AttributeError\n        If symbolic dimension references non-existent node attribute.\n    \"\"\"\n    resolved: list[int] = []\n    for dim in shape:\n        if isinstance(dim, int):\n            # Flexible (-1) or fixed (int) dimension\n            resolved.append(dim)\n            continue\n\n        if isinstance(dim, str):\n            # Symbolic dimension - resolve from node\n            if node is None:\n                raise ValueError(\n                    f\"Cannot resolve symbolic dimension '{dim}' without node instance\"\n                )\n            if not hasattr(node, dim):\n                node_label = getattr(node, \"id\", None) or node\n                raise AttributeError(\n                    f\"Node {node_label} has no attribute '{dim}' for dimension resolution\"\n                )\n\n            value = getattr(node, dim)\n            if not isinstance(value, int):\n                raise TypeError(f\"Dimension '{dim}' resolved to {type(value)}, expected int\")\n            resolved.append(value)\n            continue\n\n        raise TypeError(f\"Invalid dimension type: {type(dim)}\")\n\n    return tuple(resolved)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.InputPort","title":"InputPort","text":"<pre><code>InputPort(node, name, spec)\n</code></pre> <p>Proxy object representing a node's input port.</p> <p>Initialize an input port proxy.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Any</code> <p>The node instance that owns this port.</p> required <code>name</code> <code>str</code> <p>The name of the port on the node.</p> required <code>spec</code> <code>PortSpec</code> <p>The port specification defining type and shape constraints.</p> required Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/ports.py</code> <pre><code>def __init__(self, node: Any, name: str, spec: PortSpec) -&gt; None:\n    \"\"\"Initialize an input port proxy.\n\n    Parameters\n    ----------\n    node : Any\n        The node instance that owns this port.\n    name : str\n        The name of the port on the node.\n    spec : PortSpec\n        The port specification defining type and shape constraints.\n    \"\"\"\n    self.node = node\n    self.name = name\n    self.spec = spec\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.OutputPort","title":"OutputPort","text":"<pre><code>OutputPort(node, name, spec)\n</code></pre> <p>Proxy object representing a node's output port.</p> <p>Initialize an output port proxy.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Any</code> <p>The node instance that owns this port.</p> required <code>name</code> <code>str</code> <p>The name of the port on the node.</p> required <code>spec</code> <code>PortSpec</code> <p>The port specification defining type and shape constraints.</p> required Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/ports.py</code> <pre><code>def __init__(self, node: Any, name: str, spec: PortSpec) -&gt; None:\n    \"\"\"Initialize an output port proxy.\n\n    Parameters\n    ----------\n    node : Any\n        The node instance that owns this port.\n    name : str\n        The name of the port on the node.\n    spec : PortSpec\n        The port specification defining type and shape constraints.\n    \"\"\"\n    self.node = node\n    self.name = name\n    self.spec = spec\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PortSpec","title":"PortSpec  <code>dataclass</code>","text":"<pre><code>PortSpec(dtype, shape, description='', optional=False)\n</code></pre> <p>Specification for a node input or output port.</p>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PortSpec.resolve_shape","title":"resolve_shape","text":"<pre><code>resolve_shape(node)\n</code></pre> <p>Resolve symbolic dimensions in shape using node attributes.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Any</code> <p>Node instance to resolve symbolic dimensions from.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Resolved shape with all symbolic dimensions replaced by concrete integer values.</p> See Also <p>DimensionResolver.resolve : Underlying resolution logic.</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/ports.py</code> <pre><code>def resolve_shape(self, node: Any) -&gt; tuple[int, ...]:\n    \"\"\"Resolve symbolic dimensions in shape using node attributes.\n\n    Parameters\n    ----------\n    node : Any\n        Node instance to resolve symbolic dimensions from.\n\n    Returns\n    -------\n    tuple[int, ...]\n        Resolved shape with all symbolic dimensions replaced by concrete integer values.\n\n    See Also\n    --------\n    DimensionResolver.resolve : Underlying resolution logic.\n    \"\"\"\n    return DimensionResolver.resolve(self.shape, node)\n</code></pre>"},{"location":"api/ports/#cuvis_ai_schemas.pipeline.PortSpec.is_compatible_with","title":"is_compatible_with","text":"<pre><code>is_compatible_with(other, source_node, target_node)\n</code></pre> <p>Check if this port can connect to another port.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>PortSpec | list[PortSpec]</code> <p>Target port spec. If a list, it's a variadic port - extract the spec.</p> required <code>source_node</code> <code>Any | None</code> <p>Source node for dimension resolution</p> required <code>target_node</code> <code>Any | None</code> <p>Target node for dimension resolution</p> required <p>Returns:</p> Type Description <code>tuple[bool, str]</code> <p>(is_compatible, error_message)</p> Source code in <code>.venv/lib/python3.11/site-packages/cuvis_ai_schemas/pipeline/ports.py</code> <pre><code>def is_compatible_with(\n    self,\n    other: PortSpec | list[PortSpec],\n    source_node: Any | None,\n    target_node: Any | None,\n) -&gt; tuple[bool, str]:\n    \"\"\"Check if this port can connect to another port.\n\n    Parameters\n    ----------\n    other : PortSpec | list[PortSpec]\n        Target port spec. If a list, it's a variadic port - extract the spec.\n    source_node : Any | None\n        Source node for dimension resolution\n    target_node : Any | None\n        Target node for dimension resolution\n\n    Returns\n    -------\n    tuple[bool, str]\n        (is_compatible, error_message)\n    \"\"\"\n\n    def _format_dtype(value: Any) -&gt; str:\n        \"\"\"Format a dtype value for display in error messages.\n\n        Parameters\n        ----------\n        value : Any\n            A dtype value (torch.dtype, type, or other).\n\n        Returns\n        -------\n        str\n            Human-readable string representation of the dtype.\n        \"\"\"\n        if isinstance(value, torch.dtype):\n            return str(value)\n        return getattr(value, \"__name__\", str(value))\n\n    def _is_tensor_related(dtype: Any) -&gt; bool:\n        \"\"\"Check if dtype is torch.Tensor or a specific torch.dtype.\n\n        Parameters\n        ----------\n        dtype : Any\n            The dtype to check.\n\n        Returns\n        -------\n        bool\n            True if dtype is torch.Tensor or a torch.dtype instance.\n        \"\"\"\n        return dtype is torch.Tensor or isinstance(dtype, torch.dtype)\n\n    # Handle variadic ports (list-based specs)\n    if isinstance(other, list):\n        if not other:\n            return False, \"Variadic port has empty spec list\"\n        # Extract the actual PortSpec from the list\n        other = other[0]\n\n    # Check dtype compatibility with smart tensor handling\n    source_is_tensor = _is_tensor_related(self.dtype)\n    target_is_tensor = _is_tensor_related(other.dtype)\n\n    if source_is_tensor and target_is_tensor:\n        # Both tensor-related types\n        # Allow if either is generic torch.Tensor OR both are same dtype\n        if not (\n            self.dtype is torch.Tensor\n            or other.dtype is torch.Tensor\n            or self.dtype == other.dtype\n        ):\n            return False, (\n                f\"Dtype mismatch: source has {_format_dtype(self.dtype)}, \"\n                f\"target expects {_format_dtype(other.dtype)}\"\n            )\n    elif self.dtype != other.dtype:\n        # Non-tensor types must match exactly\n        return False, (\n            f\"Dtype mismatch: source has {_format_dtype(self.dtype)}, \"\n            f\"target expects {_format_dtype(other.dtype)}\"\n        )\n\n    # Resolve shapes\n    try:\n        source_shape = self.resolve_shape(source_node) if source_node else self.shape\n        target_shape = other.resolve_shape(target_node) if target_node else other.shape\n    except (AttributeError, ValueError, TypeError) as exc:\n        return False, f\"Shape resolution failed: {exc}\"\n\n    # Check rank compatibility\n    if len(source_shape) != len(target_shape):\n        return False, (\n            f\"Shape rank mismatch: source has {len(source_shape)} dimensions, \"\n            f\"target expects {len(target_shape)}\"\n        )\n\n    # Check dimension-by-dimension compatibility\n    for idx, (src_dim, tgt_dim) in enumerate(zip(source_shape, target_shape, strict=True)):\n        # -1 means flexible, always compatible\n        if src_dim == -1 or tgt_dim == -1:\n            continue\n\n        # Both fixed - must match exactly\n        if src_dim != tgt_dim:\n            return False, (\n                f\"Dimension {idx} mismatch: source has size {src_dim}, target expects {tgt_dim}\"\n            )\n\n    return True, \"\"\n</code></pre>"},{"location":"api/ports/#see-also","title":"See Also","text":"<ul> <li>Nodes API: Node implementations with port specifications</li> <li>Pipeline API: Pipeline and connection management</li> <li>Core Concepts: Understand the architecture</li> <li>Quickstart: Practical port usage examples</li> </ul>"},{"location":"api/training/","title":"Training","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/training/#training-api","title":"Training API","text":"<p>Training-related components including losses and metrics.</p>"},{"location":"api/training/#overview","title":"Overview","text":"<p>Training functionality in CUVIS.AI is provided through loss functions, metrics, and monitoring nodes that integrate with PyTorch Lightning.</p>"},{"location":"api/training/#loss-functions","title":"Loss Functions","text":""},{"location":"api/training/#cuvis_ai.node.losses","title":"losses","text":"<p>Loss nodes for training pipeline (port-based architecture).</p>"},{"location":"api/training/#cuvis_ai.node.losses.LossNode","title":"LossNode","text":"<pre><code>LossNode(**kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Base class for loss nodes that restricts execution to training stages.</p> <p>Loss nodes should not execute during inference - only during train, val, and test.</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    # Default to train/val/test stages, but allow override\n    assert \"execution_stages\" not in kwargs, (\n        \"Loss nodes can only execute in train, val, and test stages.\"\n    )\n\n    super().__init__(\n        execution_stages={\n            ExecutionStage.TRAIN,\n            ExecutionStage.VAL,\n            ExecutionStage.TEST,\n        },\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.OrthogonalityLoss","title":"OrthogonalityLoss","text":"<pre><code>OrthogonalityLoss(weight=1.0, **kwargs)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Orthogonality regularization loss for TrainablePCA.</p> <p>Encourages PCA components to remain orthonormal during training. Loss = weight * ||W @ W.T - I||^2_F</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Weight for orthogonality loss (default: 1.0)</p> <code>1.0</code> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(self, weight: float = 1.0, **kwargs) -&gt; None:\n    self.weight = weight\n\n    super().__init__(\n        weight=weight,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.OrthogonalityLoss.forward","title":"forward","text":"<pre><code>forward(components, **_)\n</code></pre> <p>Compute weighted orthogonality loss from PCA components.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Tensor</code> <p>PCA components matrix [n_components, n_features]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing weighted loss</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, components: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute weighted orthogonality loss from PCA components.\n\n    Parameters\n    ----------\n    components : Tensor\n        PCA components matrix [n_components, n_features]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing weighted loss\n    \"\"\"\n    # Compute gram matrix: W @ W.T\n    gram = components @ components.T\n\n    # Target: identity matrix\n    n_components = components.shape[0]\n    eye = torch.eye(\n        n_components,\n        device=components.device,\n        dtype=components.dtype,\n    )\n\n    # Frobenius norm of difference\n    orth_loss = torch.sum((gram - eye) ** 2)\n\n    return {\"loss\": self.weight * orth_loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.AnomalyBCEWithLogits","title":"AnomalyBCEWithLogits","text":"<pre><code>AnomalyBCEWithLogits(\n    weight=1.0, pos_weight=None, reduction=\"mean\", **kwargs\n)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Binary cross-entropy loss for anomaly detection with logits.</p> <p>Computes BCE loss between predicted anomaly scores and ground truth masks. Uses BCEWithLogitsLoss for numerical stability.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Overall weight for this loss component (default: 1.0)</p> <code>1.0</code> <code>pos_weight</code> <code>float</code> <p>Weight for positive class (anomaly) to handle class imbalance (default: None)</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Reduction method: 'mean', 'sum', or 'none' (default: 'mean')</p> <code>'mean'</code> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(\n    self,\n    weight: float = 1.0,\n    pos_weight: float | None = None,\n    reduction: str = \"mean\",\n    **kwargs,\n) -&gt; None:\n    self.weight = weight\n    self.pos_weight = pos_weight\n    self.reduction = reduction\n\n    super().__init__(\n        weight=weight,\n        pos_weight=pos_weight,\n        reduction=reduction,\n        **kwargs,\n    )\n\n    # Create loss function\n    if pos_weight is not None:\n        pos_weight_tensor = torch.tensor([pos_weight])\n        self.register_buffer(\"_pos_weight\", pos_weight_tensor)\n        self.loss_fn = nn.BCEWithLogitsLoss(\n            pos_weight=self._pos_weight,\n            reduction=reduction,\n        )\n    else:\n        self.loss_fn = nn.BCEWithLogitsLoss(reduction=reduction)\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.AnomalyBCEWithLogits.forward","title":"forward","text":"<pre><code>forward(predictions, targets, **_)\n</code></pre> <p>Compute weighted BCE loss.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Tensor</code> <p>Predicted scores [B, H, W, 1]</p> required <code>targets</code> <code>Tensor</code> <p>Ground truth masks [B, H, W, 1]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing scalar loss</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, predictions: Tensor, targets: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute weighted BCE loss.\n\n    Parameters\n    ----------\n    predictions : Tensor\n        Predicted scores [B, H, W, 1]\n    targets : Tensor\n        Ground truth masks [B, H, W, 1]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing scalar loss\n    \"\"\"\n    # Squeeze channel dimension to [B, H, W] for BCEWithLogitsLoss\n    if predictions.dim() == 4 and predictions.shape[-1] == 1:\n        predictions = predictions.squeeze(-1)\n\n    if targets.dim() == 4 and targets.shape[-1] == 1:\n        targets = targets.squeeze(-1)\n\n    # Convert labels to float\n    targets = targets.float()\n\n    # Compute loss\n    loss = self.loss_fn(predictions, targets)\n\n    # Apply weight\n    weighted_loss = self.weight * loss\n\n    return {\"loss\": weighted_loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.MSEReconstructionLoss","title":"MSEReconstructionLoss","text":"<pre><code>MSEReconstructionLoss(\n    weight=1.0, reduction=\"mean\", **kwargs\n)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Mean squared error reconstruction loss.</p> <p>Computes MSE between reconstruction and target. Useful for autoencoder-style architectures.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Weight for this loss component (default: 1.0)</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>Reduction method: 'mean', 'sum', or 'none' (default: 'mean')</p> <code>'mean'</code> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(self, weight: float = 1.0, reduction: str = \"mean\", **kwargs) -&gt; None:\n    self.weight = weight\n    self.reduction = reduction\n    # Extract Node base parameters from kwargs to avoid duplication\n    super().__init__(\n        weight=weight,\n        reduction=reduction,\n        **kwargs,\n    )\n    self.loss_fn = nn.MSELoss(reduction=reduction)\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.MSEReconstructionLoss.forward","title":"forward","text":"<pre><code>forward(reconstruction, target, **_)\n</code></pre> <p>Compute MSE reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>reconstruction</code> <code>Tensor</code> <p>Reconstructed data</p> required <code>target</code> <code>Tensor</code> <p>Target for reconstruction</p> required <code>**_</code> <code>Any</code> <p>Additional arguments (e.g., context) - ignored but accepted for compatibility</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing scalar loss</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, reconstruction: Tensor, target: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute MSE reconstruction loss.\n\n    Parameters\n    ----------\n    reconstruction : Tensor\n        Reconstructed data\n    target : Tensor\n        Target for reconstruction\n    **_ : Any\n        Additional arguments (e.g., context) - ignored but accepted for compatibility\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing scalar loss\n    \"\"\"\n    # Ensure consistent shapes\n    if target.shape != reconstruction.shape:\n        raise ValueError(\n            f\"Shape mismatch: reconstruction {reconstruction.shape} vs target {target.shape}\"\n        )\n\n    # Compute loss\n    loss = self.loss_fn(reconstruction, target)\n\n    # Apply weight\n    return {\"loss\": self.weight * loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.DistinctnessLoss","title":"DistinctnessLoss","text":"<pre><code>DistinctnessLoss(weight=0.1, eps=1e-06, **kwargs)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Repulsion loss encouraging different selectors to choose different bands.</p> <p>This loss is designed for band/channel selector nodes that output a 2D weight matrix <code>[output_channels, input_channels]</code>. It computes the mean pairwise cosine similarity between all pairs of selector weight vectors and penalizes high similarity:</p> <p>.. math::</p> <pre><code>L_\\text{repel} = \\frac{1}{N_\\text{pairs}} \\sum_{i &lt; j}\n    \\cos(\\mathbf{w}_i, \\mathbf{w}_j)\n</code></pre> <p>Minimizing this loss encourages selectors to focus on different bands, preventing the common failure mode where all channels collapse onto the same band.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Overall weight for this loss component (default: 0.1).</p> <code>0.1</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability when normalizing (default: 1e-6).</p> <code>1e-06</code> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(self, weight: float = 0.1, eps: float = 1e-6, **kwargs) -&gt; None:\n    self.weight = float(weight)\n    self.eps = float(eps)\n\n    super().__init__(weight=self.weight, eps=self.eps, **kwargs)\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.DistinctnessLoss.forward","title":"forward","text":"<pre><code>forward(selection_weights, **_)\n</code></pre> <p>Compute mean pairwise cosine similarity penalty.</p> <p>Parameters:</p> Name Type Description Default <code>selection_weights</code> <code>Tensor</code> <p>Weight matrix of shape [output_channels, input_channels].</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with a single key <code>\"loss\"</code> containing the scalar loss.</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, selection_weights: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute mean pairwise cosine similarity penalty.\n\n    Parameters\n    ----------\n    selection_weights : Tensor\n        Weight matrix of shape [output_channels, input_channels].\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with a single key ``\"loss\"`` containing the scalar loss.\n    \"\"\"\n    # Normalize each selector vector to unit length\n    w = selection_weights\n    w_norm = F.normalize(w, p=2, dim=-1, eps=self.eps)  # [C, T]\n\n    num_channels = w_norm.shape[0]\n    if num_channels &lt; 2:\n        # Nothing to compare - no repulsion needed\n        return {\"loss\": torch.zeros((), device=w_norm.device, dtype=w_norm.dtype)}\n\n    # Compute all pairwise cosine similarities using matrix multiplication (optimized)\n    similarity_matrix = w_norm @ w_norm.T  # [C, C] matrix of cosine similarities\n\n    # Extract upper triangular part (i &lt; j pairs), excluding diagonal\n    upper_tri = torch.triu(similarity_matrix, diagonal=1)\n\n    # Compute mean of non-zero elements (i &lt; j pairs)\n    mean_cos = upper_tri[upper_tri != 0].mean()\n\n    # Minimize mean cosine similarity (repulsion)\n    loss = self.weight * mean_cos\n    return {\"loss\": loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.SelectorEntropyRegularizer","title":"SelectorEntropyRegularizer","text":"<pre><code>SelectorEntropyRegularizer(\n    weight=0.01, target_entropy=None, eps=1e-06, **kwargs\n)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Entropy regularization for SoftChannelSelector.</p> <p>Encourages exploration by penalizing low-entropy (over-confident) selections. Computes entropy from selection weights and applies regularization.</p> <p>Higher entropy = more uniform selection (encouraged early in training) Lower entropy = more peaked selection (emerges naturally as training progresses)</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Weight for entropy regularization (default: 0.01) Positive weight encourages exploration (maximizes entropy) Negative weight encourages exploitation (minimizes entropy)</p> <code>0.01</code> <code>target_entropy</code> <code>float</code> <p>Target entropy for regularization (default: None, no target) If set, uses squared error: (entropy - target)^2</p> <code>None</code> <code>eps</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6)</p> <code>1e-06</code> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(\n    self,\n    weight: float = 0.01,\n    target_entropy: float | None = None,\n    eps: float = 1e-6,\n    **kwargs,\n) -&gt; None:\n    self.weight = weight\n    self.target_entropy = target_entropy\n    self.eps = eps\n\n    super().__init__(\n        weight=weight,\n        target_entropy=target_entropy,\n        eps=eps,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.SelectorEntropyRegularizer.forward","title":"forward","text":"<pre><code>forward(weights, **_)\n</code></pre> <p>Compute entropy regularization loss from selection weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Tensor</code> <p>Channel selection weights [n_channels]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing regularization loss</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, weights: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute entropy regularization loss from selection weights.\n\n    Parameters\n    ----------\n    weights : Tensor\n        Channel selection weights [n_channels]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing regularization loss\n    \"\"\"\n    # Normalize weights to probabilities\n    probs = weights / (weights.sum() + self.eps)\n\n    # Compute entropy: -sum(p * log(p))\n    entropy = -(probs * torch.log(probs + self.eps)).sum()\n\n    # Compute loss\n    if self.target_entropy is not None:\n        # Target-based regularization: minimize distance to target\n        loss = (entropy - self.target_entropy) ** 2\n    else:\n        # Simple regularization:\n        # maximize (positive weight) or minimize (negative weight) entropy\n        loss = -entropy\n\n    # Apply weight\n    return {\"loss\": self.weight * loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.SelectorDiversityRegularizer","title":"SelectorDiversityRegularizer","text":"<pre><code>SelectorDiversityRegularizer(weight=0.01, **kwargs)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Diversity regularization for SoftChannelSelector.</p> <p>Encourages diverse channel selection by penalizing concentration on few channels. Uses negative variance to encourage spread (higher variance = more diverse).</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Weight for diversity regularization (default: 0.01)</p> <code>0.01</code> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(self, weight: float = 0.01, **kwargs) -&gt; None:\n    self.weight = weight\n    super().__init__(\n        weight=weight,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.SelectorDiversityRegularizer.forward","title":"forward","text":"<pre><code>forward(weights, **_)\n</code></pre> <p>Compute weighted diversity loss from selection weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Tensor</code> <p>Channel selection weights [n_channels]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing weighted loss</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, weights: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute weighted diversity loss from selection weights.\n\n    Parameters\n    ----------\n    weights : Tensor\n        Channel selection weights [n_channels]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing weighted loss\n    \"\"\"\n    # Compute variance of weights (high variance = diverse selection)\n    mean_weight = weights.mean()\n    variance = ((weights - mean_weight) ** 2).mean()\n\n    # Return negative variance (minimizing loss = maximizing variance = maximizing diversity)\n    diversity_loss = -variance\n\n    return {\"loss\": self.weight * diversity_loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.DeepSVDDSoftBoundaryLoss","title":"DeepSVDDSoftBoundaryLoss","text":"<pre><code>DeepSVDDSoftBoundaryLoss(nu=0.05, weight=1.0, **kwargs)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Soft-boundary Deep SVDD objective operating on BHWD embeddings.</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(self, nu: float = 0.05, weight: float = 1.0, **kwargs) -&gt; None:\n    if not (0.0 &lt; nu &lt; 1.0):\n        raise ValueError(\"nu must be in (0, 1)\")\n    self.nu = float(nu)\n    self.weight = float(weight)\n\n    super().__init__(nu=self.nu, weight=self.weight, **kwargs)\n\n    self.r_unconstrained = nn.Parameter(torch.tensor(0.0))\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.DeepSVDDSoftBoundaryLoss.forward","title":"forward","text":"<pre><code>forward(embeddings, center, **_)\n</code></pre> <p>Compute Deep SVDD soft-boundary loss.</p> <p>The loss consists of the hypersphere radius R\u00b2 plus a slack penalty for points outside the hypersphere. The radius R is learned via an unconstrained parameter with softplus activation.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Tensor</code> <p>Embedded feature representations [B, H, W, D] from the network.</p> required <code>center</code> <code>Tensor</code> <p>Center of the hypersphere [D] computed during initialization.</p> required <code>**_</code> <code>Any</code> <p>Additional unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing the scalar loss value.</p> Notes <p>The loss formula is: loss = weight * (R\u00b2 + (1/\u03bd) * mean(ReLU(dist - R\u00b2))) where dist is the squared distance from embeddings to the center.</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, embeddings: Tensor, center: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute Deep SVDD soft-boundary loss.\n\n    The loss consists of the hypersphere radius R\u00b2 plus a slack penalty\n    for points outside the hypersphere. The radius R is learned via\n    an unconstrained parameter with softplus activation.\n\n    Parameters\n    ----------\n    embeddings : Tensor\n        Embedded feature representations [B, H, W, D] from the network.\n    center : Tensor\n        Center of the hypersphere [D] computed during initialization.\n    **_ : Any\n        Additional unused keyword arguments.\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing the scalar loss value.\n\n    Notes\n    -----\n    The loss formula is: loss = weight * (R\u00b2 + (1/\u03bd) * mean(ReLU(dist - R\u00b2)))\n    where dist is the squared distance from embeddings to the center.\n    \"\"\"\n    B, H, W, D = embeddings.shape\n    z = embeddings.reshape(B * H * W, D)\n    R = torch.nn.functional.softplus(self.r_unconstrained, beta=10.0)\n    dist = torch.sum((z - center.view(1, -1)) ** 2, dim=1)\n    slack = torch.relu(dist - R**2)\n    base_loss = R**2 + (1.0 / self.nu) * slack.mean()\n    loss = self.weight * base_loss\n\n    return {\"loss\": loss}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.IoULoss","title":"IoULoss","text":"<pre><code>IoULoss(\n    weight=1.0,\n    smooth=1e-06,\n    normalize_method=\"sigmoid\",\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>LossNode</code></p> <p>Differentiable IoU (Intersection over Union) loss.</p> <p>Computes: 1 - (|A \u2229 B| + smooth) / (|A U B| + smooth) Works directly on continuous scores (not binary decisions), preserving gradients.</p> <p>The scores are normalized to [0, 1] range using sigmoid or clamp before computing IoU, ensuring differentiability.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>float</code> <p>Overall weight for this loss component (default: 1.0)</p> <code>1.0</code> <code>smooth</code> <code>float</code> <p>Small constant for numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>normalize_method</code> <code>('sigmoid', 'clamp', 'minmax')</code> <p>Method to normalize predictions to [0, 1] range (default: \"sigmoid\") - \"sigmoid\": Apply sigmoid activation (good for unbounded scores) - \"clamp\": Clamp to [0, 1] (good for scores already in reasonable range) - \"minmax\": Min-max normalization per batch (good for varying score ranges)</p> <code>\"sigmoid\"</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; iou_loss = IoULoss(weight=1.0, smooth=1e-6)\n&gt;&gt;&gt; # Use with AdaClip scores directly (no thresholding needed)\n&gt;&gt;&gt; loss = iou_loss.forward(predictions=adaclip_scores, targets=ground_truth_mask)\n</code></pre> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def __init__(\n    self,\n    weight: float = 1.0,\n    smooth: float = 1e-6,\n    normalize_method: str = \"sigmoid\",\n    **kwargs,\n) -&gt; None:\n    self.weight = weight\n    self.smooth = smooth\n    self.normalize_method = normalize_method\n\n    if normalize_method not in [\"sigmoid\", \"clamp\", \"minmax\"]:\n        raise ValueError(\n            f\"normalize_method must be one of ['sigmoid', 'clamp', 'minmax'], got {normalize_method}\"\n        )\n\n    super().__init__(\n        weight=weight,\n        smooth=smooth,\n        normalize_method=normalize_method,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.losses.IoULoss.forward","title":"forward","text":"<pre><code>forward(predictions, targets, **_)\n</code></pre> <p>Compute differentiable IoU loss.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Tensor</code> <p>Predicted anomaly scores [B, H, W, 1] (any real values)</p> required <code>targets</code> <code>Tensor</code> <p>Ground truth binary masks [B, H, W, 1]</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary with \"loss\" key containing scalar IoU loss</p> Source code in <code>cuvis_ai/node/losses.py</code> <pre><code>def forward(self, predictions: Tensor, targets: Tensor, **_: Any) -&gt; dict[str, Tensor]:\n    \"\"\"Compute differentiable IoU loss.\n\n    Parameters\n    ----------\n    predictions : Tensor\n        Predicted anomaly scores [B, H, W, 1] (any real values)\n    targets : Tensor\n        Ground truth binary masks [B, H, W, 1]\n\n    Returns\n    -------\n    dict[str, Tensor]\n        Dictionary with \"loss\" key containing scalar IoU loss\n    \"\"\"\n    # Normalize predictions to [0, 1] range based on method\n    if self.normalize_method == \"sigmoid\":\n        # Sigmoid: good for unbounded scores (e.g., logits)\n        pred = torch.sigmoid(predictions)\n    elif self.normalize_method == \"clamp\":\n        # Clamp: good for scores already in reasonable range\n        pred = torch.clamp(predictions, 0.0, 1.0)\n    elif self.normalize_method == \"minmax\":\n        # Min-max normalization per batch\n        pred_min = predictions.min()\n        pred_max = predictions.max()\n        if pred_max &gt; pred_min:\n            pred = (predictions - pred_min) / (pred_max - pred_min + self.smooth)\n        else:\n            pred = torch.ones_like(predictions) * 0.5\n    else:\n        raise ValueError(f\"Unknown normalize_method: {self.normalize_method}\")\n\n    # Convert targets to float\n    target = targets.float()\n\n    # Flatten for computation\n    pred_flat = pred.view(-1)  # [B*H*W]\n    target_flat = target.view(-1)  # [B*H*W]\n\n    # Compute IoU: intersection / union\n    # intersection = |A \u2229 B| = sum(pred * target)\n    # union = |A \u222a B| = sum(pred) + sum(target) - intersection\n    intersection = (pred_flat * target_flat).sum()\n    union = pred_flat.sum() + target_flat.sum() - intersection\n\n    # IoU coefficient\n    iou = (intersection + self.smooth) / (union + self.smooth)\n\n    # IoU loss: 1 - IoU (minimize loss = maximize IoU)\n    loss = 1.0 - iou\n\n    return {\"loss\": self.weight * loss}\n</code></pre>"},{"location":"api/training/#metrics","title":"Metrics","text":""},{"location":"api/training/#cuvis_ai.node.metrics","title":"metrics","text":"<p>Metric nodes for training pipeline (port-based architecture).</p>"},{"location":"api/training/#cuvis_ai.node.metrics.ExplainedVarianceMetric","title":"ExplainedVarianceMetric","text":"<pre><code>ExplainedVarianceMetric(execution_stages=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Track explained variance ratio for PCA components.</p> <p>Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.ExplainedVarianceMetric.forward","title":"forward","text":"<pre><code>forward(explained_variance_ratio, context)\n</code></pre> <p>Compute explained variance metrics.</p> <p>Parameters:</p> Name Type Description Default <code>explained_variance_ratio</code> <code>Tensor</code> <p>Explained variance ratios from PCA node</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(self, explained_variance_ratio: Tensor, context: Context) -&gt; dict[str, Any]:\n    \"\"\"Compute explained variance metrics.\n\n    Parameters\n    ----------\n    explained_variance_ratio : Tensor\n        Explained variance ratios from PCA node\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    metrics = []\n\n    # Per-component variance\n    for i, ratio in enumerate(explained_variance_ratio):\n        metrics.append(\n            Metric(\n                name=f\"explained_variance_pc{i + 1}\",\n                value=ratio.item(),\n                stage=context.stage,\n                epoch=context.epoch,\n                batch_idx=context.batch_idx,\n            )\n        )\n\n    # Total variance explained\n    metrics.append(\n        Metric(\n            name=\"total_explained_variance\",\n            value=explained_variance_ratio.sum().item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        )\n    )\n\n    # Cumulative variance\n    cumulative = torch.cumsum(explained_variance_ratio, dim=0)\n    for i, cum_var in enumerate(cumulative):\n        metrics.append(\n            Metric(\n                name=f\"cumulative_variance_pc{i + 1}\",\n                value=cum_var.item(),\n                stage=context.stage,\n                epoch=context.epoch,\n                batch_idx=context.batch_idx,\n            )\n        )\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.AnomalyDetectionMetrics","title":"AnomalyDetectionMetrics","text":"<pre><code>AnomalyDetectionMetrics(execution_stages=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Compute anomaly detection metrics (precision, recall, F1, etc.).</p> <p>Uses torchmetrics for GPU-optimized, robust metric computation. Expects binary decisions and targets to be binary masks. Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        **kwargs,\n    )\n\n    # Initialize torchmetrics for binary classification\n    # These are stateless (compute per-batch) since we don't call update()\n    self.precision_metric = BinaryPrecision()\n    self.recall_metric = BinaryRecall()\n    self.f1_metric = BinaryF1Score()\n    self.iou_metric = BinaryJaccardIndex()\n    self.average_precision_metric = BinaryAveragePrecision()\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.AnomalyDetectionMetrics.forward","title":"forward","text":"<pre><code>forward(decisions, targets, context, logits=None)\n</code></pre> <p>Compute anomaly detection metrics using torchmetrics.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>Tensor</code> <p>Binary anomaly decisions [B, H, W, 1]</p> required <code>targets</code> <code>Tensor</code> <p>Ground truth binary masks [B, H, W, 1]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(\n    self,\n    decisions: Tensor,\n    targets: Tensor,\n    context: Context,\n    logits: Tensor | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Compute anomaly detection metrics using torchmetrics.\n\n    Parameters\n    ----------\n    decisions : Tensor\n        Binary anomaly decisions [B, H, W, 1]\n    targets : Tensor\n        Ground truth binary masks [B, H, W, 1]\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    # Ensure consistent shapes and flatten spatial dimensions\n    decisions = decisions.squeeze(-1)  # [B, H, W]\n    targets = targets.squeeze(-1)  # [B, H, W]\n\n    # Flatten to [N] where N = B*H*W for torchmetrics\n    preds_flat = decisions.flatten()  # [B*H*W]\n    targets_flat = targets.flatten()  # [B*H*W]\n\n    # Compute metrics using torchmetrics (they handle edge cases robustly)\n    precision = self.precision_metric(preds_flat, targets_flat)\n    recall = self.recall_metric(preds_flat, targets_flat)\n    f1 = self.f1_metric(preds_flat, targets_flat)\n    iou = self.iou_metric(preds_flat, targets_flat)\n\n    metrics = [\n        Metric(\n            name=\"precision\",\n            value=precision.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"recall\",\n            value=recall.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"f1_score\",\n            value=f1.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"iou\",\n            value=iou.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n    ]\n\n    if logits is not None:\n        raw_scores = logits.squeeze(-1).flatten().float()\n        probs_for_ap = torch.sigmoid(raw_scores)\n        average_precision = self.average_precision_metric(probs_for_ap, targets_flat)\n\n        metrics.append(\n            Metric(\n                name=\"average_precision\",\n                value=average_precision.item(),\n                stage=context.stage,\n                epoch=context.epoch,\n                batch_idx=context.batch_idx,\n            )\n        )\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.ScoreStatisticsMetric","title":"ScoreStatisticsMetric","text":"<pre><code>ScoreStatisticsMetric(execution_stages=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Compute statistical properties of score distributions.</p> <p>Tracks mean, std, min, max, median, and quantiles of scores. Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.ScoreStatisticsMetric.forward","title":"forward","text":"<pre><code>forward(scores, context)\n</code></pre> <p>Compute score statistics.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>Score values [B, H, W]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(self, scores: Tensor, context: Context) -&gt; dict[str, Any]:\n    \"\"\"Compute score statistics.\n\n    Parameters\n    ----------\n    scores : Tensor\n        Score values [B, H, W]\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    # Flatten scores\n    scores_flat = scores.reshape(-1)\n\n    metrics = [\n        Metric(\n            name=\"scores/mean\",\n            value=scores_flat.mean().item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/std\",\n            value=scores_flat.std().item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/min\",\n            value=scores_flat.min().item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/max\",\n            value=scores_flat.max().item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/median\",\n            value=scores_flat.median().item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/q25\",\n            value=torch.quantile(scores_flat, 0.25).item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/q75\",\n            value=torch.quantile(scores_flat, 0.75).item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/q95\",\n            value=torch.quantile(scores_flat, 0.95).item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"scores/q99\",\n            value=torch.quantile(scores_flat, 0.99).item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n    ]\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.ComponentOrthogonalityMetric","title":"ComponentOrthogonalityMetric","text":"<pre><code>ComponentOrthogonalityMetric(\n    execution_stages=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Track orthogonality of PCA components during training.</p> <p>Measures how close the component matrix is to being orthonormal. Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.ComponentOrthogonalityMetric.forward","title":"forward","text":"<pre><code>forward(components, context)\n</code></pre> <p>Compute component orthogonality metrics.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Tensor</code> <p>PCA components matrix [n_components, n_features]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(self, components: Tensor, context: Context) -&gt; dict[str, Any]:\n    \"\"\"Compute component orthogonality metrics.\n\n    Parameters\n    ----------\n    components : Tensor\n        PCA components matrix [n_components, n_features]\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    # Compute gram matrix: W @ W.T\n    gram = components @ components.T\n    n = components.shape[0]\n\n    # Target: identity matrix\n    eye = torch.eye(n, device=components.device, dtype=components.dtype)\n\n    # Frobenius norm of difference\n    orth_error = torch.norm(gram - eye, p=\"fro\").item()\n\n    # Average absolute deviation from identity\n    avg_off_diagonal = (gram - eye).abs().mean().item()\n\n    # Diagonal elements (should be close to 1)\n    diagonal = torch.diagonal(gram)\n    diagonal_mean = diagonal.mean().item()\n    diagonal_std = diagonal.std().item()\n\n    metrics = [\n        Metric(\n            name=\"orthogonality_error\",\n            value=orth_error,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"avg_off_diagonal\",\n            value=avg_off_diagonal,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"diagonal_mean\",\n            value=diagonal_mean,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"diagonal_std\",\n            value=diagonal_std,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n    ]\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.SelectorEntropyMetric","title":"SelectorEntropyMetric","text":"<pre><code>SelectorEntropyMetric(\n    eps=1e-06, execution_stages=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Track entropy of channel selection distribution.</p> <p>Measures the uncertainty/diversity in channel selection weights. Higher entropy indicates more uniform selection (less confident). Lower entropy indicates more peaked selection (more confident).</p> <p>Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    eps: float = 1e-6,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    self.eps = eps\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        eps=eps,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.SelectorEntropyMetric.forward","title":"forward","text":"<pre><code>forward(weights, context)\n</code></pre> <p>Compute entropy of selection weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Tensor</code> <p>Channel selection weights [n_channels]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(self, weights: Tensor, context: Context) -&gt; dict[str, Any]:\n    \"\"\"Compute entropy of selection weights.\n\n    Parameters\n    ----------\n    weights : Tensor\n        Channel selection weights [n_channels]\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    # Normalize weights to probabilities\n    probs = weights / (weights.sum() + self.eps)\n\n    # Compute entropy: -sum(p * log(p))\n    entropy = -(probs * torch.log(probs + self.eps)).sum()\n\n    metrics = [\n        Metric(\n            name=\"selector/entropy\",\n            value=entropy.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n    ]\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.SelectorDiversityMetric","title":"SelectorDiversityMetric","text":"<pre><code>SelectorDiversityMetric(execution_stages=None, **kwargs)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Track diversity of channel selection.</p> <p>Measures how spread out the selection weights are across channels. Uses Gini coefficient - lower values indicate more diverse selection.</p> <p>Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.SelectorDiversityMetric.forward","title":"forward","text":"<pre><code>forward(weights, context)\n</code></pre> <p>Compute diversity metrics for selection weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Tensor</code> <p>Channel selection weights [n_channels]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(self, weights: Tensor, context: Context) -&gt; dict[str, Any]:\n    \"\"\"Compute diversity metrics for selection weights.\n\n    Parameters\n    ----------\n    weights : Tensor\n        Channel selection weights [n_channels]\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    # Compute variance (measure of spread)\n    mean_weight = weights.mean()\n    variance = ((weights - mean_weight) ** 2).mean()\n\n    # Compute Gini coefficient (0 = perfect equality, 1 = perfect inequality)\n    # Lower Gini = more diverse selection\n    sorted_weights, _ = torch.sort(weights)\n    n = len(sorted_weights)\n    index = torch.arange(1, n + 1, device=weights.device, dtype=weights.dtype)\n    gini = (2 * (sorted_weights * index).sum()) / (n * sorted_weights.sum()) - (n + 1) / n\n\n    metrics = [\n        Metric(\n            name=\"weight_variance\",\n            value=variance.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"gini_coefficient\",\n            value=gini.item(),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n    ]\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.AnomalyPixelStatisticsMetric","title":"AnomalyPixelStatisticsMetric","text":"<pre><code>AnomalyPixelStatisticsMetric(\n    execution_stages=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>Compute anomaly pixel statistics from binary decisions.</p> <p>Calculates total pixels, anomalous pixels count, and anomaly percentage. Useful for monitoring the proportion of detected anomalies in batches. Executes only during validation and test stages.</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def __init__(\n    self,\n    execution_stages: set[ExecutionStage] | None = None,\n    **kwargs,\n) -&gt; None:\n    name, execution_stages = Node.consume_base_kwargs(\n        kwargs, execution_stages or {ExecutionStage.VAL, ExecutionStage.TEST}\n    )\n    super().__init__(\n        name=name,\n        execution_stages=execution_stages,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.metrics.AnomalyPixelStatisticsMetric.forward","title":"forward","text":"<pre><code>forward(decisions, context)\n</code></pre> <p>Compute anomaly pixel statistics.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>Tensor</code> <p>Binary anomaly decisions [B, H, W, 1]</p> required <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with \"metrics\" key containing list of Metric objects</p> Source code in <code>cuvis_ai/node/metrics.py</code> <pre><code>def forward(self, decisions: Tensor, context: Context) -&gt; dict[str, Any]:\n    \"\"\"Compute anomaly pixel statistics.\n\n    Parameters\n    ----------\n    decisions : Tensor\n        Binary anomaly decisions [B, H, W, 1]\n    context : Context\n        Execution context with stage, epoch, batch_idx\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary with \"metrics\" key containing list of Metric objects\n    \"\"\"\n    # Calculate statistics\n    total_pixels = decisions.numel()\n    anomalous_pixels = int(decisions.sum().item())\n    anomaly_percentage = (anomalous_pixels / total_pixels) * 100 if total_pixels &gt; 0 else 0.0\n\n    metrics = [\n        Metric(\n            name=\"anomaly/total_pixels\",\n            value=float(total_pixels),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"anomaly/anomalous_pixels\",\n            value=float(anomalous_pixels),\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n        Metric(\n            name=\"anomaly/anomaly_percentage\",\n            value=anomaly_percentage,\n            stage=context.stage,\n            epoch=context.epoch,\n            batch_idx=context.batch_idx,\n        ),\n    ]\n\n    return {\"metrics\": metrics}\n</code></pre>"},{"location":"api/training/#monitoring","title":"Monitoring","text":""},{"location":"api/training/#cuvis_ai.node.monitor","title":"monitor","text":"<p>TensorBoard Monitoring Nodes.</p> <p>This module provides nodes for logging artifacts and metrics to TensorBoard during pipeline execution. The monitoring nodes are sink nodes that accept artifacts (visualizations) and metrics from upstream nodes and write them to TensorBoard logs for visualization and analysis.</p> <p>The primary use case is logging training and validation metrics, along with visualizations like heatmaps, RGB renderings, and PCA plots during model training.</p> See Also <p>cuvis_ai.node.visualizations : Nodes that generate artifacts for monitoring</p>"},{"location":"api/training/#cuvis_ai.node.monitor.TensorBoardMonitorNode","title":"TensorBoardMonitorNode","text":"<pre><code>TensorBoardMonitorNode(\n    output_dir=\"./runs\",\n    run_name=None,\n    comment=\"\",\n    flush_secs=120,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>Node</code></p> <p>TensorBoard monitoring node for logging artifacts and metrics.</p> <p>This is a SINK node that logs visualizations (artifacts) and metrics to TensorBoard. Accepts optional inputs for artifacts and metrics, allowing predecessors to be filtered by execution_stage without causing errors.</p> <p>Executes during all stages (ALWAYS).</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory for TensorBoard logs (default: \"./runs\")</p> <code>'./runs'</code> <code>comment</code> <code>str</code> <p>Comment to append to log directory name (default: \"\")</p> <code>''</code> <code>flush_secs</code> <code>int</code> <p>How often to flush pending events to disk (default: 120)</p> <code>120</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; heatmap_viz = AnomalyHeatmap(cmap='hot', up_to=10)\n&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; graph.connect(\n...     (heatmap_viz.artifacts, tensorboard_node.artifacts),\n... )\n</code></pre> Source code in <code>cuvis_ai/node/monitor.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str = \"./runs\",\n    run_name: str | None = None,\n    comment: str = \"\",\n    flush_secs: int = 120,\n    **kwargs,\n) -&gt; None:\n    self.output_dir = Path(output_dir)\n    self.run_name = run_name\n    self.comment = comment\n    self.flush_secs = flush_secs\n    self._writer = None\n    self._tensorboard_available = False\n\n    super().__init__(\n        execution_stages={ExecutionStage.ALWAYS},\n        output_dir=str(output_dir),\n        run_name=run_name,\n        comment=comment,\n        flush_secs=flush_secs,\n        **kwargs,\n    )\n\n    # Check if tensorboard is available\n\n    self._SummaryWriter = SummaryWriter\n\n    # Determine the log directory with run name\n    self.log_dir = self._resolve_log_dir()\n\n    # Initialize TensorBoard writer\n    self.log_dir.mkdir(parents=True, exist_ok=True)\n    self._writer = self._SummaryWriter(\n        log_dir=str(self.log_dir),\n        comment=self.comment,\n        flush_secs=self.flush_secs,\n    )\n    logger.info(f\"TensorBoard writer initialized: {self.log_dir}\")\n    logger.info(f\"To view visualizations, run: uv run tensorboard --logdir={self.output_dir}\")\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.monitor.TensorBoardMonitorNode.forward","title":"forward","text":"<pre><code>forward(artifacts=None, metrics=None, context=None)\n</code></pre> <p>Log artifacts and metrics to TensorBoard.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>Execution context with stage, epoch, batch_idx, global_step</p> <code>None</code> <code>artifacts</code> <code>list[Artifact]</code> <p>List of artifacts to log (default: None)</p> <code>None</code> <code>metrics</code> <code>list[Metric]</code> <p>List of metrics to log (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Empty dict (sink node has no outputs)</p> Source code in <code>cuvis_ai/node/monitor.py</code> <pre><code>def forward(\n    self,\n    artifacts: list[Artifact] | None = None,\n    metrics: list[Metric] | None = None,\n    context: Context | None = None,\n) -&gt; dict:\n    \"\"\"Log artifacts and metrics to TensorBoard.\n\n    Parameters\n    ----------\n    context : Context\n        Execution context with stage, epoch, batch_idx, global_step\n    artifacts : list[Artifact], optional\n        List of artifacts to log (default: None)\n    metrics : list[Metric], optional\n        List of metrics to log (default: None)\n\n    Returns\n    -------\n    dict\n        Empty dict (sink node has no outputs)\n    \"\"\"\n    if context is None:\n        context = Context()\n\n    stage = context.stage.value\n    step = context.global_step\n\n    # Flatten artifacts if it's a list of lists (variadic port)\n    if artifacts is not None:\n        if (\n            isinstance(artifacts, list)\n            and len(artifacts) &gt; 0\n            and isinstance(artifacts[0], list)\n        ):\n            artifacts = [item for sublist in artifacts for item in sublist]\n\n    # Log artifacts\n    if artifacts is not None:\n        for artifact in artifacts:\n            self._log_artifact(artifact, stage, step)\n        logger.debug(f\"Logged {len(artifacts)} artifacts to TensorBoard at step {step}\")\n\n    # Flatten metrics if variadic input provided\n    if (\n        metrics is not None\n        and isinstance(metrics, list)\n        and metrics\n        and isinstance(metrics[0], list)\n    ):\n        metrics = [item for sublist in metrics for item in sublist]\n\n    # Log metrics\n    if metrics is not None:\n        for metric in metrics:\n            self._log_metric(metric, stage, step)\n        logger.debug(f\"Logged {len(metrics)} metrics to TensorBoard at step {step}\")\n\n    return {}\n</code></pre>"},{"location":"api/training/#cuvis_ai.node.monitor.TensorBoardMonitorNode.log","title":"log","text":"<pre><code>log(name, value, step)\n</code></pre> <p>Log a scalar value to TensorBoard.</p> <p>This method provides a simple interface for external trainers to log metrics directly, complementing the port-based logging. Used by GradientTrainer to log train/val losses to the same TensorBoard directory as graph metrics and artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name/tag for the scalar (e.g., \"train/loss\", \"val/accuracy\")</p> required <code>value</code> <code>float</code> <p>Scalar value to log</p> required <code>step</code> <code>int</code> <p>Global step number</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n&gt;&gt;&gt; # From external trainer\n&gt;&gt;&gt; tensorboard_node.log(\"train/loss\", 0.5, step=100)\n</code></pre> Source code in <code>cuvis_ai/node/monitor.py</code> <pre><code>def log(self, name: str, value: float, step: int) -&gt; None:\n    \"\"\"Log a scalar value to TensorBoard.\n\n    This method provides a simple interface for external trainers\n    to log metrics directly, complementing the port-based logging.\n    Used by GradientTrainer to log train/val losses to the same\n    TensorBoard directory as graph metrics and artifacts.\n\n    Parameters\n    ----------\n    name : str\n        Name/tag for the scalar (e.g., \"train/loss\", \"val/accuracy\")\n    value : float\n        Scalar value to log\n    step : int\n        Global step number\n\n    Examples\n    --------\n    &gt;&gt;&gt; tensorboard_node = TensorBoardMonitorNode(output_dir=\"./runs\")\n    &gt;&gt;&gt; # From external trainer\n    &gt;&gt;&gt; tensorboard_node.log(\"train/loss\", 0.5, step=100)\n    \"\"\"\n    self._writer.add_scalar(name, value, step)\n</code></pre>"},{"location":"api/training/#related-pages","title":"Related Pages","text":"<ul> <li>Two-Phase Training</li> <li>Execution Stages</li> <li>Loss &amp; Metrics Catalog</li> </ul>"},{"location":"api/utilities/","title":"Utilities","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"api/utilities/#utilities-api","title":"Utilities API","text":"<p>Helper functions and utilities for CUVIS.AI.</p>"},{"location":"api/utilities/#overview","title":"Overview","text":"<p>This page documents utility modules that provide helper functions for various tasks.</p>"},{"location":"api/utilities/#deep-svdd-factory","title":"Deep SVDD Factory","text":"<p>Utility functions for Deep SVDD channel configuration and model setup.</p>"},{"location":"api/utilities/#cuvis_ai.utils.deep_svdd_factory","title":"deep_svdd_factory","text":"<p>Deep SVDD Channel Configuration Utilities.</p> <p>This module provides utilities for inferring channel counts after bandpass filtering for Deep SVDD networks. This is useful for automatically configuring network architectures based on the data pipeline's preprocessing steps.</p> See Also <p>cuvis_ai.node.preprocessors : Bandpass filtering nodes cuvis_ai.anomaly.deep_svdd : Deep SVDD anomaly detection</p>"},{"location":"api/utilities/#cuvis_ai.utils.deep_svdd_factory.ChannelConfig","title":"ChannelConfig  <code>dataclass</code>","text":"<pre><code>ChannelConfig(num_channels, in_channels)\n</code></pre> <p>Configuration for network channel counts.</p> <p>Stores the number of input and output channels for network layers, typically determined after bandpass filtering.</p> <p>Attributes:</p> Name Type Description <code>num_channels</code> <code>int</code> <p>Total number of channels in the network.</p> <code>in_channels</code> <code>int</code> <p>Number of input channels to the network.</p>"},{"location":"api/utilities/#cuvis_ai.utils.deep_svdd_factory.infer_channels_after_bandpass","title":"infer_channels_after_bandpass","text":"<pre><code>infer_channels_after_bandpass(datamodule, bandpass_cfg)\n</code></pre> <p>Infer post-bandpass channel count from a sample batch.</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <code>object</code> <p>Datamodule with a train_dataloader() method returning batches with \"wavelengths\".</p> required <code>bandpass_cfg</code> <code>object</code> <p>Config with min_wavelength_nm and max_wavelength_nm fields.</p> required <p>Returns:</p> Type Description <code>ChannelConfig</code> <p>num_channels and in_channels set to the filtered channel count.</p> Source code in <code>cuvis_ai/utils/deep_svdd_factory.py</code> <pre><code>def infer_channels_after_bandpass(datamodule, bandpass_cfg) -&gt; ChannelConfig:\n    \"\"\"Infer post-bandpass channel count from a sample batch.\n\n    Parameters\n    ----------\n    datamodule : object\n        Datamodule with a train_dataloader() method returning batches with \"wavelengths\".\n    bandpass_cfg : object\n        Config with min_wavelength_nm and max_wavelength_nm fields.\n\n    Returns\n    -------\n    ChannelConfig\n        num_channels and in_channels set to the filtered channel count.\n    \"\"\"\n    sample_batch = next(iter(datamodule.train_dataloader()))\n    wavelengths = sample_batch[\"wavelengths\"]\n    keep_mask = wavelengths &gt;= bandpass_cfg.min_wavelength_nm\n    if bandpass_cfg.max_wavelength_nm is not None:\n        keep_mask = keep_mask &amp; (wavelengths &lt;= bandpass_cfg.max_wavelength_nm)\n    num_channels_after_bandpass = int(keep_mask.sum().item())\n    return ChannelConfig(\n        num_channels=num_channels_after_bandpass, in_channels=num_channels_after_bandpass\n    )\n</code></pre>"},{"location":"api/utilities/#visualization-helpers","title":"Visualization Helpers","text":"<p>Helper functions for creating visualizations.</p>"},{"location":"api/utilities/#cuvis_ai.utils.vis_helpers","title":"vis_helpers","text":"<p>Visualization helper utilities for converting figures and tensors to arrays.</p>"},{"location":"api/utilities/#cuvis_ai.utils.vis_helpers.fig_to_array","title":"fig_to_array","text":"<pre><code>fig_to_array(fig, dpi=150)\n</code></pre> <p>Convert matplotlib figure to numpy array in RGB format.</p> <p>This utility handles the conversion of a matplotlib figure to a numpy array by saving it to a BytesIO buffer, loading it with PIL, and converting to a numpy array. The figure is automatically closed after conversion.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The matplotlib figure to convert</p> required <code>dpi</code> <code>int</code> <p>Resolution for the saved image (default: 150)</p> <code>150</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>RGB image as numpy array with shape (H, W, 3) and dtype uint8</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; ax.plot([1, 2, 3], [1, 4, 9])\n&gt;&gt;&gt; img_array = fig_to_array(fig, dpi=150)\n&gt;&gt;&gt; img_array.shape\n(height, width, 3)\n</code></pre> Source code in <code>cuvis_ai/utils/vis_helpers.py</code> <pre><code>def fig_to_array(fig: matplotlib.figure.Figure, dpi: int = 150) -&gt; np.ndarray:\n    \"\"\"Convert matplotlib figure to numpy array in RGB format.\n\n    This utility handles the conversion of a matplotlib figure to a numpy array\n    by saving it to a BytesIO buffer, loading it with PIL, and converting to\n    a numpy array. The figure is automatically closed after conversion.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure\n        The matplotlib figure to convert\n    dpi : int, optional\n        Resolution for the saved image (default: 150)\n\n    Returns\n    -------\n    np.ndarray\n        RGB image as numpy array with shape (H, W, 3) and dtype uint8\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; ax.plot([1, 2, 3], [1, 4, 9])\n    &gt;&gt;&gt; img_array = fig_to_array(fig, dpi=150)\n    &gt;&gt;&gt; img_array.shape\n    (height, width, 3)\n    \"\"\"\n    buf = BytesIO()\n    fig.savefig(buf, format=\"png\", dpi=dpi, bbox_inches=\"tight\")\n    buf.seek(0)\n    img = Image.open(buf)\n    img_array = np.array(img.convert(\"RGB\"))\n    buf.close()\n\n    # Close the figure to free memory\n    import matplotlib.pyplot as plt\n\n    plt.close(fig)\n\n    return img_array\n</code></pre>"},{"location":"api/utilities/#cuvis_ai.utils.vis_helpers.tensor_to_uint8","title":"tensor_to_uint8","text":"<pre><code>tensor_to_uint8(tensor)\n</code></pre> <p>Convert float tensor [0, 1] to uint8 [0, 255].</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor with values in [0, 1]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor converted to uint8 in range [0, 255], stays on original device</p> Source code in <code>cuvis_ai/utils/vis_helpers.py</code> <pre><code>def tensor_to_uint8(tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert float tensor [0, 1] to uint8 [0, 255].\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Input tensor with values in [0, 1]\n\n    Returns\n    -------\n    torch.Tensor\n        Tensor converted to uint8 in range [0, 255], stays on original device\n    \"\"\"\n    return (tensor.clamp(0, 1) * 255).to(torch.uint8)\n</code></pre>"},{"location":"api/utilities/#cuvis_ai.utils.vis_helpers.tensor_to_numpy","title":"tensor_to_numpy","text":"<pre><code>tensor_to_numpy(tensor)\n</code></pre> <p>Convert torch tensor to numpy array on CPU.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Input tensor (can be on any device)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array representation</p> Source code in <code>cuvis_ai/utils/vis_helpers.py</code> <pre><code>def tensor_to_numpy(tensor: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"Convert torch tensor to numpy array on CPU.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Input tensor (can be on any device)\n\n    Returns\n    -------\n    np.ndarray\n        Numpy array representation\n    \"\"\"\n    return tensor.detach().cpu().numpy()\n</code></pre>"},{"location":"api/utilities/#related-pages","title":"Related Pages","text":"<ul> <li>Utility Nodes</li> </ul>"},{"location":"concepts/execution-stages/","title":"Execution Stages","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"concepts/execution-stages/#execution-stages","title":"Execution Stages","text":"<p>Control when nodes execute during pipeline operation via stage-aware graph execution.</p> <p>Execution stages enable conditional node execution based on context (training, validation, testing, or inference). Essential for loss nodes (training only), metrics (validation/test only), and optimization-specific operations.</p> <p>Key capabilities:</p> <ul> <li>Five execution stages: ALWAYS, TRAIN, VAL, TEST, INFERENCE</li> <li>Stage-aware filtering in pipeline executor</li> <li>Context object carries stage, epoch, batch_idx, global_step</li> <li>Default to ALWAYS for core processing nodes</li> <li>Restrict stages only when needed</li> </ul>"},{"location":"concepts/execution-stages/#stage-system-overview","title":"Stage System Overview","text":"<p>Every node specifies execution stages via the <code>execution_stages</code> parameter:</p> <pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.enums import ExecutionStage\n\nclass MyNode(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            execution_stages={ExecutionStage.TRAIN, ExecutionStage.VAL},\n            **kwargs\n        )\n</code></pre> <p>Key Concepts:</p> Concept Description ExecutionStage Enum ALWAYS, TRAIN, VAL, TEST, INFERENCE execution_stages Parameter Set of stages when node should execute Context Object Runtime info (stage, epoch, batch_idx) passed to nodes Stage Filtering Pipeline skips nodes not matching current stage Default Nodes default to <code>{ExecutionStage.ALWAYS}</code>"},{"location":"concepts/execution-stages/#execution-flow","title":"Execution Flow","text":"<pre><code>graph LR\n    A[Pipeline.forward&lt;br/&gt;context.stage=TRAIN] --&gt; B{For Each Node}\n    B --&gt; C{Node.execution_stages&lt;br/&gt;contains stage?}\n    C --&gt;|Yes| D[Execute Node]\n    C --&gt;|No| E[Skip Node]\n    D --&gt; F[Continue]\n    E --&gt; F\n    style D fill:#d4edda\n    style E fill:#f8d7da</code></pre>"},{"location":"concepts/execution-stages/#the-five-execution-stages","title":"The Five Execution Stages","text":""},{"location":"concepts/execution-stages/#1-always-default","title":"1. ALWAYS (Default)","text":"<p>Node executes unconditionally in all stages.</p> <p>Use Cases: - Core data transformation - Feature extraction - Essential preprocessing</p> <pre><code>class FeatureExtractor(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.ALWAYS}, **kwargs)\n\n    def forward(self, data, **_):\n        # Runs during TRAIN, VAL, TEST, INFERENCE\n        features = self.extract_features(data)\n        return {\"features\": features}\n\n# Default behavior\nnode1 = MyNode()  # Equivalent to ExecutionStage.ALWAYS\n</code></pre>"},{"location":"concepts/execution-stages/#2-train","title":"2. TRAIN","text":"<p>Node only executes during training.</p> <p>Use Cases: - Training-specific data augmentation - Dropout layers - Training loss computation</p> <pre><code>class TrainingAugmentation(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.TRAIN}, **kwargs)\n\n    def forward(self, image, **_):\n        # Only applies during training\n        image = self.random_flip(image)\n        image = self.color_jitter(image)\n        return {\"augmented\": image}\n</code></pre>"},{"location":"concepts/execution-stages/#3-val-validation","title":"3. VAL (Validation)","text":"<p>Node only executes during validation.</p> <p>Use Cases: - Validation metrics - Validation visualizations - Model selection criteria</p> <pre><code>class ValidationMetrics(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.VAL}, **kwargs)\n\n    def forward(self, predictions, targets, context, **_):\n        accuracy = (predictions.argmax(dim=1) == targets).float().mean()\n        return {\"metrics\": [\n            Metric(name=\"val/accuracy\", value=float(accuracy), stage=context.stage)\n        ]}\n</code></pre>"},{"location":"concepts/execution-stages/#4-test","title":"4. TEST","text":"<p>Node only executes during testing.</p> <p>Use Cases: - Final test metrics - Performance benchmarking - Confusion matrices</p> <pre><code>class TestEvaluator(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.TEST}, **kwargs)\n\n    def forward(self, predictions, ground_truth, **_):\n        test_results = self.compute_comprehensive_metrics(predictions, ground_truth)\n        return {\"test_results\": test_results}\n</code></pre>"},{"location":"concepts/execution-stages/#5-inference","title":"5. INFERENCE","text":"<p>Node only executes during inference/prediction.</p> <p>Use Cases: - Production-only post-processing - Inference-specific output formatting - Deployment-specific optimizations</p> <pre><code>class InferencePostProcessor(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.INFERENCE}, **kwargs)\n\n    def forward(self, raw_output, **_):\n        probabilities = torch.softmax(raw_output, dim=-1)\n        top_k_probs, top_k_indices = torch.topk(probabilities, k=5, dim=-1)\n\n        return {\"formatted_results\": {\n            \"probabilities\": top_k_probs.tolist(),\n            \"class_indices\": top_k_indices.tolist(),\n        }}\n</code></pre>"},{"location":"concepts/execution-stages/#stage-aware-node-patterns","title":"Stage-Aware Node Patterns","text":""},{"location":"concepts/execution-stages/#pattern-1-single-stage","title":"Pattern 1: Single Stage","text":"<pre><code>class TrainOnlyAugmentation(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.TRAIN}, **kwargs)\n</code></pre>"},{"location":"concepts/execution-stages/#pattern-2-multiple-stages","title":"Pattern 2: Multiple Stages","text":"<pre><code>class MetricsNode(Node):\n    def __init__(self, **kwargs):\n        # Execute during validation and test only\n        super().__init__(\n            execution_stages={ExecutionStage.VAL, ExecutionStage.TEST},\n            **kwargs\n        )\n</code></pre>"},{"location":"concepts/execution-stages/#pattern-3-training-aware-common-for-loss","title":"Pattern 3: Training-Aware (Common for Loss)","text":"<pre><code>from cuvis_ai.node.losses import LossNode\n\nclass MyLoss(LossNode):\n    # Auto-configured to {TRAIN, VAL, TEST}\n    def forward(self, predictions, targets, **_):\n        return {\"loss\": self.compute_loss(predictions, targets)}\n</code></pre>"},{"location":"concepts/execution-stages/#pattern-4-stage-conditional-behavior","title":"Pattern 4: Stage-Conditional Behavior","text":"<pre><code>class AdaptiveNormalizer(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            execution_stages={ExecutionStage.TRAIN, ExecutionStage.INFERENCE},\n            **kwargs\n        )\n\n    def forward(self, data, context: Context, **_):\n        if context.stage == ExecutionStage.TRAIN:\n            self.update_running_stats(data)\n        normalized = self.normalize(data)\n        return {\"normalized\": normalized}\n</code></pre>"},{"location":"concepts/execution-stages/#pipeline-execution-with-context","title":"Pipeline Execution with Context","text":""},{"location":"concepts/execution-stages/#context-object","title":"Context Object","text":"<p>Runtime information passed to pipeline.</p> <pre><code>from cuvis_ai_schemas.enums import ExecutionStage\nfrom cuvis_ai_schemas.execution import Context\n\ncontext = Context(\n    stage=ExecutionStage.TRAIN,  # Current stage\n    epoch=5,                     # Training epoch\n    batch_idx=42,                # Batch index\n    global_step=1337             # Global training step\n)\n</code></pre> <p>Attributes:</p> Attribute Type Description <code>stage</code> <code>ExecutionStage</code> Current stage <code>epoch</code> <code>int</code> Training epoch (0-indexed) <code>batch_idx</code> <code>int</code> Batch index in epoch (0-indexed) <code>global_step</code> <code>int</code> Global step counter"},{"location":"concepts/execution-stages/#passing-context-to-pipeline","title":"Passing Context to Pipeline","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_schemas.enums import ExecutionStage\nfrom cuvis_ai_schemas.execution import Context\n\npipeline = CuvisPipeline()\n\n# Training\ntrain_context = Context(stage=ExecutionStage.TRAIN, epoch=0, batch_idx=0)\ntrain_outputs = pipeline.forward(batch=train_batch, context=train_context)\n\n# Validation\nval_context = Context(stage=ExecutionStage.VAL, epoch=0, batch_idx=0)\nval_outputs = pipeline.forward(batch=val_batch, context=val_context)\n\n# Inference\ninference_context = Context(stage=ExecutionStage.INFERENCE)\ninference_outputs = pipeline.forward(batch=inference_batch, context=inference_context)\n</code></pre>"},{"location":"concepts/execution-stages/#using-context-in-nodes","title":"Using Context in Nodes","text":"<pre><code>class ContextAwareNode(Node):\n    def forward(self, data, context: Context, **_):\n        if context.stage == ExecutionStage.TRAIN:\n            result = self.train_transform(data)\n        else:\n            result = self.eval_transform(data)\n\n        metadata = {\n            \"stage\": context.stage.value,\n            \"epoch\": context.epoch,\n            \"batch_idx\": context.batch_idx,\n        }\n\n        return {\"result\": result, \"metadata\": metadata}\n</code></pre>"},{"location":"concepts/execution-stages/#data-flow-patterns","title":"Data Flow Patterns","text":""},{"location":"concepts/execution-stages/#pattern-1-loss-and-metric-separation","title":"Pattern 1: Loss and Metric Separation","text":"<p>Losses compute during train/val/test, metrics only during val/test.</p> <pre><code>class BCELoss(LossNode):\n    # Auto-configured {TRAIN, VAL, TEST}\n    def forward(self, predictions, targets, **_):\n        return {\"loss\": F.binary_cross_entropy(predictions, targets)}\n\nclass AccuracyMetric(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            execution_stages={ExecutionStage.VAL, ExecutionStage.TEST},\n            **kwargs\n        )\n\n    def forward(self, predictions, targets, context, **_):\n        accuracy = (predictions.round() == targets).float().mean()\n        return {\"metrics\": [\n            Metric(name=\"accuracy\", value=float(accuracy), stage=context.stage)\n        ]}\n</code></pre>"},{"location":"concepts/execution-stages/#pattern-2-training-vs-inference-paths","title":"Pattern 2: Training vs Inference Paths","text":"<pre><code>graph TD\n    A[Data Input&lt;br/&gt;ALWAYS] --&gt; B[Augmentation&lt;br/&gt;TRAIN]\n    A --&gt; C[Feature Extraction&lt;br/&gt;ALWAYS]\n    B --&gt; C\n    C --&gt; D[Model&lt;br/&gt;ALWAYS]\n    D --&gt; E[Loss&lt;br/&gt;TRAIN/VAL/TEST]\n    D --&gt; F[Metrics&lt;br/&gt;VAL/TEST]\n    D --&gt; G[Inference Optimizer&lt;br/&gt;INFERENCE]\n    style E fill:#d4edda\n    style F fill:#d4edda\n    style G fill:#f8d7da</code></pre>"},{"location":"concepts/execution-stages/#best-practices","title":"Best Practices","text":"<ol> <li>Use Semantic Stage Selection</li> </ol> <pre><code># GOOD: Loss computes during training phases\nclass TrainingLoss(LossNode):\n    pass  # Automatically {TRAIN, VAL, TEST}\n\n# BAD: Loss executing during inference\nclass BadLoss(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.ALWAYS}, **kwargs)\n</code></pre> <ol> <li>Default to ALWAYS for Core Nodes</li> </ol> <pre><code># GOOD: Applies everywhere\nclass FeatureExtractor(Node):\n    pass  # Defaults to ALWAYS\n\n# BAD: Unnecessary restriction\nclass OverRestricted(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            execution_stages={ExecutionStage.TRAIN, ExecutionStage.INFERENCE},\n            **kwargs\n        )\n</code></pre> <ol> <li>Separate Concerns with Multiple Nodes</li> </ol> <pre><code># GOOD: Separate nodes for different behaviors\nclass TrainingPostProcessor(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.TRAIN}, **kwargs)\n\nclass InferencePostProcessor(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.INFERENCE}, **kwargs)\n</code></pre> <ol> <li>Document Stage Decisions</li> </ol> <pre><code>class ValidationOnlyMetric(Node):\n    \"\"\"Compute expensive metric only during validation.\n\n    This metric requires full dataset aggregation and is too expensive\n    during training. Computed during validation for model selection.\n\n    Execution Stages: VAL, TEST\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(\n            execution_stages={ExecutionStage.VAL, ExecutionStage.TEST},\n            **kwargs\n        )\n</code></pre> <ol> <li>Test All Stages</li> </ol> <pre><code>def test_pipeline_all_stages(pipeline, sample_batch):\n    \"\"\"Verify pipeline behavior in all stages.\"\"\"\n    stages = [\n        ExecutionStage.TRAIN,\n        ExecutionStage.VAL,\n        ExecutionStage.TEST,\n        ExecutionStage.INFERENCE,\n    ]\n\n    for stage in stages:\n        context = Context(stage=stage, epoch=0, batch_idx=0)\n        outputs = pipeline.forward(batch=sample_batch, context=context)\n\n        if stage == ExecutionStage.TRAIN:\n            assert \"loss\" in outputs\n        if stage in {ExecutionStage.VAL, ExecutionStage.TEST}:\n            assert \"metrics\" in outputs\n        if stage == ExecutionStage.INFERENCE:\n            assert \"predictions\" in outputs\n</code></pre>"},{"location":"concepts/execution-stages/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/execution-stages/#node-not-executing","title":"Node Not Executing","text":"<p>Diagnosis: <pre><code>logger.info(f\"Node {node.name} stages: {node.execution_stages}\")\nlogger.info(f\"Current stage: {context.stage}\")\n</code></pre></p> <p>Solution: Verify stage match <pre><code># Node restricted to TRAIN\nnode = MyNode(execution_stages={ExecutionStage.TRAIN})\n\n# But running VAL - won't execute!\ncontext = Context(stage=ExecutionStage.VAL)\n\n# Fix: Add VAL to stages\nnode = MyNode(execution_stages={ExecutionStage.TRAIN, ExecutionStage.VAL})\n</code></pre></p>"},{"location":"concepts/execution-stages/#unexpected-node-execution","title":"Unexpected Node Execution","text":"<p>Solution: Check parent class constructor <pre><code>class MyMetric(MetricNode):  # Parent sets VAL/TEST\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)  # Call parent to preserve stages\n        self.threshold = 0.5\n</code></pre></p>"},{"location":"concepts/execution-stages/#loss-not-computing-in-validation","title":"Loss Not Computing in Validation","text":"<p>Solution: Use LossNode base class <pre><code>from cuvis_ai.node.losses import LossNode\n\nclass MyLoss(LossNode):\n    # Auto-configured {TRAIN, VAL, TEST}\n    def forward(self, predictions, targets, **_):\n        return {\"loss\": self.compute_loss(predictions, targets)}\n</code></pre></p>"},{"location":"concepts/execution-stages/#context-not-available-in-node","title":"Context Not Available in Node","text":"<p>Fix: Add context to INPUT_SPECS <pre><code>class MyNode(Node):\n    INPUT_SPECS = {\n        \"data\": PortSpec(dtype=torch.float32, shape=(-1, -1)),\n        \"context\": PortSpec(dtype=Context, shape=()),  # Add this\n    }\n\n    def forward(self, data, context: Context, **_):\n        if context.stage == ExecutionStage.TRAIN:\n            return {\"output\": self.train_process(data)}\n        return {\"output\": self.eval_process(data)}\n</code></pre></p> <p>Note: Pipeline automatically injects context for nodes declaring it in INPUT_SPECS.</p>"},{"location":"concepts/execution-stages/#stage-enum-comparison-failing","title":"Stage Enum Comparison Failing","text":"<p>Solution: Compare enum to enum, not string <pre><code># BAD\nif context.stage == \"train\":  # Never matches!\n    pass\n\n# GOOD\nif context.stage == ExecutionStage.TRAIN:\n    pass\n\n# ALSO GOOD\nif context.stage.value == \"train\":\n    pass\n</code></pre></p>"},{"location":"concepts/execution-stages/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Minimize Overhead in High-Frequency Stages</li> </ol> <pre><code># BAD: Expensive visualization in training\nclass BadVisualizer(Node):\n    def __init__(self, **kwargs):\n        super().__init__(execution_stages={ExecutionStage.ALWAYS}, **kwargs)\n\n    def forward(self, data, **_):\n        expensive_viz = self.render_3d_plot(data)  # Runs every batch!\n        return {\"visualization\": expensive_viz}\n\n# GOOD: Visualization only during validation\nclass GoodVisualizer(Node):\n    def __init__(self, **kwargs):\n        super().__init__(\n            execution_stages={ExecutionStage.VAL, ExecutionStage.TEST},\n            **kwargs\n        )\n</code></pre> <ol> <li>Reduce Complexity in Inference</li> </ol> <pre><code>class AdaptiveProcessor(Node):\n    def forward(self, data, context: Context, **_):\n        if context.stage == ExecutionStage.INFERENCE:\n            return {\"result\": self.fast_forward(data)}\n        else:\n            return {\"result\": self.full_forward(data)}\n</code></pre> <ol> <li>Skip Gradients in Non-Training</li> </ol> <pre><code># Automatically handled by trainer\nwith torch.no_grad():\n    outputs = pipeline.forward(batch=val_batch, context=val_context)\n</code></pre>"},{"location":"concepts/execution-stages/#related-documentation","title":"Related Documentation","text":"<ul> <li>\u2192 Node System Deep Dive - Node lifecycle and implementation</li> <li>\u2192 Pipeline Lifecycle - Pipeline states and execution</li> <li>\u2192 Two-Phase Training - Statistical initialization and gradient training</li> <li>\u2192 Core Concepts Overview - High-level framework concepts</li> </ul>"},{"location":"concepts/node-system-deep-dive/","title":"Node System Deep Dive","text":"<p>Fundamental processing units in CUVIS.AI pipelines.</p> <p>A Node represents a single processing unit in a pipeline. Each node performs a specific task, declares typed input/output ports, manages internal state, and supports both CPU and GPU execution.</p> <p>Key capabilities:</p> <ul> <li>Typed I/O via port specifications</li> <li>Optional statistical initialization from data</li> <li>Freeze/unfreeze for two-phase training</li> <li>Stage-aware execution control</li> <li>Serialization and restoration</li> </ul>"},{"location":"concepts/node-system-deep-dive/#node-lifecycle","title":"Node Lifecycle","text":"<p>Complete lifecycle from creation to cleanup:</p> <pre><code>flowchart TB\n    A[Node Creation] --&gt; B[Port Initialization]\n    B --&gt; C{Requires Statistical&lt;br/&gt;Initialization?}\n    C --&gt;|Yes| D[statistical_initialization&lt;br/&gt;from data]\n    C --&gt;|No| E[Ready for Use]\n    D --&gt; F{Convert to&lt;br/&gt;Trainable?}\n    F --&gt;|Yes| G[unfreeze&lt;br/&gt;buffers \u2192 parameters]\n    F --&gt;|No| H[Use Frozen&lt;br/&gt;Statistics]\n    G --&gt; I[Gradient Training]\n    H --&gt; E\n    I --&gt; E\n    E --&gt; J[Forward Pass&lt;br/&gt;Execution]\n    J --&gt; K[Output Generation]\n    K --&gt; L{More Data?}\n    L --&gt;|Yes| J\n    L --&gt;|No| M[Serialization&lt;br/&gt;save state_dict]\n    M --&gt; N[Cleanup&lt;br/&gt;release resources]\n\n    style A fill:#e1f5ff\n    style D fill:#fff3cd\n    style G fill:#ffe66d\n    style J fill:#f3e5f5\n    style M fill:#d4edda\n    style N fill:#ffc107</code></pre>"},{"location":"concepts/node-system-deep-dive/#base-node-architecture","title":"Base Node Architecture","text":"<p>All nodes inherit from <code>Node</code> base class (itself inheriting from <code>nn.Module</code>, <code>ABC</code>, <code>Serializable</code>):</p> <pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nimport torch\n\nclass Node(nn.Module, ABC, Serializable):\n    \"\"\"Base class for all nodes.\"\"\"\n\n    # Class-level port specifications\n    INPUT_SPECS: dict[str, PortSpec | list[PortSpec]] = {}\n    OUTPUT_SPECS: dict[str, PortSpec | list[PortSpec]] = {}\n\n    def __init__(self, name: str | None = None, **kwargs):\n        super().__init__()\n        self.name = name\n        self._input_ports = {}   # Created from INPUT_SPECS\n        self._output_ports = {}  # Created from OUTPUT_SPECS\n\n    @abstractmethod\n    def forward(self, **inputs) -&gt; dict[str, Any]:\n        \"\"\"Process inputs and return outputs.\"\"\"\n        pass\n</code></pre> <p>Key properties:</p> <ul> <li><code>requires_initial_fit</code>: Auto-detects if node needs statistical initialization</li> <li><code>execution_stages</code>: Controls when node executes (TRAIN, VAL, TEST, INFERENCE, ALWAYS)</li> <li><code>freezed</code>: Tracks frozen vs trainable state</li> </ul>"},{"location":"concepts/node-system-deep-dive/#common-node-patterns","title":"Common Node Patterns","text":""},{"location":"concepts/node-system-deep-dive/#1-data-loading-pattern","title":"1. Data Loading Pattern","text":"<p>Load and validate input data</p> <pre><code>from cuvis_ai.node.data import LentilsAnomalyDataNode\n\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\n\n# Characteristics: Stateless, executes in all stages\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#2-processing-pattern","title":"2. Processing Pattern","text":"<p>Transform and normalize data</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training import StatisticalTrainer\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\n\n# Create pipeline and add nodes\npipeline = CuvisPipeline(\"Normalization_Pipeline\")\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True)\n\n# Connect nodes\npipeline.connect(\n    (data_node.outputs.cube, normalizer.data),\n)\n\n# Statistical initialization via trainer\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes normalizer with statistics\n\n# Characteristics: Can be stateless or stateful\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#3-statistical-pattern","title":"3. Statistical Pattern","text":"<p>Anomaly detection using statistical methods</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training import StatisticalTrainer\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\n\n# Create pipeline with statistical nodes\npipeline = CuvisPipeline(\"RX_Statistical\")\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True)\nrx_node = RXGlobal(num_channels=61, eps=1e-6)\n\n# Connect the pipeline\npipeline.connect(\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, rx_node.data),\n)\n\n# Phase 1: Statistical initialization\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Initializes all statistical nodes (normalizer, rx_node)\n\n# Phase 2 (optional): Enable gradient training\npipeline.unfreeze_nodes_by_name([rx_node.name])\n</code></pre> <p>Statistical initialization pattern:</p> <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Compute statistics from initialization data.\"\"\"\n    self.reset()\n\n    for batch_data in input_stream:\n        data_tensor = batch_data[\"data\"]\n        self._update_statistics(data_tensor)  # Welford's algorithm\n\n    # Store as buffers (frozen by default)\n    self.register_buffer(\"mu\", computed_mean)\n    self.register_buffer(\"sigma\", computed_covariance)\n    self._statistically_initialized = True\n</code></pre> <p>Characteristics: Requires initialization, stores statistics as buffers, can be unfrozen for training</p>"},{"location":"concepts/node-system-deep-dive/#4-deep-learning-pattern","title":"4. Deep Learning Pattern","text":"<p>Neural network-based analysis</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training import StatisticalTrainer, GradientTrainer\nfrom cuvis_ai.anomaly.deep_svdd import (\n    DeepSVDDProjection,\n    DeepSVDDCenterTracker,\n    DeepSVDDScores,\n    ZScoreNormalizerGlobal,\n)\nfrom cuvis_ai.node.losses import DeepSVDDSoftBoundaryLoss\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\n\n# Create pipeline with deep learning nodes\npipeline = CuvisPipeline(\"DeepSVDD\")\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nencoder = ZScoreNormalizerGlobal(num_channels=61, hidden=32, sample_n=100)\nprojection = DeepSVDDProjection(in_channels=61, rep_dim=16, hidden=[32, 16])\ncenter_tracker = DeepSVDDCenterTracker(rep_dim=16)\nloss_node = DeepSVDDSoftBoundaryLoss(name=\"deepsvdd_loss\")\n\n# Connect the pipeline\npipeline.connect(\n    (data_node.outputs.cube, encoder.data),\n    (encoder.normalized, projection.data),\n    (projection.embeddings, center_tracker.embeddings),\n    (projection.embeddings, loss_node.embeddings),\n    (center_tracker.center, loss_node.center),\n)\n\n# Phase 1: Statistical initialization of encoder\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()\n\n# Phase 2: Gradient training\npipeline.unfreeze_nodes_by_name([encoder.name])\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[loss_node],\n    trainer_config=training_config,\n)\ngrad_trainer.fit()\n</code></pre> <p>Characteristics: Trainable neural networks, GPU-accelerated, require gradient optimization</p>"},{"location":"concepts/node-system-deep-dive/#creating-custom-nodes","title":"Creating Custom Nodes","text":""},{"location":"concepts/node-system-deep-dive/#basic-custom-node-template","title":"Basic Custom Node Template","text":"<pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nimport torch\nfrom torch import nn\n\nclass MyCustomNode(Node):\n    \"\"\"Custom node for specific processing.\"\"\"\n\n    INPUT_SPECS = {\n        \"features\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1),\n            description=\"Input feature vectors\"\n        )\n    }\n\n    OUTPUT_SPECS = {\n        \"transformed\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1),\n            description=\"Transformed features\"\n        )\n    }\n\n    def __init__(self, input_dim: int, output_dim: int, **kwargs):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        super().__init__(input_dim=input_dim, output_dim=output_dim, **kwargs)\n\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, features: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Process input features.\"\"\"\n        transformed = self.linear(features)\n        return {\"transformed\": transformed}\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#adding-statistical-initialization-optional","title":"Adding Statistical Initialization (Optional)","text":"<pre><code>    def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n        \"\"\"Initialize parameters from data.\"\"\"\n        feature_sum = torch.zeros(self.input_dim)\n        count = 0\n\n        for batch_data in input_stream:\n            features = batch_data[\"features\"]\n            feature_sum += features.sum(dim=0)\n            count += features.shape[0]\n\n        mean = feature_sum / count\n        self.register_buffer(\"running_mean\", mean)\n        self._statistically_initialized = True\n\n    def unfreeze(self) -&gt; None:\n        \"\"\"Convert buffers to parameters for gradient training.\"\"\"\n        if hasattr(self, \"running_mean\"):\n            self.running_mean = nn.Parameter(self.running_mean.clone())\n        super().unfreeze()\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#node-registration","title":"Node Registration","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n@NodeRegistry.register\nclass MyCustomNode(Node):\n    \"\"\"Now discoverable via NodeRegistry.get(\"MyCustomNode\")\"\"\"\n    pass\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#node-registry-and-discovery","title":"Node Registry and Discovery","text":"<p>Built-in node access:</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Get node class\nRXGlobal = NodeRegistry.get(\"RXGlobal\")\n\n# List all nodes\nall_nodes = NodeRegistry.list_builtin_nodes()\n</code></pre> <p>Plugin support:</p> <pre><code># Create registry instance\nregistry = NodeRegistry()\nregistry.load_plugins(\"path/to/plugins.yaml\")\n\n# Get plugin node\nAdaCLIPDetector = registry.get(\"AdaCLIPDetector\")\n\n# Use with PipelineBuilder\nfrom cuvis_ai.pipeline.pipeline_builder import PipelineBuilder\nbuilder = PipelineBuilder(node_registry=registry)\n</code></pre> <p>Plugin configuration (<code>plugins.yaml</code>):</p> <pre><code>plugins:\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.0\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n</code></pre> <p>Resolution order: Instance plugins \u2192 Built-in registry \u2192 Import from module path</p>"},{"location":"concepts/node-system-deep-dive/#state-management","title":"State Management","text":""},{"location":"concepts/node-system-deep-dive/#buffers-vs-parameters","title":"Buffers vs Parameters","text":"<p>Parameters are trainable (receive gradients). Buffers are non-trainable state. Both are serialized and moved with <code>.to(device)</code>.</p> <pre><code># Buffers: statistics, running means, constants\nself.register_buffer(\"mean\", torch.zeros(num_features))\nself.register_buffer(\"covariance\", torch.eye(num_features))\n\n# Parameters: weights, learnable transformations\nself.linear = nn.Linear(in_features, out_features)\nself.register_parameter(\"custom_weight\", nn.Parameter(torch.randn(10)))\n\n# Two-phase: buffer \u2192 parameter\nself.register_buffer(\"scale\", torch.ones(1))  # Phase 1: statistical init\nself.scale = nn.Parameter(self.scale.clone())  # Phase 2: unfreeze for training\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#freezeunfreeze-pattern","title":"Freeze/Unfreeze Pattern","text":"<pre><code>from cuvis_ai_core.training import StatisticalTrainer\n\n# Phase 1: Statistical initialization (frozen by default)\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Computes statistics, stores as buffers\nnode.freezed  # True - no gradients, values constant\n\n# Phase 2: Unfreeze for gradient training (optional)\nnode.unfreeze()  # Converts buffers \u2192 parameters, enables gradient updates\nnode.freezed  # False - now trainable\n\n# Benefits: Fast statistical init + optional gradient refinement\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#best-practices","title":"Best Practices","text":""},{"location":"concepts/node-system-deep-dive/#1-keep-nodes-focused","title":"1. Keep Nodes Focused","text":"<p>Single responsibility - one node, one task. Compose complex behavior from simple nodes.</p> <pre><code>class NormalizationNode(Node):\n    \"\"\"Normalizes input data to [0, 1] range.\"\"\"\n    pass\n\nclass AnomalyDetectionNode(Node):\n    \"\"\"Detects anomalies using RX algorithm.\"\"\"\n    pass\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#2-trust-port-validation-dont-over-validate","title":"2. Trust Port Validation (Don't Over-Validate)","text":"<p>Port schema validation is automatic - the pipeline validates data types and shapes. Do NOT duplicate these checks:</p> <pre><code># \u274c BAD: Duplicates automatic port validation\ndef forward(self, data: torch.Tensor, **_):\n    if data.ndim != 4:  # Port spec already validates this\n        raise ValueError(f\"Expected 4D tensor (BHWC), got {data.shape}\")\n    if data.dtype != torch.float32:  # Port spec already validates this\n        raise TypeError(f\"Expected float32, got {data.dtype}\")\n    return {\"output\": self.process(data)}\n\n# \u2705 GOOD: Trust port specs, only check node-specific state\nINPUT_SPECS = {\n    \"data\": PortSpec(\n        dtype=torch.float32,\n        shape=(-1, -1, -1, 61),  # Framework validates this\n        description=\"Hyperspectral cube in BHWC format\"\n    )\n}\n\ndef forward(self, data: torch.Tensor, **_):\n    # DO check node-specific initialization state (not automatic)\n    if not self._statistically_initialized:\n        raise RuntimeError(f\"{self.__class__.__name__} requires initialization\")\n    return {\"output\": self.process(data)}\n</code></pre> <p>What's automatic: Port shape/dtype validation What's manual: Statistical initialization checks (node responsibility)</p>"},{"location":"concepts/node-system-deep-dive/#3-avoid-to-in-forward-pipeline-handles-device-placement","title":"3. Avoid <code>.to()</code> in Forward (Pipeline Handles Device Placement)","text":"<p>The pipeline automatically moves nodes, parameters, and data to the correct device when <code>pipeline.to(device)</code> is called. Do NOT use <code>.to()</code> calls in <code>forward()</code>:</p> <pre><code># \u274c BAD: Manual device placement breaks multi-device training\ndef forward(self, data: torch.Tensor, **_):\n    data = data.to(\"cuda\")  # DON'T DO THIS!\n    weights = self.weights.to(\"cuda\")  # DON'T DO THIS!\n    result = torch.matmul(data, weights)\n    return {\"output\": result}\n\n# \u2705 GOOD: Let the pipeline handle device placement\ndef forward(self, data: torch.Tensor, **_):\n    # data and self.weights are already on the correct device\n    result = torch.matmul(data, self.weights)\n    return {\"output\": result}\n</code></pre> <p>Why avoid <code>.to()</code> in forward:</p> <ul> <li>Breaks multi-device training (model parallelism)</li> <li>Unnecessary overhead (data already on correct device)</li> <li>Pipeline manages device placement via <code>pipeline.to(device)</code></li> <li>The framework ensures consistency across all nodes</li> </ul>"},{"location":"concepts/node-system-deep-dive/#4-document-port-requirements","title":"4. Document Port Requirements","text":"<pre><code>INPUT_SPECS = {\n    \"data\": PortSpec(\n        dtype=torch.float32,\n        shape=(-1, -1, -1, 61),  # BHWC with 61 channels\n        description=\"Hyperspectral cube in BHWC format, normalized to [0, 1]\"\n    )\n}\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#5-use-context-for-training-metadata","title":"5. Use Context for Training Metadata","text":"<p>Context provides training metadata passed as a parameter to <code>forward()</code>. Contains stage, epoch, batch_idx, and global_step:</p> <pre><code>from cuvis_ai_schemas.execution import Context, Metric\nfrom cuvis_ai_schemas.enums import ExecutionStage\n\n# \u2705 GOOD: Context passed as parameter\ndef forward(self, predictions: torch.Tensor, targets: torch.Tensor,\n            context: Context) -&gt; dict:\n    # Context fields\n    stage = context.stage           # ExecutionStage enum: \"train\", \"val\", \"test\", \"inference\"\n    epoch = context.epoch           # Current epoch (0-indexed)\n    batch_idx = context.batch_idx   # Current batch in epoch (0-indexed)\n    step = context.global_step      # Global step across all epochs (for logging)\n\n    # Use for metrics\n    loss_value = self.compute_loss(predictions, targets)\n    metric = Metric(\n        name=\"loss\",\n        value=loss_value,\n        stage=stage,\n        epoch=epoch,\n        batch_idx=batch_idx,\n    )\n\n    # Conditional behavior per stage\n    if context.stage == ExecutionStage.TRAIN:\n        self.update_running_stats(predictions)\n\n    return {\"metrics\": [metric]}\n\n# \u274c BAD: Manual metadata parameters\ndef forward(self, predictions: torch.Tensor, targets: torch.Tensor,\n            epoch: int, batch_idx: int, stage: str, **_):  # Don't do this\n    pass\n</code></pre> <p>Context fields:</p> <ul> <li><code>stage</code>: ExecutionStage enum (\"train\", \"val\", \"test\", \"inference\")</li> <li><code>epoch</code>: Current epoch number (0-indexed)</li> <li><code>batch_idx</code>: Batch index within epoch (0-indexed)</li> <li><code>global_step</code>: Global step counter for monitoring (0-indexed)</li> </ul> <p>Common use cases:</p> <ul> <li>Metric/artifact logging with training metadata</li> <li>Conditional behavior per stage (train vs inference)</li> <li>TensorBoard step tracking via <code>global_step</code></li> </ul>"},{"location":"concepts/node-system-deep-dive/#6-initialize-buffers-and-parameters-properly","title":"6. Initialize Buffers and Parameters Properly","text":"<p>All buffers and parameters MUST be fully initialized in <code>__init__</code> with correct dimensions. Initializing them as <code>None</code> or empty and rewriting later breaks serialization.</p> <p>Get all required arguments (like <code>num_channels</code>) upfront in the constructor:</p> <pre><code># \u274c BAD: Deferred initialization breaks serialization\nclass BadNode(Node):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.register_buffer(\"mu\", None)  # Breaks state_dict!\n        self.register_buffer(\"cov\", torch.zeros(0))\n\n    def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n        num_channels = next(iter(input_stream))[\"data\"].shape[-1]\n        self.mu = torch.zeros(num_channels)  # Too late - breaks serialization\n\n# \u2705 GOOD: Proper initialization with required arguments\nclass GoodNode(Node):\n    def __init__(self, num_channels: int, **kwargs):\n        super().__init__(num_channels=num_channels, **kwargs)\n        # Buffers initialized with correct shapes immediately\n        self.register_buffer(\"mu\", torch.zeros(num_channels, dtype=torch.float32))\n        self.register_buffer(\"cov\", torch.eye(num_channels, dtype=torch.float32))\n\n    def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n        for batch in input_stream:\n            self._update_statistics(batch[\"data\"])\n        # Update in-place (maintains buffer identity)\n        self.mu.copy_(computed_mean)\n        self.cov.copy_(computed_covariance)\n</code></pre> <p>Why: PyTorch's <code>state_dict()</code> captures buffers at <code>register_buffer()</code> time. Reassigning later breaks serialization completely.</p>"},{"location":"concepts/node-system-deep-dive/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/node-system-deep-dive/#node-not-initialized-error","title":"\"Node not initialized\" Error","text":"<p>Problem: RuntimeError when using statistical nodes without initialization</p> <p>Solution: Use <code>StatisticalTrainer</code> to initialize nodes before use</p> <pre><code>from cuvis_ai_core.training import StatisticalTrainer\n\nrx = RXGlobal(num_channels=61)\npipeline.add_node(rx)\n\n# Initialize using trainer\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Initializes rx node\n\n# Now ready for inference\noutputs = rx.forward(data=test_data)\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#port-type-mismatch","title":"Port Type Mismatch","text":"<p>Problem: PortCompatibilityError with dtype mismatch</p> <p>Solution: Ensure consistent dtypes across pipeline</p> <pre><code>normalizer = MinMaxNormalizer(dtype=torch.float32)\nrx = RXGlobal(num_channels=61, dtype=torch.float32)  # Match dtype\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#shape-mismatch","title":"Shape Mismatch","text":"<p>Problem: ValueError with unexpected input shape</p> <p>Solution: Check channel dimension matches node expectations</p> <pre><code>selector = SoftChannelSelector(\n    n_select=10,\n    input_channels=30  # Match actual input channels\n)\n</code></pre>"},{"location":"concepts/node-system-deep-dive/#related-documentation","title":"Related Documentation","text":"<ul> <li>\u2192 Port System Deep Dive - Node I/O and data flow</li> <li>\u2192 Pipeline Lifecycle - Node integration in pipelines</li> <li>\u2192 Two-Phase Training - Statistical initialization and gradient training</li> <li>\u2192 Node Catalog - Built-in node reference</li> <li>\u2192 Creating Custom Nodes - Step-by-step tutorial</li> </ul>"},{"location":"concepts/overview/","title":"Core Concepts Overview","text":"<p>CUVIS.AI is built on five interconnected concepts that work together to create flexible, type-safe ML pipelines:</p> <pre><code>graph TB\n    subgraph \" \"\n        N[Nodes&lt;br/&gt;Processing units with typed I/O]\n        P[Ports&lt;br/&gt;Type-safe connections]\n        PL[Pipelines&lt;br/&gt;DAG orchestration]\n        TPT[Two-Phase Training&lt;br/&gt;Statistical + Gradient]\n        ES[Execution Stages&lt;br/&gt;Conditional execution]\n    end\n\n    N --&gt;|connected via| P\n    P --&gt;|orchestrated by| PL\n    PL --&gt;|trained with| TPT\n    PL --&gt;|filtered by| ES\n\n    style N fill:#e1f5ff\n    style P fill:#fff3cd\n    style PL fill:#ffe66d\n    style TPT fill:#d4edda\n    style ES fill:#f3e5f5\n\n    click N \"../node-system-deep-dive/\"\n    click P \"../port-system-deep-dive/\"\n    click PL \"../pipeline-lifecycle/\"\n    click TPT \"../two-phase-training/\"\n    click ES \"../execution-stages/\"</code></pre> <p>Each concept below has a dedicated deep-dive page with comprehensive diagrams and examples.</p>"},{"location":"concepts/pipeline-lifecycle/","title":"Pipeline Lifecycle","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"concepts/pipeline-lifecycle/#pipeline-lifecycle","title":"Pipeline Lifecycle","text":"<p>Central orchestrator managing node connections, data flow, and execution through distinct lifecycle phases.</p> <p>A Pipeline in CUVIS.AI is a directed acyclic graph (DAG) of connected nodes that processes data through transformations. It manages connections, validates ports, executes nodes in topological order, handles initialization, and serializes trained models.</p> <p>Key capabilities:</p> <ul> <li>Port-based connections with validation</li> <li>Topological execution ordering</li> <li>Statistical initialization and gradient training</li> <li>Serialization and restoration</li> <li>Stage-aware execution filtering</li> <li>Introspection and debugging tools</li> </ul>"},{"location":"concepts/pipeline-lifecycle/#pipeline-states","title":"Pipeline States","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Construction: create pipeline\n    Construction --&gt; Validation: verify()\n    Validation --&gt; Initialization: statistical_initialization()\n    Initialization --&gt; Execution: forward() / train()\n    Execution --&gt; Serialization: save()\n    Serialization --&gt; [*]\n    Serialization --&gt; Restoration: load()\n    Restoration --&gt; Validation\n    Execution --&gt; Cleanup: release resources\n    Cleanup --&gt; [*]</code></pre> <p>States: 1. Construction: Add nodes and connections 2. Validation: Verify graph integrity and port compatibility 3. Initialization: Statistical init, GPU transfer 4. Execution: Training, validation, inference 5. Serialization: Save structure and weights 6. Restoration: Load saved pipelines 7. Cleanup: Release resources</p>"},{"location":"concepts/pipeline-lifecycle/#construction-phase","title":"Construction Phase","text":""},{"location":"concepts/pipeline-lifecycle/#creating-a-pipeline","title":"Creating a Pipeline","text":"<pre><code>from cuvis_ai.pipeline.pipeline import CuvisPipeline\n\npipeline = CuvisPipeline(\n    name=\"my_anomaly_detector\",\n    strict_runtime_io_validation=True\n)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#adding-nodes","title":"Adding Nodes","text":"<pre><code># Explicit addition\npipeline.add_node(data_loader)\npipeline.add_node(normalizer)\n\n# Or automatic during connection\npipeline.connect((data_loader.cube, normalizer.data))\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#connecting-nodes","title":"Connecting Nodes","text":"<pre><code>data_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1e-6)\nrx_node = RXGlobal(num_channels=61)\n\npipeline.connect(\n    (data_node.cube, normalizer.data),\n    (normalizer.normalized, rx_node.data)\n)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#pipeline-builder-yaml","title":"Pipeline Builder (YAML)","text":"<pre><code>from cuvis_ai.pipeline.pipeline_builder import PipelineBuilder\n\nbuilder = PipelineBuilder()\npipeline = builder.build_from_config(\"configs/pipeline/my_pipeline.yaml\")\n</code></pre> <p>YAML Format: <pre><code>name: RX_Anomaly_Detector\n\nnodes:\n  - name: data_loader\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: rx_detector\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n\nconnections:\n  - from: data_loader.cube\n    to: rx_detector.data\n</code></pre></p>"},{"location":"concepts/pipeline-lifecycle/#validation-phase","title":"Validation Phase","text":"<p>Automatic during construction, manual via verify().</p> <pre><code>pipeline.verify()  # Check all constraints\n</code></pre> <p>Validation Checks:</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Required ports&lt;br/&gt;connected?}\n    B --&gt;|No| C[ValidationError]\n    B --&gt;|Yes| D{Type&lt;br/&gt;compatible?}\n    D --&gt;|No| E[PortCompatibilityError]\n    D --&gt;|Yes| F{Shape&lt;br/&gt;compatible?}\n    F --&gt;|No| E\n    F --&gt;|Yes| G{No cycles&lt;br/&gt;in graph?}\n    G --&gt;|Cycle| H[CycleError]\n    G --&gt;|No cycle| I{Config&lt;br/&gt;valid?}\n    I --&gt;|No| J[ConfigError]\n    I --&gt;|Yes| K[\u2713 Valid]\n    style K fill:#d4edda\n    style C fill:#f8d7da\n    style E fill:#f8d7da\n    style H fill:#f8d7da</code></pre> <p>Validates: 1. All required ports connected 2. Port dtype/shape compatibility 3. No circular dependencies 4. Valid node configuration</p>"},{"location":"concepts/pipeline-lifecycle/#initialization-phase","title":"Initialization Phase","text":"<p>Prepare nodes for execution.</p>"},{"location":"concepts/pipeline-lifecycle/#node-initialization-order","title":"Node Initialization Order","text":"<pre><code># Nodes initialized in topological order\nsorted_nodes = pipeline._get_topologically_sorted_nodes()\n\nfor node in sorted_nodes:\n    if node.requires_initial_fit:\n        node.statistical_initialization(initialization_stream)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#two-phase-initialization","title":"Two-Phase Initialization","text":"<pre><code>from cuvis_ai.trainer.statistical_trainer import StatisticalTrainer\n\n# Phase 1: Statistical initialization\ndatamodule = SingleCu3sDataModule(...)\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()  # Initialize statistical nodes\n\n# Phase 2: Unfreeze for gradient training\npipeline.unfreeze_nodes_by_name([\"rx_detector\", \"selector\"])\n</code></pre> <p>See Two-Phase Training for details.</p>"},{"location":"concepts/pipeline-lifecycle/#gpu-transfer","title":"GPU Transfer","text":"<pre><code>pipeline = pipeline.to(\"cuda\")\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#execution-phase","title":"Execution Phase","text":""},{"location":"concepts/pipeline-lifecycle/#sequential-execution","title":"Sequential Execution","text":"<pre><code>from cuvis_ai_core.pipeline.context import Context, ExecutionStage\n\ncontext = Context(stage=ExecutionStage.INFERENCE)\noutputs = pipeline.forward(\n    batch={\"cube\": hyperspectral_data},\n    context=context\n)\n\nanomaly_scores = outputs[(\"rx_detector\", \"scores\")]\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#batch-processing","title":"Batch Processing","text":"<pre><code>results = []\nfor batch in dataloader:\n    outputs = pipeline.forward(batch=batch, context=context)\n    results.append(outputs[(\"rx_detector\", \"scores\")])\n\nall_scores = torch.cat(results, dim=0)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#trainer-managed-execution","title":"Trainer-Managed Execution","text":"<pre><code>from cuvis_ai.trainer.gradient_trainer import GradientTrainer\n\ntrainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss],\n    metric_nodes=[metrics_node],\n    trainer_config=training_config\n)\n\ntrainer.fit()\ntrainer.test()\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#execution-flow","title":"Execution Flow","text":"<p>High-Level Sequence:</p> <pre><code>sequenceDiagram\n    User-&gt;&gt;Pipeline: forward(batch, context)\n    Pipeline-&gt;&gt;Pipeline: Topological sort\n    Pipeline-&gt;&gt;Pipeline: Filter by stage\n    loop For each node\n        Pipeline-&gt;&gt;Node: Gather inputs\n        Pipeline-&gt;&gt;Node: forward(**inputs)\n        Node--&gt;&gt;Pipeline: Return outputs\n        Pipeline-&gt;&gt;Pipeline: Store in port_data\n    end\n    Pipeline--&gt;&gt;User: Return results</code></pre> <p>Detailed Forward Pass Mechanics:</p> <p>This diagram shows the internal port-based routing and validation that occurs during <code>pipeline.forward()</code>:</p> <pre><code>%%{init: {'flowchart': {'nodeSpacing': 20, 'rankSpacing': 30}} }%%\nflowchart TD\n    A[\"pipeline.forward(batch=...)\"] --&gt; B[Port-Based Batch Distribution]\n    B --&gt; C[Resolve Input Ports]\n    C --&gt; D[Topological Sort by Port Connections]\n    D --&gt; E[For each node in order]\n    E --&gt; F[Collect inputs from connected output ports]\n    F --&gt; G[Execute node.forward(**inputs)]\n    G --&gt; H[Store outputs in port dictionary]\n    H --&gt; I{More nodes?}\n    I --&gt;|Yes| E\n    I --&gt;|No| J[Return port output dictionary]\n\n    B --&gt; K[Port Validation]\n    K --&gt; L[Type Checking]\n    K --&gt; M[Shape Compatibility]\n    K --&gt; N[Stage Filtering]\n\n    style A fill:#e1f5ff\n    style J fill:#d4edda\n    style K fill:#fff3cd</code></pre> <p>Step-by-Step Breakdown:</p> <ol> <li>Batch Distribution: Input batch is distributed to entry nodes based on port specifications</li> <li>Port Resolution: All input ports are resolved to their connected output ports</li> <li>Topological Sorting: Nodes are sorted by dependency order based on port connections</li> <li>Per-Node Execution:</li> <li>Gather all inputs from connected upstream output ports</li> <li>Execute the node's <code>forward()</code> method with gathered inputs</li> <li>Store outputs in port dictionary keyed by <code>(node_id, port_name)</code></li> <li>Validation (Parallel):</li> <li>Type checking ensures dtype compatibility</li> <li>Shape validation confirms dimension compatibility</li> <li>Stage filtering excludes nodes not active in current execution stage</li> <li>Result Return: Complete port output dictionary returned to caller</li> </ol> <p>Example Port Dictionary:</p> <pre><code>outputs = pipeline.forward(batch={\"cube\": data})\n\n# Returns dictionary with port keys:\n{\n    (\"data_node\", \"cube\"): tensor(...),           # Raw data\n    (\"normalizer\", \"normalized\"): tensor(...),    # Normalized cube\n    (\"rx_detector\", \"scores\"): tensor(...),       # Anomaly scores\n    (\"rx_detector\", \"logits\"): tensor(...),       # Logits output\n}\n\n# Access outputs by port:\nscores = outputs[(\"rx_detector\", \"scores\")]\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#stage-aware-execution","title":"Stage-Aware Execution","text":"<pre><code># Different nodes execute per stage\ncontext = Context(stage=ExecutionStage.TRAIN)\npipeline.forward(batch=data, context=context)  # All nodes\n\ncontext = Context(stage=ExecutionStage.VAL)\npipeline.forward(batch=data, context=context)  # Only VAL/ALWAYS nodes\n\ncontext = Context(stage=ExecutionStage.INFERENCE)\npipeline.forward(batch=data, context=context)  # Only INFERENCE/ALWAYS nodes\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#partial-execution","title":"Partial Execution","text":"<pre><code># Run only up to normalizer (debugging)\noutputs = pipeline.forward(\n    batch={\"cube\": data},\n    context=context,\n    upto_node=normalizer\n)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#serialization-phase","title":"Serialization Phase","text":"<p>Save pipeline structure and trained weights.</p>"},{"location":"concepts/pipeline-lifecycle/#saving-pipelines","title":"Saving Pipelines","text":"<pre><code>from cuvis_ai.pipeline.config import PipelineMetadata\n\npipeline.save_to_file(\n    config_path=\"outputs/my_pipeline.yaml\",\n    metadata=PipelineMetadata(\n        name=\"RX_Anomaly_Detector_v1\",\n        description=\"Trained on Lentils dataset\",\n        tags=[\"anomaly-detection\", \"production\"]\n    ),\n    validate_nodes=True,\n    include_optimizer=False,\n    include_scheduler=False\n)\n\n# Generates: outputs/my_pipeline.yaml, outputs/my_pipeline.pt\n</code></pre> <p>YAML Structure: <pre><code>version: '1.0'\nmetadata:\n  name: RX_Anomaly_Detector_v1\n  description: Trained on Lentils dataset\n  tags: [anomaly-detection, production]\n\nnodes:\n  - name: data_loader\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: rx_detector\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n\nconnections:\n  - from: data_loader.cube\n    to: rx_detector.data\n</code></pre></p> <p>PyTorch Checkpoint: <pre><code>{\n    \"state_dict\": {\n        \"normalizer\": {\n            \"running_min\": tensor(...),\n            \"running_max\": tensor(...)\n        },\n        \"rx_detector\": {\n            \"mu\": tensor(...),\n            \"sigma\": tensor(...)\n        }\n    },\n    \"metadata\": {...}\n}\n</code></pre></p> <p>What Gets Saved: \u2705 Pipeline structure, node configs, trained weights, statistics, metadata, optimizer/scheduler (optional) \u274c Training data, intermediate activations, Python code, external plugins</p>"},{"location":"concepts/pipeline-lifecycle/#restoration-phase","title":"Restoration Phase","text":"<p>Load saved pipelines and resume work.</p>"},{"location":"concepts/pipeline-lifecycle/#loading-pipelines","title":"Loading Pipelines","text":"<pre><code>pipeline = CuvisPipeline.load_from_file(\n    config_path=\"outputs/my_pipeline.yaml\",\n    weights_path=\"outputs/my_pipeline.pt\",  # Auto-detected if None\n    device=\"cuda\",\n    strict_weight_loading=True\n)\n\noutputs = pipeline.forward(batch=test_data)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#with-plugins","title":"With Plugins","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\nregistry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n\nbuilder = PipelineBuilder(node_registry=registry)\npipeline = builder.build_from_config(\"outputs/my_pipeline.yaml\")\npipeline.load_state_dict(torch.load(\"outputs/my_pipeline.pt\")[\"state_dict\"])\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#configuration-overrides","title":"Configuration Overrides","text":"<pre><code>pipeline = CuvisPipeline.load_from_file(\n    config_path=\"outputs/my_pipeline.yaml\",\n    config_overrides={\"nodes\": [{\"params\": {\"threshold\": 0.8}}]}\n)\n\n# Or list syntax\npipeline = CuvisPipeline.load_from_file(\n    config_path=\"outputs/my_pipeline.yaml\",\n    config_overrides=[\"nodes.0.params.threshold=0.8\"]\n)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#cleanup-phase","title":"Cleanup Phase","text":"<p>Release resources (GPU memory, file handles, caches).</p> <pre><code>pipeline.cleanup()\n\n# Or context manager (automatic)\nwith CuvisPipeline.load_from_file(\"pipeline.yaml\") as pipeline:\n    results = pipeline.forward(batch=data)\n# Auto-cleanup on exit\n\n# Manual resource release\npipeline = pipeline.to(\"cpu\")\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#training-workflow","title":"Training Workflow","text":""},{"location":"concepts/pipeline-lifecycle/#statistical-training-only","title":"Statistical Training Only","text":"<pre><code>from cuvis_ai.trainer.statistical_trainer import StatisticalTrainer\n\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()\nstat_trainer.validate()\nstat_trainer.test()\n\npipeline.save_to_file(\"outputs/statistical_pipeline.yaml\")\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#gradient-training-two-phase","title":"Gradient Training (Two-Phase)","text":"<pre><code>from cuvis_ai.trainer.gradient_trainer import GradientTrainer\n\n# Phase 1: Statistical init (if needed)\nif any(node.requires_initial_fit for node in pipeline.nodes):\n    stat_trainer = StatisticalTrainer(pipeline, datamodule)\n    stat_trainer.fit()\n\n# Phase 2: Unfreeze and train\npipeline.unfreeze_nodes_by_name([\"selector\", \"rx_detector\"])\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss],\n    metric_nodes=[metrics_node],\n    trainer_config=training_config,\n    optimizer_config=optimizer_config\n)\n\ngrad_trainer.fit()\ngrad_trainer.test()\n\npipeline.save_to_file(\"outputs/gradient_trained_pipeline.yaml\")\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#monitoring-debugging","title":"Monitoring &amp; Debugging","text":""},{"location":"concepts/pipeline-lifecycle/#pipeline-introspection","title":"Pipeline Introspection","text":"<pre><code>summary = pipeline.summary()\nprint(summary)\n\ninput_specs = pipeline.get_input_specs()\noutput_specs = pipeline.get_output_specs()\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#visualization","title":"Visualization","text":"<pre><code>pipeline.visualize(output_path=\"pipeline_graph.png\", format=\"png\")\ndot_source = pipeline.to_dot()\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#execution-profiling","title":"Execution Profiling","text":"<pre><code>pipeline.enable_profiling()\noutputs = pipeline.forward(batch=data)\n\ntimings = pipeline.get_execution_stats()\nfor node_name, time_ms in timings.items():\n    print(f\"{node_name}: {time_ms:.2f}ms\")\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#logging","title":"Logging","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\npipeline.set_log_level(logging.DEBUG)\n\noutputs = pipeline.forward(batch=data)\n</code></pre>"},{"location":"concepts/pipeline-lifecycle/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Validate Early <pre><code>pipeline = build_pipeline()\npipeline.verify()  # Catch errors before training\nstat_trainer.fit()\n</code></pre></p> </li> <li> <p>Check Statistical Initialization <pre><code>uninit_nodes = [\n    node for node in pipeline.nodes\n    if node.requires_initial_fit and not node._statistically_initialized\n]\nif uninit_nodes:\n    stat_trainer.fit()\n</code></pre></p> </li> <li> <p>Save Checkpoints <pre><code>for epoch in range(num_epochs):\n    train_one_epoch()\n    if (epoch + 1) % save_interval == 0:\n        pipeline.save_to_file(f\"checkpoints/epoch_{epoch+1}.yaml\")\n</code></pre></p> </li> <li> <p>Use Context Managers <pre><code>with CuvisPipeline.load_from_file(\"pipeline.yaml\") as pipeline:\n    results = process_data(pipeline, test_data)\n# Auto-cleanup\n</code></pre></p> </li> <li> <p>Monitor Performance <pre><code>pipeline.enable_profiling()\nfor batch in dataloader:\n    pipeline.forward(batch=batch)\nstats = pipeline.get_execution_stats()\n</code></pre></p> </li> <li> <p>Version Pipelines <pre><code>pipeline.save_to_file(\n    \"outputs/pipeline_v2.yaml\",\n    metadata=PipelineMetadata(\n        name=\"RX_Detector\",\n        version=\"2.1.0\",\n        tags=[\"v2\", \"production\"]\n    )\n)\n</code></pre></p> </li> </ol>"},{"location":"concepts/pipeline-lifecycle/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Forgetting Statistical Initialization <pre><code># Missing: stat_trainer.fit()\ngrad_trainer.fit()  # Error: RXGlobal not initialized!\n</code></pre></p> </li> <li> <p>Type Mismatches <pre><code>normalizer = MinMaxNormalizer(dtype=torch.float32)\nrx = RXGlobal(dtype=torch.float64)  # Mismatch!\n</code></pre></p> </li> <li> <p>Cyclic Dependencies <pre><code>pipeline.connect(\n    (node_a.data, node_b.data),\n    (node_b.result, node_c.data),\n    (node_c.final, node_a.data)  # Cycle!\n)\n</code></pre></p> </li> <li> <p>Memory Leaks <pre><code># Bad: No cleanup\nfor i in range(1000):\n    pipeline = CuvisPipeline.load_from_file(\"pipeline.yaml\")\n    pipeline.forward(batch=data)\n\n# Good: Context manager\nfor i in range(1000):\n    with CuvisPipeline.load_from_file(\"pipeline.yaml\") as pipeline:\n        pipeline.forward(batch=data)\n</code></pre></p> </li> </ol>"},{"location":"concepts/pipeline-lifecycle/#related-documentation","title":"Related Documentation","text":"<ul> <li>\u2192 Node System Deep Dive - Pipeline components</li> <li>\u2192 Port System Deep Dive - Node communication</li> <li>\u2192 Two-Phase Training - Training workflow</li> <li>\u2192 Execution Stages - Stage-aware execution</li> <li>\u2192 Building Pipelines (Python) - Practical guide</li> <li>\u2192 Building Pipelines (YAML) - Config-based construction</li> </ul>"},{"location":"concepts/port-system-deep-dive/","title":"Port System","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"concepts/port-system-deep-dive/#port-system-deep-dive","title":"Port System Deep Dive","text":"<p>Typed, validated connections between nodes enable robust data flow in CUVIS.AI pipelines.</p> <p>Ports provide explicit communication interfaces with type safety, shape validation, optional connections, and symbolic dimension resolution.</p> <p>Key capabilities:</p> <ul> <li>Typed I/O with dtype and shape specifications</li> <li>Automatic connection compatibility validation</li> <li>Support for optional and variadic ports</li> <li>Symbolic dimensions resolved from node attributes</li> <li>Data flow patterns (fan-in, fan-out, multi-port)</li> </ul>"},{"location":"concepts/port-system-deep-dive/#port-architecture","title":"Port Architecture","text":""},{"location":"concepts/port-system-deep-dive/#portspec-definition","title":"PortSpec Definition","text":"<p>Dataclass specifying the contract for a port.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass PortSpec:\n    dtype: Any                          # Data type (torch.float32, etc.)\n    shape: tuple[int | str, ...]        # Shape with -1 for flexible, strings for symbolic\n    description: str = \"\"               # Documentation\n    optional: bool = False              # Connection required?\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#port-types","title":"Port Types","text":"<p>InputPort: Receives data into a node <pre><code>node.inputs.data  # InputPort(node, \"data\", spec)\npipeline.connect(source_port, target_node.inputs.data)\n</code></pre></p> <p>OutputPort: Emits data from a node <pre><code>node.outputs.scores  # OutputPort(node, \"scores\", spec)\npipeline.connect(source_node.outputs.scores, target_port)\n</code></pre></p>"},{"location":"concepts/port-system-deep-dive/#shape-specifications","title":"Shape Specifications","text":"<p>Three dimension types:</p> <p>Flexible (<code>-1</code>): Accept any size <pre><code>shape=(-1, -1, -1, -1)  # Any 4D tensor\n</code></pre></p> <p>Fixed (integers): Require exact size <pre><code>shape=(128, 128, 3)  # Exactly 128x128 RGB\n</code></pre></p> <p>Symbolic (strings): Resolve from node attributes <pre><code>shape=(-1, \"n_select\")  # Second dim from self.n_select\n</code></pre></p>"},{"location":"concepts/port-system-deep-dive/#connection-validation","title":"Connection Validation","text":"<pre><code>flowchart LR\n    A[Source Port] --&gt;|dtype check| B{Compatible?}\n    B --&gt;|No| C[PortCompatibilityError]\n    B --&gt;|Yes| D[Resolve symbolic dims]\n    D --&gt;|shape check| E{Compatible?}\n    E --&gt;|No| C\n    E --&gt;|Yes| F[Connection Valid]\n    style A fill:#e1f5ff\n    style F fill:#d4edda\n    style C fill:#f8d7da</code></pre> <p>Compatibility Rules:</p> Source Target Compatible? <code>torch.float32</code> <code>torch.float32</code> \u2705 Yes <code>torch.float32</code> <code>torch.float64</code> \u274c No <code>torch.Tensor</code> <code>torch.float32</code> \u2705 Yes <code>(-1, -1, 61)</code> <code>(-1, -1, 61)</code> \u2705 Yes <code>(-1, -1, 61)</code> <code>(-1, -1, -1)</code> \u2705 Yes (flexible target) <code>(-1, -1, 61)</code> <code>(-1, -1, 30)</code> \u274c No (fixed mismatch)"},{"location":"concepts/port-system-deep-dive/#connection-api","title":"Connection API","text":"<pre><code># Single connection\npipeline.connect((source_node.result, target_node.data))\n\n# Batch connections\npipeline.connect(\n    (node1.data, node2.data),\n    (node2.result, node3.features),\n    (node3.scores, node4.predictions)\n)\n\n# Nodes auto-added if not present\npipeline.connect((new_node1.data, new_node2.data))\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#simplified-syntax","title":"Simplified Syntax","text":"<pre><code># Explicit (always works)\npipeline.connect((normalizer.outputs.normalized, rx_node.inputs.data))\n\n# Simplified (when unambiguous)\npipeline.connect((normalizer.normalized, rx_node.data))\n\n# Use explicit syntax only when node has same-named input and output port\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#port-definition","title":"Port Definition","text":""},{"location":"concepts/port-system-deep-dive/#input-ports","title":"Input Ports","text":"<pre><code>class ProcessingNode(Node):\n    INPUT_SPECS = {\n        \"data\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),\n            description=\"Input hyperspectral cube\",\n            optional=False\n        ),\n        \"mask\": PortSpec(\n            dtype=torch.bool,\n            shape=(-1, -1, -1),\n            description=\"Optional binary mask\",\n            optional=True\n        ),\n        \"weights\": PortSpec(\n            dtype=torch.float32,\n            shape=(\"n_channels\",),  # Symbolic\n            description=\"Per-channel weights\"\n        )\n    }\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#output-ports","title":"Output Ports","text":"<pre><code>class AnalysisNode(Node):\n    OUTPUT_SPECS = {\n        \"scores\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1,),\n            description=\"Anomaly scores per sample\"\n        ),\n        \"embeddings\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, 128),\n            description=\"Feature embeddings\"\n        ),\n        \"metadata\": PortSpec(\n            dtype=dict,\n            shape=(),  # Scalar\n            description=\"Processing metadata\"\n        )\n    }\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#symbolic-dimension-resolution","title":"Symbolic Dimension Resolution","text":"<pre><code>from cuvis_ai.pipeline.ports import DimensionResolver\n\nclass SelectorNode(Node):\n    def __init__(self, n_select: int, **kwargs):\n        self.n_select = n_select  # Store before super().__init__()\n        super().__init__(**kwargs)\n\n    OUTPUT_SPECS = {\n        \"selected\": PortSpec(\n            torch.float32,\n            (-1, -1, -1, \"n_select\")  # Resolved at runtime\n        )\n    }\n\n# Resolution\nselector = SelectorNode(n_select=10)\nresolved_shape = DimensionResolver.resolve(\n    shape=(-1, -1, -1, \"n_select\"),\n    node=selector\n)\n# Result: (-1, -1, -1, 10)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#variadic-ports-fan-in","title":"Variadic Ports (Fan-in)","text":"<p>Multiple connections to one port.</p> <pre><code>class AggregatorNode(Node):\n    INPUT_SPECS = {\n        \"features\": [PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1),\n            description=\"Feature tensors to aggregate\"\n        )]\n    }\n\n    def forward(self, features: list[torch.Tensor], **_):\n        concatenated = torch.cat(features, dim=-1)\n        return {\"aggregated\": concatenated}\n\n# Connect multiple sources\npipeline.connect(\n    (node1.feat1, aggregator.features),\n    (node2.feat2, aggregator.features),\n    (node3.feat3, aggregator.features)\n)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#data-flow-patterns","title":"Data Flow Patterns","text":""},{"location":"concepts/port-system-deep-dive/#pattern-1-linear-single-inputoutput","title":"Pattern 1: Linear (Single Input/Output)","text":"<pre><code>graph LR\n    A[DataLoader] --&gt;|data| B[Normalizer]\n    B --&gt;|normalized| C[Analyzer]\n    C --&gt;|scores| D[Output]\n    style A fill:#e1f5ff\n    style D fill:#d4edda</code></pre> <pre><code>pipeline.connect(\n    (loader.data, normalizer.data),\n    (normalizer.normalized, analyzer.data),\n    (analyzer.scores, output_node.data)\n)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#pattern-2-fan-in-multiple-sources","title":"Pattern 2: Fan-in (Multiple Sources)","text":"<pre><code>graph LR\n    A[Source A] --&gt;|features_a| C[Merger]\n    B[Source B] --&gt;|features_b| C\n    C --&gt;|merged| D[Downstream]\n    style A fill:#e1f5ff\n    style B fill:#e1f5ff\n    style D fill:#d4edda</code></pre> <pre><code>pipeline.connect(\n    (source_a.features, merger.features_a),\n    (source_b.features, merger.features_b)\n)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#pattern-3-fan-out-multiple-targets","title":"Pattern 3: Fan-out (Multiple Targets)","text":"<pre><code>graph LR\n    A[Preprocessor] --&gt;|data| B[Branch A]\n    A --&gt;|data| C[Branch B]\n    A --&gt;|data| D[Branch C]\n    style A fill:#e1f5ff\n    style B fill:#d4edda\n    style C fill:#d4edda\n    style D fill:#d4edda</code></pre> <pre><code>pipeline.connect(\n    (preprocessor.data, branch_a.data),\n    (preprocessor.data, branch_b.data),\n    (preprocessor.data, branch_c.data)\n)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#pattern-4-multi-port-node","title":"Pattern 4: Multi-Port Node","text":"<pre><code>graph LR\n    A[Analyzer] --&gt;|scores| B[Decider]\n    A --&gt;|embeddings| C[Visualizer]\n    A --&gt;|metadata| D[Logger]\n    style A fill:#f3e5f5\n    style B fill:#d4edda\n    style C fill:#d4edda\n    style D fill:#d4edda</code></pre> <pre><code>pipeline.connect(\n    (analyzer.scores, decider.predictions),\n    (analyzer.embeddings, visualizer.features),\n    (analyzer.metadata, logger.data)\n)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#pattern-5-complete-pipeline-example","title":"Pattern 5: Complete Pipeline Example","text":"<p>This diagram shows a real anomaly detection pipeline with port-based data flow, including processing nodes, loss computation, and visualization:</p> <pre><code>graph LR\n    A[Input Batch] --&gt; B[Port Distribution]\n    B --&gt; C[MinMaxNormalizer.data]\n    C --&gt; D[MinMaxNormalizer.normalized]\n    D --&gt; E[SoftChannelSelector.data]\n    E --&gt; F[SoftChannelSelector.selected]\n    F --&gt; G[TrainablePCA.features]\n    G --&gt; H[TrainablePCA.projected]\n    H --&gt; I[RXGlobal.data]\n    I --&gt; J[RXGlobal.scores]\n\n    E --&gt; K[SelectorEntropyReg.weights]\n    F --&gt; L[SelectorDiversityReg.weights]\n    J --&gt; M[AnomalyBCEWithLogits.predictions]\n    J --&gt; N[AnomalyHeatmap.data]\n\n    K --&gt; O[SelectorEntropyReg.loss]\n    L --&gt; P[SelectorDiversityReg.loss]\n    M --&gt; Q[AnomalyBCEWithLogits.loss]\n\n    O --&gt; R[GradientTrainer loss_nodes]\n    P --&gt; R\n    Q --&gt; R\n\n    N --&gt; S[VisualizationManager.inputs]\n    R --&gt; T[Backward Pass]\n    S --&gt; U[Monitor Output]\n\n    style A fill:#e1f5ff\n    style T fill:#ffc107\n    style U fill:#2196f3</code></pre> <p>Data Flow Explanation:</p> <ol> <li>Main Processing Chain: Input \u2192 MinMaxNormalizer \u2192 SoftChannelSelector \u2192 TrainablePCA \u2192 RXGlobal \u2192 Anomaly Scores</li> <li>Regularization Branch: Selector weights feed into entropy and diversity regularizers</li> <li>Loss Aggregation: Three loss nodes (BCE, entropy, diversity) are registered with GradientTrainer</li> <li>Visualization Branch: Scores and heatmaps feed into monitoring system</li> </ol> <p>Code Example:</p> <pre><code>from cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.node.selector import SoftChannelSelector\nfrom cuvis_ai.node.pca import TrainablePCA\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.losses import AnomalyBCEWithLogits, SelectorEntropyRegularizer, SelectorDiversityRegularizer\nfrom cuvis_ai.node.visualizations import AnomalyHeatmap\n\n# Create nodes\nnormalizer = MinMaxNormalizer(eps=1e-6)\nselector = SoftChannelSelector(n_select=15, input_channels=61)\npca = TrainablePCA(n_components=10, input_dim=15)\nrx = RXGlobal(num_channels=10)\nbce_loss = AnomalyBCEWithLogits(name=\"bce\", weight=10.0)\nentropy_loss = SelectorEntropyRegularizer(name=\"entropy\", weight=0.1)\ndiversity_loss = SelectorDiversityRegularizer(name=\"diversity\", weight=0.01)\nheatmap = AnomalyHeatmap(name=\"heatmap\")\n\n# Connect pipeline\npipeline.connect(\n    # Main processing chain\n    (data_node.cube, normalizer.data),\n    (normalizer.normalized, selector.data),\n    (selector.selected, pca.features),\n    (pca.projected, rx.data),\n    # Loss connections\n    (rx.scores, bce_loss.predictions),\n    (data_node.mask, bce_loss.targets),\n    (selector.weights, entropy_loss.weights),\n    (selector.weights, diversity_loss.weights),\n    # Visualization\n    (rx.scores, heatmap.scores),\n)\n\n# Register loss nodes with trainer\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss, entropy_loss, diversity_loss],\n    ...\n)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#advanced-features","title":"Advanced Features","text":""},{"location":"concepts/port-system-deep-dive/#optional-ports","title":"Optional Ports","text":"<pre><code>class FlexibleNode(Node):\n    INPUT_SPECS = {\n        \"data\": PortSpec(..., optional=False),\n        \"mask\": PortSpec(..., optional=True),\n    }\n\n    def forward(self, data: torch.Tensor, mask: torch.Tensor | None = None, **_):\n        if mask is not None:\n            processed = data * mask.unsqueeze(-1)\n        else:\n            processed = data\n        return {\"processed\": processed}\n\n# mask can be left unconnected\npipeline.connect((source.data, node.data))\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#dynamic-port-resolution","title":"Dynamic Port Resolution","text":"<pre><code>class AdaptiveNode(Node):\n    def __init__(self, output_dim: int, **kwargs):\n        self.output_dim = output_dim\n        super().__init__(**kwargs)\n\n    INPUT_SPECS = {\n        \"features\": PortSpec(torch.float32, (-1, \"input_dim\"))\n    }\n\n    OUTPUT_SPECS = {\n        \"transformed\": PortSpec(torch.float32, (-1, \"output_dim\"))\n    }\n\n    @property\n    def input_dim(self):\n        return self.linear.in_features\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#best-practices","title":"Best Practices","text":"<ol> <li>Descriptive Port Names</li> </ol> <pre><code>OUTPUT_SPECS = {\n    \"anomaly_scores\": PortSpec(...),\n    \"feature_embeddings\": PortSpec(...),\n    \"class_probabilities\": PortSpec(...)\n}\n</code></pre> <ol> <li>Explicit Data Types</li> </ol> <pre><code>PortSpec(dtype=torch.float32, shape=(-1, 128))\n# NOT: dtype=torch.Tensor (too generic)\n</code></pre> <ol> <li>Document Port Requirements</li> </ol> <pre><code>INPUT_SPECS = {\n    \"hyperspectral_cube\": PortSpec(\n        dtype=torch.float32,\n        shape=(-1, -1, -1, 61),\n        description=\"\"\"\n        Format: BHWC (Batch, Height, Width, Channels)\n        Channels: 61 spectral bands (400-1000nm)\n        Normalization: Values in [0, 1]\n        \"\"\"\n    )\n}\n</code></pre> <ol> <li>Validate in forward()</li> </ol> <pre><code>def forward(self, data: torch.Tensor, **_):\n    if data.ndim != 4:\n        raise ValueError(f\"Expected 4D tensor (BHWC), got {data.ndim}D\")\n    if data.shape[-1] != self.expected_channels:\n        raise ValueError(f\"Expected {self.expected_channels} channels, got {data.shape[-1]}\")\n    if data.min() &lt; 0 or data.max() &gt; 1:\n        raise ValueError(\"Data must be in [0, 1] range\")\n</code></pre> <ol> <li>Handle Optional Ports Gracefully</li> </ol> <pre><code>def forward(\n    self,\n    data: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    weights: torch.Tensor | None = None,\n    **_\n):\n    if mask is None:\n        mask = torch.ones_like(data[..., 0], dtype=torch.bool)\n    if weights is None:\n        weights = torch.ones(data.shape[-1], device=data.device)\n    return self._process(data, mask, weights)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/port-system-deep-dive/#type-mismatch","title":"Type Mismatch","text":"<pre><code>PortCompatibilityError: Cannot connect ports: dtype mismatch\n  Source: normalizer (torch.float32)\n  Target: analyzer (torch.float64)\n</code></pre> <p>Fix: Use consistent dtypes</p> <pre><code>normalizer = MinMaxNormalizer(dtype=torch.float32)\nanalyzer = RXGlobal(dtype=torch.float32)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#shape-mismatch","title":"Shape Mismatch","text":"<pre><code>PortCompatibilityError: Cannot connect ports: shape mismatch\n  Source: selector (shape: (-1, -1, -1, 10))\n  Target: rx_node (shape: (-1, -1, -1, 61))\n</code></pre> <p>Fix: Update node configuration</p> <pre><code>selector = SoftChannelSelector(n_select=61, input_channels=61)\nrx_node = RXGlobal(num_channels=61)\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#missing-required-port","title":"Missing Required Port","text":"<p>Fix: Connect all required ports</p> <pre><code>pipeline.connect((source.result, rx_node.data))\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#symbolic-dimension-not-found","title":"Symbolic Dimension Not Found","text":"<pre><code>AttributeError: 'SelectorNode' has no attribute 'n_channels'\n</code></pre> <p>Fix: Store attribute before super().init()</p> <pre><code>class SelectorNode(Node):\n    def __init__(self, n_channels: int, **kwargs):\n        self.n_channels = n_channels  # MUST be before super()\n        super().__init__(**kwargs)\n\n    OUTPUT_SPECS = {\n        \"selected\": PortSpec(torch.float32, (-1, -1, -1, \"n_channels\"))\n    }\n</code></pre>"},{"location":"concepts/port-system-deep-dive/#related-documentation","title":"Related Documentation","text":"<ul> <li>\u2192 Node System Deep Dive - Node I/O and port usage</li> <li>\u2192 Pipeline Lifecycle - Node integration in pipelines</li> <li>\u2192 Two-Phase Training - Statistical initialization</li> <li>\u2192 Building Pipelines (Python) - Practical construction</li> <li>\u2192 Building Pipelines (YAML) - Configuration-based construction</li> </ul>"},{"location":"concepts/two-phase-training/","title":"Two-Phase Training","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"concepts/two-phase-training/#two-phase-training","title":"Two-Phase Training","text":"<p>Many pipelines use two phases: statistical initialization from initialization data, then optional gradient training via backpropagation.</p> <p>Two-phase training combines statistical methods and deep learning. Phase 1 computes statistics fast and deterministically. Phase 2 refines parameters via gradients for task-specific optimization.</p> <p>Phase 1 Benefits: * \u26a1 Fast (seconds to minutes) * \ud83d\udcbe Memory efficient * \ud83c\udfaf Strong initialization for Phase 2 * \ud83d\udd12 Deterministic and reproducible</p> <p>Phase 2 Benefits: * \ud83d\udcc8 Learns task-specific features * \ud83c\udfa8 Flexible optimization objectives * \ud83d\udd2c Fine-grained parameter refinement</p>"},{"location":"concepts/two-phase-training/#phase-1-statistical-initialization","title":"Phase 1: Statistical Initialization","text":"<p>Compute statistics from initialization data without gradients using Welford's algorithm.</p>"},{"location":"concepts/two-phase-training/#statistical-training-lifecycle","title":"Statistical Training Lifecycle","text":"<p>This flowchart shows the complete Phase 1 training process, from trainer creation through validation:</p> <pre><code>flowchart TD\n    A[Create StatisticalTrainer] --&gt; B[\"stat_trainer = StatisticalTrainer(pipeline, datamodule)\"]\n    B --&gt; C[\"stat_trainer.fit() called\"]\n    C --&gt; D{Find nodes with&lt;br/&gt;requires_initial_fit?}\n    D --&gt;|Yes| E[Sort nodes by&lt;br/&gt;port connections]\n    D --&gt;|No| M[Skip Phase 1]\n    E --&gt; F[For each statistical node]\n    F --&gt; G[Get transformed data&lt;br/&gt;via port routing]\n    G --&gt; H[\"Call node.statistical_initialization(input_stream)\"]\n    H --&gt; I[Accumulate statistics&lt;br/&gt;using Welford's algorithm]\n    I --&gt; J[Store as frozen buffers&lt;br/&gt;register_buffer()]\n    J --&gt; K[\"Call node.prepare_for_gradient_train()\"]\n    K --&gt; L{More nodes?}\n    L --&gt;|Yes| F\n    L --&gt;|No| M[Statistical initialization complete]\n    M --&gt; N[\"Optional: stat_trainer.validate()\"]\n    N --&gt; O[Validation metrics]\n\n    style A fill:#e1f5ff\n    style C fill:#fff3cd\n    style H fill:#f3e5f5\n    style M fill:#d4edda\n    style O fill:#d4edda</code></pre> <p>Key Steps:</p> <ol> <li>Trainer Creation: Instantiate <code>StatisticalTrainer</code> with pipeline and datamodule</li> <li>fit() Execution: Calls the statistical initialization process</li> <li>Node Discovery: Finds all nodes with <code>requires_initial_fit=True</code></li> <li>Topological Ordering: Sorts nodes by port connections for proper data flow</li> <li>Per-Node Initialization:</li> <li>Routes data through preceding nodes via port connections</li> <li>Calls <code>statistical_initialization()</code> with data stream</li> <li>Accumulates statistics using Welford's online algorithm</li> <li>Stores statistics as PyTorch buffers (frozen, non-trainable)</li> <li>Prepares node for potential gradient training phase</li> <li>Validation (Optional): Test initialized pipeline on validation set</li> </ol>"},{"location":"concepts/two-phase-training/#loading-initialization-data","title":"Loading Initialization Data","text":"<pre><code>from cuvis_ai.data.datamodule import SingleCu3sDataModule\n\ndatamodule = SingleCu3sDataModule(\n    cu3s_file_path=\"data/initialization/samples.cu3s\",\n    annotation_json_path=\"data/initialization/annotations.json\",\n    train_ids=[0, 1, 2],  # Use for initialization\n    val_ids=[3],\n    test_ids=[4],\n    batch_size=4\n)\n\ndatamodule.setup(stage=\"fit\")\n</code></pre>"},{"location":"concepts/two-phase-training/#computing-statistics","title":"Computing Statistics","text":"<pre><code>from cuvis_ai.trainer.statistical_trainer import StatisticalTrainer\n\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()  # Process all batches\n\n# Nodes now initialized\nassert pipeline.nodes[\"rx_detector\"]._statistically_initialized\n</code></pre> <p>What happens during fit(): 1. Iterate through training batches 2. For each node with <code>requires_initial_fit=True</code>:    - Pass data through preceding nodes    - Call <code>node.statistical_initialization(input_stream)</code>    - Accumulate statistics (mean, covariance, etc.) 3. Store statistics as frozen buffers 4. Mark node as initialized</p>"},{"location":"concepts/two-phase-training/#statistical-nodes","title":"Statistical Nodes","text":""},{"location":"concepts/two-phase-training/#rxglobal-anomaly-detection","title":"RXGlobal (Anomaly Detection)","text":"<pre><code>from cuvis_ai.anomaly.rx_detector import RXGlobal\n\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nrx_node = RXGlobal(num_channels=61, eps=1e-6)\npipeline.add_node(rx_node)\n\n# Use StatisticalTrainer to initialize\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically calls statistical_initialization on rx_node\n\nprint(rx_node.mu.shape)      # (61,) - Background mean\nprint(rx_node.sigma.shape)   # (61, 61) - Covariance matrix\n</code></pre> <p>Implementation Pattern: <pre><code>def statistical_initialization(self, input_stream: InputStream) -&gt; None:\n    \"\"\"Compute statistics using Welford's algorithm.\"\"\"\n    n = 0\n    mean = torch.zeros(self.num_channels, dtype=torch.float64)\n    M2 = torch.zeros((self.num_channels, self.num_channels), dtype=torch.float64)\n\n    for batch_data in input_stream:\n        data = batch_data[\"data\"]\n        flattened = data.flatten(0, 2).double()\n\n        for sample in flattened:\n            n += 1\n            delta = sample - mean\n            mean += delta / n\n            delta2 = sample - mean\n            M2 += torch.outer(delta, delta2)\n\n    covariance = M2 / (n - 1) if n &gt; 1 else M2\n    self.register_buffer(\"mu\", mean.float())\n    self.register_buffer(\"sigma\", covariance.float())\n    self._statistically_initialized = True\n</code></pre></p>"},{"location":"concepts/two-phase-training/#minmaxnormalizer","title":"MinMaxNormalizer","text":"<pre><code>from cuvis_ai.node.normalization import MinMaxNormalizer\n\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True)\npipeline.add_node(normalizer)\n\n# Use StatisticalTrainer to initialize\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically calls statistical_initialization on normalizer\n\nprint(normalizer.running_min)  # Global min values\nprint(normalizer.running_max)  # Global max values\n</code></pre>"},{"location":"concepts/two-phase-training/#softchannelselector","title":"SoftChannelSelector","text":"<pre><code>from cuvis_ai.node.selector import SoftChannelSelector\n\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nselector = SoftChannelSelector(\n    n_select=10,\n    input_channels=61,\n    init_method=\"variance\",\n    temperature_init=5.0\n)\npipeline.add_node(selector)\n\n# Use StatisticalTrainer to initialize\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically calls statistical_initialization on selector\n</code></pre>"},{"location":"concepts/two-phase-training/#trainablepca","title":"TrainablePCA","text":"<pre><code>from cuvis_ai.node.pca import TrainablePCA\n\nfrom cuvis_ai_core.training import StatisticalTrainer\n\npca = TrainablePCA(n_components=10, input_dim=61)\npipeline.add_node(pca)\n\n# Use StatisticalTrainer to initialize\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically calls statistical_initialization on pca\n\nprint(pca.components.shape)      # (10, 61)\nprint(pca.explained_variance)    # Variance per component\n</code></pre>"},{"location":"concepts/two-phase-training/#validation-after-phase-1","title":"Validation After Phase 1","text":"<pre><code>val_results = stat_trainer.validate()\n\nfor metric in val_results[\"metrics\"]:\n    print(f\"{metric.name}: {metric.value:.4f}\")\n\n# Expected: Reasonable performance with just statistics\n# Example: IoU &gt; 0.5, F1 &gt; 0.6\n</code></pre>"},{"location":"concepts/two-phase-training/#phase-2-gradient-training","title":"Phase 2: Gradient Training","text":"<p>Refine parameters via gradient descent.</p>"},{"location":"concepts/two-phase-training/#gradient-training-lifecycle","title":"Gradient Training Lifecycle","text":"<p>This flowchart shows the complete Phase 2 training process, from unfreezing nodes through test evaluation:</p> <pre><code>flowchart TD\n    A[After Phase 1 completion] --&gt; B[\"Select nodes to unfreeze\"]\n    B --&gt; C[\"pipeline.unfreeze_nodes_by_name(['selector', 'rx_detector'])\"]\n    C --&gt; D[\"Buffers \u2192 Parameters&lt;br/&gt;(requires_grad=True)\"]\n    D --&gt; E[\"Create GradientTrainer\"]\n    E --&gt; F[\"grad_trainer = GradientTrainer(&lt;br/&gt;pipeline, datamodule,&lt;br/&gt;loss_nodes, metric_nodes,&lt;br/&gt;trainer_config, optimizer_config)\"]\n    F --&gt; G[\"grad_trainer.fit() called\"]\n    G --&gt; H[Create CuvisLightningModule]\n    H --&gt; I[Register loss nodes via ports]\n    I --&gt; J[Register metric nodes via ports]\n    J --&gt; K[Create PyTorch Lightning Trainer]\n    K --&gt; L[Training Loop Start]\n    L --&gt; M[training_step:&lt;br/&gt;Port-based forward pass]\n    M --&gt; N[Aggregate loss from loss_nodes]\n    N --&gt; O[Backward pass + optimizer step]\n    O --&gt; P[validation_step:&lt;br/&gt;Forward pass + metrics]\n    P --&gt; Q{Execute callbacks?}\n    Q --&gt;|Early stopping| R[Check stopping criteria]\n    Q --&gt;|Checkpoint| S[Save best model]\n    Q --&gt;|LR scheduler| T[Adjust learning rate]\n    R --&gt; U{Continue training?}\n    S --&gt; U\n    T --&gt; U\n    U --&gt;|Yes| M\n    U --&gt;|No| V[Training Complete]\n    V --&gt; W[\"Optional: grad_trainer.test()\"]\n    W --&gt; X[Test metrics with best checkpoint]\n\n    style A fill:#e1f5ff\n    style G fill:#fff3cd\n    style M fill:#f3e5f5\n    style O fill:#ffc107\n    style V fill:#d4edda\n    style X fill:#d4edda</code></pre> <p>Key Steps:</p> <ol> <li>Node Unfreezing: Select which nodes to train via <code>unfreeze_nodes_by_name()</code></li> <li>Buffer Conversion: Frozen buffers converted to trainable parameters</li> <li>Trainer Creation: Instantiate <code>GradientTrainer</code> with:</li> <li>Pipeline and datamodule</li> <li>Loss nodes (compute training objective)</li> <li>Metric nodes (track performance)</li> <li>Training config (epochs, accelerator, callbacks)</li> <li>Optimizer config (learning rate, weight decay)</li> <li>Lightning Setup:</li> <li>Create <code>CuvisLightningModule</code> wrapper</li> <li>Register loss/metric nodes via port connections</li> <li>Create PyTorch Lightning <code>Trainer</code> instance</li> <li>Training Loop:</li> <li>training_step: Forward pass through pipeline, aggregate losses, backward pass</li> <li>validation_step: Forward pass, compute metrics</li> <li>Callbacks: Early stopping, checkpointing, learning rate scheduling</li> <li>Test Evaluation (Optional): Final evaluation on test set with best checkpoint</li> </ol>"},{"location":"concepts/two-phase-training/#unfreezing-nodes","title":"Unfreezing Nodes","text":"<pre><code># After Phase 1\nstat_trainer.fit()\n\n# Select nodes to train\nunfreeze_node_names = [\"rx_detector\", \"selector\"]\npipeline.unfreeze_nodes_by_name(unfreeze_node_names)\n\ntrainable_params = sum(p.numel() for p in pipeline.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params:,}\")\n</code></pre> <p>What unfreeze() does: <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"Convert buffers to parameters for gradient training.\"\"\"\n    for name, buffer in list(self._buffers.items()):\n        if buffer is not None and buffer.numel() &gt; 0:\n            param = nn.Parameter(buffer.clone())\n            delattr(self, name)\n            setattr(self, name, param)\n    self.freezed = False\n</code></pre></p>"},{"location":"concepts/two-phase-training/#defining-loss-nodes","title":"Defining Loss Nodes","text":"<pre><code>from cuvis_ai.node.losses import AnomalyBCEWithLogits\n\nbce_loss = AnomalyBCEWithLogits(name=\"bce\", weight=10.0)\n\npipeline.connect(\n    (logit_head.logits, bce_loss.predictions),\n    (data_node.mask, bce_loss.targets)\n)\n</code></pre>"},{"location":"concepts/two-phase-training/#gradient-training","title":"Gradient Training","text":"<pre><code>from cuvis_ai.trainer.gradient_trainer import GradientTrainer\nfrom cuvis_ai.config.trainrun import TrainingConfig, OptimizerConfig, SchedulerConfig\n\ntraining_config = TrainingConfig(\n    seed=42,\n    optimizer=OptimizerConfig(\n        name=\"adamw\",\n        lr=0.001,\n        weight_decay=1e-4\n    ),\n    scheduler=SchedulerConfig(\n        name=\"reduce_on_plateau\",\n        monitor=\"metrics_anomaly/iou\",\n        mode=\"max\",\n        factor=0.5,\n        patience=5\n    ),\n    trainer=TrainerConfig(\n        max_epochs=50,\n        accelerator=\"auto\"\n    )\n)\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss],\n    metric_nodes=[metrics_node],\n    trainer_config=training_config.trainer,\n    optimizer_config=training_config.optimizer,\n    scheduler_config=training_config.scheduler\n)\n\ngrad_trainer.fit()\ntest_results = grad_trainer.test()\n</code></pre>"},{"location":"concepts/two-phase-training/#callbacks","title":"Callbacks","text":"<p>Hooks into training loop for monitoring, checkpointing, and early stopping.</p>"},{"location":"concepts/two-phase-training/#modelcheckpoint","title":"ModelCheckpoint","text":"<p>Automatically save best models:</p> <pre><code>from cuvis_ai.config.trainrun import CallbacksConfig, ModelCheckpointConfig\n\ncallbacks_config = CallbacksConfig(\n    checkpoint=ModelCheckpointConfig(\n        monitor=\"metrics_anomaly/iou\",\n        mode=\"max\",\n        save_top_k=3,\n        filename=\"best-{epoch:02d}-{val_iou:.3f}\",\n        dirpath=\"outputs/checkpoints\"\n    )\n)\n</code></pre>"},{"location":"concepts/two-phase-training/#earlystopping","title":"EarlyStopping","text":"<p>Stop when metric stops improving:</p> <pre><code>from cuvis_ai.config.trainrun import EarlyStoppingConfig\n\ncallbacks_config = CallbacksConfig(\n    early_stopping=[\n        EarlyStoppingConfig(\n            monitor=\"val/bce\",\n            mode=\"min\",\n            patience=10,\n            min_delta=0.001\n        )\n    ]\n)\n</code></pre>"},{"location":"concepts/two-phase-training/#complete-callback-config","title":"Complete Callback Config","text":"<pre><code>training_config = TrainingConfig(\n    trainer=TrainerConfig(\n        max_epochs=50,\n        accelerator=\"auto\",\n        callbacks=CallbacksConfig(\n            checkpoint=ModelCheckpointConfig(\n                monitor=\"metrics_anomaly/iou\",\n                mode=\"max\",\n                save_top_k=3\n            ),\n            early_stopping=[\n                EarlyStoppingConfig(\n                    monitor=\"val/bce\",\n                    mode=\"min\",\n                    patience=10,\n                    min_delta=0.001\n                )\n            ],\n            lr_monitor=LearningRateMonitorConfig(logging_interval=\"epoch\")\n        )\n    )\n)\n</code></pre>"},{"location":"concepts/two-phase-training/#complete-two-phase-example","title":"Complete Two-Phase Example","text":"<pre><code>from cuvis_ai.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.node.selector import SoftChannelSelector\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.losses import AnomalyBCEWithLogits\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.trainer.statistical_trainer import StatisticalTrainer\nfrom cuvis_ai.trainer.gradient_trainer import GradientTrainer\n\n# ============ SETUP ============\n\npipeline = CuvisPipeline(\"Channel_Selector\")\n\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True)\nselector = SoftChannelSelector(n_select=10, input_channels=61, init_method=\"variance\")\nrx_node = RXGlobal(num_channels=10, eps=1e-6)\nlogit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\ndecider = BinaryDecider(threshold=0.5)\n\nbce_loss = AnomalyBCEWithLogits(name=\"bce\", weight=10.0)\nmetrics_node = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n\npipeline.connect(\n    (data_node.cube, normalizer.data),\n    (normalizer.normalized, selector.data),\n    (selector.selected, rx_node.data),\n    (rx_node.scores, logit_head.scores),\n    (logit_head.logits, bce_loss.predictions),\n    (data_node.mask, bce_loss.targets),\n    (logit_head.logits, decider.logits),\n    (decider.decisions, metrics_node.decisions),\n    (data_node.mask, metrics_node.targets)\n)\n\ndatamodule = SingleCu3sDataModule(\n    cu3s_file_path=\"data/Lentils/Lentils_000.cu3s\",\n    annotation_json_path=\"data/Lentils/Lentils_000.json\",\n    train_ids=[0, 2, 3],\n    val_ids=[1],\n    test_ids=[1, 5],\n    batch_size=2\n)\n\n# ============ PHASE 1: STATISTICAL INITIALIZATION ============\n\nprint(\"Phase 1: Statistical initialization...\")\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()\n\nprint(\"\u2713 Statistical initialization complete\")\nval_results = stat_trainer.validate()\nprint(f\"  Validation IoU: {val_results['metrics_anomaly/iou']:.3f}\")\n\n# ============ PHASE 2: GRADIENT TRAINING ============\n\nprint(\"\\nPhase 2: Gradient training...\")\nunfreeze_nodes = [selector.name, rx_node.name, logit_head.name]\npipeline.unfreeze_nodes_by_name(unfreeze_nodes)\n\ntrainable_params = sum(p.numel() for p in pipeline.parameters() if p.requires_grad)\nprint(f\"\u2713 Unfrozen {len(unfreeze_nodes)} nodes ({trainable_params:,} parameters)\")\n\ntraining_config = TrainingConfig(\n    seed=42,\n    optimizer=OptimizerConfig(name=\"adamw\", lr=0.001),\n    scheduler=SchedulerConfig(name=\"reduce_on_plateau\", monitor=\"metrics_anomaly/iou\", mode=\"max\"),\n    trainer=TrainerConfig(max_epochs=20, accelerator=\"auto\")\n)\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss],\n    metric_nodes=[metrics_node],\n    trainer_config=training_config.trainer,\n    optimizer_config=training_config.optimizer,\n    scheduler_config=training_config.scheduler\n)\n\ngrad_trainer.fit()\ntest_results = grad_trainer.test()\nprint(f\"\u2713 Test IoU: {test_results['metrics_anomaly/iou']:.3f}\")\n\n# ============ SAVE ============\n\nfrom cuvis_ai.pipeline.config import PipelineMetadata\n\npipeline.save_to_file(\n    \"outputs/channel_selector.yaml\",\n    metadata=PipelineMetadata(\n        name=\"Channel_Selector_v1\",\n        description=\"RX + learnable channel selection\",\n        tags=[\"two-phase\", \"gradient-trained\", \"production\"]\n    )\n)\n\nprint(\"\u2713 Saved to outputs/channel_selector.yaml\")\n</code></pre>"},{"location":"concepts/two-phase-training/#saving-loading","title":"Saving &amp; Loading","text":""},{"location":"concepts/two-phase-training/#option-1-save-full-pipeline","title":"Option 1: Save Full Pipeline","text":"<pre><code>pipeline.save_to_file(\n    \"outputs/trained_pipeline.yaml\",\n    metadata=PipelineMetadata(name=\"Full_Pipeline\")\n)\n# Generates: outputs/trained_pipeline.yaml, outputs/trained_pipeline.pt\n</code></pre>"},{"location":"concepts/two-phase-training/#option-2-save-initialization-state","title":"Option 2: Save Initialization State","text":"<pre><code>initialization_state = {\n    \"rx_mean\": pipeline.nodes[\"rx_detector\"].mu.clone(),\n    \"rx_cov\": pipeline.nodes[\"rx_detector\"].sigma.clone(),\n    \"normalizer_min\": pipeline.nodes[\"normalizer\"].running_min.clone(),\n    \"normalizer_max\": pipeline.nodes[\"normalizer\"].running_max.clone(),\n}\ntorch.save(initialization_state, \"outputs/initialization.pt\")\n</code></pre>"},{"location":"concepts/two-phase-training/#load-complete-pipeline","title":"Load Complete Pipeline","text":"<pre><code>pipeline = CuvisPipeline.load_from_file(\n    config_path=\"outputs/trained_pipeline.yaml\",\n    weights_path=\"outputs/trained_pipeline.pt\",\n    device=\"cuda\"\n)\n\noutputs = pipeline.forward(batch=test_data)\n</code></pre>"},{"location":"concepts/two-phase-training/#load-with-separate-initialization","title":"Load with Separate Initialization","text":"<pre><code>init_state = torch.load(\"outputs/initialization.pt\")\n\npipeline = build_pipeline()\npipeline.nodes[\"rx_detector\"].mu = init_state[\"rx_mean\"]\npipeline.nodes[\"rx_detector\"].sigma = init_state[\"rx_cov\"]\npipeline.nodes[\"rx_detector\"]._statistically_initialized = True\n</code></pre>"},{"location":"concepts/two-phase-training/#best-practices","title":"Best Practices","text":"<ol> <li>Separate Initialization Data</li> </ol> <p>Use \"normal\" samples for initialization, diverse data for training:    <pre><code>initialization_ids = [0, 1, 2]      # Clean, representative\ntraining_ids = [0, 1, 2, 3, 4, 5]  # Includes anomalies\n</code></pre></p> <ol> <li>Validate Initialization Quality</li> </ol> <pre><code>def validate_initialization(rx_node, validation_data):\n    with torch.no_grad():\n        scores = []\n        for batch in validation_data:\n            outputs = rx_node.forward(data=batch[\"data\"])\n            scores.append(outputs[\"scores\"])\n\n        all_scores = torch.cat(scores)\n        mean_score = all_scores.mean().item()\n\n        if mean_score &gt; 1.0:\n            print(\"\u26a0\ufe0f Warning: High mean score indicates poor initialization\")\n</code></pre> <ol> <li>Detect Data Drift</li> </ol> <pre><code>def needs_reinitialization(node, new_data_stats, threshold=0.1):\n    if not node._statistically_initialized:\n        return True\n\n    mean_drift = torch.abs(node.mu - new_data_stats[\"mean\"]).mean()\n    drift_ratio = mean_drift / torch.abs(node.mu).mean()\n\n    return drift_ratio &gt; threshold\n</code></pre> <ol> <li>Version Initialization</li> </ol> <pre><code>initialization_meta = {\n    \"version\": \"1.0\",\n    \"date\": \"2026-01-29\",\n    \"source\": \"data/initialization_v1/\",\n    \"n_samples\": len(initialization_data),\n}\ntorch.save(initialization_meta, \"outputs/initialization_v1_meta.pt\")\n</code></pre>"},{"location":"concepts/two-phase-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/two-phase-training/#node-not-initialized","title":"\"Node not initialized\"","text":"<p>Solution: Use <code>StatisticalTrainer</code> for Phase 1 before Phase 2</p> <pre><code>from cuvis_ai_core.training import StatisticalTrainer\n\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()  # Phase 1\n\ngrad_trainer.fit()  # Phase 2\n</code></pre>"},{"location":"concepts/two-phase-training/#poor-detection-performance","title":"Poor Detection Performance","text":"<p>Causes: Bad initialization data, small sample size, contaminated data</p> <p>Solution: <pre><code>print(f\"Calibration samples: {len(calib_data)}\")\nprint(f\"Data range: [{calib_data.min():.2f}, {calib_data.max():.2f}]\")\n\n# Check for outliers\npercentiles = torch.quantile(calib_data, torch.tensor([0.01, 0.99]))\n\n# Clean or increase data\nclean_data = remove_outliers(calib_data)\n</code></pre></p>"},{"location":"concepts/two-phase-training/#gradient-training-degrades-performance","title":"Gradient Training Degrades Performance","text":"<p>Solution: <pre><code># 1. Reduce learning rate\noptimizer_config = OptimizerConfig(lr=0.0001)\n\n# 2. Freeze some nodes\nunfreeze_nodes = [\"selector\"]\n\n# 3. Use early stopping\nearly_stopping = EarlyStoppingConfig(\n    monitor=\"val/metrics_anomaly/iou\",\n    mode=\"max\",\n    patience=5\n)\n</code></pre></p>"},{"location":"concepts/two-phase-training/#calibration-takes-too-long","title":"Calibration Takes Too Long","text":"<p>Solution: <pre><code># Option 1: Sample data\nif len(calib_data) &gt; 10000:\n    indices = torch.randperm(len(calib_data))[:10000]\n    calib_data = calib_data[indices]\n\n# Option 2: Larger batch size\ndatamodule = SingleCu3sDataModule(..., batch_size=32)\n\n# Option 3: Use GPU\npipeline = pipeline.to(\"cuda\")\n</code></pre></p>"},{"location":"concepts/two-phase-training/#related-documentation","title":"Related Documentation","text":"<ul> <li>\u2192 Pipeline Lifecycle - Complete pipeline workflow</li> <li>\u2192 Node System Deep Dive - Statistical nodes</li> <li>\u2192 Execution Stages - Training vs inference</li> <li>\u2192 RX Tutorial - Statistical training example</li> <li>\u2192 Channel Selector Tutorial - Two-phase workflow</li> </ul>"},{"location":"config/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"config/#configuration-system","title":"Configuration System","text":"<p>CUVIS.AI uses Hydra for powerful configuration management with support for composition, overrides, and config groups.</p>"},{"location":"config/#configuration-documentation","title":"Configuration Documentation","text":"<ul> <li> <p> Config Groups</p> <p>Organize configurations using Hydra config groups</p> </li> <li> <p> TrainRun Schema</p> <p>Complete schema for training run configurations</p> </li> <li> <p> Pipeline Schema</p> <p>Schema for pipeline YAML configurations</p> </li> <li> <p> Hydra Composition</p> <p>Advanced composition and override patterns</p> </li> </ul> <p>Related Pages: - Configuration Guide - Build Pipeline (YAML)</p>"},{"location":"config/config-groups/","title":"Config Groups","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"config/config-groups/#configuration-groups","title":"Configuration Groups","text":"<p>Organize and manage CUVIS.AI configurations using Hydra config groups for modular, composable experiment setups.</p>"},{"location":"config/config-groups/#overview","title":"Overview","text":"<p>Configuration groups provide modular organization of experiment parameters:</p> <ul> <li>pipeline: Pipeline architecture definitions (nodes + connections)</li> <li>data: Data loading and splitting configurations</li> <li>training: Trainer, optimizer, and scheduler settings</li> <li>trainrun: Composed experiments combining all groups</li> </ul> <p>Benefits: - Reusable configurations across experiments - Easy parameter sweeps and comparisons - Clear separation of concerns - Version control friendly</p>"},{"location":"config/config-groups/#quick-start","title":"Quick Start","text":""},{"location":"config/config-groups/#directory-structure","title":"Directory Structure","text":"<pre><code>configs/\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 lentils.yaml\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 rx_statistical.yaml\n\u2502   \u251c\u2500\u2500 channel_selector.yaml\n\u2502   \u251c\u2500\u2500 deep_svdd.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 training/\n\u2502   \u2514\u2500\u2500 default.yaml\n\u2514\u2500\u2500 trainrun/\n    \u251c\u2500\u2500 rx_statistical.yaml\n    \u251c\u2500\u2500 channel_selector.yaml\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"config/config-groups/#basic-usage","title":"Basic Usage","text":"<p>Select configs in trainrun: <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: my_experiment\n</code></pre></p> <p>Override from command line: <pre><code>uv run python train.py \\\n    training.trainer.max_epochs=100 \\\n    data.batch_size=4\n</code></pre></p>"},{"location":"config/config-groups/#config-groups","title":"Config Groups","text":""},{"location":"config/config-groups/#1-pipeline-group","title":"1. Pipeline Group","text":"<p>Location: <code>configs/pipeline/</code></p> <p>Pipeline group defines the computational graph (nodes + connections).</p> <p>Available pipelines: - <code>rx_statistical.yaml</code> - RX anomaly detector (statistical) - <code>channel_selector.yaml</code> - Channel selection + RX (gradient) - <code>deep_svdd.yaml</code> - Deep SVDD anomaly detection - <code>drcnn_adaclip.yaml</code> - DRCNN + AdaClip integration - <code>concrete_adaclip.yaml</code> - Concrete band selector + AdaClip - <code>adaclip_baseline.yaml</code> - AdaClip baseline</p> <p>Selection pattern: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical\n</code></pre></p> <p>Example: rx_statistical.yaml <pre><code>metadata:\n  name: RX_Statistical\n  description: RX anomaly detector with statistical initialization\n  tags:\n    - statistical\n    - rx\n  author: cuvis.ai\n\nnodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: MinMaxNormalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: RXGlobal\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\nconnections:\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n  - from: MinMaxNormalizer.outputs.normalized\n    to: RXGlobal.inputs.data\n</code></pre></p> <p>Override pipeline parameters: <pre><code># In trainrun config\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - _self_\n\n# Override specific node params\npipeline:\n  nodes:\n    - name: RXGlobal\n      params:\n        eps: 1.0e-08  # Override from 1.0e-06\n</code></pre></p>"},{"location":"config/config-groups/#2-data-group","title":"2. Data Group","text":"<p>Location: <code>configs/data/</code></p> <p>Data group defines data loading, splitting, and preprocessing.</p> <p>Available configs: - <code>lentils.yaml</code> - Lentils anomaly detection dataset</p> <p>Selection pattern: <pre><code>defaults:\n  - /data@data: lentils\n</code></pre></p> <p>Example: lentils.yaml <pre><code>cu3s_file_path: data/Lentils/Lentils_000.cu3s\nannotation_json_path: data/Lentils/Lentils_000.json\n\ntrain_ids: [0, 2, 3]\nval_ids: [1, 5]\ntest_ids: [1, 5]\n\nbatch_size: 2\nshuffle: true\nnum_workers: 0\nprocessing_mode: Reflectance\n</code></pre></p> <p>Override data parameters: <pre><code># In trainrun config\ndefaults:\n  - /data@data: lentils\n  - _self_\n\ndata:\n  train_ids: [0, 1, 2]    # Override\n  val_ids: [3, 4]         # Override\n  batch_size: 4           # Override\n</code></pre></p> <p>Command-line overrides: <pre><code>uv run python train.py \\\n    data.train_ids=[0,1,2] \\\n    data.batch_size=8\n</code></pre></p>"},{"location":"config/config-groups/#3-training-group","title":"3. Training Group","text":"<p>Location: <code>configs/training/</code></p> <p>Training group defines trainer, optimizer, scheduler, and callback settings.</p> <p>Available configs: - <code>default.yaml</code> - Default training configuration</p> <p>Selection pattern: <pre><code>defaults:\n  - /training@training: default\n</code></pre></p> <p>Example: default.yaml <pre><code>seed: 42\n\ntrainer:\n  max_epochs: 5\n  accelerator: auto\n  devices: 1\n  precision: \"32-true\"\n  log_every_n_steps: 10\n  val_check_interval: 1.0\n  enable_checkpointing: true\n  gradient_clip_val: 1.0\n\n  callbacks:\n    learning_rate_monitor:\n      logging_interval: epoch\n      log_momentum: false\n\noptimizer:\n  name: adamw\n  lr: 0.001\n  weight_decay: 0.01\n  betas: [0.9, 0.999]\n</code></pre></p> <p>Override training parameters: <pre><code># In trainrun config\ndefaults:\n  - /training@training: default\n  - _self_\n\ntraining:\n  trainer:\n    max_epochs: 100           # Override\n    accelerator: gpu          # Override\n\n  optimizer:\n    lr: 0.0001                # Override\n\n  scheduler:                  # Add new field\n    name: reduce_on_plateau\n    monitor: metrics_anomaly/iou\n    mode: max\n</code></pre></p> <p>Command-line overrides: <pre><code>uv run python train.py \\\n    training.trainer.max_epochs=100 \\\n    training.optimizer.lr=0.0001 \\\n    training.scheduler.patience=10\n</code></pre></p>"},{"location":"config/config-groups/#4-trainrun-group","title":"4. TrainRun Group","text":"<p>Location: <code>configs/trainrun/</code></p> <p>TrainRun group composes pipeline, data, and training configs into complete experiments.</p> <p>Available trainruns: - <code>rx_statistical.yaml</code> - Statistical RX training - <code>channel_selector.yaml</code> - Channel selector gradient training - <code>deep_svdd.yaml</code> - Deep SVDD training - <code>drcnn_adaclip.yaml</code> - DRCNN + AdaClip training - <code>concrete_adaclip.yaml</code> - Concrete band selector training</p> <p>Selection pattern: <pre><code>@hydra.main(config_path=\"../configs\", config_name=\"trainrun/rx_statistical\", version_base=None)\ndef main(cfg: DictConfig):\n    ...\n</code></pre></p> <p>Example: rx_statistical.yaml <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: rx_statistical\noutput_dir: ./outputs/${name}\n\n# Statistical training (no losses)\nloss_nodes: []\nmetric_nodes: [metrics_anomaly]\nfreeze_nodes: []\nunfreeze_nodes: []\n\ntags:\n  method: rx\n  training: statistical\n  dataset: lentils\n</code></pre></p> <p>Example: drcnn_adaclip.yaml <pre><code># @package _global_\n\ndefaults:\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: drcnn_adaclip\noutput_dir: ./outputs/${name}\n\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 20\n    precision: \"32-true\"\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n  scheduler:\n    name: reduce_on_plateau\n    monitor: metrics_anomaly/iou\n    mode: max\n    factor: 0.5\n    patience: 5\n\n# Gradient training configuration\nloss_nodes: [iou_loss]\nmetric_nodes: [metrics_anomaly]\nunfreeze_nodes: [channel_mixer]\n\ntags:\n  method: drcnn_adaclip\n  training: gradient\n  dataset: lentils\n</code></pre></p>"},{"location":"config/config-groups/#group-selection-patterns","title":"Group Selection Patterns","text":""},{"location":"config/config-groups/#pattern-1-standard-composition","title":"Pattern 1: Standard Composition","text":"<p>Most common pattern for complete experiments:</p> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: channel_selector\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: my_experiment\noutput_dir: ./outputs/${name}\n</code></pre> <p>Execution: <pre><code>uv run python train.py\n</code></pre></p>"},{"location":"config/config-groups/#pattern-2-selective-overrides","title":"Pattern 2: Selective Overrides","text":"<p>Override specific sections without full replacement:</p> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\n# Override only data splits\ndata:\n  train_ids: [0, 2, 3]\n  val_ids: [1]\n  test_ids: [5]\n\n# Override only optimizer learning rate\ntraining:\n  optimizer:\n    lr: 0.0001\n</code></pre>"},{"location":"config/config-groups/#pattern-3-command-line-selection","title":"Pattern 3: Command-Line Selection","text":"<p>Select config groups at runtime:</p> <pre><code># Select different pipeline\nuv run python train.py pipeline=deep_svdd\n\n# Select different data config\nuv run python train.py data=custom_dataset\n\n# Combine selections\nuv run python train.py \\\n    pipeline=channel_selector \\\n    data=lentils \\\n    training=default \\\n    training.optimizer.lr=0.0001\n</code></pre>"},{"location":"config/config-groups/#pattern-4-multi-run-sweeps","title":"Pattern 4: Multi-Run Sweeps","text":"<p>Sweep over multiple config combinations:</p> <pre><code># Sweep over pipelines\nuv run python train.py -m pipeline=rx_statistical,channel_selector,deep_svdd\n\n# Sweep over hyperparameters\nuv run python train.py -m training.optimizer.lr=0.001,0.0001,0.00001\n\n# Combined sweep\nuv run python train.py -m \\\n    pipeline=rx_statistical,channel_selector \\\n    training.optimizer.lr=0.001,0.0001\n</code></pre> <p>Hydra creates separate output directories: <pre><code>outputs/\n\u251c\u2500\u2500 pipeline=rx_statistical,lr=0.001/\n\u251c\u2500\u2500 pipeline=rx_statistical,lr=0.0001/\n\u251c\u2500\u2500 pipeline=channel_selector,lr=0.001/\n\u2514\u2500\u2500 pipeline=channel_selector,lr=0.0001/\n</code></pre></p>"},{"location":"config/config-groups/#override-syntax","title":"Override Syntax","text":""},{"location":"config/config-groups/#dot-notation","title":"Dot Notation","text":"<p>Access nested fields using dot notation:</p> <pre><code>training.trainer.max_epochs=100\ntraining.optimizer.lr=0.001\ndata.batch_size=16\npipeline.nodes.RXGlobal.params.eps=1e-08\n</code></pre>"},{"location":"config/config-groups/#list-assignment","title":"List Assignment","text":"<p>Assign lists using bracket notation:</p> <pre><code>data.train_ids=[0,1,2]\ndata.val_ids=[3,4]\nloss_nodes=[bce_loss,entropy_loss]\n</code></pre>"},{"location":"config/config-groups/#dictionary-assignment","title":"Dictionary Assignment","text":"<p>Assign dictionaries using brace notation:</p> <pre><code>training.scheduler={name:reduce_on_plateau,monitor:val/loss,mode:min}\n</code></pre>"},{"location":"config/config-groups/#multiple-overrides","title":"Multiple Overrides","text":"<p>Chain multiple overrides separated by spaces:</p> <pre><code>uv run python train.py \\\n    training.trainer.max_epochs=100 \\\n    training.optimizer.lr=0.0001 \\\n    training.optimizer.weight_decay=0.01 \\\n    data.batch_size=8 \\\n    data.train_ids=[0,1,2] \\\n    output_dir=outputs/custom_experiment\n</code></pre>"},{"location":"config/config-groups/#creating-custom-config-groups","title":"Creating Custom Config Groups","text":""},{"location":"config/config-groups/#step-1-create-group-directory","title":"Step 1: Create Group Directory","text":"<pre><code>mkdir configs/data/my_dataset\n</code></pre>"},{"location":"config/config-groups/#step-2-create-config-file","title":"Step 2: Create Config File","text":"<p>File: <code>configs/data/my_dataset/default.yaml</code></p> <pre><code>cu3s_file_path: data/MyDataset/dataset.cu3s\nannotation_json_path: data/MyDataset/annotations.json\n\ntrain_ids: [0, 1, 2, 3]\nval_ids: [4, 5]\ntest_ids: [6, 7, 8]\n\nbatch_size: 4\nshuffle: true\nnum_workers: 4\nprocessing_mode: Reflectance\n</code></pre>"},{"location":"config/config-groups/#step-3-use-in-trainrun","title":"Step 3: Use in TrainRun","text":"<pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: my_dataset/default\n  - /training@training: default\n  - _self_\n\nname: my_experiment\n</code></pre>"},{"location":"config/config-groups/#step-4-create-variants","title":"Step 4: Create Variants","text":"<p>File: <code>configs/data/my_dataset/augmented.yaml</code></p> <pre><code>defaults:\n  - default  # Inherit from default.yaml\n  - _self_\n\n# Override augmentation settings\nuse_augmentation: true\naugmentation:\n  horizontal_flip: true\n  vertical_flip: true\n  rotation_degrees: 15\n</code></pre> <p>Usage: <pre><code>defaults:\n  - /data@data: my_dataset/augmented\n</code></pre></p>"},{"location":"config/config-groups/#best-practices","title":"Best Practices","text":""},{"location":"config/config-groups/#1-config-organization","title":"1. Config Organization","text":"<p>Keep related configs together: <pre><code>configs/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 lentils/\n\u2502   \u2502   \u251c\u2500\u2500 default.yaml\n\u2502   \u2502   \u251c\u2500\u2500 augmented.yaml\n\u2502   \u2502   \u2514\u2500\u2500 small_subset.yaml\n\u2502   \u2514\u2500\u2500 tomatoes/\n\u2502       \u2514\u2500\u2500 default.yaml\n</code></pre></p> <p>Use descriptive names: <pre><code># Good\nconfigs/pipeline/channel_selector_with_rx.yaml\n\n# Avoid\nconfigs/pipeline/pipeline1.yaml\n</code></pre></p>"},{"location":"config/config-groups/#2-defaults-ordering","title":"2. Defaults Ordering","text":"<p>Always put <code>_self_</code> last: <pre><code># Correct\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - _self_  # \u2190 LAST\n\n# Wrong\ndefaults:\n  - _self_\n  - /pipeline@pipeline: rx_statistical  # Won't override _self_\n</code></pre></p>"},{"location":"config/config-groups/#3-minimal-overrides","title":"3. Minimal Overrides","text":"<p>Only override what you need: <pre><code># Good: Minimal changes\ndefaults:\n  - /training@training: default\n  - _self_\n\ntraining:\n  optimizer:\n    lr: 0.0001  # Only override LR\n\n# Avoid: Duplicating entire config\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 5\n    accelerator: auto\n    devices: 1\n    # ... (duplicates default.yaml)\n</code></pre></p>"},{"location":"config/config-groups/#4-variable-interpolation","title":"4. Variable Interpolation","text":"<p>Use variable references: <pre><code>name: my_experiment\noutput_dir: ./outputs/${name}\n\ntraining:\n  trainer:\n    default_root_dir: ${output_dir}\n</code></pre></p> <p>Environment variables: <pre><code>data:\n  cu3s_file_path: ${oc.env:DATA_ROOT}/Lentils_000.cu3s\n  # Falls back if DATA_ROOT not set:\n  cu3s_file_path: ${oc.env:DATA_ROOT,./data/Lentils}/Lentils_000.cu3s\n</code></pre></p>"},{"location":"config/config-groups/#5-documentation","title":"5. Documentation","text":"<p>Add comments to configs: <pre><code># RX Statistical Training\n# This trainrun performs statistical initialization of RX detector\n# without gradient-based training.\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - _self_\n\n# Experiment identifier\nname: rx_statistical\n\n# Empty loss nodes = statistical training only\nloss_nodes: []\n</code></pre></p>"},{"location":"config/config-groups/#common-patterns","title":"Common Patterns","text":""},{"location":"config/config-groups/#pattern-1-hyperparameter-sweep","title":"Pattern 1: Hyperparameter Sweep","text":"<p>Base config: <code>configs/trainrun/sweep_base.yaml</code></p> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: channel_selector\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: sweep_lr_${training.optimizer.lr}\noutput_dir: ./outputs/sweep/${name}\n</code></pre> <p>Execution: <pre><code>uv run python train.py -m \\\n    --config-name=trainrun/sweep_base \\\n    training.optimizer.lr=0.001,0.0001,0.00001\n</code></pre></p>"},{"location":"config/config-groups/#pattern-2-dataset-comparison","title":"Pattern 2: Dataset Comparison","text":"<pre><code>uv run python train.py -m \\\n    data=lentils,tomatoes,cucumbers \\\n    name=comparison_${data}\n</code></pre>"},{"location":"config/config-groups/#pattern-3-pipeline-comparison","title":"Pattern 3: Pipeline Comparison","text":"<pre><code>uv run python train.py -m \\\n    pipeline=rx_statistical,channel_selector,deep_svdd \\\n    name=${pipeline}_comparison\n</code></pre>"},{"location":"config/config-groups/#pattern-4-configuration-inheritance","title":"Pattern 4: Configuration Inheritance","text":"<p>Base config: <code>configs/training/base_optimizer.yaml</code></p> <pre><code>optimizer:\n  name: adamw\n  betas: [0.9, 0.999]\n</code></pre> <p>Variant: <code>configs/training/high_lr.yaml</code></p> <pre><code>defaults:\n  - base_optimizer\n  - _self_\n\noptimizer:\n  lr: 0.001\n  weight_decay: 0.01\n</code></pre> <p>Variant: <code>configs/training/low_lr.yaml</code></p> <pre><code>defaults:\n  - base_optimizer\n  - _self_\n\noptimizer:\n  lr: 0.00001\n  weight_decay: 0.001\n</code></pre>"},{"location":"config/config-groups/#troubleshooting","title":"Troubleshooting","text":""},{"location":"config/config-groups/#config-not-found","title":"Config Not Found","text":"<p>Problem: <code>ConfigAttributeError: Key 'pipeline' is not in struct</code></p> <p>Solution: Check config group path and filename: <pre><code># Check available configs\nls configs/pipeline/\n\n# Verify path in defaults\ndefaults:\n  - /pipeline@pipeline: rx_statistical  # \u2190 Must match filename\n</code></pre></p>"},{"location":"config/config-groups/#override-not-applied","title":"Override Not Applied","text":"<p>Problem: Override in trainrun doesn't apply.</p> <p>Solution: Ensure <code>_self_</code> is last in defaults: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - _self_  # \u2190 MUST BE LAST\n\n# Overrides below\ndata:\n  batch_size: 16  # \u2190 Now works\n</code></pre></p>"},{"location":"config/config-groups/#missing-required-field","title":"Missing Required Field","text":"<p>Problem: <code>MissingMandatoryValue: Missing mandatory value: data.cu3s_file_path</code></p> <p>Solution: Verify data config has required fields: <pre><code># configs/data/lentils.yaml\ncu3s_file_path: data/Lentils/Lentils_000.cu3s  # \u2190 Required\nannotation_json_path: data/Lentils/Lentils_000.json\n</code></pre></p>"},{"location":"config/config-groups/#package-directive-errors","title":"Package Directive Errors","text":"<p>Problem: Configs merged at wrong level.</p> <p>Solution: Use <code>@package _global_</code> for trainrun configs: <pre><code># @package _global_  \u2190 Required for trainruns\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n</code></pre></p>"},{"location":"config/config-groups/#see-also","title":"See Also","text":"<ul> <li>Configuration Guides:</li> <li>Hydra Composition - Composition patterns and inheritance</li> <li>TrainRun Schema - Complete trainrun configuration reference</li> <li>Pipeline Schema - Pipeline YAML structure</li> <li>How-To Guides:</li> <li>Build Pipelines in YAML - Create pipeline configs</li> <li>Configuration Guide - Configuration overview</li> <li>Examples:</li> <li><code>configs/trainrun/</code> - Example trainrun configurations</li> <li><code>configs/pipeline/</code> - Example pipeline configurations</li> <li><code>examples/rx_statistical.py</code> - Using config groups in code</li> </ul>"},{"location":"config/hydra-composition/","title":"Hydra Composition","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"config/hydra-composition/#hydra-composition-patterns","title":"Hydra Composition Patterns","text":"<p>Master Hydra composition for flexible, reusable, and modular configuration management in CUVIS.AI.</p>"},{"location":"config/hydra-composition/#overview","title":"Overview","text":"<p>Hydra enables powerful configuration composition:</p> <ul> <li>Defaults List: Compose multiple configs into one</li> <li>Package Directives: Control config placement in hierarchy</li> <li>Inheritance: Reuse and extend base configurations</li> <li>Variable Interpolation: Reference and compute values dynamically</li> <li>Multi-Run Sweeps: Hyperparameter optimization and grid search</li> <li>Command-Line Overrides: Runtime configuration changes</li> </ul> <p>Benefits: - Eliminate configuration duplication - Compose experiments from reusable pieces - Easy hyperparameter sweeps - Clear configuration hierarchy</p>"},{"location":"config/hydra-composition/#quick-start","title":"Quick Start","text":""},{"location":"config/hydra-composition/#basic-composition","title":"Basic Composition","text":"<p>Trainrun config: <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: my_experiment\noutput_dir: ./outputs/${name}\n</code></pre></p> <p>Usage: <pre><code>@hydra.main(config_path=\"../configs\", config_name=\"trainrun/my_experiment\", version_base=None)\ndef main(cfg: DictConfig):\n    print(cfg.name)  # Access composed config\n</code></pre></p>"},{"location":"config/hydra-composition/#package-directives","title":"Package Directives","text":""},{"location":"config/hydra-composition/#package-global","title":"@package global","text":"<p>Merges config at the root level.</p> <p>Most common for trainrun configs: <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\nname: experiment_1\n</code></pre></p> <p>Result: <pre><code>pipeline:\n  metadata:\n    name: RX_Statistical\n  nodes: [...]\nname: experiment_1  # \u2190 Merged at root\n</code></pre></p>"},{"location":"config/hydra-composition/#package-group","title":"@package group","text":"<p>Merges config under the group name.</p> <p>Example: <pre><code># @package training\n\ndefaults:\n  - /optimizer: adamw\n  - /scheduler: reduce_on_plateau\n\nseed: 42\n</code></pre></p> <p>Result: <pre><code>training:\n  seed: 42\n  optimizer: {...}\n  scheduler: {...}\n</code></pre></p>"},{"location":"config/hydra-composition/#explicit-package-paths","title":"Explicit Package Paths","text":"<p>Control exactly where configs are placed:</p> <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical       # \u2192 pipeline: {...}\n  - /data@data: lentils                      # \u2192 data: {...}\n  - /training@training: default              # \u2192 training: {...}\n</code></pre> <p>Path format: <code>/source_group@target_key: config_name</code></p>"},{"location":"config/hydra-composition/#defaults-list","title":"Defaults List","text":""},{"location":"config/hydra-composition/#structure-and-ordering","title":"Structure and Ordering","text":"<p>The <code>defaults</code> list determines config composition order:</p> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical   # Load first\n  - /data@data: lentils                  # Load second\n  - /training@training: default          # Load third\n  - _self_                               # THIS CONFIG (must be last)\n\n# Overrides below (only applied because _self_ is last)\ndata:\n  batch_size: 16\n</code></pre> <p>Critical rule: <code>_self_</code> must be last to allow overrides in the current file.</p>"},{"location":"config/hydra-composition/#absolute-vs-relative-paths","title":"Absolute vs Relative Paths","text":"<p>Absolute paths (start with <code>/</code>): <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical\n  # Searches: configs/pipeline/rx_statistical.yaml\n</code></pre></p> <p>Relative paths (no leading <code>/</code>): <pre><code># In configs/training/high_lr.yaml\ndefaults:\n  - base_optimizer  # Searches: configs/training/base_optimizer.yaml\n  - _self_\n</code></pre></p>"},{"location":"config/hydra-composition/#conditional-defaults","title":"Conditional Defaults","text":"<p>Exclude defaults using <code>optional</code>:</p> <pre><code>defaults:\n  - /pipeline@pipeline: ${pipeline_name}\n  - /training/scheduler@training.scheduler: ${scheduler_name}\n  - optional /augmentation@data.augmentation: ${augmentation}\n</code></pre>"},{"location":"config/hydra-composition/#config-inheritance","title":"Config Inheritance","text":""},{"location":"config/hydra-composition/#simple-inheritance","title":"Simple Inheritance","text":"<p>Base config: <code>configs/training/base.yaml</code> <pre><code>seed: 42\n\ntrainer:\n  max_epochs: 5\n  accelerator: auto\n  devices: 1\n\noptimizer:\n  name: adamw\n  betas: [0.9, 0.999]\n</code></pre></p> <p>Variant: <code>configs/training/high_lr.yaml</code> <pre><code>defaults:\n  - base\n  - _self_\n\noptimizer:\n  lr: 0.001           # Add new field\n  weight_decay: 0.01  # Add new field\n</code></pre></p> <p>Result: <pre><code>seed: 42                        # From base\ntrainer:\n  max_epochs: 5                 # From base\n  accelerator: auto             # From base\n  devices: 1                    # From base\noptimizer:\n  name: adamw                   # From base\n  betas: [0.9, 0.999]          # From base\n  lr: 0.001                     # From high_lr\n  weight_decay: 0.01            # From high_lr\n</code></pre></p>"},{"location":"config/hydra-composition/#multi-level-inheritance","title":"Multi-Level Inheritance","text":"<p>Level 1: <code>configs/training/base_optimizer.yaml</code> <pre><code>optimizer:\n  name: adamw\n  betas: [0.9, 0.999]\n</code></pre></p> <p>Level 2: <code>configs/training/base_training.yaml</code> <pre><code>defaults:\n  - base_optimizer\n  - _self_\n\nseed: 42\ntrainer:\n  max_epochs: 5\n</code></pre></p> <p>Level 3: <code>configs/training/custom_training.yaml</code> <pre><code>defaults:\n  - base_training\n  - _self_\n\noptimizer:\n  lr: 0.001\ntrainer:\n  max_epochs: 100  # Override base\n</code></pre></p> <p>Result: Combines all three levels with later configs overriding earlier ones.</p>"},{"location":"config/hydra-composition/#override-behavior","title":"Override Behavior","text":"<p>Hydra uses merge strategy by default:</p> <p>Base: <pre><code>data:\n  train_ids: [0, 2, 3]\n  val_ids: [1, 5]\n  batch_size: 2\n</code></pre></p> <p>Override: <pre><code>defaults:\n  - base\n  - _self_\n\ndata:\n  batch_size: 16  # Only override batch_size\n</code></pre></p> <p>Result: <pre><code>data:\n  train_ids: [0, 2, 3]    # From base\n  val_ids: [1, 5]         # From base\n  batch_size: 16          # Overridden\n</code></pre></p>"},{"location":"config/hydra-composition/#variable-interpolation","title":"Variable Interpolation","text":""},{"location":"config/hydra-composition/#simple-interpolation","title":"Simple Interpolation","text":"<p>Reference other values in the config:</p> <pre><code>name: my_experiment\noutput_dir: ./outputs/${name}\n# Resolves to: ./outputs/my_experiment\n\ntraining:\n  trainer:\n    default_root_dir: ${output_dir}\n    # Resolves to: ./outputs/my_experiment\n</code></pre>"},{"location":"config/hydra-composition/#environment-variables","title":"Environment Variables","text":"<p>Access environment variables with <code>oc.env</code>:</p> <pre><code>data:\n  cu3s_file_path: ${oc.env:DATA_ROOT}/Lentils_000.cu3s\n</code></pre> <p>With fallback: <pre><code>data:\n  cu3s_file_path: ${oc.env:DATA_ROOT,./data/Lentils}/Lentils_000.cu3s\n  # Use $DATA_ROOT if set, otherwise use ./data/Lentils\n</code></pre></p>"},{"location":"config/hydra-composition/#computed-values","title":"Computed Values","text":"<p>Conditional values: <pre><code>training:\n  accelerator: ${oc.env:ACCELERATOR,auto}\n  devices: ${oc.decode:\"1 if '${training.accelerator}' == 'cpu' else -1\"}\n</code></pre></p> <p>Path manipulation: <pre><code>name: experiment_01\ncheckpoint_dir: ${output_dir}/checkpoints\nlatest_checkpoint: ${checkpoint_dir}/last.ckpt\n</code></pre></p>"},{"location":"config/hydra-composition/#omegaconf-resolvers","title":"OmegaConf Resolvers","text":"<p>Built-in resolvers:</p> <p><code>oc.env</code> - Environment variable: <pre><code>data_root: ${oc.env:DATA_ROOT}\n</code></pre></p> <p><code>oc.decode</code> - Python expression: <pre><code>use_gpu: ${oc.decode:\"'cuda' if torch.cuda.is_available() else 'cpu'\"}\n</code></pre></p> <p><code>oc.create</code> - Create object: <pre><code>timestamp: ${oc.create:datetime.datetime.now}\n</code></pre></p>"},{"location":"config/hydra-composition/#cross-group-references","title":"Cross-Group References","text":"<p>Reference values from other config groups:</p> <pre><code># In trainrun config\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /training@training: default\n  - _self_\n\n# Reference pipeline name\noutput_dir: ./outputs/${pipeline.metadata.name}\n\n# Reference training seed\nexperiment_seed: ${training.seed}\n</code></pre>"},{"location":"config/hydra-composition/#override-mechanisms","title":"Override Mechanisms","text":""},{"location":"config/hydra-composition/#1-config-level-overrides","title":"1. Config-Level Overrides","text":"<p>In trainrun config file: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_  # \u2190 Must be last\n\n# Override specific fields\ndata:\n  train_ids: [0, 1, 2]\n  batch_size: 16\n\ntraining:\n  optimizer:\n    lr: 0.0001\n  trainer:\n    max_epochs: 100\n</code></pre></p>"},{"location":"config/hydra-composition/#2-command-line-overrides","title":"2. Command-Line Overrides","text":"<p>Dot notation: <pre><code>python train.py training.optimizer.lr=0.001\n</code></pre></p> <p>Nested overrides: <pre><code>python train.py \\\n    training.trainer.max_epochs=100 \\\n    training.optimizer.lr=0.001 \\\n    training.optimizer.weight_decay=0.01 \\\n    data.batch_size=16\n</code></pre></p> <p>List assignment: <pre><code>python train.py data.train_ids=[0,1,2,3]\n</code></pre></p> <p>Dictionary assignment: <pre><code>python train.py training.scheduler={name:reduce_on_plateau,patience:10}\n</code></pre></p>"},{"location":"config/hydra-composition/#3-config-group-selection","title":"3. Config Group Selection","text":"<p>Switch entire config groups: <pre><code>python train.py pipeline=channel_selector\npython train.py data=custom_dataset\npython train.py training=high_lr\n</code></pre></p>"},{"location":"config/hydra-composition/#4-programmatic-overrides","title":"4. Programmatic Overrides","text":"<p>In Python code: <pre><code>from omegaconf import OmegaConf\n\n@hydra.main(config_path=\"../configs\", config_name=\"trainrun/default_gradient\")\ndef main(cfg: DictConfig):\n    # Override via OmegaConf\n    cfg.training.optimizer.lr = 0.0001\n    cfg.data.batch_size = 32\n\n    # Or use merge\n    overrides = OmegaConf.create({\n        \"training\": {\n            \"optimizer\": {\"lr\": 0.0001},\n            \"trainer\": {\"max_epochs\": 100}\n        }\n    })\n    cfg = OmegaConf.merge(cfg, overrides)\n\n    # Convert to dict for usage\n    config_dict = OmegaConf.to_container(cfg, resolve=True)\n</code></pre></p>"},{"location":"config/hydra-composition/#multi-run-sweeps","title":"Multi-Run Sweeps","text":""},{"location":"config/hydra-composition/#basic-sweep-syntax","title":"Basic Sweep Syntax","text":"<p>Use <code>-m</code> flag to enable multi-run mode:</p> <pre><code>python train.py -m training.optimizer.lr=0.001,0.0001,0.00001\n</code></pre> <p>Hydra creates separate runs: <pre><code>outputs/\n\u251c\u2500\u2500 multirun/\n\u2502   \u2514\u2500\u2500 2026-02-04/\n\u2502       \u251c\u2500\u2500 10-30-00/\n\u2502       \u2502   \u251c\u2500\u2500 0/  # lr=0.001\n\u2502       \u2502   \u251c\u2500\u2500 1/  # lr=0.0001\n\u2502       \u2502   \u2514\u2500\u2500 2/  # lr=0.00001\n</code></pre></p>"},{"location":"config/hydra-composition/#sweep-multiple-parameters","title":"Sweep Multiple Parameters","text":"<p>Cartesian product: <pre><code>python train.py -m \\\n    training.optimizer.lr=0.001,0.0001 \\\n    training.optimizer.weight_decay=0.01,0.001\n</code></pre></p> <p>Creates 4 runs: 1. lr=0.001, weight_decay=0.01 2. lr=0.001, weight_decay=0.001 3. lr=0.0001, weight_decay=0.01 4. lr=0.0001, weight_decay=0.001</p>"},{"location":"config/hydra-composition/#sweep-config-groups","title":"Sweep Config Groups","text":"<pre><code>python train.py -m pipeline=rx_statistical,channel_selector,deep_svdd\n</code></pre> <p>Or combine: <pre><code>python train.py -m \\\n    pipeline=rx_statistical,channel_selector \\\n    training.optimizer.lr=0.001,0.0001\n</code></pre></p>"},{"location":"config/hydra-composition/#custom-sweep-configurations","title":"Custom Sweep Configurations","text":"<p>Base config: <code>configs/trainrun/sweep_base.yaml</code> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: ${pipeline_name}\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: sweep_${pipeline_name}_lr_${training.optimizer.lr}\noutput_dir: ./outputs/sweeps/${name}\n</code></pre></p> <p>Execute sweep: <pre><code>python train.py \\\n    --config-name=trainrun/sweep_base \\\n    -m \\\n    pipeline_name=rx_statistical,channel_selector \\\n    training.optimizer.lr=0.001,0.0001,0.00001\n</code></pre></p>"},{"location":"config/hydra-composition/#sweep-output-organization","title":"Sweep Output Organization","text":"<p>Hydra creates hierarchical directories:</p> <pre><code>outputs/\n\u2514\u2500\u2500 multirun/\n    \u2514\u2500\u2500 2026-02-04/\n        \u2514\u2500\u2500 10-30-00/\n            \u251c\u2500\u2500 .hydra/\n            \u2502   \u251c\u2500\u2500 config.yaml\n            \u2502   \u251c\u2500\u2500 hydra.yaml\n            \u2502   \u2514\u2500\u2500 overrides.yaml\n            \u251c\u2500\u2500 0/  # First combination\n            \u2502   \u251c\u2500\u2500 .hydra/\n            \u2502   \u251c\u2500\u2500 pipeline/\n            \u2502   \u2514\u2500\u2500 trained_models/\n            \u251c\u2500\u2500 1/  # Second combination\n            \u2514\u2500\u2500 2/  # Third combination\n</code></pre>"},{"location":"config/hydra-composition/#advanced-composition-patterns","title":"Advanced Composition Patterns","text":""},{"location":"config/hydra-composition/#pattern-1-base-variants","title":"Pattern 1: Base + Variants","text":"<p>Base config: <code>configs/trainrun/base_experiment.yaml</code> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: ${pipeline_name}\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\nname: ${pipeline_name}_experiment\noutput_dir: ./outputs/${name}\n\ntags:\n  dataset: lentils\n  method: ${pipeline_name}\n</code></pre></p> <p>Variant configs: - <code>configs/trainrun/rx_experiment.yaml</code> - <code>configs/trainrun/channel_selector_experiment.yaml</code></p> <pre><code># Each variant\ndefaults:\n  - base_experiment\n  - _self_\n\npipeline_name: rx_statistical\n</code></pre>"},{"location":"config/hydra-composition/#pattern-2-conditional-composition","title":"Pattern 2: Conditional Composition","text":"<p>Based on mode: <pre><code>defaults:\n  - /pipeline@pipeline: ${pipeline_name}\n  - /training/optimizer@training.optimizer: ${optimizer_type}\n  - /training/scheduler@training.scheduler: ${scheduler_type}\n  - optional /training/callbacks@training.callbacks: ${callbacks_preset}\n  - _self_\n\npipeline_name: rx_statistical\noptimizer_type: adamw\nscheduler_type: reduce_on_plateau\ncallbacks_preset: null  # Optional\n</code></pre></p>"},{"location":"config/hydra-composition/#pattern-3-hierarchical-configs","title":"Pattern 3: Hierarchical Configs","text":"<p>Directory structure: <pre><code>configs/\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 statistical/\n\u2502   \u2502   \u251c\u2500\u2500 rx.yaml\n\u2502   \u2502   \u2514\u2500\u2500 lad.yaml\n\u2502   \u2514\u2500\u2500 gradient/\n\u2502       \u251c\u2500\u2500 channel_selector.yaml\n\u2502       \u2514\u2500\u2500 deep_svdd.yaml\n</code></pre></p> <p>Usage: <pre><code>defaults:\n  - /pipeline@pipeline: statistical/rx\n  # or\n  - /pipeline@pipeline: gradient/channel_selector\n</code></pre></p>"},{"location":"config/hydra-composition/#pattern-4-config-recipes","title":"Pattern 4: Config Recipes","text":"<p>Recipe: <code>configs/recipes/fast_prototype.yaml</code> <pre><code># @package _global_\n\ndefaults:\n  - /trainrun@_here_: default\n  - _self_\n\ntraining:\n  trainer:\n    max_epochs: 3\n    fast_dev_run: false\n\ndata:\n  batch_size: 1\n  num_workers: 0\n\noutput_dir: ./outputs/quick_test\n</code></pre></p> <p>Usage: <pre><code>python train.py --config-name=recipes/fast_prototype\n</code></pre></p>"},{"location":"config/hydra-composition/#pattern-5-mixin-configs","title":"Pattern 5: Mixin Configs","text":"<p>Mixin: <code>configs/mixins/debug.yaml</code> <pre><code># @package training\n\ntrainer:\n  fast_dev_run: true\n  limit_train_batches: 10\n  limit_val_batches: 5\n  enable_progress_bar: true\n\noptimizer:\n  lr: 0.01  # Higher LR for fast debugging\n</code></pre></p> <p>Usage: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - /mixins/debug@training:_here_  # Merge debug settings\n  - _self_\n</code></pre></p>"},{"location":"config/hydra-composition/#pattern-6-dynamic-experiment-generation","title":"Pattern 6: Dynamic Experiment Generation","text":"<p>Generator config: <code>configs/experiments/generate.yaml</code> <pre><code># @package _global_\n\ndefaults:\n  - /pipeline@pipeline: ${experiment.pipeline}\n  - /data@data: ${experiment.dataset}\n  - /training@training: ${experiment.training_preset}\n  - _self_\n\nexperiment:\n  pipeline: rx_statistical\n  dataset: lentils\n  training_preset: default\n\nname: ${experiment.pipeline}_on_${experiment.dataset}\noutput_dir: ./outputs/${name}\n</code></pre></p> <p>Sweep different experiments: <pre><code>python train.py \\\n    --config-name=experiments/generate \\\n    -m \\\n    experiment.pipeline=rx_statistical,channel_selector \\\n    experiment.dataset=lentils,tomatoes\n</code></pre></p>"},{"location":"config/hydra-composition/#best-practices","title":"Best Practices","text":""},{"location":"config/hydra-composition/#1-defaults-ordering","title":"1. Defaults Ordering","text":"<p>Always put <code>_self_</code> last: <pre><code># \u2713 Correct\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - _self_\n\ndata:\n  batch_size: 16  # Overrides work\n\n# \u2717 Wrong\ndefaults:\n  - _self_\n  - /pipeline@pipeline: rx_statistical\n\ndata:\n  batch_size: 16  # Doesn't override!\n</code></pre></p>"},{"location":"config/hydra-composition/#2-package-directives","title":"2. Package Directives","text":"<p>Use <code>@package _global_</code> for trainrun configs: <pre><code># configs/trainrun/my_experiment.yaml\n# @package _global_  # \u2190 Always add this\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n</code></pre></p> <p>Avoid mixing package directives in the same config.</p>"},{"location":"config/hydra-composition/#3-clear-variable-names","title":"3. Clear Variable Names","text":"<p>Good: <pre><code>name: ${pipeline.metadata.name}_experiment\noutput_dir: ./outputs/${name}\ncheckpoint_path: ${output_dir}/checkpoints\n</code></pre></p> <p>Avoid: <pre><code>name: ${a}\noutput_dir: ${b}/${c}\n</code></pre></p>"},{"location":"config/hydra-composition/#4-minimal-overrides","title":"4. Minimal Overrides","text":"<p>Only override what you need: <pre><code># \u2713 Good\ndefaults:\n  - /training@training: default\n  - _self_\n\ntraining:\n  optimizer:\n    lr: 0.0001  # Only override LR\n\n# \u2717 Bad (duplicates entire config)\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 5\n    accelerator: auto\n    # ... (all fields repeated)\n</code></pre></p>"},{"location":"config/hydra-composition/#5-document-complex-compositions","title":"5. Document Complex Compositions","text":"<pre><code># Base Experiment Template\n#\n# This config composes:\n# - Pipeline: Specified via pipeline_name variable\n# - Data: Lentils dataset with custom splits\n# - Training: Default settings with overrideable LR\n#\n# Usage:\n#   python train.py --config-name=base_experiment pipeline_name=rx_statistical\n\n# @package _global_\n\ndefaults:\n  - /pipeline@pipeline: ${pipeline_name}\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n</code></pre>"},{"location":"config/hydra-composition/#6-validate-interpolations","title":"6. Validate Interpolations","text":"<p>Check for typos: <pre><code># \u2713 Correct\noutput_dir: ./outputs/${name}\n\n# \u2717 Typo\noutput_dir: ./outputs/${nmae}  # Will error at runtime\n</code></pre></p> <p>Use resolve=True when converting: <pre><code>config_dict = OmegaConf.to_container(cfg, resolve=True)\n</code></pre></p>"},{"location":"config/hydra-composition/#troubleshooting","title":"Troubleshooting","text":""},{"location":"config/hydra-composition/#missing-key-error","title":"Missing Key Error","text":"<p>Problem: <code>KeyError: 'pipeline'</code></p> <p>Solution: Check defaults list and package directives: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical  # \u2190 Ensure @pipeline part\n</code></pre></p>"},{"location":"config/hydra-composition/#interpolation-error","title":"Interpolation Error","text":"<p>Problem: <code>InterpolationResolutionError: Could not resolve ${name}</code></p> <p>Solution: Ensure referenced key exists: <pre><code>name: my_experiment  # \u2190 Must be defined\noutput_dir: ./outputs/${name}\n</code></pre></p>"},{"location":"config/hydra-composition/#override-not-applied","title":"Override Not Applied","text":"<p>Problem: Override in config doesn't work.</p> <p>Solution: Ensure <code>_self_</code> is last: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical\n  - _self_  # \u2190 MUST BE LAST\n\n# Overrides below\npipeline:\n  nodes: [...]\n</code></pre></p>"},{"location":"config/hydra-composition/#package-directive-confusion","title":"Package Directive Confusion","text":"<p>Problem: Config appears at wrong level in hierarchy.</p> <p>Solution: Check package directive: <pre><code># For trainrun configs, use:\n# @package _global_\n\n# For group-specific configs, use:\n# @package training\n# or\n# @package data\n</code></pre></p>"},{"location":"config/hydra-composition/#circular-dependency","title":"Circular Dependency","text":"<p>Problem: <code>CircularReferenceError</code></p> <p>Solution: Avoid circular references: <pre><code># \u2717 Circular\na: ${b}\nb: ${a}\n\n# \u2713 Fixed\na: base_value\nb: ${a}_extended\n</code></pre></p>"},{"location":"config/hydra-composition/#see-also","title":"See Also","text":"<ul> <li>Configuration Guides:</li> <li>Config Groups - Organizing configuration groups</li> <li>TrainRun Schema - Complete trainrun reference</li> <li>Pipeline Schema - Pipeline YAML structure</li> <li>User Guide:</li> <li>Configuration Overview - Configuration system overview</li> <li>External Resources:</li> <li>Hydra Documentation - Official Hydra docs</li> <li>OmegaConf Documentation - OmegaConf reference</li> <li>Examples:</li> <li><code>configs/trainrun/</code> - Example trainrun compositions</li> <li><code>examples/rx_statistical.py</code> - Using Hydra in code</li> </ul>"},{"location":"config/pipeline-schema/","title":"Pipeline Schema","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"config/pipeline-schema/#pipeline-configuration-schema","title":"Pipeline Configuration Schema","text":"<p>Complete reference for pipeline YAML structure, fields, validation rules, and examples.</p>"},{"location":"config/pipeline-schema/#overview","title":"Overview","text":"<p>Pipeline configurations define the computational graph for hyperspectral image processing:</p> <ul> <li>metadata: Pipeline identification and documentation</li> <li>nodes: Processing components with parameters</li> <li>connections: Data flow between nodes</li> <li>frozen_nodes: Nodes excluded from training</li> </ul> <p>File location: <code>configs/pipeline/</code></p> <p>Usage: Referenced in trainrun configs via Hydra composition</p>"},{"location":"config/pipeline-schema/#quick-reference","title":"Quick Reference","text":""},{"location":"config/pipeline-schema/#minimal-pipeline","title":"Minimal Pipeline","text":"<pre><code>metadata:\n  name: My_Pipeline\n  description: Pipeline description\n  author: cuvis.ai\n\nnodes:\n  - name: data_loader\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: detector\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\nconnections:\n  - from: data_loader.outputs.cube\n    to: detector.inputs.data\n\nfrozen_nodes: []\n</code></pre>"},{"location":"config/pipeline-schema/#complete-pipeline-structure","title":"Complete Pipeline Structure","text":"<pre><code>metadata:\n  name: string                 # Required\n  description: string           # Required\n  created: string              # Optional\n  tags: [...]                  # Optional\n  author: string               # Required\n  version: string              # Optional\n  cuvis_ai_version: string     # Optional\n\nnodes:\n  - name: string               # Required, unique\n    class: string              # Required, importable Python path\n    params: {}                 # Optional, node-specific parameters\n\nconnections:\n  - from: node.outputs.port    # Required\n    to: node.inputs.port       # Required\n\nfrozen_nodes: []               # Optional, list of node names\n</code></pre>"},{"location":"config/pipeline-schema/#metadata-section","title":"Metadata Section","text":""},{"location":"config/pipeline-schema/#required-fields","title":"Required Fields","text":"<p>name (string) - Pipeline identifier - Used in logs and saved models - Should be descriptive and unique</p> <pre><code>metadata:\n  name: RX_Statistical\n</code></pre> <p>description (string) - Brief description of pipeline purpose - Helps identify pipelines in multi-experiment setups</p> <pre><code>metadata:\n  description: RX anomaly detector with statistical initialization\n</code></pre> <p>author (string) - Creator or organization name - Used for tracking and attribution</p> <pre><code>metadata:\n  author: cuvis.ai\n</code></pre>"},{"location":"config/pipeline-schema/#optional-fields","title":"Optional Fields","text":"<p>created (string) - Creation timestamp - Format: ISO 8601 or any consistent format</p> <pre><code>metadata:\n  created: '2026-02-04'\n</code></pre> <p>tags (list of strings) - Classification tags for filtering and search - Common tags: <code>statistical</code>, <code>gradient</code>, <code>anomaly</code>, <code>classification</code></p> <pre><code>metadata:\n  tags:\n    - statistical\n    - rx\n    - anomaly_detection\n</code></pre> <p>version (string) - Pipeline version for tracking changes - Use semantic versioning: <code>major.minor.patch</code></p> <pre><code>metadata:\n  version: '1.2.0'\n</code></pre> <p>cuvis_ai_version (string) - Framework version used to create pipeline - Helps with compatibility tracking</p> <pre><code>metadata:\n  cuvis_ai_version: '0.1.5'\n</code></pre>"},{"location":"config/pipeline-schema/#complete-metadata-example","title":"Complete Metadata Example","text":"<pre><code>metadata:\n  name: DRCNN_AdaClip_Gradient\n  description: DRCNN channel mixer with AdaClip anomaly detection\n  created: '2026-02-04T10:30:00'\n  tags:\n    - gradient\n    - drcnn\n    - adaclip\n    - anomaly\n  author: cuvis.ai\n  version: '2.1.0'\n  cuvis_ai_version: '0.1.5.post26'\n</code></pre>"},{"location":"config/pipeline-schema/#nodes-section","title":"Nodes Section","text":""},{"location":"config/pipeline-schema/#node-structure","title":"Node Structure","text":"<p>Each node has three components:</p> <pre><code>nodes:\n  - name: &lt;unique_identifier&gt;\n    class: &lt;fully_qualified_class_path&gt;\n    params: &lt;dict_of_parameters&gt;\n</code></pre>"},{"location":"config/pipeline-schema/#name-string-required","title":"name (string, required)","text":"<ul> <li>Unique identifier within the pipeline</li> <li>Used in connections to reference the node</li> <li>Case-sensitive</li> <li>Must be valid Python identifier (no spaces, special chars)</li> </ul> <p>Good names: <pre><code>name: data_loader\nname: MinMaxNormalizer\nname: rx_detector\nname: metrics_anomaly\n</code></pre></p> <p>Avoid: <pre><code>name: node1              # Not descriptive\nname: my node            # Contains space\nname: detector-rx        # Contains dash (not Python identifier)\n</code></pre></p>"},{"location":"config/pipeline-schema/#class-string-required","title":"class (string, required)","text":"<ul> <li>Fully qualified Python class path</li> <li>Must be importable from Python path</li> <li>Format: <code>module.submodule.ClassName</code></li> </ul> <p>Common node classes:</p> <p>Data nodes: <pre><code>class: cuvis_ai.node.data.LentilsAnomalyDataNode\nclass: cuvis_ai.node.data.Cu3sDataNode\n</code></pre></p> <p>Preprocessing nodes: <pre><code>class: cuvis_ai.node.normalization.MinMaxNormalizer\nclass: cuvis_ai.node.normalization.StandardScaler\nclass: cuvis_ai.node.preprocessing.PCANode\n</code></pre></p> <p>Detection nodes: <pre><code>class: cuvis_ai.anomaly.rx_detector.RXGlobal\nclass: cuvis_ai.anomaly.lad_detector.LADDetector\nclass: cuvis_ai.node.conversion.ScoreToLogit\n</code></pre></p> <p>Decision nodes: <pre><code>class: cuvis_ai.deciders.binary_decider.BinaryDecider\n</code></pre></p> <p>Loss nodes: <pre><code>class: cuvis_ai.anomaly.iou_loss.IoULoss\nclass: cuvis_ai.anomaly.bce_loss.AnomalyBCEWithLogits\n</code></pre></p> <p>Metric nodes: <pre><code>class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n</code></pre></p> <p>Monitoring nodes: <pre><code>class: cuvis_ai.node.monitor.TensorBoardMonitorNode\nclass: cuvis_ai.node.visualizations.AnomalyMask\nclass: cuvis_ai.node.visualizations.ScoreHeatmapVisualizer\n</code></pre></p>"},{"location":"config/pipeline-schema/#params-dict-optional","title":"params (dict, optional)","text":"<p>Node-specific configuration parameters. Each node class defines its own parameters.</p> <p>Example: Data node parameters <pre><code>- name: LentilsAnomalyDataNode\n  class: cuvis_ai.node.data.LentilsAnomalyDataNode\n  params:\n    normal_class_ids: [0, 1]\n</code></pre></p> <p>Example: Normalization parameters <pre><code>- name: MinMaxNormalizer\n  class: cuvis_ai.node.normalization.MinMaxNormalizer\n  params:\n    eps: 1.0e-06\n    use_running_stats: true\n</code></pre></p> <p>Example: Detector parameters <pre><code>- name: RXGlobal\n  class: cuvis_ai.anomaly.rx_detector.RXGlobal\n  params:\n    num_channels: 61\n    eps: 1.0e-06\n    cache_inverse: true\n</code></pre></p> <p>Example: Complex node with many parameters <pre><code>- name: concrete_selector\n  class: cuvis_ai.node.concrete_selector.ConcreteBandSelector\n  params:\n    input_channels: 61\n    output_channels: 3\n    tau_start: 10.0\n    tau_end: 0.1\n    max_epochs: 20\n    use_hard_inference: true\n    eps: 1.0e-06\n</code></pre></p> <p>Example: Node with no parameters <pre><code>- name: metrics_anomaly\n  class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n  params: {}\n</code></pre></p> <p>Or simply omit <code>params</code>: <pre><code>- name: metrics_anomaly\n  class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n</code></pre></p>"},{"location":"config/pipeline-schema/#complete-node-examples","title":"Complete Node Examples","text":"<p>Statistical RX Pipeline Nodes: <pre><code>nodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: MinMaxNormalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: RXGlobal\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\n  - name: ScoreToLogit\n    class: cuvis_ai.node.conversion.ScoreToLogit\n    params:\n      init_scale: 1.0\n      init_bias: 0.0\n\n  - name: BinaryDecider\n    class: cuvis_ai.deciders.binary_decider.BinaryDecider\n    params:\n      threshold: 0.5\n\n  - name: metrics_anomaly\n    class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n</code></pre></p>"},{"location":"config/pipeline-schema/#connections-section","title":"Connections Section","text":""},{"location":"config/pipeline-schema/#connection-structure","title":"Connection Structure","text":"<p>Connections define data flow between nodes using port references:</p> <pre><code>connections:\n  - from: &lt;source_node&gt;.&lt;port_type&gt;.&lt;port_name&gt;\n    to: &lt;target_node&gt;.&lt;port_type&gt;.&lt;port_name&gt;\n</code></pre> <p>Port types: - <code>outputs</code> - Source ports (produce data) - <code>inputs</code> - Target ports (consume data)</p>"},{"location":"config/pipeline-schema/#connection-syntax","title":"Connection Syntax","text":"<p>Pattern: <pre><code>&lt;node_name&gt;.&lt;port_type&gt;.&lt;port_name&gt;\n</code></pre></p> <p>Components: 1. node_name: Must match a node name defined in <code>nodes</code> section 2. port_type: Either <code>outputs</code> (source) or <code>inputs</code> (target) 3. port_name: Port identifier defined by the node class</p> <p>Valid connection: <pre><code>- from: data_loader.outputs.cube\n  to: normalizer.inputs.data\n</code></pre></p> <p>Invalid connections: <pre><code># Wrong: Using inputs as source\n- from: normalizer.inputs.data\n  to: detector.inputs.data\n\n# Wrong: Using outputs as target\n- from: data_loader.outputs.cube\n  to: normalizer.outputs.normalized\n\n# Wrong: Mismatched port names\n- from: data_loader.outputs.cube\n  to: normalizer.inputs.wrong_port_name\n</code></pre></p>"},{"location":"config/pipeline-schema/#common-port-names","title":"Common Port Names","text":"<p>Data node outputs: <pre><code>LentilsAnomalyDataNode.outputs.cube         # Hyperspectral cube\nLentilsAnomalyDataNode.outputs.mask         # Ground truth mask\nLentilsAnomalyDataNode.outputs.wavelengths  # Wavelength array\n</code></pre></p> <p>Normalizer ports: <pre><code>MinMaxNormalizer.inputs.data        # Input data\nMinMaxNormalizer.outputs.normalized # Normalized output\n</code></pre></p> <p>Detector ports: <pre><code>RXGlobal.inputs.data          # Input hyperspectral data\nRXGlobal.outputs.scores       # Anomaly scores\nRXGlobal.outputs.mean         # Computed mean (statistical nodes)\nRXGlobal.outputs.covariance   # Computed covariance (statistical nodes)\n</code></pre></p> <p>Decision ports: <pre><code>BinaryDecider.inputs.logits       # Input logits or scores\nBinaryDecider.outputs.decisions   # Binary decisions\n</code></pre></p> <p>Metric ports: <pre><code>AnomalyDetectionMetrics.inputs.decisions  # Binary predictions\nAnomalyDetectionMetrics.inputs.targets    # Ground truth\nAnomalyDetectionMetrics.inputs.logits     # Optional scores\nAnomalyDetectionMetrics.outputs.metrics   # Computed metrics\n</code></pre></p> <p>Monitor ports: <pre><code>TensorBoardMonitorNode.inputs.metrics    # Metric objects\nTensorBoardMonitorNode.inputs.artifacts  # Visualization artifacts\n</code></pre></p>"},{"location":"config/pipeline-schema/#connection-patterns","title":"Connection Patterns","text":"<p>Linear flow: <pre><code>connections:\n  - from: data_loader.outputs.cube\n    to: normalizer.inputs.data\n  - from: normalizer.outputs.normalized\n    to: detector.inputs.data\n  - from: detector.outputs.scores\n    to: decider.inputs.scores\n</code></pre></p> <p>Multi-branch flow: <pre><code>connections:\n  # Main flow\n  - from: data_loader.outputs.cube\n    to: normalizer.inputs.data\n  - from: normalizer.outputs.normalized\n    to: detector.inputs.data\n\n  # Branch 1: Metrics\n  - from: decider.outputs.decisions\n    to: metrics.inputs.decisions\n  - from: data_loader.outputs.mask\n    to: metrics.inputs.targets\n\n  # Branch 2: Visualization\n  - from: decider.outputs.decisions\n    to: viz.inputs.decisions\n  - from: data_loader.outputs.cube\n    to: viz.inputs.cube\n</code></pre></p> <p>Fan-out pattern (one source \u2192 multiple targets): <pre><code>connections:\n  # RX scores go to multiple destinations\n  - from: detector.outputs.scores\n    to: decider.inputs.scores\n  - from: detector.outputs.scores\n    to: score_viz.inputs.scores\n  - from: detector.outputs.scores\n    to: metrics.inputs.logits\n</code></pre></p> <p>Convergence pattern (multiple sources \u2192 one target): <pre><code>connections:\n  # Monitoring receives from multiple sources\n  - from: metrics.outputs.metrics\n    to: monitor.inputs.metrics\n  - from: viz_mask.outputs.artifacts\n    to: monitor.inputs.artifacts\n  - from: score_viz.outputs.artifacts\n    to: monitor.inputs.artifacts\n</code></pre></p>"},{"location":"config/pipeline-schema/#complete-connection-examples","title":"Complete Connection Examples","text":"<p>RX Statistical Pipeline: <pre><code>connections:\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: metrics_anomaly.inputs.targets\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: mask.inputs.mask\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: mask.inputs.cube\n  - from: MinMaxNormalizer.outputs.normalized\n    to: RXGlobal.inputs.data\n  - from: RXGlobal.outputs.scores\n    to: ScoreToLogit.inputs.scores\n  - from: ScoreToLogit.outputs.logits\n    to: BinaryDecider.inputs.logits\n  - from: BinaryDecider.outputs.decisions\n    to: metrics_anomaly.inputs.decisions\n  - from: BinaryDecider.outputs.decisions\n    to: mask.inputs.decisions\n  - from: metrics_anomaly.outputs.metrics\n    to: TensorBoardMonitorNode.inputs.metrics\n  - from: mask.outputs.artifacts\n    to: TensorBoardMonitorNode.inputs.artifacts\n</code></pre></p> <p>DRCNN + AdaClip Pipeline (multi-branch): <pre><code>connections:\n  # Data loading\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n\n  # Main processing\n  - from: MinMaxNormalizer.outputs.normalized\n    to: channel_mixer.inputs.data\n  - from: channel_mixer.outputs.output\n    to: adaclip.inputs.image\n\n  # Loss computation\n  - from: adaclip.outputs.scores\n    to: iou_loss.inputs.predictions\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: iou_loss.inputs.targets\n\n  # Decisions\n  - from: adaclip.outputs.scores\n    to: decider.inputs.logits\n\n  # Metrics\n  - from: decider.outputs.decisions\n    to: metrics_anomaly.inputs.decisions\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: metrics_anomaly.inputs.targets\n  - from: adaclip.outputs.scores\n    to: metrics_anomaly.inputs.logits\n\n  # Monitoring\n  - from: metrics_anomaly.outputs.metrics\n    to: TensorBoardMonitorNode.inputs.metrics\n</code></pre></p>"},{"location":"config/pipeline-schema/#frozen-nodes-section","title":"Frozen Nodes Section","text":""},{"location":"config/pipeline-schema/#purpose","title":"Purpose","text":"<p>The <code>frozen_nodes</code> list specifies nodes that should not be updated during training.</p> <p>Use cases: - Data loaders (always frozen) - Statistical nodes after initialization - Pretrained components that should remain fixed</p>"},{"location":"config/pipeline-schema/#syntax","title":"Syntax","text":"<pre><code>frozen_nodes: []  # Empty list (all nodes trainable if applicable)\n</code></pre> <p>Or: <pre><code>frozen_nodes:\n  - data_loader\n  - normalizer\n  - pretrained_detector\n</code></pre></p>"},{"location":"config/pipeline-schema/#examples","title":"Examples","text":"<p>Statistical training (no gradient training): <pre><code>frozen_nodes: []  # Not used in statistical training\n</code></pre></p> <p>Gradient training with selective unfreezing: <pre><code># In trainrun config (not pipeline)\nfreeze_nodes: [data_loader, normalizer]\nunfreeze_nodes: [channel_selector, rx_detector]\n</code></pre></p> <p>Note: The <code>frozen_nodes</code> field in pipeline configs is typically empty. Freezing/unfreezing is controlled by <code>freeze_nodes</code> and <code>unfreeze_nodes</code> in trainrun configs.</p>"},{"location":"config/pipeline-schema/#complete-pipeline-examples","title":"Complete Pipeline Examples","text":""},{"location":"config/pipeline-schema/#example-1-statistical-rx-pipeline","title":"Example 1: Statistical RX Pipeline","text":"<pre><code>metadata:\n  name: RX_Statistical\n  description: RX anomaly detector with statistical initialization\n  tags:\n    - statistical\n    - rx\n  author: cuvis.ai\n\nnodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: MinMaxNormalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: RXGlobal\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\n  - name: ScoreToLogit\n    class: cuvis_ai.node.conversion.ScoreToLogit\n    params:\n      init_scale: 1.0\n      init_bias: 0.0\n\n  - name: BinaryDecider\n    class: cuvis_ai.deciders.binary_decider.BinaryDecider\n    params:\n      threshold: 0.5\n\n  - name: metrics_anomaly\n    class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n\n  - name: TensorBoardMonitorNode\n    class: cuvis_ai.node.monitor.TensorBoardMonitorNode\n    params:\n      output_dir: outputs/rx_statistical/tensorboard\n      run_name: RX_Statistical\n\nconnections:\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n  - from: MinMaxNormalizer.outputs.normalized\n    to: RXGlobal.inputs.data\n  - from: RXGlobal.outputs.scores\n    to: ScoreToLogit.inputs.scores\n  - from: ScoreToLogit.outputs.logits\n    to: BinaryDecider.inputs.logits\n  - from: BinaryDecider.outputs.decisions\n    to: metrics_anomaly.inputs.decisions\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: metrics_anomaly.inputs.targets\n  - from: metrics_anomaly.outputs.metrics\n    to: TensorBoardMonitorNode.inputs.metrics\n\nfrozen_nodes: []\n</code></pre>"},{"location":"config/pipeline-schema/#example-2-channel-selector-gradient-pipeline","title":"Example 2: Channel Selector Gradient Pipeline","text":"<pre><code>metadata:\n  name: Channel_Selector\n  description: Learnable channel selection with RX detection\n  tags:\n    - gradient\n    - channel_selection\n    - rx\n  author: cuvis.ai\n  version: '1.0.0'\n\nnodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: MinMaxNormalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: selector\n    class: cuvis_ai.node.channel_selection.ChannelSelector\n    params:\n      num_channels: 61\n      tau_start: 8.0\n      tau_end: 0.05\n\n  - name: rx_global\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\n  - name: logit_head\n    class: cuvis_ai.node.conversion.ScoreToLogit\n    params:\n      init_scale: 1.0\n      init_bias: 0.0\n\n  - name: decider\n    class: cuvis_ai.deciders.binary_decider.BinaryDecider\n    params:\n      threshold: 0.5\n\n  - name: bce_loss\n    class: cuvis_ai.anomaly.bce_loss.AnomalyBCEWithLogits\n    params:\n      weight: 1.0\n\n  - name: entropy_loss\n    class: cuvis_ai.anomaly.entropy_loss.SelectorEntropyLoss\n    params:\n      weight: 0.001\n\n  - name: metrics_anomaly\n    class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n\nconnections:\n  # Data flow\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n  - from: MinMaxNormalizer.outputs.normalized\n    to: selector.inputs.data\n  - from: selector.outputs.selected\n    to: rx_global.inputs.data\n  - from: rx_global.outputs.scores\n    to: logit_head.inputs.scores\n  - from: logit_head.outputs.logits\n    to: decider.inputs.logits\n\n  # Loss computation\n  - from: logit_head.outputs.logits\n    to: bce_loss.inputs.predictions\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: bce_loss.inputs.targets\n  - from: selector.outputs.weights\n    to: entropy_loss.inputs.weights\n\n  # Metrics\n  - from: decider.outputs.decisions\n    to: metrics_anomaly.inputs.decisions\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: metrics_anomaly.inputs.targets\n\nfrozen_nodes: []\n</code></pre>"},{"location":"config/pipeline-schema/#example-3-deep-svdd-pipeline","title":"Example 3: Deep SVDD Pipeline","text":"<pre><code>metadata:\n  name: Deep_SVDD\n  description: Deep Support Vector Data Description for anomaly detection\n  tags:\n    - gradient\n    - deep_learning\n    - svdd\n  author: cuvis.ai\n\nnodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: normalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: projection\n    class: cuvis_ai.node.deep_svdd.ProjectionNetwork\n    params:\n      input_dim: 61\n      hidden_dims: [128, 64, 32]\n      output_dim: 16\n\n  - name: deepsvdd_loss\n    class: cuvis_ai.anomaly.deep_svdd_loss.DeepSVDDLoss\n    params:\n      radius: 0.0\n      nu: 0.1\n\n  - name: metrics_anomaly\n    class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n\nconnections:\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: normalizer.inputs.data\n  - from: normalizer.outputs.normalized\n    to: projection.inputs.data\n  - from: projection.outputs.embeddings\n    to: deepsvdd_loss.inputs.embeddings\n  - from: deepsvdd_loss.outputs.scores\n    to: metrics_anomaly.inputs.logits\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: metrics_anomaly.inputs.targets\n\nfrozen_nodes: []\n</code></pre>"},{"location":"config/pipeline-schema/#validation-rules","title":"Validation Rules","text":""},{"location":"config/pipeline-schema/#1-node-name-uniqueness","title":"1. Node Name Uniqueness","text":"<p>All node names must be unique within a pipeline.</p> <p>Valid: <pre><code>nodes:\n  - name: normalizer_1\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n  - name: normalizer_2\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n</code></pre></p> <p>Invalid: <pre><code>nodes:\n  - name: normalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n  - name: normalizer  # \u2717 Duplicate name\n    class: cuvis_ai.node.normalization.StandardScaler\n</code></pre></p>"},{"location":"config/pipeline-schema/#2-class-importability","title":"2. Class Importability","text":"<p>All node classes must be importable from Python path.</p> <p>Valid: <pre><code>class: cuvis_ai.anomaly.rx_detector.RXGlobal  # \u2713 Exists\n</code></pre></p> <p>Invalid: <pre><code>class: cuvis_ai.anomaly.NonexistentNode  # \u2717 Import error\nclass: RXGlobal                          # \u2717 Not fully qualified\n</code></pre></p>"},{"location":"config/pipeline-schema/#3-connection-validity","title":"3. Connection Validity","text":"<p>Source must be output port: <pre><code># Valid\n- from: detector.outputs.scores\n  to: decider.inputs.scores\n\n# Invalid\n- from: detector.inputs.data  # \u2717 Can't use input as source\n  to: decider.inputs.scores\n</code></pre></p> <p>Target must be input port: <pre><code># Valid\n- from: detector.outputs.scores\n  to: decider.inputs.scores\n\n# Invalid\n- from: detector.outputs.scores\n  to: decider.outputs.decisions  # \u2717 Can't use output as target\n</code></pre></p> <p>Referenced nodes must exist: <pre><code># Valid\nnodes:\n  - name: detector\n    ...\nconnections:\n  - from: detector.outputs.scores\n    to: decider.inputs.scores\n\n# Invalid\nconnections:\n  - from: nonexistent_node.outputs.data  # \u2717 Node not defined\n    to: decider.inputs.scores\n</code></pre></p>"},{"location":"config/pipeline-schema/#4-parameter-types","title":"4. Parameter Types","text":"<p>Node parameters must match expected types.</p> <p>Valid: <pre><code>params:\n  num_channels: 61         # int\n  eps: 1.0e-06             # float\n  use_running_stats: true  # bool\n  normal_class_ids: [0, 1] # list\n</code></pre></p> <p>Invalid: <pre><code>params:\n  num_channels: \"61\"       # \u2717 String instead of int\n  eps: true                # \u2717 Bool instead of float\n  normal_class_ids: 0      # \u2717 Int instead of list\n</code></pre></p>"},{"location":"config/pipeline-schema/#best-practices","title":"Best Practices","text":""},{"location":"config/pipeline-schema/#1-descriptive-node-names","title":"1. Descriptive Node Names","text":"<p>Good: <pre><code>name: data_loader\nname: MinMaxNormalizer\nname: rx_detector\nname: metrics_anomaly\n</code></pre></p> <p>Avoid: <pre><code>name: node1\nname: n1\nname: temp\n</code></pre></p>"},{"location":"config/pipeline-schema/#2-consistent-naming-convention","title":"2. Consistent Naming Convention","text":"<p>Pick a style and stick to it: <pre><code># snake_case (recommended)\nname: data_loader\nname: rx_detector\nname: score_visualizer\n\n# PascalCase (alternative)\nname: DataLoader\nname: RXDetector\nname: ScoreVisualizer\n</code></pre></p>"},{"location":"config/pipeline-schema/#3-organized-connections","title":"3. Organized Connections","text":"<p>Group connections by purpose: <pre><code>connections:\n  # Main processing flow\n  - from: data_loader.outputs.cube\n    to: normalizer.inputs.data\n  - from: normalizer.outputs.normalized\n    to: detector.inputs.data\n\n  # Loss computation\n  - from: detector.outputs.scores\n    to: loss.inputs.predictions\n\n  # Metrics and monitoring\n  - from: metrics.outputs.metrics\n    to: monitor.inputs.metrics\n</code></pre></p>"},{"location":"config/pipeline-schema/#4-comments-for-complex-pipelines","title":"4. Comments for Complex Pipelines","text":"<pre><code>nodes:\n  # Data loading and preprocessing\n  - name: data_loader\n    ...\n  - name: normalizer\n    ...\n\n  # Feature extraction\n  - name: channel_selector\n    ...\n  - name: pca\n    ...\n\n  # Anomaly detection\n  - name: rx_detector\n    ...\n  - name: threshold_decider\n    ...\n</code></pre>"},{"location":"config/pipeline-schema/#5-parameter-documentation","title":"5. Parameter Documentation","text":"<pre><code>- name: concrete_selector\n  class: cuvis_ai.node.concrete_selector.ConcreteBandSelector\n  params:\n    input_channels: 61\n    output_channels: 3\n    tau_start: 10.0        # Initial temperature for Gumbel-Softmax\n    tau_end: 0.1           # Final temperature after annealing\n    max_epochs: 20         # Epochs for temperature schedule\n    use_hard_inference: true  # Use hard selection during inference\n</code></pre>"},{"location":"config/pipeline-schema/#see-also","title":"See Also","text":"<ul> <li>Configuration Guides:</li> <li>Config Groups - Organizing pipeline configs</li> <li>TrainRun Schema - Complete experiment configuration</li> <li>Hydra Composition - Composition patterns</li> <li>How-To Guides:</li> <li>Build Pipelines in YAML - Creating pipeline configs</li> <li>Build Pipelines in Python - Programmatic construction</li> <li>Add Builtin Node - Creating custom nodes</li> <li>Node Catalog:</li> <li>Data Nodes - Available data loading nodes</li> <li>Processing Nodes - Normalization, PCA, etc.</li> <li>Statistical Nodes - Anomaly detector implementations</li> <li>Loss &amp; Metrics Nodes - Loss and metric computations</li> </ul>"},{"location":"config/trainrun-schema/","title":"TrainRun Schema","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"config/trainrun-schema/#trainrun-configuration-schema","title":"TrainRun Configuration Schema","text":""},{"location":"config/trainrun-schema/#overview","title":"Overview","text":"<p>Complete reference for TrainRun configuration structure, fields, validation rules, and examples.</p>"},{"location":"config/trainrun-schema/#trainrunconfig-structure","title":"TrainRunConfig Structure","text":"<pre><code>TrainRunConfig(\n    name: str,                      # Experiment identifier\n    pipeline: Dict,                 # Full pipeline configuration\n    data: Dict,                     # Data loading configuration\n    training: TrainingConfig,       # Training settings\n    output_dir: str,                # Results directory path\n    loss_nodes: List[str] = [],     # Loss computation nodes\n    metric_nodes: List[str] = [],   # Metric computation nodes\n    freeze_nodes: List[str] = [],   # Initially frozen nodes\n    unfreeze_nodes: List[str] = [], # Nodes to unfreeze for training\n    tags: Dict = {}                 # Optional metadata\n)\n</code></pre>"},{"location":"config/trainrun-schema/#required-fields","title":"Required Fields","text":""},{"location":"config/trainrun-schema/#name-string","title":"name (string)","text":"<p>Unique experiment identifier used for organizing outputs and logging.</p> <p>Example: <pre><code>name: channel_selector_experiment\n</code></pre></p>"},{"location":"config/trainrun-schema/#pipeline-dict","title":"pipeline (dict)","text":"<p>Complete pipeline specification with nodes and connections. Can be inline or composed via Hydra.</p> <p>Inline format: <pre><code>pipeline:\n  name: My_Pipeline\n  nodes:\n    - name: data_loader\n      class: cuvis_ai.node.data.LentilsAnomalyDataNode\n      params:\n        normal_class_ids: [0, 1]\n  connections:\n    - from: data_loader.outputs.cube\n      to: normalizer.inputs.data\n</code></pre></p> <p>Composition format: <pre><code>defaults:\n  - /pipeline@pipeline: rx_statistical  # Load from configs/pipeline/\n</code></pre></p>"},{"location":"config/trainrun-schema/#data-dict","title":"data (dict)","text":"<p>Data loading and splitting configuration.</p> <p>Required fields: - <code>cu3s_file_path</code> (string): Path to .cu3s data file - <code>train_ids</code>, <code>val_ids</code>, <code>test_ids</code> (list of int): Sample indices for splits - <code>batch_size</code> (int): Batch size for training</p> <p>Example: <pre><code>data:\n  cu3s_file_path: data/Lentils/Lentils_000.cu3s\n  annotation_json_path: data/Lentils/Lentils_000.json\n  train_ids: [0, 2, 3]\n  val_ids: [1, 5]\n  test_ids: [1, 5]\n  batch_size: 2\n  processing_mode: Reflectance\n  shuffle: true\n</code></pre></p>"},{"location":"config/trainrun-schema/#training-dict","title":"training (dict)","text":"<p>Training configuration including optimizer, scheduler, and callbacks.</p> <p>Required fields: - <code>trainer</code> (dict): PyTorch Lightning trainer settings - <code>optimizer</code> (dict): Optimizer configuration - <code>scheduler</code> (dict, optional): Learning rate scheduler</p> <p>Example: <pre><code>training:\n  seed: 42\n  trainer:\n    max_epochs: 50\n    accelerator: auto\n    devices: 1\n    precision: \"32-true\"\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n  scheduler:\n    name: reduce_on_plateau\n    monitor: metrics_anomaly/iou\n    mode: max\n</code></pre></p>"},{"location":"config/trainrun-schema/#output_dir-string","title":"output_dir (string)","text":"<p>Directory path for saving results, checkpoints, and logs.</p> <p>Example: <pre><code>output_dir: outputs/my_experiment\n# Or with variable interpolation:\noutput_dir: ./outputs/${name}\n</code></pre></p>"},{"location":"config/trainrun-schema/#optional-fields","title":"Optional Fields","text":""},{"location":"config/trainrun-schema/#loss_nodes-list-of-string","title":"loss_nodes (list of string)","text":"<p>Names of loss nodes for gradient training. Empty for statistical-only training.</p> <p>Example: <pre><code>loss_nodes:\n  - bce_loss\n  - entropy_loss\n</code></pre></p>"},{"location":"config/trainrun-schema/#metric_nodes-list-of-string","title":"metric_nodes (list of string)","text":"<p>Names of metric nodes for evaluation.</p> <p>Example: <pre><code>metric_nodes:\n  - metrics_anomaly\n</code></pre></p>"},{"location":"config/trainrun-schema/#freeze_nodes-list-of-string","title":"freeze_nodes (list of string)","text":"<p>Nodes to keep frozen (non-trainable) throughout training.</p> <p>Example: <pre><code>freeze_nodes:\n  - data_loader\n  - normalizer\n</code></pre></p>"},{"location":"config/trainrun-schema/#unfreeze_nodes-list-of-string","title":"unfreeze_nodes (list of string)","text":"<p>Nodes to unfreeze for gradient training after statistical initialization.</p> <p>Example: <pre><code>unfreeze_nodes:\n  - channel_selector\n  - rx_detector\n  - logit_head\n</code></pre></p>"},{"location":"config/trainrun-schema/#tags-dict","title":"tags (dict)","text":"<p>Optional metadata for experiment organization.</p> <p>Example: <pre><code>tags:\n  dataset: lentils\n  method: channel_selection\n  version: v2\n</code></pre></p>"},{"location":"config/trainrun-schema/#complete-examples","title":"Complete Examples","text":""},{"location":"config/trainrun-schema/#example-1-statistical-training-rx-detector","title":"Example 1: Statistical Training (RX Detector)","text":"<pre><code># @package _global_\n\nname: rx_statistical_experiment\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\ndata:\n  train_ids: [0, 2, 3]\n  val_ids: [1, 5]\n  test_ids: [1, 5]\n  batch_size: 1\n\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 1  # Only statistical initialization\n    accelerator: auto\n\noutput_dir: outputs/rx_statistical\n\n# No loss nodes = statistical training only\nloss_nodes: []\nmetric_nodes: [metrics]\nfreeze_nodes: []\nunfreeze_nodes: []\n</code></pre>"},{"location":"config/trainrun-schema/#example-2-gradient-training-channel-selector","title":"Example 2: Gradient Training (Channel Selector)","text":"<pre><code># @package _global_\n\nname: channel_selector_experiment\n\ndefaults:\n  - /pipeline@pipeline: channel_selector\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\ndata:\n  train_ids: [0]\n  val_ids: [3, 4]\n  test_ids: [1, 5]\n  batch_size: 1\n\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 50\n    accelerator: auto\n    devices: 1\n    precision: \"32-true\"\n\n    callbacks:\n      model_checkpoint:\n        dirpath: outputs/channel_selector/checkpoints\n        monitor: metrics_anomaly/iou\n        mode: max\n        save_top_k: 3\n        save_last: true\n\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n\n  scheduler:\n    name: reduce_on_plateau\n    monitor: metrics_anomaly/iou\n    mode: max\n    patience: 5\n\noutput_dir: outputs/channel_selector\n\n# Gradient training configuration\nloss_nodes:\n  - bce_loss\n  - entropy_loss\nmetric_nodes:\n  - metrics_anomaly\nunfreeze_nodes:\n  - selector\n  - rx_global\n  - logit_head\n\ntags:\n  method: channel_selection\n  dataset: lentils\n</code></pre>"},{"location":"config/trainrun-schema/#example-3-two-phase-training-deep-svdd","title":"Example 3: Two-Phase Training (Deep SVDD)","text":"<pre><code># @package _global_\n\nname: deep_svdd_experiment\n\ndefaults:\n  - /pipeline@pipeline: deep_svdd\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\ndata:\n  train_ids: [0, 2, 3]\n  val_ids: [1]\n  test_ids: [5]\n  batch_size: 2\n\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 100\n    accelerator: gpu\n    devices: 1\n    precision: \"16-mixed\"  # Use mixed precision for speed\n    gradient_clip_val: 1.0\n\n    callbacks:\n      early_stopping:\n        monitor: metrics_anomaly/auc\n        patience: 10\n        mode: max\n        min_delta: 0.001\n\n  optimizer:\n    name: adam\n    lr: 0.0001\n    weight_decay: 0.0001\n\n  scheduler:\n    name: cosine_annealing\n    T_max: 100\n    eta_min: 1e-6\n\noutput_dir: outputs/deep_svdd\n\nloss_nodes:\n  - deepsvdd_loss\nmetric_nodes:\n  - metrics_anomaly\nunfreeze_nodes:\n  - normalizer\n  - projection\nfreeze_nodes:\n  - data_loader\n\ntags:\n  method: deep_svdd\n  dataset: lentils\n  gpu: true\n</code></pre>"},{"location":"config/trainrun-schema/#validation-rules","title":"Validation Rules","text":""},{"location":"config/trainrun-schema/#1-file-structure-requirements","title":"1. File Structure Requirements","text":"<p>Required Hydra directive: <pre><code># @package _global_\n</code></pre> Must be the first line of every trainrun configuration file.</p> <p>Required sections: - <code>defaults</code> (list) - <code>name</code> (string) - <code>output_dir</code> (string)</p>"},{"location":"config/trainrun-schema/#2-composition-directives","title":"2. Composition Directives","text":"<p>defaults list must include: <pre><code>defaults:\n  - /pipeline@pipeline: &lt;pipeline_name&gt;\n  - /data@data: &lt;data_name&gt;\n  - /training@training: &lt;training_name&gt;\n  - _self_  # Must be last for overrides to work\n</code></pre></p>"},{"location":"config/trainrun-schema/#3-node-name-validation","title":"3. Node Name Validation","text":"<ul> <li><code>loss_nodes</code>, <code>metric_nodes</code>, <code>freeze_nodes</code>, <code>unfreeze_nodes</code> must reference valid node names defined in pipeline</li> <li>Names are case-sensitive and must match exactly</li> </ul> <p>Example validation: <pre><code># Pipeline defines these nodes:\nnodes = [\"data_loader\", \"normalizer\", \"selector\", \"rx\", \"bce_loss\", \"metrics\"]\n\n# Valid unfreeze_nodes:\nunfreeze_nodes = [\"selector\", \"rx\"]  # \u2713\n\n# Invalid:\nunfreeze_nodes = [\"Selector\", \"RX\"]  # \u2717 (wrong case)\nunfreeze_nodes = [\"unknown_node\"]     # \u2717 (not in pipeline)\n</code></pre></p>"},{"location":"config/trainrun-schema/#4-training-type-detection","title":"4. Training Type Detection","text":"<p>Statistical training: <pre><code>loss_nodes: []  # Empty or omitted\n# System uses StatisticalTrainer\n</code></pre></p> <p>Gradient training: <pre><code>loss_nodes: [loss1, loss2]  # Non-empty\n# System uses GradientTrainer\n</code></pre></p>"},{"location":"config/trainrun-schema/#5-output-directory-rules","title":"5. Output Directory Rules","text":"<ul> <li>Must be writable path</li> <li>Supports variable interpolation: <code>${name}</code>, <code>${oc.env:USER}</code></li> <li>Automatically creates subdirectories:</li> <li><code>trained_models/</code></li> <li><code>checkpoints/</code></li> <li><code>tensorboard/</code></li> </ul>"},{"location":"config/trainrun-schema/#common-patterns","title":"Common Patterns","text":""},{"location":"config/trainrun-schema/#pattern-1-quick-experiment","title":"Pattern 1: Quick Experiment","text":"<p>Minimal configuration for rapid iteration:</p> <pre><code># @package _global_\n\nname: quick_test\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\noutput_dir: outputs/${name}\n</code></pre>"},{"location":"config/trainrun-schema/#pattern-2-hyperparameter-sweep","title":"Pattern 2: Hyperparameter Sweep","text":"<p>Base configuration for multi-run experiments:</p> <pre><code># @package _global_\n\nname: hp_sweep_lr_${training.optimizer.lr}\n\ndefaults:\n  - /pipeline@pipeline: channel_selector\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\ntraining:\n  optimizer:\n    lr: 0.001  # Override from command line\n\noutput_dir: outputs/${name}\n</code></pre> <p>Usage: <pre><code>python train.py -m training.optimizer.lr=0.001,0.0001,0.00001\n</code></pre></p>"},{"location":"config/trainrun-schema/#pattern-3-production-configuration","title":"Pattern 3: Production Configuration","text":"<p>Comprehensive configuration with all settings explicit:</p> <pre><code># @package _global_\n\nname: production_model_v1\n\ndefaults:\n  - /pipeline@pipeline: channel_selector\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\ndata:\n  cu3s_file_path: data/production/train.cu3s\n  train_ids: [0, 1, 2, 3, 4]\n  val_ids: [5, 6]\n  test_ids: [7, 8, 9]\n  batch_size: 4\n  shuffle: true\n\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 200\n    accelerator: gpu\n    devices: 1\n    precision: \"16-mixed\"\n    deterministic: true\n    gradient_clip_val: 1.0\n\n    callbacks:\n      model_checkpoint:\n        dirpath: outputs/${name}/checkpoints\n        monitor: metrics_anomaly/iou\n        mode: max\n        save_top_k: 1\n        save_last: true\n\n      early_stopping:\n        monitor: metrics_anomaly/iou\n        patience: 20\n        mode: max\n        min_delta: 0.001\n\n  optimizer:\n    name: adamw\n    lr: 0.0005\n    weight_decay: 0.01\n\n  scheduler:\n    name: reduce_on_plateau\n    monitor: metrics_anomaly/iou\n    mode: max\n    factor: 0.5\n    patience: 10\n\noutput_dir: outputs/${name}\n\nloss_nodes:\n  - bce_loss\n  - entropy_loss\nmetric_nodes:\n  - metrics_anomaly\nunfreeze_nodes:\n  - selector\n  - rx_global\n  - logit_head\n\ntags:\n  version: v1\n  purpose: production\n  dataset: production_dataset\n  created: 2026-02-04\n</code></pre>"},{"location":"config/trainrun-schema/#see-also","title":"See Also","text":"<ul> <li>Build Pipelines in YAML</li> <li>Restore Pipeline from TrainRun</li> <li>Hydra Composition</li> <li>Pipeline Schema</li> <li>Config Groups</li> </ul>"},{"location":"deployment/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"deployment/#deployment-guide","title":"Deployment Guide","text":"<p>Deploy CUVIS.AI pipelines in production environments with confidence.</p>"},{"location":"deployment/#deployment-topics","title":"Deployment Topics","text":"<ul> <li> <p> Docker &amp; Kubernetes</p> <p>Containerize and orchestrate CUVIS.AI deployments</p> </li> <li> <p> Logging &amp; Monitoring</p> <p>Set up comprehensive logging and monitoring</p> </li> <li> <p> Stress Testing</p> <p>Test and benchmark your deployments</p> </li> <li> <p> gRPC Deployment</p> <p>Deploy gRPC services for remote access</p> </li> </ul> <p>Related Pages: - Configuration Guide - GPU Acceleration</p>"},{"location":"deployment/docker-kubernetes/","title":"Docker & Kubernetes","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"deployment/docker-kubernetes/#docker-kubernetes-deployment","title":"Docker &amp; Kubernetes Deployment","text":"<p>Status: Under Development (Phase 6) Expected Completion: Week 6</p>"},{"location":"deployment/docker-kubernetes/#coming-soon","title":"Coming Soon","text":"<p>This guide will cover: - Creating Docker images for CUVIS.AI - Multi-stage Docker builds - Kubernetes deployment manifests - Scaling and resource management - Health checks and readiness probes - CI/CD integration</p> <p>Related Pages: - Deployment Overview - gRPC Deployment - Logging &amp; Monitoring</p>"},{"location":"deployment/grpc_deployment/","title":"Deployment","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"deployment/grpc_deployment/#grpc-api-deployment-guide","title":"gRPC API Deployment Guide","text":"<p>This guide covers running the cuvis.ai gRPC service in local, containerized, and clustered environments.</p>"},{"location":"deployment/grpc_deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>PyTorch and PyTorch Lightning dependencies installed via project tooling</li> <li>gRPC runtime (<code>grpcio</code>)</li> <li>Access to <code>.cu3s</code> data and optional annotation JSON files</li> </ul>"},{"location":"deployment/grpc_deployment/#installation","title":"Installation","text":""},{"location":"deployment/grpc_deployment/#using-uv-recommended","title":"Using UV (recommended)","text":"<pre><code>git clone git@gitlab.cubert.local:cubert/cuvis.ai.git\ncd cuvis.ai\nuv sync\ncd proto &amp;&amp; buf generate\n</code></pre>"},{"location":"deployment/grpc_deployment/#using-pip","title":"Using pip","text":"<pre><code>git clone git@gitlab.cubert.local:cubert/cuvis.ai.git\ncd cuvis.ai\npip install -e .\npip install grpcio grpcio-tools\ncd proto &amp;&amp; buf generate\n</code></pre>"},{"location":"deployment/grpc_deployment/#running-the-server","title":"Running the Server","text":"<p>The reference server lives in <code>examples/grpc/server.py</code>: <pre><code>uv run python examples/grpc/server.py\n</code></pre></p> <p>Configure message sizes or thread pool depth as needed: <pre><code>server = grpc.server(\n    futures.ThreadPoolExecutor(max_workers=10),\n    options=[\n        (\"grpc.max_send_message_length\", 100 * 1024 * 1024),\n        (\"grpc.max_receive_message_length\", 100 * 1024 * 1024),\n    ],\n)\n</code></pre></p>"},{"location":"deployment/grpc_deployment/#tls","title":"TLS","text":"<p>Generate a key/cert pair and enable TLS on the server: <pre><code>openssl genrsa -out server.key 2048\nopenssl req -new -x509 -key server.key -out server.crt -days 365\n</code></pre> <pre><code>credentials = grpc.ssl_server_credentials([(open(\"server.key\", \"rb\").read(), open(\"server.crt\", \"rb\").read())])\nserver.add_secure_port(\"[::]:50051\", credentials)\n</code></pre></p>"},{"location":"deployment/grpc_deployment/#docker","title":"Docker","text":"<p><code>Dockerfile</code> example: <pre><code>FROM python:3.10-slim\n\nWORKDIR /app\nCOPY pyproject.toml uv.lock ./\nRUN pip install uv &amp;&amp; uv sync\n\nCOPY . .\nRUN cd proto &amp;&amp; buf generate\n\nEXPOSE 50051\nCMD [\"uv\", \"run\", \"python\", \"examples/grpc/server.py\"]\n</code></pre></p> <p>Build and run: <pre><code>docker build -t cuvis-ai-grpc .\ndocker run -p 50051:50051 -v /data:/data cuvis-ai-grpc\n</code></pre></p>"},{"location":"deployment/grpc_deployment/#kubernetes","title":"Kubernetes","text":"<p>Minimal deployment + service: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cuvis-ai-grpc\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cuvis-ai-grpc\n  template:\n    metadata:\n      labels:\n        app: cuvis-ai-grpc\n    spec:\n      containers:\n      - name: grpc-server\n        image: cuvis-ai-grpc:latest\n        ports:\n        - containerPort: 50051\n        resources:\n          requests:\n            cpu: \"2\"\n            memory: \"4Gi\"\n          limits:\n            cpu: \"4\"\n            memory: \"8Gi\"\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: cuvis-data-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cuvis-ai-grpc\nspec:\n  selector:\n    app: cuvis-ai-grpc\n  ports:\n  - port: 50051\n    targetPort: 50051\n  type: LoadBalancer\n</code></pre></p>"},{"location":"deployment/grpc_deployment/#monitoring-and-health","title":"Monitoring and Health","text":"<ul> <li>Enable structured logging with <code>logging.basicConfig</code>.</li> <li>Expose Prometheus metrics (e.g., via <code>prometheus-client</code> counters/histograms).</li> <li>Add gRPC health checks using <code>grpc_health.v1.health_pb2_grpc</code>.</li> </ul>"},{"location":"deployment/grpc_deployment/#security","title":"Security","text":"<ul> <li>Add an authentication interceptor (token or mTLS).</li> <li>Enforce rate limiting per client to avoid resource exhaustion.</li> <li>Restrict message sizes and concurrency to fit hardware budgets.</li> </ul>"},{"location":"deployment/grpc_deployment/#troubleshooting","title":"Troubleshooting","text":"<ul> <li><code>NOT_FOUND</code> errors: verify <code>session_id</code> exists and was not closed.</li> <li>Message size exceeded: raise <code>grpc.max_send_message_length</code> / <code>grpc.max_receive_message_length</code> on both client and server.</li> <li>Slow throughput: increase thread pool size or reduce batch sizes.</li> <li>Port conflicts: choose an available port or free the existing listener.</li> </ul>"},{"location":"deployment/logging-monitoring/","title":"Logging & Monitoring","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"deployment/logging-monitoring/#logging-monitoring","title":"Logging &amp; Monitoring","text":"<p>Status: Under Development (Phase 6) Expected Completion: Week 6</p>"},{"location":"deployment/logging-monitoring/#coming-soon","title":"Coming Soon","text":"<p>This guide will cover: - Structured logging configuration - Log aggregation with ELK/EFK stack - Metrics collection and exposition - Prometheus integration - Grafana dashboards - Alerting and notifications</p> <p>Related Pages: - Deployment Overview - Monitoring &amp; Visualization - Stress Testing</p>"},{"location":"development/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"development/#development-guide","title":"Development Guide","text":"<p>Contributing to CUVIS.AI development and following best practices.</p>"},{"location":"development/#development-documentation","title":"Development Documentation","text":"<ul> <li> <p> Contributing</p> <p>Guidelines for contributing to the project</p> </li> <li> <p> Docstrings Guide</p> <p>Write consistent numpy-style docstrings</p> </li> <li> <p> Git Hooks</p> <p>Set up pre-commit hooks and automation</p> </li> </ul> <p>Related Pages: - Add Built-in Node - Plugin Development</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"development/contributing/#contributing-guide","title":"Contributing Guide","text":"<p>Welcome to the cuvis-ai community! We're excited to have you contribute to our open-source hyperspectral image analysis framework.</p>"},{"location":"development/contributing/#introduction-philosophy","title":"Introduction &amp; Philosophy","text":"<p>cuvis-ai is built on the principle of community-driven open source development. We believe the best software is created through collaboration and diverse perspectives. Cubert GmbH is committed to fostering an open, inclusive, and positive community where everyone can contribute meaningfully.</p>"},{"location":"development/contributing/#primary-contribution-path-plugins","title":"Primary Contribution Path: Plugins","text":"<p>The recommended way to extend cuvis-ai is through the plugin system. By creating plugins in your own repository, you can:</p> <ul> <li>Maintain independent version control and release cycles</li> <li>Manage your own dependencies without affecting the core framework</li> <li>Share your work with the community through the central plugin registry</li> <li>Keep your proprietary algorithms separate while still integrating with cuvis-ai pipelines</li> </ul>"},{"location":"development/contributing/#secondary-path-built-in-nodes","title":"Secondary Path: Built-in Nodes","text":"<p>Contributing directly to the core framework is also valuable for:</p> <ul> <li>Core functionality improvements</li> <li>General-purpose nodes that benefit all users</li> <li>Performance optimizations to the framework itself</li> </ul> <p>Built-in contributions require more rigorous review and integration testing, but they become part of the official cuvis-ai distribution.</p>"},{"location":"development/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"development/contributing/#plugin-nodes-recommended","title":"\ud83d\udd0c Plugin Nodes (Recommended)","text":"<p>What: Create custom nodes in your own repository that extend cuvis-ai functionality.</p> <p>When to use: When you want to add domain-specific algorithms, experimental methods, or proprietary techniques.</p> <p>How: 1. Follow the Plugin Development Guide to create your plugin 2. Implement nodes inheriting from <code>cuvis_ai_core.node.Node</code> 3. Test with cuvis-ai pipelines using local plugin loading 4. Publish to GitHub and submit to the central registry (see Plugin Contribution Workflow below)</p> <p>Benefits: Independent versioning, faster development cycles, easier maintenance, community sharing.</p>"},{"location":"development/contributing/#built-in-nodes","title":"\ud83c\udfd7\ufe0f Built-in Nodes","text":"<p>What: Contribute nodes directly to the cuvis-ai core codebase.</p> <p>When to use: When your node provides general-purpose functionality that benefits all users.</p> <p>How: 1. Follow the Add Built-in Node Guide 2. Submit a pull request with comprehensive tests 3. Undergo code review and integration testing</p> <p>Requirements: Must follow core coding standards, include thorough documentation, pass all tests.</p>"},{"location":"development/contributing/#documentation","title":"\ud83d\udcd6 Documentation","text":"<p>Improve guides, tutorials, API documentation, or fix typos. Documentation PRs are always welcome!</p>"},{"location":"development/contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>Found a bug? Open an issue with: - Clear description of the problem - Steps to reproduce - Expected vs actual behavior - Environment details (OS, Python version, CUDA version) - Minimal code example demonstrating the bug</p>"},{"location":"development/contributing/#feature-requests","title":"\ud83d\udca1 Feature Requests","text":"<p>Have an idea for improvement? Open a discussion to: - Describe the feature and its use case - Explain why it would benefit the community - Discuss potential implementation approaches</p>"},{"location":"development/contributing/#plugin-contribution-workflow","title":"Plugin Contribution Workflow","text":"<p>This section describes the complete process for contributing a plugin to the central registry, making your custom nodes discoverable and usable by the entire cuvis-ai community.</p>"},{"location":"development/contributing/#step-1-develop-your-plugin","title":"Step 1: Develop Your Plugin","text":"<p>Follow the comprehensive Plugin Development Guide to create your plugin from scratch.</p> <p>Key requirements: - Node Implementation: Inherit from <code>cuvis_ai_core.node.Node</code> - Project Structure: Create proper <code>pyproject.toml</code> with dependencies and metadata - Documentation: Write comprehensive README with usage examples - Testing: Add tests using pytest</p> <p>Example plugin structure: <pre><code>my-plugin/\n\u251c\u2500\u2500 my_plugin/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 nodes/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 custom_detector.py  # Your node classes\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_custom_detector.py\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 LICENSE\n</code></pre></p> <p>Example node implementation: <pre><code>from cuvis_ai_core.node import Node\n\nclass CustomDetector(Node):\n    \"\"\"Custom anomaly detector using proprietary algorithm.\"\"\"\n\n    def __init__(self, threshold: float = 0.5, **kwargs):\n        super().__init__(**kwargs)\n        self.threshold = threshold\n\n    def process(self, data):\n        # Your implementation here\n        return processed_data\n</code></pre></p>"},{"location":"development/contributing/#step-2-test-locally","title":"Step 2: Test Locally","text":"<p>Before publishing, thoroughly test your plugin with cuvis-ai pipelines.</p> <p>Create a local manifest (<code>plugins.yaml</code>): <pre><code>plugins:\n  my_plugin:\n    path: \"../my-plugin\"  # Local directory path for development\n    provides:\n      - my_plugin.nodes.CustomDetector\n</code></pre></p> <p>Test with example pipelines: <pre><code># Test plugin loading\nuv run python -c \"\nfrom cuvis_ai_core.utils.node_registry import NodeRegistry\nregistry = NodeRegistry()\nregistry.load_plugins('plugins.yaml')\nCustomDetector = NodeRegistry.get('CustomDetector', instance=registry)\nprint('Plugin loaded successfully!')\n\"\n\n# Test with full pipeline\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_test_pipeline.yaml \\\n    --plugins-path plugins.yaml\n</code></pre></p> <p>Verify: - \u2705 All dependencies install correctly - \u2705 Nodes load without errors - \u2705 Nodes work in actual pipelines - \u2705 All tests pass: <code>pytest tests/</code></p>"},{"location":"development/contributing/#step-3-publish-to-github","title":"Step 3: Publish to GitHub","text":"<p>Prepare your plugin for public distribution.</p> <p>Requirements: - Public GitHub repository (required for central registry) - Semantic version tag: <code>v1.0.0</code>, <code>v0.1.0-beta</code>, <code>v2.0.0-rc.1</code> - LICENSE file: Permissive open source license recommended (MIT, Apache-2.0, BSD)</p> <p>Create comprehensive README: <pre><code># My Custom Plugin\n\nBrief description of your plugin and its purpose.\n\n## Installation\n\n\\`\\`\\`yaml\nplugins:\n  my_plugin:\n    repo: \"https://github.com/yourorg/my-plugin.git\"\n    tag: \"v1.0.0\"\n    provides:\n      - my_plugin.nodes.CustomDetector\n\\`\\`\\`\n\n## Nodes Provided\n\n### CustomDetector\n\nDescription of the node, its inputs, outputs, and parameters.\n\n## Usage Example\n\n\\`\\`\\`python\n# Example pipeline configuration\n...\n\\`\\`\\`\n\n## Dependencies\n\n- numpy&gt;=1.20.0\n- scikit-learn&gt;=1.0.0\n\n## License\n\nMIT License\n</code></pre></p> <p>Create Git tag: <pre><code>git tag -a v1.0.0 -m \"Initial release\"\ngit push origin v1.0.0\n</code></pre></p>"},{"location":"development/contributing/#step-4-submit-to-central-registry","title":"Step 4: Submit to Central Registry","text":"<p>Add your plugin to the cuvis-ai central registry so users can discover and use it.</p> <p>4.1 Fork the cuvis-ai repository</p> <p>Navigate to github.com/cubert-hyperspectral/cuvis-ai and click \"Fork\".</p> <p>4.2 Edit <code>configs/plugins/registry.yaml</code></p> <p>Add your plugin entry: <pre><code>plugins:\n  # ... existing plugins ...\n\n  my_plugin:\n    repo: \"https://github.com/yourorg/my-plugin.git\"\n    tag: \"v1.0.0\"  # Your release tag\n    provides:\n      - my_plugin.nodes.CustomDetector\n      - my_plugin.nodes.HelperNode  # List all public nodes\n</code></pre></p> <p>4.3 Add documentation entry</p> <p>Edit <code>docs/plugin-system/index.md</code> under the \"Community Plugins\" section: <pre><code>### Community Plugins\n\n- **[cuvis-ai-adaclip](https://github.com/cubert-hyperspectral/cuvis-ai-adaclip)** - AdaCLIP vision-language anomaly detection\n- **[my-plugin](https://github.com/yourorg/my-plugin)** - Brief description of your plugin's functionality and use cases\n</code></pre></p> <p>4.4 (Optional) Add showcase example</p> <p>Create <code>examples/my-plugin/</code> directory with: - Sample pipeline configuration using your plugin - Example data or instructions - README explaining the example</p>"},{"location":"development/contributing/#step-5-create-pull-request","title":"Step 5: Create Pull Request","text":"<p>Submit your plugin registration for review.</p> <p>PR title format: <pre><code>plugin: add [my-plugin] to registry\n</code></pre></p> <p>PR description template: <pre><code>## Plugin Information\n\n**Name:** my-plugin\n**Repository:** https://github.com/yourorg/my-plugin\n**Version:** v1.0.0\n\n## Purpose\n\nBrief description of what the plugin does and why it's useful.\n\n## Nodes Provided\n\n- `CustomDetector`: Description of this node\n- `HelperNode`: Description of this node\n\n## Example Usage\n\n\\`\\`\\`yaml\nplugins:\n  my_plugin:\n    repo: \"https://github.com/yourorg/my-plugin.git\"\n    tag: \"v1.0.0\"\n    provides:\n      - my_plugin.nodes.CustomDetector\n\n# Use in pipeline\nnodes:\n  detector:\n    _target_: CustomDetector\n    threshold: 0.75\n\\`\\`\\`\n\n## Testing Performed\n\n- [x] Plugin loads successfully with NodeRegistry\n- [x] All tests pass (`pytest tests/`)\n- [x] Tested with example pipeline (attached in examples/)\n- [x] Documentation is comprehensive\n\n## Dependencies\n\nNo significant dependencies beyond cuvis-ai-core requirements.\n\n## Checklist\n\n- [x] Added entry to `configs/plugins/registry.yaml`\n- [x] Added documentation to `docs/plugin-system/index.md`\n- [x] Included LICENSE file (MIT)\n- [x] README has installation and usage examples\n- [ ] Added showcase example in `examples/` (optional)\n</code></pre></p> <p>Add label: <code>plugin-contribution</code></p>"},{"location":"development/contributing/#step-6-review-process","title":"Step 6: Review Process","text":"<p>The core team will review your submission.</p> <p>What we check: - \u2705 YAML manifest syntax is valid - \u2705 Plugin loads successfully with NodeRegistry - \u2705 Plugin follows development best practices - \u2705 Appropriate open source licensing - \u2705 Documentation is comprehensive and clear - \u2705 Tests exist and pass (if example provided)</p> <p>Timeline: - Typical review turnaround: 3-5 business days - We may request changes to manifest formatting or documentation - Once approved, your plugin will be merged into the registry</p> <p>Communication: - All feedback will be provided through PR comments - Please respond to review comments promptly - Feel free to ask questions or request clarification</p>"},{"location":"development/contributing/#step-7-post-acceptance-maintenance","title":"Step 7: Post-Acceptance Maintenance","text":"<p>After your plugin is accepted, keep it maintained for the community.</p> <p>Ongoing responsibilities: - Respond to issues: Help users who encounter problems with your plugin - Keep it working: Ensure compatibility with new cuvis-ai releases - Update registry: When releasing new versions, submit PRs to update the <code>tag:</code> field</p> <p>Releasing updates: <pre><code># Create new version tag\ngit tag -a v1.1.0 -m \"Add new feature X\"\ngit push origin v1.1.0\n\n# Update registry (submit PR)\n# Edit configs/plugins/registry.yaml:\nmy_plugin:\n  repo: \"https://github.com/yourorg/my-plugin.git\"\n  tag: \"v1.1.0\"  # Update to new version\n  provides:\n    - my_plugin.nodes.CustomDetector\n</code></pre></p> <p>Best practices: - Use GitHub Releases for better visibility - Follow semantic versioning (MAJOR.MINOR.PATCH) - Write clear release notes describing changes - Update your plugin's README when adding features</p>"},{"location":"development/contributing/#important-requirements-for-plugins","title":"Important Requirements for Plugins","text":"<p>When developing and submitting plugins, ensure they meet these requirements:</p> <ul> <li>\u2705 Only Git tags supported (no branches/commits) for reproducibility</li> <li>\u2705 Valid tag formats: <code>v1.2.3</code>, <code>v0.1.0-alpha</code>, <code>2.0.0-rc.1</code> (semantic versioning)</li> <li>\u2705 Proper <code>pyproject.toml</code> with all dependencies declared</li> <li>\u2705 Comprehensive README with usage examples and API documentation</li> <li>\u2705 Permissive open source license (MIT, Apache-2.0, BSD preferred)</li> <li>\u2705 Loading test passes (NodeRegistry can import all provided classes)</li> </ul> <p>Local development support:</p> <p>During development, you can use local directory paths instead of Git repositories:</p> <pre><code>plugins:\n  # Production plugin from Git\n  production_plugin:\n    repo: \"https://github.com/org/plugin.git\"\n    tag: \"v1.0.0\"\n    provides:\n      - production_plugin.nodes.Node1\n\n  # Local plugin for development\n  dev_plugin:\n    path: \"../my-plugin\"  # Relative or absolute directory path\n    provides:\n      - dev_plugin.nodes.TestNode\n</code></pre> <p>This is particularly useful for: - Testing changes before publishing - Private plugins that won't be on GitHub - Quick iterations during development</p>"},{"location":"development/contributing/#development-environment-setup","title":"Development Environment Setup","text":"<p>To contribute to cuvis-ai core or develop plugins, set up your development environment.</p>"},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (3.11 recommended)</li> <li>uv package manager: Install instructions</li> <li>Git for version control</li> <li>CUDA (optional, for GPU acceleration)</li> </ul>"},{"location":"development/contributing/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/cubert-hyperspectral/cuvis-ai.git\ncd cuvis-ai\n\n# Install with all dependencies including dev tools\nuv sync --all-extras --dev\n</code></pre>"},{"location":"development/contributing/#set-up-git-hooks","title":"Set Up Git Hooks","text":"<p>We use pre-commit hooks to ensure code quality:</p> <pre><code># Install pre-commit hooks\npre-commit install\n\n# Run manually on all files (optional)\npre-commit run --all-files\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest -v\n\n# Run specific test file\npytest tests/test_node_registry.py -v\n\n# Run with coverage\npytest --cov=cuvis_ai_core --cov-report=html\n</code></pre>"},{"location":"development/contributing/#ide-setup","title":"IDE Setup","text":"<p>VS Code: Install recommended extensions (Python, Pylance, Ruff)</p> <p>PyCharm: Configure interpreter to use the uv-managed virtual environment</p>"},{"location":"development/contributing/#code-standards","title":"Code Standards","text":""},{"location":"development/contributing/#style-guide","title":"Style Guide","text":"<p>We follow PEP 8 with enforcement through Ruff:</p> <ul> <li>Line length: 100 characters (not the PEP 8 default of 79)</li> <li>Use double quotes for strings</li> <li>4 spaces for indentation (no tabs)</li> </ul> <p>The pre-commit hooks will automatically format your code.</p>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>Type hints are required for all public functions:</p> <pre><code>def process_data(\n    data: np.ndarray,\n    threshold: float = 0.5\n) -&gt; tuple[np.ndarray, dict[str, Any]]:\n    \"\"\"Process hyperspectral data.\"\"\"\n    ...\n</code></pre>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>We use Google-style docstrings. See the Docstrings Guide for detailed formatting requirements.</p> <p>Example: <pre><code>class CustomNode(Node):\n    \"\"\"Brief one-line description.\n\n    More detailed explanation of what the node does,\n    its purpose, and how it fits into pipelines.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Attributes:\n        attribute1: Description of attribute1\n\n    Example:\n        ```python\n        node = CustomNode(param1=value1)\n        result = node.process(data)\n        ```\n    \"\"\"\n    ...\n</code></pre></p>"},{"location":"development/contributing/#testing","title":"Testing","text":"<p>All new code requires tests:</p> <ul> <li>Unit tests for individual functions/methods</li> <li>Integration tests for node interactions</li> <li>Use pytest fixtures for common setup</li> <li>Aim for 80%+ code coverage</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#branch-naming","title":"Branch Naming","text":"<p>Use descriptive branch names with prefixes:</p> <ul> <li><code>feature/add-custom-detector</code> - New features</li> <li><code>fix/memory-leak-in-pipeline</code> - Bug fixes</li> <li><code>docs/update-plugin-guide</code> - Documentation updates</li> <li><code>refactor/simplify-node-registry</code> - Code refactoring</li> </ul>"},{"location":"development/contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow Conventional Commits style:</p> <pre><code>type(scope): brief description\n\nLonger explanation if needed (wrap at 72 chars).\n\n- Bullet points for multiple changes\n- Use present tense (\"add\" not \"added\")\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>style</code>, <code>refactor</code>, <code>test</code>, <code>chore</code></p> <p>Examples: <pre><code>feat(nodes): add spectral angle mapper node\n\ndocs(plugin-system): update contribution workflow\n\nfix(pipeline): resolve memory leak in batch processing\n</code></pre></p>"},{"location":"development/contributing/#pr-description","title":"PR Description","text":"<p>Your PR description should include:</p> <ol> <li>What: Brief summary of changes</li> <li>Why: Motivation and context</li> <li>How: Implementation approach (if non-obvious)</li> <li>Testing: What tests were added/modified</li> <li>Breaking changes: Any backwards-incompatible changes</li> <li>Screenshots: If UI changes (for docs/examples)</li> </ol>"},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<p>Checklist:</p> <ul> <li> All tests pass: <code>pytest -v</code></li> <li> Pre-commit hooks pass: <code>pre-commit run --all-files</code></li> <li> Documentation updated (if applicable)</li> <li> Docstrings added for new functions/classes</li> <li> Type hints included</li> <li> CHANGELOG updated (for significant changes)</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":""},{"location":"development/contributing/#community-channels","title":"Community Channels","text":"<ul> <li>Discord/Slack: (coming soon!) Real-time chat with other contributors</li> <li>GitHub Discussions: Ask questions, share ideas, get feedback</li> <li>Issue Tracker: Report bugs or request features</li> </ul>"},{"location":"development/contributing/#asking-questions","title":"Asking Questions","text":"<p>When asking for help:</p> <ol> <li>Search first: Check if your question has been answered</li> <li>Be specific: Provide context, code examples, error messages</li> <li>Be respectful: Everyone is volunteering their time</li> </ol>"},{"location":"development/contributing/#contact","title":"Contact","text":"<p>For private inquiries or collaboration opportunities: - Email: [contact info from README or website] - Open a GitHub Discussion</p>"},{"location":"development/contributing/#community-values","title":"Community Values","text":"<p>Cubert GmbH is committed to creating an open, inclusive, and positive community. We expect all contributors to:</p> <ul> <li>Be respectful and welcoming to newcomers</li> <li>Provide constructive feedback</li> <li>Focus on what's best for the community</li> <li>Show empathy towards others</li> </ul> <p>We have zero tolerance for harassment, discrimination, or abusive behavior.</p>"},{"location":"development/contributing/#thank-you","title":"Thank You!","text":"<p>Thank you for contributing to cuvis-ai! Your work helps build a better tool for the hyperspectral imaging community. We're excited to see what you create!</p> <p>Quick Links: - Plugin Development Guide - Complete guide to creating plugins - Add Built-in Node Guide - Contributing to core - Node System Deep Dive - Understanding node architecture - Central Plugin Registry - Browse registered plugins</p>"},{"location":"development/docstrings/","title":"Docstrings Guide","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"development/docstrings/#docstring-standards","title":"Docstring Standards","text":""},{"location":"development/docstrings/#overview","title":"Overview","text":"<p>This guide provides comprehensive standards for writing high-quality docstrings in CUVIS.AI. Following these standards ensures consistent, well-documented code that generates excellent API documentation.</p>"},{"location":"development/docstrings/#docstring-style","title":"Docstring Style","text":"<p>CUVIS.AI uses NumPy style docstrings for consistency with the scientific Python ecosystem and compatibility with mkdocstrings.</p>"},{"location":"development/docstrings/#why-numpy-style","title":"Why NumPy Style?","text":"<ul> <li>Readability: Clear section headers and structured format</li> <li>Compatibility: Works seamlessly with mkdocstrings and Sphinx</li> <li>Scientific Standard: Widely adopted in NumPy, SciPy, scikit-learn, and PyTorch</li> <li>Rich Features: Supports all common documentation needs (parameters, returns, raises, examples, notes, references)</li> </ul>"},{"location":"development/docstrings/#required-sections-by-component-type","title":"Required Sections by Component Type","text":""},{"location":"development/docstrings/#for-modules","title":"For Modules","text":"<p>Every Python module should have a module-level docstring at the top of the file:</p> <pre><code>\"\"\"\nOne-line module summary (under 80 characters).\n\nExtended description of module purpose and contents.\nCan span multiple paragraphs to provide context about\nwhat the module contains and when to use it.\n\nSee Also\n--------\nrelated_module : Brief description\nanother_module : Brief description\n\"\"\"\n</code></pre> <p>Example:</p> <pre><code>\"\"\"\nAnomaly Detection Nodes.\n\nThis module provides anomaly detection nodes for hyperspectral image analysis,\nincluding both statistical methods (RX, LAD) and deep learning approaches\n(Deep SVDD). Each node implements the BaseNode interface and can be composed\ninto processing pipelines.\n\nSee Also\n--------\ncuvis_ai.deciders : Binary decision nodes for classification\ncuvis_ai.node.normalization : Preprocessing nodes\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#for-classes","title":"For Classes","text":"<p>All public classes must have comprehensive docstrings:</p> <pre><code>class MyNode(BaseNode):\n    \"\"\"\n    One-line class summary (under 80 characters).\n\n    Extended description explaining what this class does, when to use it,\n    and any important behavioral characteristics. Can span multiple\n    paragraphs if needed.\n\n    Parameters\n    ----------\n    param1 : type\n        Description of param1. Explain what it controls and valid values.\n    param2 : type, optional\n        Description of param2. Mention default behavior.\n        Default is ``default_value``.\n    param3 : type or None, optional\n        Description of param3. Explain None behavior.\n        If None, the behavior is... Default is None.\n\n    Attributes\n    ----------\n    attribute1 : type\n        Description of public attribute and what it stores.\n    attribute2 : type\n        Description of public attribute.\n\n    Raises\n    ------\n    ValueError\n        If param1 is negative or out of valid range.\n    RuntimeError\n        If node is not initialized before processing.\n\n    See Also\n    --------\n    RelatedNode : Brief description of relationship\n    AnotherNode : Brief description of relationship\n\n    Notes\n    -----\n    Additional implementation notes, algorithm details, or important\n    considerations for users. Can include mathematical formulas,\n    performance characteristics, or usage guidelines.\n\n    References\n    ----------\n    .. [1] Author, \"Paper Title,\" Journal, Year.\n           URL or DOI if applicable.\n\n    Examples\n    --------\n    Basic usage:\n\n    &gt;&gt;&gt; node = MyNode(param1=10)\n    &gt;&gt;&gt; result = node.forward(data)\n    &gt;&gt;&gt; print(result[\"output\"])\n\n    Advanced usage with initialization:\n\n    &gt;&gt;&gt; from cuvis_ai_core.training import StatisticalTrainer\n    &gt;&gt;&gt; node = MyNode(param1=20, param2=0.5)\n    &gt;&gt;&gt; pipeline.add_node(node)\n    &gt;&gt;&gt; trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n    &gt;&gt;&gt; trainer.fit()  # Initializes node\n    &gt;&gt;&gt; result = node.forward(test_data)\n    \"\"\"\n</code></pre>"},{"location":"development/docstrings/#for-methods-and-functions","title":"For Methods and Functions","text":"<p>All public methods must document parameters, returns, and exceptions:</p> <pre><code>def forward(self, data: np.ndarray, mask: Optional[np.ndarray] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process input data through the node.\n\n    Extended description if the method needs more explanation\n    about what it does and how it works.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data with shape ``(batch, channels, height, width)``.\n        Values should be in range [0, 1] after normalization.\n    mask : np.ndarray or None, optional\n        Binary mask with shape ``(batch, 1, height, width)``.\n        If None, no masking is applied. Default is None.\n\n    Returns\n    -------\n    dict\n        Dictionary containing:\n\n        - 'output' : np.ndarray\n            Processed data with shape ``(batch, out_channels, height, width)``.\n        - 'scores' : np.ndarray\n            Anomaly scores with shape ``(batch, height, width)``.\n        - 'metadata' : dict\n            Processing metadata including computation time and statistics.\n\n    Raises\n    ------\n    ValueError\n        If input data has incorrect shape or invalid values.\n    RuntimeError\n        If node is not initialized via ``statistical_initialization()``.\n\n    See Also\n    --------\n    statistical_initialization : Initialize node with initialization data\n\n    Notes\n    -----\n    This method processes data in batches for efficiency. For large\n    datasets, consider using batch_size &lt;= 32 to avoid memory issues.\n\n    Examples\n    --------\n    Basic usage:\n\n    &gt;&gt;&gt; data = torch.randn(1, 3, 64, 64)\n    &gt;&gt;&gt; result = node.forward(data)\n    &gt;&gt;&gt; result['output'].shape\n    torch.Size([1, 10, 64, 64])\n\n    With masking:\n\n    &gt;&gt;&gt; mask = torch.ones(1, 1, 64, 64)\n    &gt;&gt;&gt; result = node.forward(data, mask=mask)\n    \"\"\"\n</code></pre>"},{"location":"development/docstrings/#for-properties","title":"For Properties","text":"<p>Properties should have concise docstrings:</p> <pre><code>@property\ndef requires_initial_fit(self) -&gt; bool:\n    \"\"\"\n    Whether this node requires statistical initialization.\n\n    Returns\n    -------\n    bool\n        True if ``statistical_initialization()`` must be called before\n        ``forward()``, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"development/docstrings/#best-practices","title":"Best Practices","text":""},{"location":"development/docstrings/#1-be-concise-but-complete","title":"1. Be Concise but Complete","text":"<ul> <li>First line: Brief summary in one sentence (under 80 characters)</li> <li>Blank line: Always follow first line with blank line</li> <li>Extended description: Add more details if needed in subsequent paragraphs</li> </ul> <p>Good:</p> <pre><code>def process(data):\n    \"\"\"\n    Process hyperspectral data through RX anomaly detection.\n\n    Applies the RX algorithm to compute pixel-wise anomaly scores\n    based on Mahalanobis distance from the background distribution.\n    \"\"\"\n</code></pre> <p>Bad:</p> <pre><code>def process(data):\n    \"\"\"This function processes data.\"\"\"  # Too vague\n</code></pre>"},{"location":"development/docstrings/#2-document-parameters-thoroughly","title":"2. Document Parameters Thoroughly","text":"<p>For each parameter, include:</p> <ul> <li>Type: Clear type annotation</li> <li>Purpose: What the parameter controls</li> <li>Valid range/values: Constraints or valid options</li> <li>Default behavior: For optional parameters</li> <li>Units: If applicable (e.g., pixels, seconds, degrees)</li> </ul> <p>Good:</p> <pre><code>\"\"\"\nParameters\n----------\nthreshold : float\n    Anomaly score threshold in range [0, 1]. Pixels with scores\n    above this value are classified as anomalous. Default is 0.5.\nchannels : list of int or None, optional\n    Channel indices to process. If None, all channels are used.\n    Default is None.\n\"\"\"\n</code></pre> <p>Bad:</p> <pre><code>\"\"\"\nParameters\n----------\nthreshold : float\n    The threshold.  # Incomplete - missing range, purpose, default\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#3-describe-return-values-clearly","title":"3. Describe Return Values Clearly","text":"<p>For dictionary returns, document all keys and their meanings:</p> <p>Good:</p> <pre><code>\"\"\"\nReturns\n-------\ndict\n    Dictionary containing:\n\n    - 'scores' : torch.Tensor\n        Anomaly scores with shape ``(batch, height, width)``.\n        Higher values indicate more anomalous pixels.\n    - 'threshold' : float\n        Adaptive threshold value used for classification.\n    - 'decisions' : torch.Tensor\n        Binary decisions (0=normal, 1=anomaly) with shape\n        ``(batch, height, width)``.\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#4-add-meaningful-examples","title":"4. Add Meaningful Examples","text":"<p>Include examples that demonstrate:</p> <ul> <li>Basic usage: Simplest way to use the component</li> <li>Common patterns: Typical use cases</li> <li>Edge cases: How to handle special situations</li> </ul> <p>Make examples runnable when possible:</p> <pre><code>\"\"\"\nExamples\n--------\nBasic RX detection:\n\n&gt;&gt;&gt; from cuvis_ai.anomaly import RXDetector\n&gt;&gt;&gt; detector = RXDetector()\n&gt;&gt;&gt; data = torch.randn(1, 150, 64, 64)  # (batch, channels, H, W)\n&gt;&gt;&gt; result = detector.forward(data)\n&gt;&gt;&gt; result['scores'].shape\ntorch.Size([1, 64, 64])\n\nWith custom parameters:\n\n&gt;&gt;&gt; from cuvis_ai_core.training import StatisticalTrainer\n&gt;&gt;&gt; detector = RXDetector(use_global_covariance=True)\n&gt;&gt;&gt; pipeline.add_node(detector)\n&gt;&gt;&gt; trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n&gt;&gt;&gt; trainer.fit()  # Initializes detector with background data\n&gt;&gt;&gt; result = detector.forward(test_data)\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#5-cross-reference-related-items","title":"5. Cross-Reference Related Items","text":"<p>Use \"See Also\" to link related functionality:</p> <pre><code>\"\"\"\nSee Also\n--------\nRXDetector : Reed-Xiaoli anomaly detector\nLADDetector : Local anomaly density detector\ncuvis_ai.deciders.BinaryDecider : Convert scores to binary decisions\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#6-document-exceptions","title":"6. Document Exceptions","text":"<p>List exceptions that can be raised and explain when:</p> <pre><code>\"\"\"\nRaises\n------\nValueError\n    If ``data`` has fewer than 2 dimensions.\n    If ``channels`` contains indices &gt;= data.shape[1].\nRuntimeError\n    If ``statistical_initialization()`` was not called when\n    ``requires_initial_fit=True``.\nFileNotFoundError\n    If checkpoint file specified in ``load_path`` does not exist.\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#7-add-notes-for-important-details","title":"7. Add Notes for Important Details","text":"<p>Use Notes section for:</p> <ul> <li>Algorithm details</li> <li>Performance considerations</li> <li>Memory requirements</li> <li>Thread safety</li> <li>Version compatibility</li> </ul> <pre><code>\"\"\"\nNotes\n-----\nThis implementation uses Welford's online algorithm for numerical\nstability when computing covariance matrices. Memory usage is\nO(C^2) where C is the number of channels.\n\nFor datasets with &gt;500 channels, consider using PCA dimensionality\nreduction first to improve performance.\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#type-hints","title":"Type Hints","text":""},{"location":"development/docstrings/#use-type-hints-consistently","title":"Use Type Hints Consistently","text":"<p>Always include type hints in function signatures:</p> <pre><code>from typing import Optional, Dict, List, Any, Union, Tuple\nimport numpy as np\nimport torch\n\ndef process(\n    self,\n    data: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    channels: Optional[List[int]] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Process data with optional mask and channel selection.\"\"\"\n</code></pre>"},{"location":"development/docstrings/#common-types","title":"Common Types","text":"Type Usage <code>torch.Tensor</code> PyTorch tensors <code>np.ndarray</code> NumPy arrays <code>Dict[str, Any]</code> Dictionaries with string keys <code>List[T]</code> Lists of type T <code>Tuple[T1, T2]</code> Tuples with specific types <code>Optional[T]</code> T or None <code>Union[T1, T2]</code> Either T1 or T2 <code>Callable[[ArgTypes], ReturnType]</code> Function types"},{"location":"development/docstrings/#examples-in-docstrings","title":"Examples in Docstrings","text":""},{"location":"development/docstrings/#doctest-format","title":"Doctest Format","text":"<p>Use doctest format for executable examples:</p> <pre><code>\"\"\"\nExamples\n--------\n&gt;&gt;&gt; node = RXDetector()\n&gt;&gt;&gt; data = torch.randn(1, 150, 64, 64)\n&gt;&gt;&gt; result = node.forward(data)\n&gt;&gt;&gt; result['scores'].shape\ntorch.Size([1, 64, 64])\n&gt;&gt;&gt; result['scores'].min() &gt;= 0\nTrue\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#narrative-examples","title":"Narrative Examples","text":"<p>For more complex examples, use narrative style:</p> <pre><code>\"\"\"\nExamples\n--------\nBasic usage with statistical initialization:\n\n&gt;&gt;&gt; # Create detector and load background data\n&gt;&gt;&gt; from cuvis_ai_core.training import StatisticalTrainer\n&gt;&gt;&gt; detector = RXDetector()\n&gt;&gt;&gt; pipeline.add_node(detector)\n&gt;&gt;&gt; background = load_hyperspectral_data(\"background.npy\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize with background statistics\n&gt;&gt;&gt; trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n&gt;&gt;&gt; trainer.fit()  # Initializes detector\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Process test data\n&gt;&gt;&gt; test_data = load_hyperspectral_data(\"test.npy\")\n&gt;&gt;&gt; result = detector.forward(test_data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply threshold for binary decisions\n&gt;&gt;&gt; decisions = result['scores'] &gt; 0.5\n\nComplete pipeline example:\n\n&gt;&gt;&gt; from cuvis_ai.anomaly import RXDetector\n&gt;&gt;&gt; from cuvis_ai.deciders import BinaryDecider\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Build detection pipeline\n&gt;&gt;&gt; detector = RXDetector(use_global_covariance=True)\n&gt;&gt;&gt; decider = BinaryDecider(threshold=0.5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Process data\n&gt;&gt;&gt; scores = detector.forward(data)\n&gt;&gt;&gt; decisions = decider.forward(scores)\n\"\"\"\n</code></pre>"},{"location":"development/docstrings/#testing-docstrings","title":"Testing Docstrings","text":""},{"location":"development/docstrings/#check-coverage","title":"Check Coverage","text":"<p>Use <code>interrogate</code> to measure docstring coverage:</p> <pre><code># Check entire package\ninterrogate -v cuvis_ai/\n\n# Check specific module\ninterrogate -vv cuvis_ai/anomaly/rx_detector.py\n\n# Require 95% coverage\ninterrogate -v cuvis_ai/ --fail-under 95\n</code></pre>"},{"location":"development/docstrings/#run-doctests","title":"Run Doctests","text":"<p>Test that examples in docstrings actually work:</p> <pre><code># Test single file\npython -m doctest cuvis_ai/anomaly/rx_detector.py -v\n\n# Test all files\npytest --doctest-modules cuvis_ai/\n</code></pre>"},{"location":"development/docstrings/#validate-style","title":"Validate Style","text":"<p>Check docstring style compliance:</p> <pre><code># Install pydocstyle\npip install pydocstyle\n\n# Check style\npydocstyle cuvis_ai/\n</code></pre>"},{"location":"development/docstrings/#tools","title":"Tools","text":""},{"location":"development/docstrings/#interrogate","title":"interrogate","text":"<p>Measures docstring coverage:</p> <pre><code># Install\npip install interrogate\n\n# Basic usage\ninterrogate -v cuvis_ai/\n\n# Detailed report with missing items\ninterrogate -vv cuvis_ai/\n\n# Generate badge\ninterrogate --generate-badge docs/badges/ cuvis_ai/\n\n# Fail if coverage below threshold\ninterrogate -v cuvis_ai/ --fail-under 95\n</code></pre> <p>Configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.interrogate]\nignore-init-method = true\nignore-init-module = false\nignore-magic = false\nignore-semiprivate = false\nignore-private = false\nignore-property-decorators = false\nignore-module = false\nignore-nested-functions = false\nignore-nested-classes = true\nignore-setters = false\nfail-under = 95\nverbose = 1\n</code></pre>"},{"location":"development/docstrings/#pydocstyle","title":"pydocstyle","text":"<p>Checks docstring style compliance:</p> <pre><code># Install\npip install pydocstyle\n\n# Check style\npydocstyle cuvis_ai/\n\n# Check specific convention\npydocstyle --convention=numpy cuvis_ai/\n</code></pre>"},{"location":"development/docstrings/#mkdocstrings","title":"mkdocstrings","text":"<p>Generates API documentation from docstrings:</p> <pre><code># Build documentation\nmkdocs build\n\n# Serve documentation locally\nmkdocs serve\n\n# Build with strict error checking\nmkdocs build --strict\n</code></pre>"},{"location":"development/docstrings/#common-patterns","title":"Common Patterns","text":""},{"location":"development/docstrings/#node-classes","title":"Node Classes","text":"<p>Standard pattern for node docstrings:</p> <pre><code>class MyDetectorNode(BaseNode):\n    \"\"\"\n    Brief one-line description of what this node does.\n\n    Extended description explaining the algorithm, when to use\n    this node, and key characteristics.\n\n    Parameters\n    ----------\n    param1 : type\n        Description with valid range and purpose.\n    param2 : type, optional\n        Description with default behavior. Default is value.\n\n    Attributes\n    ----------\n    requires_initial_fit : bool\n        Whether statistical initialization is required.\n    output_ports : Dict[str, Port]\n        Output port specifications.\n\n    See Also\n    --------\n    RelatedNode : Alternative approach\n    PreprocessingNode : Recommended preprocessing\n\n    Notes\n    -----\n    Implementation notes and algorithm details.\n\n    References\n    ----------\n    .. [1] Algorithm paper citation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; node = MyDetectorNode(param1=value)\n    &gt;&gt;&gt; result = node.forward(data)\n    \"\"\"\n</code></pre>"},{"location":"development/docstrings/#forward-methods","title":"forward() Methods","text":"<p>Standard pattern for forward method docstrings:</p> <pre><code>def forward(self, **inputs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process data through the node.\n\n    Parameters\n    ----------\n    **inputs : Any\n        Input data from connected ports. Expected keys:\n\n        - 'data' : torch.Tensor\n            Input data with shape ``(batch, channels, H, W)``.\n        - 'mask' : torch.Tensor, optional\n            Binary mask with shape ``(batch, 1, H, W)``.\n\n    Returns\n    -------\n    dict\n        Output data for connected ports:\n\n        - 'output' : torch.Tensor\n            Processed data with shape ``(batch, out_channels, H, W)``.\n        - 'scores' : torch.Tensor\n            Computed scores with shape ``(batch, H, W)``.\n\n    Raises\n    ------\n    ValueError\n        If required inputs are missing or have invalid shapes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = node.forward(data=input_tensor)\n    &gt;&gt;&gt; result['output'].shape\n    torch.Size([1, 10, 64, 64])\n    \"\"\"\n</code></pre>"},{"location":"development/docstrings/#see-also","title":"See Also","text":"<ul> <li>Numpy Docstring Guide - Official NumPy style guide</li> <li>Node Development - How to create new nodes</li> <li>Contributing Guidelines - Contribution workflow</li> <li>API Reference - Auto-generated API documentation</li> </ul>"},{"location":"development/docstrings/#related-pages","title":"Related Pages","text":"<ul> <li>Development Overview</li> <li>Contributing Guide</li> <li>Git Hooks</li> </ul>"},{"location":"development/documentation-guidelines/","title":"Documentation guidelines","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"development/documentation-guidelines/#cuv","title":"CUV","text":"<p>IS.AI Documentation Guidelines</p> <p>Created: 2026-02-05 Last Updated: 2026-02-05</p>"},{"location":"development/documentation-guidelines/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Documentation Philosophy</li> <li>Documentation Architecture</li> <li>Docstring Guidelines</li> <li>Maintaining Curated API Pages</li> <li>Adding New Modules</li> <li>Missing Anchor Fix Strategy</li> <li>Build and Verification</li> </ol>"},{"location":"development/documentation-guidelines/#documentation-philosophy","title":"Documentation Philosophy","text":""},{"location":"development/documentation-guidelines/#following-pytorchs-model","title":"Following PyTorch's Model","text":"<p>CUVIS.AI follows the PyTorch documentation model:</p> <p>\"Autosummary generates concise summary tables for modules, classes, and functions... making it easier for users to get an overview of the API. Autodoc generates a one pager documentation for all functions in a class which is often overwhelming and hard for users to read. In most cases, autosummary is a better way of organizing API documentation.\"</p> <p>\u2014 PyTorch Documentation Guidelines</p>"},{"location":"development/documentation-guidelines/#core-principles","title":"Core Principles","text":"<ol> <li>Hybrid Approach: Combine hand-written structure with auto-generated content</li> <li>User-Centered: Organize documentation for discoverability, not just completeness</li> <li>Single Source of Truth: Docstrings in code are the authoritative source</li> <li>Stay Current: Documentation auto-updates when code docstrings change</li> <li>Meaningful Organization: Group by functionality, not just alphabetically</li> </ol>"},{"location":"development/documentation-guidelines/#documentation-architecture","title":"Documentation Architecture","text":""},{"location":"development/documentation-guidelines/#two-layer-system","title":"Two-Layer System","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 1: Curated Pages (Manual Structure)                  \u2502\n\u2502  - docs/api/nodes.md                                        \u2502\n\u2502  - docs/api/training.md                                     \u2502\n\u2502  - Organized categories, context, navigation                \u2502\n\u2502  - Uses ::: directives to pull API content                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 2: Python Docstrings (Auto-Generated Content)        \u2502\n\u2502  - cuvis_ai/anomaly/rx_detector.py                          \u2502\n\u2502  - cuvis_ai/node/losses.py                                  \u2502\n\u2502  - Google-style docstrings                                  \u2502\n\u2502  - Pulled at build time via mkdocstrings                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/documentation-guidelines/#whats-manual-vs-auto-generated","title":"What's Manual vs Auto-Generated","text":"Aspect Manual Auto-Generated Organization \u2705 Category headers, navigation \u274c Context \u2705 Overview sections, explanations \u274c API Content \u274c \u2705 Pulled from docstrings Parameter Docs \u274c \u2705 Pulled from docstrings Examples \u274c \u2705 Pulled from docstrings Stays Current \u26a0\ufe0f When adding new modules \u2705 Automatically"},{"location":"development/documentation-guidelines/#docstring-guidelines","title":"Docstring Guidelines","text":""},{"location":"development/documentation-guidelines/#style-google-format","title":"Style: Google Format","text":"<p>CUVIS.AI uses Google-style docstrings (same as PyTorch).</p>"},{"location":"development/documentation-guidelines/#module-level-docstrings","title":"Module-Level Docstrings","text":"<p>Purpose: Provide overview, context, and references</p> <pre><code>\"\"\"RX anomaly detection nodes for hyperspectral imaging.\n\nThis module implements the Reed-Xiaoli (RX) anomaly detection algorithm, a widely used\nstatistical method for detecting anomalies in hyperspectral images. The RX algorithm\ncomputes squared Mahalanobis distance from the background distribution, treating\npixels with large distances as potential anomalies.\n\nThe module provides two variants:\n\n- **RXGlobal**: Uses global statistics (mean, covariance) estimated from training data.\n  Supports two-phase training: statistical initialization followed by optional gradient-based\n  fine-tuning via unfreeze().\n\n- **RXPerBatch**: Computes statistics independently for each batch on-the-fly without\n  requiring initialization. Useful for real-time processing or when training data is unavailable.\n\nExamples:\n    Basic usage with global statistics:\n\n    ```python\n    from cuvis_ai.anomaly.rx_detector import RXGlobal\n\n    detector = RXGlobal(\n        in_channels=224,\n        normalize=True,\n        epsilon=1e-6\n    )\n    ```\n\nReference:\n    Reed, I. S., &amp; Yu, X. (1990). \"Adaptive multiple-band CFAR detection of an optical\n    pattern with unknown spectral distribution.\" IEEE Transactions on Acoustics, Speech,\n    and Signal Processing, 38(10), 1760-1770.\n\"\"\"\n</code></pre> <p>Required Sections: - Brief description (1-2 sentences) - Detailed explanation - Available classes/functions overview - Examples (optional but recommended) - References (for research-based implementations)</p>"},{"location":"development/documentation-guidelines/#class-level-docstrings","title":"Class-Level Docstrings","text":"<pre><code>class RXGlobal(Node):\n    \"\"\"RX anomaly detector using global statistics.\n\n    Computes anomaly scores using the Reed-Xiaoli (RX) algorithm with global\n    mean and covariance estimated during training. Supports two-phase training\n    where statistical initialization is followed by optional gradient-based\n    fine-tuning.\n\n    Attributes:\n        in_channels: Number of input spectral channels\n        normalize: Whether to normalize anomaly scores\n        epsilon: Small constant for numerical stability\n\n    Examples:\n        Create and initialize detector:\n\n        ```python\n        detector = RXGlobal(in_channels=224)\n        detector.init(data_iterator)\n        ```\n    \"\"\"\n</code></pre>"},{"location":"development/documentation-guidelines/#methodfunction-docstrings","title":"Method/Function Docstrings","text":"<pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute RX anomaly scores.\n\n    Args:\n        x: Input tensor of shape (B, H, W, C) where:\n            - B: batch size\n            - H: height\n            - W: width\n            - C: channels (must equal in_channels)\n\n    Returns:\n        Anomaly score tensor of shape (B, H, W, 1). Higher scores\n        indicate greater likelihood of anomaly.\n\n    Raises:\n        ValueError: If input channels don't match in_channels.\n        RuntimeError: If detector hasn't been initialized.\n    \"\"\"\n</code></pre> <p>Required Sections: - Brief description - Args: All parameters with types and descriptions - Returns: Return value with type and meaning - Raises: Exceptions that can be raised (if applicable) - Examples: Usage examples (optional but recommended)</p>"},{"location":"development/documentation-guidelines/#docstring-best-practices","title":"Docstring Best Practices","text":"<ol> <li>Be Specific About Tensor Shapes: Use notation like <code>(B, H, W, C)</code> with legend</li> <li>Explain Units: If values have units (meters, seconds, etc.), specify them</li> <li>Link Related Components: Reference related classes/functions</li> <li>Provide Context: Explain when/why to use this over alternatives</li> <li>Include Equations: For algorithms, show key equations in LaTeX</li> <li>Add Warnings: Document gotchas, performance considerations, limitations</li> </ol>"},{"location":"development/documentation-guidelines/#maintaining-curated-api-pages","title":"Maintaining Curated API Pages","text":""},{"location":"development/documentation-guidelines/#structure-of-curated-pages","title":"Structure of Curated Pages","text":"<p>Curated pages (e.g., <code>docs/api/nodes.md</code>) have this structure:</p> <pre><code># Page Title\n\nBrief introduction explaining what this API section covers.\n\n## Category Name\n\nDescription of this category and when to use these components.\n\n### Component Name\n\n::: full.module.path\n    options:\n      show_root_heading: true\n      heading_level: 4\n</code></pre>"},{"location":"development/documentation-guidelines/#when-to-update-curated-pages","title":"When to Update Curated Pages","text":"<p>Update curated pages when:</p> <ol> <li>New Module Added: Add entry with <code>:::</code> directive</li> <li>Module Moved: Update import path in <code>:::</code> directive</li> <li>Category Changed: Move <code>:::</code> directive to new category section</li> <li>Module Deprecated: Add deprecation notice, consider removing</li> <li>New Category Needed: Add category header with description</li> </ol>"},{"location":"development/documentation-guidelines/#example-adding-a-new-module","title":"Example: Adding a New Module","text":"<p>Step 1: Create Python module with rich docstrings</p> <pre><code># cuvis_ai/anomaly/new_detector.py\n\"\"\"New anomaly detection method.\n\nThis module implements...\n\"\"\"\n</code></pre> <p>Step 2: Add to appropriate curated page</p> <pre><code>&lt;!-- docs/api/nodes.md --&gt;\n\n## Anomaly Detection Nodes\n\nStatistical and deep learning methods for detecting anomalies in hyperspectral data.\n\n### RX Detector\n::: cuvis_ai.anomaly.rx_detector\n    options:\n      show_root_heading: true\n      heading_level: 4\n\n### New Detector  \u2190 ADD THIS\n::: cuvis_ai.anomaly.new_detector\n    options:\n      show_root_heading: true\n      heading_level: 4\n</code></pre> <p>Step 3: Verify build</p> <pre><code>uv run mkdocs build --strict\n</code></pre>"},{"location":"development/documentation-guidelines/#adding-new-modules","title":"Adding New Modules","text":""},{"location":"development/documentation-guidelines/#checklist-for-new-modules","title":"Checklist for New Modules","text":"<p>When adding a new module to CUVIS.AI:</p> <ul> <li> Write module-level docstring with overview and examples</li> <li> Write class/function docstrings following Google style</li> <li> Add to appropriate curated page (<code>docs/api/nodes.md</code>, etc.)</li> <li> Place in correct category section</li> <li> Build docs locally to verify</li> <li> Check for broken cross-references</li> <li> Update related tutorial/guide if applicable</li> </ul>"},{"location":"development/documentation-guidelines/#choosing-the-right-curated-page","title":"Choosing the Right Curated Page","text":"Module Type Curated Page Example Node implementations <code>docs/api/nodes.md</code> RXDetector, DeepSVDD Training components <code>docs/api/training.md</code> Loss functions, metrics Data handling <code>docs/api/data.md</code> Datasets, data loaders Pipeline building <code>docs/api/pipeline.md</code> Graph, Pipeline Port definitions <code>docs/api/ports.md</code> PortSpec, StreamType Utilities <code>docs/api/utilities.md</code> Helpers, factories"},{"location":"development/documentation-guidelines/#missing-anchor-fix-strategy","title":"Missing Anchor Fix Strategy","text":""},{"location":"development/documentation-guidelines/#understanding-the-issue","title":"Understanding the Issue","text":"<p>MkDocs generates anchors from headings:</p> <ul> <li>Heading: <code>## Data Loading with LentilsAnomalyDataNode</code></li> <li>Anchor: <code>#data-loading-with-lentilsanomalydatanode</code></li> </ul> <p>Links break when: 1. Heading doesn't exist 2. Heading text doesn't match link 3. Heading uses unexpected formatting</p>"},{"location":"development/documentation-guidelines/#anchor-naming-rules","title":"Anchor Naming Rules","text":"<p>MkDocs transforms headings to anchors by:</p> <ol> <li>Converting to lowercase</li> <li>Replacing spaces with hyphens</li> <li>Removing special characters</li> <li>Removing multiple consecutive hyphens</li> </ol> <p>Examples:</p> Heading Anchor <code>## DeepSVDD Nodes</code> <code>#deepsvdd-nodes</code> <code>## Two-Phase Training Workflow</code> <code>#two-phase-training-workflow</code> <code>## Step 1: Data Loading</code> <code>#step-1-data-loading</code>"},{"location":"development/documentation-guidelines/#build-and-verification","title":"Build and Verification","text":""},{"location":"development/documentation-guidelines/#standard-build-command","title":"Standard Build Command","text":"<pre><code># Normal build (warnings displayed but not fatal)\nuv run mkdocs build\n\n# Strict build (warnings cause build failure)\nuv run mkdocs build --strict\n</code></pre>"},{"location":"development/documentation-guidelines/#expected-warnings","title":"Expected Warnings","text":"<p>After fixes, expect 14 warnings for external file references:</p> <pre><code>WARNING - Doc file contains a link '../../examples/grpc/...'\nWARNING - Doc file contains a link '../../configs/plugins/...'\n</code></pre> <p>These are acceptable - they reference legitimate source files outside <code>docs/</code>.</p>"},{"location":"development/documentation-guidelines/#verification-workflow","title":"Verification Workflow","text":"<ol> <li> <p>Baseline: Record current warning count    <pre><code>uv run mkdocs build --strict 2&gt;&amp;1 | grep \"WARNING\" | wc -l\n</code></pre></p> </li> <li> <p>After changes: Rebuild and compare    <pre><code>uv run mkdocs build --strict\n</code></pre></p> </li> <li> <p>Serve locally to visually verify:    <pre><code>uv run mkdocs serve\n# Open http://127.0.0.1:8000\n</code></pre></p> </li> </ol>"},{"location":"development/documentation-guidelines/#pre-commit-checklist","title":"Pre-Commit Checklist","text":"<p>Before committing documentation changes:</p> <ul> <li> <code>mkdocs build --strict</code> passes (or only expected warnings)</li> <li> Docstrings follow Google style</li> <li> New modules added to curated pages</li> <li> Internal links verified</li> <li> Examples tested (if code examples included)</li> <li> Spelling checked</li> </ul>"},{"location":"development/documentation-guidelines/#quick-reference","title":"Quick Reference","text":""},{"location":"development/documentation-guidelines/#docstring-template","title":"Docstring Template","text":"<pre><code>\"\"\"Brief one-line description.\n\nDetailed multi-paragraph explanation of what this does, when to use it,\nand how it fits into the larger system.\n\nArgs:\n    param1: Description with type info\n    param2: Description with type info\n\nReturns:\n    Description of return value with type\n\nRaises:\n    ErrorType: When this error occurs\n\nExamples:\n    Basic usage:\n\n    ```python\n    result = function(param1, param2)\n    ```\n\nSee Also:\n    - RelatedClass: For related functionality\n\"\"\"\n</code></pre>"},{"location":"development/documentation-guidelines/#curated-page-template","title":"Curated Page Template","text":"<pre><code># API Section Name\n\nBrief introduction to this API section.\n\n## Category 1\n\nDescription of what belongs in this category.\n\n### Component A\n::: module.path.component_a\n    options:\n      show_root_heading: true\n      heading_level: 4\n</code></pre> <p>For questions or suggestions, see: Contributing Guide</p>"},{"location":"development/git-hooks/","title":"Git Hooks","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"development/git-hooks/#git-hooks","title":"Git Hooks","text":"<p>This project uses Git hooks to enforce code quality standards before commits and pushes.</p>"},{"location":"development/git-hooks/#initial-setup-required","title":"Initial Setup (Required)","text":"<p>After cloning the repository, you must enable the hooks by running:</p> <pre><code>git config core.hooksPath .githooks\n</code></pre> <p>This tells Git to use the version-controlled hooks in <code>.githooks/</code> instead of the default <code>.git/hooks/</code> directory.</p> <p>Note: This is a one-time setup per repository clone. The hooks will then work automatically for all commits and pushes.</p>"},{"location":"development/git-hooks/#hooks-overview","title":"Hooks Overview","text":""},{"location":"development/git-hooks/#pre-commit-hook","title":"Pre-commit Hook","text":"<p>Purpose: Fast quality checks on every commit Actions: - Runs <code>uv run ruff format .</code> (formats code first) - Runs <code>uv run ruff check . --fix</code> (auto-fixes linting issues on formatted code) - Auto-stages formatted files</p> <p>Performance: Fast (~few seconds), skips test execution</p>"},{"location":"development/git-hooks/#pre-push-hook","title":"Pre-push Hook","text":"<p>Purpose: Comprehensive validation before pushing to remote Actions: - Runs <code>uv run ruff format .</code> (formatting first) - Runs <code>uv run ruff check . --fix</code> (linting on formatted code) - Runs <code>interrogate -v cuvis_ai/ --fail-under 95</code> (docstring coverage check) - Runs <code>uv run pytest tests/ -v --tb=line -m \"not gpu\"</code> (all non-GPU tests)</p> <p>Performance: Slower (~depends on test suite), ensures code quality and documentation</p>"},{"location":"development/git-hooks/#usage","title":"Usage","text":""},{"location":"development/git-hooks/#normal-workflow","title":"Normal Workflow","text":"<p>The hooks run automatically:</p> <pre><code># Commit triggers pre-commit hook\ngit commit -m \"your message\"\n\n# Push triggers pre-push hook\ngit push\n</code></pre> <p>If any check fails, the commit/push will be blocked with a clear error message.</p>"},{"location":"development/git-hooks/#bypass-hooks-emergency-only","title":"Bypass Hooks (Emergency Only)","text":"<p>Use <code>--no-verify</code> flag to skip hooks when absolutely necessary:</p> <pre><code># Skip pre-commit checks\ngit commit --no-verify -m \"emergency fix\"\n\n# Skip pre-push checks\ngit push --no-verify\n</code></pre> <p>Warning: Only use <code>--no-verify</code> in emergencies. The CI pipeline will still enforce these checks.</p>"},{"location":"development/git-hooks/#what-gets-checked","title":"What Gets Checked","text":""},{"location":"development/git-hooks/#linting-ruff","title":"Linting (Ruff)","text":"<ul> <li>Code style compliance (E, F, W, I, B, UP, C4 rules)</li> <li>Import sorting</li> <li>Type annotation requirements (for public functions)</li> <li>Configured in <code>pyproject.toml</code> under <code>[tool.ruff]</code></li> </ul>"},{"location":"development/git-hooks/#formatting-ruff","title":"Formatting (Ruff)","text":"<ul> <li>Consistent code formatting</li> <li>Line length: 100 characters</li> <li>Double quotes for strings</li> <li>4-space indentation</li> </ul>"},{"location":"development/git-hooks/#docstring-coverage-interrogate","title":"Docstring Coverage (interrogate)","text":"<ul> <li>Checks all modules in <code>cuvis_ai/</code> package</li> <li>Requires \u226595% docstring coverage</li> <li>Blocks push if coverage is below threshold</li> <li>Only runs on pre-push (not pre-commit)</li> </ul>"},{"location":"development/git-hooks/#testing-pytest","title":"Testing (pytest)","text":"<ul> <li>All tests in <code>tests/</code> directory</li> <li>Excludes GPU-marked tests (<code>-m \"not gpu\"</code>)</li> <li>Verbose output with line-level tracebacks</li> <li>Only runs on pre-push (not pre-commit)</li> </ul>"},{"location":"development/git-hooks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/git-hooks/#hook-not-running","title":"Hook Not Running","text":"<p>If hooks aren't running, verify they exist and are executable:</p> <pre><code># List hooks\nls -la .git/hooks/\n\n# On Windows with Git Bash\nls -la .git/hooks/pre-commit .git/hooks/pre-push\n</code></pre>"},{"location":"development/git-hooks/#ruff-errors","title":"Ruff Errors","text":"<p>If Ruff reports errors that can't be auto-fixed: 1. Read the error message carefully 2. Fix the issue manually 3. Stage the changes 4. Try committing again</p>"},{"location":"development/git-hooks/#test-failures","title":"Test Failures","text":"<p>If tests fail during pre-push: 1. Run tests locally: <code>uv run pytest tests/ -v --tb=line -m \"not gpu\"</code> 2. Fix the failing tests 3. Ensure all tests pass locally 4. Try pushing again</p>"},{"location":"development/git-hooks/#performance-issues","title":"Performance Issues","text":"<p>If pre-push is too slow: - Consider using <code>--no-verify</code> for WIP branches (but fix before final merge) - Or temporarily disable GPU tests marker in your local environment - Remember: CI will run full test suite anyway</p>"},{"location":"development/git-hooks/#maintenance","title":"Maintenance","text":""},{"location":"development/git-hooks/#updating-hook-scripts","title":"Updating Hook Scripts","text":"<p>Hooks are located in <code>.githooks/</code>: - <code>.githooks/pre-commit</code> - Pre-commit checks (formatting + linting) - <code>.githooks/pre-push</code> - Pre-push checks (formatting + linting + docstrings + tests)</p> <p>After modifying hooks, they take effect immediately (no installation needed). Ensure hooks are executable with <code>chmod +x .githooks/*</code>.</p>"},{"location":"development/git-hooks/#disabling-hooks-permanently-not-recommended","title":"Disabling Hooks Permanently (Not Recommended)","text":"<p>To disable hooks project-wide:</p> <pre><code># Remove or rename the hooks\nmv .git/hooks/pre-commit .git/hooks/pre-commit.disabled\nmv .git/hooks/pre-push .git/hooks/pre-push.disabled\n</code></pre> <p>Note: This is strongly discouraged. Hooks exist to catch issues early.</p>"},{"location":"grpc/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"grpc/#grpc-services","title":"gRPC Services","text":"<p>CUVIS.AI provides gRPC services for remote pipeline execution and deployment scenarios.</p>"},{"location":"grpc/#grpc-documentation","title":"gRPC Documentation","text":"<ul> <li> <p> gRPC Overview</p> <p>Introduction, architecture, and quick start guide</p> </li> <li> <p> gRPC API Reference</p> <p>Complete reference for all 46 RPC methods</p> </li> <li> <p>:material-pattern: Client Patterns</p> <p>Best practices and common usage patterns</p> </li> <li> <p> Sequence Diagrams</p> <p>Visual workflows for all major operations</p> </li> <li> <p> Deployment</p> <p>Deploy gRPC services in production</p> </li> </ul> <p>Related Pages: - gRPC Workflow Tutorial - Remote gRPC How-To</p>"},{"location":"grpc/api-reference/","title":"API Reference","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"grpc/api-reference/#grpc-api-reference","title":"gRPC API Reference","text":"<p>Complete reference documentation for the CuvisAIService with 46 RPC methods.</p>"},{"location":"grpc/api-reference/#overview","title":"Overview","text":"<p>The CUVIS.AI gRPC service exposes all functionality through a single <code>CuvisAIService</code> endpoint with 46 RPC methods organized into 6 functional categories:</p> Category Methods Purpose Session Management 3 Create, configure, and close isolated execution contexts Configuration Management 4 Resolve, validate, and apply Hydra configurations Pipeline Management 5 Load, save, and manage pipeline state Training Operations 3 Execute statistical and gradient-based training Inference Operations 1 Run predictions on trained pipelines Introspection &amp; Discovery 6 Query capabilities, inspect pipelines, visualize graphs <p>Protocol Buffers: All methods use Protocol Buffers (protobuf) for serialization.</p> <p>Service Definition: <code>CuvisAIService</code> in <code>cuvis_ai_core.proto</code></p>"},{"location":"grpc/api-reference/#connection-setup","title":"Connection &amp; Setup","text":""},{"location":"grpc/api-reference/#creating-a-grpc-stub","title":"Creating a gRPC Stub","text":"<pre><code>from cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\nimport grpc\n\n# Configure message size limits for hyperspectral data\noptions = [\n    (\"grpc.max_send_message_length\", 300 * 1024 * 1024),  # 300 MB\n    (\"grpc.max_receive_message_length\", 300 * 1024 * 1024),\n]\n\n# Create channel and stub\nchannel = grpc.insecure_channel(\"localhost:50051\", options=options)\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n</code></pre> <p>For production, use secure channels with TLS: <pre><code>credentials = grpc.ssl_channel_credentials()\nchannel = grpc.secure_channel(\"production-server:50051\", credentials, options=options)\n</code></pre></p>"},{"location":"grpc/api-reference/#helper-utilities","title":"Helper Utilities","text":"<p>The <code>examples/grpc/workflow_utils.py</code> module provides convenience functions that simplify common operations:</p> <pre><code>from examples.grpc.workflow_utils import (\n    build_stub,                         # Create configured stub\n    config_search_paths,                # Build Hydra search paths\n    create_session_with_search_paths,   # Session + search paths\n    resolve_trainrun_config,            # Resolve config with Hydra\n    apply_trainrun_config,              # Apply resolved config\n    format_progress,                    # Pretty-print training progress\n)\n\n# Quick connection\nstub = build_stub(\"localhost:50051\", max_msg_size=600*1024*1024)\n\n# Quick session setup\nsession_id = create_session_with_search_paths(stub)\n</code></pre>"},{"location":"grpc/api-reference/#session-management","title":"Session Management","text":"<p>Sessions provide isolated execution contexts for each client. Each session has independent pipeline state, training configuration, and resources.</p>"},{"location":"grpc/api-reference/#createsession","title":"CreateSession","text":"<p>Purpose: Initialize a new isolated session context.</p> <p>Request: <pre><code>message CreateSessionRequest {}\n</code></pre></p> <p>Response: <pre><code>message CreateSessionResponse {\n  string session_id = 1;  // Unique session identifier (UUID)\n}\n</code></pre></p> <p>Python Example: <pre><code>response = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\nsession_id = response.session_id\nprint(f\"Session created: {session_id}\")\n</code></pre></p> <p>Notes: - Sessions are isolated: separate pipeline, weights, data configs - Sessions automatically expire after 1 hour of inactivity - Each session consumes GPU/CPU memory until closed - Session IDs are UUIDs (e.g., <code>\"7f3e4d2c-1a9b-4c8d-9e7f-2b5a6c8d9e0f\"</code>)</p> <p>See Also: - SetSessionSearchPaths - Configure Hydra search paths - CloseSession - Release resources</p>"},{"location":"grpc/api-reference/#setsessionsearchpaths","title":"SetSessionSearchPaths","text":"<p>Purpose: Register Hydra configuration search paths for the session.</p> <p>Request: <pre><code>message SetSessionSearchPathsRequest {\n  string session_id = 1;\n  repeated string search_paths = 2;  // Absolute paths\n  bool append = 3;                   // false = replace, true = append\n}\n</code></pre></p> <p>Response: <pre><code>message SetSessionSearchPathsResponse {\n  bool success = 1;\n}\n</code></pre></p> <p>Python Example: <pre><code>from pathlib import Path\n\n# Build search paths (typical pattern)\nconfig_root = Path(__file__).parent.parent / \"configs\"\nsearch_paths = [\n    str(config_root),\n    str(config_root / \"trainrun\"),\n    str(config_root / \"pipeline\"),\n    str(config_root / \"data\"),\n    str(config_root / \"training\"),\n]\n\n# Register search paths\nstub.SetSessionSearchPaths(\n    cuvis_ai_pb2.SetSessionSearchPathsRequest(\n        session_id=session_id,\n        search_paths=search_paths,\n        append=False,  # Replace any existing paths\n    )\n)\n</code></pre></p> <p>Notes: - Must be called before <code>ResolveConfig</code> for Hydra composition to work - Paths must be absolute paths (not relative) - Use <code>append=False</code> (default) to replace existing paths - Use <code>append=True</code> to add paths to existing list - Common pattern: call immediately after <code>CreateSession</code></p> <p>Helper Function: <pre><code>from examples.grpc.workflow_utils import config_search_paths, create_session_with_search_paths\n\n# Get standard search paths\npaths = config_search_paths(extra_paths=[\"/custom/configs\"])\n\n# Create session with search paths in one call\nsession_id = create_session_with_search_paths(stub, search_paths=paths)\n</code></pre></p> <p>See Also: - ResolveConfig - Resolve configs using these paths - Hydra Composition Guide</p>"},{"location":"grpc/api-reference/#closesession","title":"CloseSession","text":"<p>Purpose: Release all resources associated with a session.</p> <p>Request: <pre><code>message CloseSessionRequest {\n  string session_id = 1;\n}\n</code></pre></p> <p>Response: <pre><code>message CloseSessionResponse {\n  bool success = 1;\n}\n</code></pre></p> <p>Python Example: <pre><code># Always close sessions when done\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\nprint(\"Session closed, resources freed\")\n</code></pre></p> <p>Best Practice - Use try/finally: <pre><code>session_id = None\ntry:\n    session_id = stub.CreateSession(...).session_id\n    # ... training or inference ...\nfinally:\n    if session_id:\n        stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre></p> <p>Notes: - Always close sessions to free GPU/CPU memory immediately - Unclosed sessions expire after 1 hour but hold resources until then - Closing releases: pipeline, weights, data loaders, CUDA memory - Safe to call even if session doesn't exist (returns success=false) - No error if session already expired</p> <p>See Also: - Client Patterns: Session Management</p>"},{"location":"grpc/api-reference/#configuration-management","title":"Configuration Management","text":"<p>The configuration service integrates with Hydra for powerful config composition, validation, and dynamic overrides.</p>"},{"location":"grpc/api-reference/#resolveconfig","title":"ResolveConfig","text":"<p>Purpose: Resolve configuration using Hydra composition with optional overrides.</p> <p>Request: <pre><code>message ResolveConfigRequest {\n  string session_id = 1;\n  string config_type = 2;        // \"trainrun\", \"pipeline\", \"training\", \"data\"\n  string path = 3;                // Relative path in search paths\n  repeated string overrides = 4;  // Hydra override syntax\n}\n</code></pre></p> <p>Response: <pre><code>message ResolveConfigResponse {\n  bytes config_bytes = 1;    // JSON-serialized config\n  string resolved_path = 2;  // Full path that was resolved\n}\n</code></pre></p> <p>Python Example: <pre><code>import json\n\n# Resolve trainrun config with overrides\nresponse = stub.ResolveConfig(\n    cuvis_ai_pb2.ResolveConfigRequest(\n        session_id=session_id,\n        config_type=\"trainrun\",\n        path=\"trainrun/deep_svdd\",  # Or just \"deep_svdd\"\n        overrides=[\n            \"training.trainer.max_epochs=50\",\n            \"training.optimizer.lr=0.0005\",\n            \"data.batch_size=8\",\n        ],\n    )\n)\n\n# Parse returned JSON\nconfig_dict = json.loads(response.config_bytes.decode(\"utf-8\"))\nprint(f\"Resolved config: {config_dict['name']}\")\nprint(f\"Pipeline: {config_dict['pipeline']['name']}\")\n</code></pre></p> <p>Config Types: - <code>\"trainrun\"</code> - Complete training run composition (pipeline + data + training) - <code>\"pipeline\"</code> - Pipeline-only configuration - <code>\"training\"</code> - Training parameters (optimizer, scheduler, trainer) - <code>\"data\"</code> - Data loading configuration</p> <p>Override Patterns: <pre><code>overrides = [\n    # Training parameters\n    \"training.trainer.max_epochs=100\",\n    \"training.trainer.accelerator=gpu\",\n    \"training.optimizer.lr=0.001\",\n    \"training.optimizer.weight_decay=0.01\",\n    \"training.scheduler.mode=min\",\n    \"training.scheduler.patience=10\",\n\n    # Data parameters\n    \"data.batch_size=16\",\n    \"data.train_ids=[0,1,2]\",\n    \"data.val_ids=[3,4]\",\n    \"data.cu3s_file_path=/data/Lentils_000.cu3s\",\n\n    # Pipeline node parameters\n    \"pipeline.nodes.channel_selector.params.tau_start=8.0\",\n    \"pipeline.nodes.rx_detector.params.eps=1e-6\",\n    \"pipeline.nodes.normalizer.params.use_running_stats=true\",\n]\n</code></pre></p> <p>Notes: - Requires <code>SetSessionSearchPaths</code> to be called first - Returns JSON bytes (decode with <code>.decode(\"utf-8\")</code> and parse with <code>json.loads()</code>) - Hydra resolves config group composition, interpolations, and overrides - Override syntax follows Hydra conventions (dot notation for nested fields)</p> <p>Helper Function: <pre><code>from examples.grpc.workflow_utils import resolve_trainrun_config\n\n# Resolve trainrun config (returns response + parsed dict)\nresolved, config_dict = resolve_trainrun_config(\n    stub,\n    session_id,\n    \"deep_svdd\",\n    overrides=[\"training.trainer.max_epochs=10\"],\n)\n</code></pre></p> <p>See Also: - SetTrainRunConfig - Apply resolved config - ValidateConfig - Pre-validate configs - Hydra Composition Guide - TrainRun Schema</p>"},{"location":"grpc/api-reference/#settrainrunconfig","title":"SetTrainRunConfig","text":"<p>Purpose: Apply resolved trainrun configuration to session (builds pipeline, sets data/training configs).</p> <p>Request: <pre><code>message SetTrainRunConfigRequest {\n  string session_id = 1;\n  TrainRunConfig config = 2;\n}\n\nmessage TrainRunConfig {\n  bytes config_bytes = 1;  // JSON from ResolveConfig\n}\n</code></pre></p> <p>Response: <pre><code>message SetTrainRunConfigResponse {\n  bool success = 1;\n}\n</code></pre></p> <p>Python Example: <pre><code># First resolve config\nresolved = stub.ResolveConfig(\n    cuvis_ai_pb2.ResolveConfigRequest(\n        session_id=session_id,\n        config_type=\"trainrun\",\n        path=\"trainrun/rx_statistical\",\n    )\n)\n\n# Apply to session\nstub.SetTrainRunConfig(\n    cuvis_ai_pb2.SetTrainRunConfigRequest(\n        session_id=session_id,\n        config=cuvis_ai_pb2.TrainRunConfig(config_bytes=resolved.config_bytes),\n    )\n)\nprint(\"TrainRun config applied, pipeline built\")\n</code></pre></p> <p>What This Does: 1. Parses trainrun configuration JSON 2. Builds pipeline from pipeline config 3. Initializes data loader from data config 4. Sets training parameters (optimizer, scheduler, trainer) 5. Prepares session for training or inference</p> <p>Notes: - Must be called after <code>ResolveConfig</code> - Replaces any existing pipeline/config in session - After this call, session is ready for <code>Train()</code> or <code>Inference()</code> - Validates config structure (raises error if malformed)</p> <p>Helper Function: <pre><code>from examples.grpc.workflow_utils import apply_trainrun_config\n\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n</code></pre></p> <p>See Also: - ResolveConfig - Resolve config first - Train - Execute training after config applied</p>"},{"location":"grpc/api-reference/#validateconfig","title":"ValidateConfig","text":"<p>Purpose: Pre-validate configuration before applying (catch errors early).</p> <p>Request: <pre><code>message ValidateConfigRequest {\n  string config_type = 1;  // \"training\", \"pipeline\", \"data\", etc.\n  bytes config_bytes = 2;   // JSON configuration\n}\n</code></pre></p> <p>Response: <pre><code>message ValidateConfigResponse {\n  bool valid = 1;\n  repeated string errors = 2;     // Fatal errors\n  repeated string warnings = 3;   // Non-fatal warnings\n}\n</code></pre></p> <p>Python Example: <pre><code>import json\n\n# Validate training config before use\ntraining_config = {\n    \"trainer\": {\"max_epochs\": 10, \"accelerator\": \"gpu\"},\n    \"optimizer\": {\"name\": \"adam\", \"lr\": 0.001},\n}\n\nvalidation = stub.ValidateConfig(\n    cuvis_ai_pb2.ValidateConfigRequest(\n        config_type=\"training\",\n        config_bytes=json.dumps(training_config).encode(\"utf-8\"),\n    )\n)\n\nif not validation.valid:\n    print(\"Configuration validation failed:\")\n    for error in validation.errors:\n        print(f\"  ERROR: {error}\")\n    raise ValueError(\"Invalid configuration\")\n\nfor warning in validation.warnings:\n    print(f\"  WARNING: {warning}\")\n</code></pre></p> <p>Config Types: - <code>\"training\"</code> - Training parameters (optimizer, scheduler, trainer) - <code>\"pipeline\"</code> - Pipeline structure and node configs - <code>\"data\"</code> - Data loading configuration - <code>\"trainrun\"</code> - Complete trainrun composition</p> <p>Common Validation Errors: - Missing required fields (e.g., <code>optimizer.lr</code>) - Invalid values (e.g., negative <code>max_epochs</code>) - Type mismatches (e.g., string for numeric field) - Unknown optimizer/scheduler names - Incompatible node connections in pipeline</p> <p>Notes: - Validation is optional but highly recommended - Catches configuration errors before starting training - Warnings are informational (config still valid) - Errors mean config is invalid and will fail if applied</p> <p>See Also: - SetTrainRunConfig - Apply config after validation - TrainRun Schema</p>"},{"location":"grpc/api-reference/#gettrainingcapabilities","title":"GetTrainingCapabilities","text":"<p>Purpose: Discover supported optimizers, schedulers, callbacks, and their parameter schemas.</p> <p>Request: <pre><code>message GetTrainingCapabilitiesRequest {}\n</code></pre></p> <p>Response: <pre><code>message GetTrainingCapabilitiesResponse {\n  repeated string optimizer_names = 1;     // e.g., [\"adam\", \"sgd\", \"adamw\"]\n  repeated string scheduler_names = 2;     // e.g., [\"step_lr\", \"reduce_on_plateau\"]\n  repeated string callback_names = 3;      // e.g., [\"early_stopping\", \"model_checkpoint\"]\n  map&lt;string, ParameterSchema&gt; optimizer_schemas = 4;\n  map&lt;string, ParameterSchema&gt; scheduler_schemas = 5;\n}\n</code></pre></p> <p>Python Example: <pre><code>capabilities = stub.GetTrainingCapabilities(\n    cuvis_ai_pb2.GetTrainingCapabilitiesRequest()\n)\n\nprint(\"Available optimizers:\", capabilities.optimizer_names)\nprint(\"Available schedulers:\", capabilities.scheduler_names)\nprint(\"Available callbacks:\", capabilities.callback_names)\n\n# Get parameter schema for Adam optimizer\nadam_schema = capabilities.optimizer_schemas[\"adam\"]\nprint(f\"Adam parameters: {adam_schema}\")\n</code></pre></p> <p>Use Cases: - Dynamic UI generation (list available options) - Config validation (check if optimizer exists) - Documentation generation - Discovery for programmatic workflows</p> <p>See Also: - ValidateConfig - Validate configs using these capabilities</p>"},{"location":"grpc/api-reference/#pipeline-management","title":"Pipeline Management","text":"<p>Manage pipeline loading, saving, and restoration.</p>"},{"location":"grpc/api-reference/#loadpipeline","title":"LoadPipeline","text":"<p>Purpose: Build pipeline from YAML configuration.</p> <p>Request: <pre><code>message LoadPipelineRequest {\n  string session_id = 1;\n  PipelineConfig pipeline = 2;\n}\n\nmessage PipelineConfig {\n  bytes config_bytes = 1;  // YAML or JSON serialized\n}\n</code></pre></p> <p>Response: <pre><code>message LoadPipelineResponse {\n  bool success = 1;\n}\n</code></pre></p> <p>Python Example: <pre><code>from pathlib import Path\nimport yaml\nimport json\n\n# Load pipeline config from YAML file\npipeline_yaml = yaml.safe_load(Path(\"pipeline.yaml\").read_text())\npipeline_json = json.dumps(pipeline_yaml).encode(\"utf-8\")\n\nstub.LoadPipeline(\n    cuvis_ai_pb2.LoadPipelineRequest(\n        session_id=session_id,\n        pipeline=cuvis_ai_pb2.PipelineConfig(config_bytes=pipeline_json),\n    )\n)\nprint(\"Pipeline loaded from config\")\n</code></pre></p> <p>Notes: - Pipeline config can be YAML or JSON (server parses both) - Builds complete pipeline graph from node definitions and connections - Does NOT load weights (use <code>LoadPipelineWeights</code> separately) - After loading, pipeline is ready for training or inference - See Pipeline Schema for config format</p> <p>See Also: - LoadPipelineWeights - Load trained weights - SavePipeline - Save pipeline config + weights - Pipeline Schema</p>"},{"location":"grpc/api-reference/#loadpipelineweights","title":"LoadPipelineWeights","text":"<p>Purpose: Load trained weights into pipeline from checkpoint file.</p> <p>Request: <pre><code>message LoadPipelineWeightsRequest {\n  string session_id = 1;\n  string weights_path = 2;  // Path to .pt file\n  bool strict = 3;          // Require exact match (default: true)\n}\n</code></pre></p> <p>Response: <pre><code>message LoadPipelineWeightsResponse {\n  bool success = 1;\n  repeated string missing_keys = 2;    // Keys in config but not in weights\n  repeated string unexpected_keys = 3; // Keys in weights but not in config\n}\n</code></pre></p> <p>Python Example: <pre><code># Load pipeline first, then weights\nstub.LoadPipeline(...)  # See LoadPipeline example\n\nresponse = stub.LoadPipelineWeights(\n    cuvis_ai_pb2.LoadPipelineWeightsRequest(\n        session_id=session_id,\n        weights_path=\"outputs/deep_svdd.pt\",\n        strict=True,  # Fail if weights don't match exactly\n    )\n)\n\nif response.missing_keys:\n    print(f\"Warning: Missing keys: {response.missing_keys}\")\nif response.unexpected_keys:\n    print(f\"Warning: Unexpected keys: {response.unexpected_keys}\")\n</code></pre></p> <p>Strict vs Non-Strict Loading: - <code>strict=True</code> (default): Fails if weights don't match pipeline exactly - <code>strict=False</code>: Loads matching weights, ignores mismatches (useful for transfer learning)</p> <p>Notes: - Must call <code>LoadPipeline</code> first to build pipeline structure - Weights file is PyTorch <code>.pt</code> checkpoint - Use <code>strict=False</code> for transfer learning or partial weight loading - Check <code>missing_keys</code> and <code>unexpected_keys</code> for debugging</p> <p>See Also: - LoadPipeline - Load pipeline config first - SavePipeline - Save weights</p>"},{"location":"grpc/api-reference/#savepipeline","title":"SavePipeline","text":"<p>Purpose: Save pipeline configuration and weights to files.</p> <p>Request: <pre><code>message SavePipelineRequest {\n  string session_id = 1;\n  string pipeline_path = 2;        // Path for YAML config\n  PipelineMetadata metadata = 3;   // Optional metadata\n}\n\nmessage PipelineMetadata {\n  string name = 1;\n  string description = 2;\n  repeated string tags = 3;\n  string author = 4;\n}\n</code></pre></p> <p>Response: <pre><code>message SavePipelineResponse {\n  string pipeline_path = 1;  // Saved YAML config path\n  string weights_path = 2;   // Saved .pt weights path\n}\n</code></pre></p> <p>Python Example: <pre><code># Save pipeline after training\nresponse = stub.SavePipeline(\n    cuvis_ai_pb2.SavePipelineRequest(\n        session_id=session_id,\n        pipeline_path=\"outputs/my_pipeline.yaml\",\n        metadata=cuvis_ai_pb2.PipelineMetadata(\n            name=\"Deep SVDD Anomaly Detector\",\n            description=\"Trained on Lentils dataset with 50 epochs\",\n            tags=[\"anomaly_detection\", \"deep_svdd\", \"production\"],\n            author=\"your_name\",\n        ),\n    )\n)\n\nprint(f\"Pipeline saved to: {response.pipeline_path}\")\nprint(f\"Weights saved to: {response.weights_path}\")\n</code></pre></p> <p>What Gets Saved: 1. YAML config - Complete pipeline structure and node parameters 2. Weights file (.pt) - PyTorch checkpoint with trained weights 3. Metadata - Optional name, description, tags, author</p> <p>Notes: - Automatically creates weights path by replacing <code>.yaml</code> with <code>.pt</code> - Metadata is embedded in YAML config file - Use this for inference-only deployment (no training state) - For full reproducibility, use <code>SaveTrainRun</code> instead</p> <p>See Also: - LoadPipeline + LoadPipelineWeights - Restore pipeline - SaveTrainRun - Save complete trainrun (includes data/training config)</p>"},{"location":"grpc/api-reference/#savetrainrun","title":"SaveTrainRun","text":"<p>Purpose: Save complete trainrun configuration (pipeline + data + training configs).</p> <p>Request: <pre><code>message SaveTrainRunRequest {\n  string session_id = 1;\n  string trainrun_path = 2;  // Path for trainrun YAML\n  bool save_weights = 3;     // Include weights (default: true)\n}\n</code></pre></p> <p>Response: <pre><code>message SaveTrainRunResponse {\n  string trainrun_path = 1;  // Saved trainrun config\n  string weights_path = 2;   // Saved weights (if save_weights=true)\n}\n</code></pre></p> <p>Python Example: <pre><code># Save complete trainrun after training\nresponse = stub.SaveTrainRun(\n    cuvis_ai_pb2.SaveTrainRunRequest(\n        session_id=session_id,\n        trainrun_path=\"outputs/deep_svdd_run.yaml\",\n        save_weights=True,\n    )\n)\n\nprint(f\"TrainRun saved to: {response.trainrun_path}\")\nprint(f\"Weights saved to: {response.weights_path}\")\n</code></pre></p> <p>What Gets Saved: - Pipeline config - Complete pipeline structure - Data config - Data loading configuration (paths, train/val/test splits, batch size) - Training config - Optimizer, scheduler, trainer parameters - Weights (optional) - Trained model weights</p> <p>TrainRun vs Pipeline: | Feature | SavePipeline | SaveTrainRun | |---------|--------------|--------------| | Pipeline config | \u2705 | \u2705 | | Weights | \u2705 | \u2705 | | Data config | \u274c | \u2705 | | Training config | \u274c | \u2705 | | Use for | Inference deployment | Reproducibility, resume training |</p> <p>Notes: - Use <code>SaveTrainRun</code> for full reproducibility (can resume training later) - Use <code>SavePipeline</code> for inference-only deployment (smaller, no training overhead) - Trainrun can be restored with <code>RestoreTrainRun</code></p> <p>See Also: - RestoreTrainRun - Restore complete trainrun - SavePipeline - Save pipeline only</p>"},{"location":"grpc/api-reference/#restoretrainrun","title":"RestoreTrainRun","text":"<p>Purpose: Restore complete training run (pipeline + data + training + weights).</p> <p>Request: <pre><code>message RestoreTrainRunRequest {\n  string trainrun_path = 1;  // Path to trainrun YAML\n  string weights_path = 2;   // Optional custom weights path\n  bool strict = 3;           // Strict weight loading (default: true)\n}\n</code></pre></p> <p>Response: <pre><code>message RestoreTrainRunResponse {\n  string session_id = 1;  // NEW session created automatically\n  bool success = 2;\n}\n</code></pre></p> <p>Python Example: <pre><code># Restore trainrun (creates new session automatically)\nresponse = stub.RestoreTrainRun(\n    cuvis_ai_pb2.RestoreTrainRunRequest(\n        trainrun_path=\"outputs/deep_svdd_run.yaml\",\n        weights_path=\"outputs/deep_svdd_run.pt\",  # Optional override\n        strict=True,\n    )\n)\n\nsession_id = response.session_id\nprint(f\"TrainRun restored in session: {session_id}\")\n\n# Now ready for inference or continued training\ninference_response = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(session_id=session_id, inputs=...)\n)\n</code></pre></p> <p>Key Feature: Automatic Session Creation - <code>RestoreTrainRun</code> creates a new session automatically - You don't need to call <code>CreateSession</code> first - Returns the new <code>session_id</code> in response - Session has pipeline + weights + configs fully loaded</p> <p>Notes: - Most convenient way to restore trained models - If <code>weights_path</code> not specified, looks for <code>.pt</code> file next to <code>.yaml</code> - Use <code>strict=False</code> for partial weight loading - Remember to <code>CloseSession</code> when done</p> <p>See Also: - SaveTrainRun - Save trainrun for restoration - Restore Pipeline Guide</p>"},{"location":"grpc/api-reference/#training-operations","title":"Training Operations","text":"<p>Execute statistical and gradient-based training with streaming progress updates.</p>"},{"location":"grpc/api-reference/#train","title":"Train","text":"<p>Purpose: Execute training with streaming progress updates (statistical or gradient).</p> <p>Request: <pre><code>message TrainRequest {\n  string session_id = 1;\n  TrainerType trainer_type = 2;  // STATISTICAL or GRADIENT\n}\n\nenum TrainerType {\n  TRAINER_TYPE_STATISTICAL = 0;\n  TRAINER_TYPE_GRADIENT = 1;\n}\n</code></pre></p> <p>Response (Server-Side Streaming): <pre><code>message TrainResponse {\n  ExecutionContext context = 1;      // Epoch, step, stage info\n  TrainStatus status = 2;            // RUNNING, COMPLETED, FAILED\n  map&lt;string, float&gt; losses = 3;     // Loss values\n  map&lt;string, float&gt; metrics = 4;    // Metric values\n  string message = 5;                // Progress message\n}\n</code></pre></p> <p>Python Example - Statistical Training: <pre><code># Phase 1: Statistical initialization (short, no backprop)\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n    print(f\"[{stage}] {status}: {progress.message}\")\n</code></pre></p> <p>Python Example - Gradient Training: <pre><code># Phase 2: Gradient-based training (full backprop)\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n\n    if progress.losses:\n        print(f\"[{stage}] Epoch {progress.context.epoch} | losses={dict(progress.losses)}\")\n    if progress.metrics:\n        print(f\"[{stage}] Epoch {progress.context.epoch} | metrics={dict(progress.metrics)}\")\n</code></pre></p> <p>Two-Phase Training Pattern: <pre><code># 1. Statistical initialization (RX, normalization stats, etc.)\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    print(f\"[statistical] {format_progress(progress)}\")\n\n# 2. Gradient-based fine-tuning (deep learning)\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    print(f\"[gradient] {format_progress(progress)}\")\n</code></pre></p> <p>Execution Stages: - <code>EXECUTION_STAGE_TRAIN</code> - Training loop - <code>EXECUTION_STAGE_VALIDATE</code> - Validation loop - <code>EXECUTION_STAGE_TEST</code> - Test loop</p> <p>Train Status: - <code>TRAIN_STATUS_RUNNING</code> - Training in progress - <code>TRAIN_STATUS_COMPLETED</code> - Training finished successfully - <code>TRAIN_STATUS_FAILED</code> - Training failed with error</p> <p>Helper Function: <pre><code>from examples.grpc.workflow_utils import format_progress\n\nfor progress in stub.Train(...):\n    print(format_progress(progress))\n# Output: \"[TRAIN] RUNNING | losses={'total': 0.42} | metrics={'iou': 0.85}\"\n</code></pre></p> <p>Notes: - Training is server-side streaming (client receives updates as they occur) - No polling required (updates pushed in real-time) - Process stream with for-loop (blocks until training completes) - Statistical training is typically fast (1-2 passes over data) - Gradient training duration depends on <code>max_epochs</code> in training config</p> <p>See Also: - GetTrainStatus - Query training status - Sequence Diagrams</p>"},{"location":"grpc/api-reference/#gettrainstatus","title":"GetTrainStatus","text":"<p>Purpose: Query current training status for a session.</p> <p>Request: <pre><code>message GetTrainStatusRequest {\n  string session_id = 1;\n}\n</code></pre></p> <p>Response: <pre><code>message GetTrainStatusResponse {\n  TrainStatus status = 1;  // Current status\n  string message = 2;      // Status message\n}\n</code></pre></p> <p>Python Example: <pre><code>response = stub.GetTrainStatus(\n    cuvis_ai_pb2.GetTrainStatusRequest(session_id=session_id)\n)\n\nstatus = cuvis_ai_pb2.TrainStatus.Name(response.status)\nprint(f\"Training status: {status}\")\nif response.message:\n    print(f\"Message: {response.message}\")\n</code></pre></p> <p>Use Cases: - Checking if training is complete before inference - Monitoring training from separate process - Debugging training issues</p> <p>Notes: - Returns last known status (may be stale if training just started) - For real-time updates, use <code>Train</code> streaming instead - Status persists in session until next training call</p>"},{"location":"grpc/api-reference/#inference-operations","title":"Inference Operations","text":"<p>Run predictions on trained pipelines.</p>"},{"location":"grpc/api-reference/#inference","title":"Inference","text":"<p>Purpose: Run predictions on trained pipeline with optional output filtering.</p> <p>Request: <pre><code>message InferenceRequest {\n  string session_id = 1;\n  InputBatch inputs = 2;\n  repeated string output_specs = 3;  // Optional: filter outputs\n}\n</code></pre></p> <p>Response: <pre><code>message InferenceResponse {\n  map&lt;string, Tensor&gt; outputs = 1;  // Output tensors by name\n  map&lt;string, float&gt; metrics = 2;   // Optional metrics\n}\n</code></pre></p> <p>Python Example - Basic Inference: <pre><code>from cuvis_ai_core.grpc import helpers\nimport numpy as np\n\n# Prepare input data\ncube = np.random.rand(1, 32, 32, 61).astype(np.float32)\nwavelengths = np.linspace(430, 910, 61).reshape(1, -1).astype(np.float32)\n\n# Run inference\nresponse = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(\n            cube=helpers.numpy_to_proto(cube),\n            wavelengths=helpers.numpy_to_proto(wavelengths),\n        ),\n    )\n)\n\n# Process outputs\nfor name, tensor_proto in response.outputs.items():\n    output_array = helpers.proto_to_numpy(tensor_proto)\n    print(f\"{name}: shape={output_array.shape}, dtype={output_array.dtype}\")\n</code></pre></p> <p>Python Example - Output Filtering: <pre><code># Request only specific outputs (reduces payload size)\nresponse = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(\n            cube=helpers.numpy_to_proto(cube),\n            wavelengths=helpers.numpy_to_proto(wavelengths),\n        ),\n        output_specs=[\n            \"selector.selected\",   # Only selected channels\n            \"detector.scores\",     # Anomaly scores\n            \"decider.decisions\",   # Final decisions\n        ],\n    )\n)\n\nselected = helpers.proto_to_numpy(response.outputs[\"selector.selected\"])\nscores = helpers.proto_to_numpy(response.outputs[\"detector.scores\"])\ndecisions = helpers.proto_to_numpy(response.outputs[\"decider.decisions\"])\n</code></pre></p> <p>Python Example - Complex Input Types: <pre><code># Inference with bounding boxes and points (e.g., SAM integration)\nresponse = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(\n            cube=helpers.numpy_to_proto(cube),\n            wavelengths=helpers.numpy_to_proto(wavelengths),\n            bboxes=cuvis_ai_pb2.BoundingBoxes(\n                boxes=[\n                    cuvis_ai_pb2.BoundingBox(\n                        element_id=0,\n                        x_min=10, y_min=10,\n                        x_max=20, y_max=20,\n                    )\n                ]\n            ),\n            points=cuvis_ai_pb2.Points(\n                points=[\n                    cuvis_ai_pb2.Point(\n                        element_id=0,\n                        x=15.5, y=15.5,\n                        type=cuvis_ai_pb2.POINT_TYPE_POSITIVE,\n                    )\n                ]\n            ),\n            text_prompt=\"Find anomalies in lentils\",\n        ),\n    )\n)\n</code></pre></p> <p>Output Filtering Benefits: - Reduces network payload (important for large tensors) - Faster response time (server skips unused computations) - Lower memory usage on client - Use when you only need subset of pipeline outputs</p> <p>Notes: - Pipeline must be loaded and trained (or weights loaded) first - <code>InputBatch</code> supports: cube, wavelengths, mask, bboxes, points, text_prompt - Output filtering is optional (omit <code>output_specs</code> to get all outputs) - Use <code>helpers.numpy_to_proto()</code> and <code>helpers.proto_to_numpy()</code> for conversions</p> <p>See Also: - InputBatch Data Type - GetPipelineInputs - Query required inputs - GetPipelineOutputs - Query available outputs</p>"},{"location":"grpc/api-reference/#introspection-discovery","title":"Introspection &amp; Discovery","text":"<p>Query pipeline capabilities, inspect structure, and visualize graphs.</p>"},{"location":"grpc/api-reference/#getpipelineinputs","title":"GetPipelineInputs","text":"<p>Purpose: Get input tensor specifications for current pipeline.</p> <p>Request: <pre><code>message GetPipelineInputsRequest {\n  string session_id = 1;\n}\n</code></pre></p> <p>Response: <pre><code>message GetPipelineInputsResponse {\n  repeated string input_names = 1;             // Input port names\n  map&lt;string, TensorSpec&gt; input_specs = 2;    // Specs by name\n}\n</code></pre></p> <p>Python Example: <pre><code>response = stub.GetPipelineInputs(\n    cuvis_ai_pb2.GetPipelineInputsRequest(session_id=session_id)\n)\n\nprint(\"Pipeline inputs:\")\nfor name in response.input_names:\n    spec = response.input_specs[name]\n    shape = list(spec.shape)\n    dtype = cuvis_ai_pb2.DType.Name(spec.dtype)\n    required = \"required\" if spec.required else \"optional\"\n    print(f\"  {name}: shape={shape}, dtype={dtype}, {required}\")\n</code></pre></p> <p>Example Output: <pre><code>Pipeline inputs:\n  cube: shape=[1, -1, -1, 61], dtype=D_TYPE_FLOAT32, required\n  wavelengths: shape=[1, 61], dtype=D_TYPE_FLOAT32, required\n  mask: shape=[1, -1, -1], dtype=D_TYPE_UINT8, optional\n</code></pre></p> <p>Notes: - Shape dimensions of <code>-1</code> indicate dynamic sizes - <code>required=true</code> means input must be provided for inference - <code>required=false</code> means input is optional - Pipeline must be loaded first</p> <p>See Also: - GetPipelineOutputs - TensorSpec Data Type</p>"},{"location":"grpc/api-reference/#getpipelineoutputs","title":"GetPipelineOutputs","text":"<p>Purpose: Get output tensor specifications for current pipeline.</p> <p>Request: <pre><code>message GetPipelineOutputsRequest {\n  string session_id = 1;\n}\n</code></pre></p> <p>Response: <pre><code>message GetPipelineOutputsResponse {\n  repeated string output_names = 1;            // Output port names\n  map&lt;string, TensorSpec&gt; output_specs = 2;   // Specs by name\n}\n</code></pre></p> <p>Python Example: <pre><code>response = stub.GetPipelineOutputs(\n    cuvis_ai_pb2.GetPipelineOutputsRequest(session_id=session_id)\n)\n\nprint(\"Pipeline outputs:\")\nfor name in response.output_names:\n    spec = response.output_specs[name]\n    shape = list(spec.shape)\n    dtype = cuvis_ai_pb2.DType.Name(spec.dtype)\n    print(f\"  {name}: shape={shape}, dtype={dtype}\")\n</code></pre></p> <p>Example Output: <pre><code>Pipeline outputs:\n  selector.selected: shape=[1, -1, -1, 4], dtype=D_TYPE_FLOAT32\n  detector.scores: shape=[1, -1, -1], dtype=D_TYPE_FLOAT32\n  decider.decisions: shape=[1, -1, -1], dtype=D_TYPE_UINT8\n</code></pre></p> <p>Use Cases: - Discovering available outputs before inference - Validating pipeline structure - Generating documentation - Dynamic UI generation</p>"},{"location":"grpc/api-reference/#getpipelinevisualization","title":"GetPipelineVisualization","text":"<p>Purpose: Render pipeline graph as image or text format.</p> <p>Request: <pre><code>message GetPipelineVisualizationRequest {\n  string session_id = 1;\n  string format = 2;  // \"png\", \"svg\", \"dot\", \"mermaid\"\n}\n</code></pre></p> <p>Response: <pre><code>message GetPipelineVisualizationResponse {\n  bytes image_data = 1;  // Binary image data or text\n  string format = 2;     // Confirmed format\n}\n</code></pre></p> <p>Python Example - PNG: <pre><code>from pathlib import Path\n\nresponse = stub.GetPipelineVisualization(\n    cuvis_ai_pb2.GetPipelineVisualizationRequest(\n        session_id=session_id,\n        format=\"png\",\n    )\n)\n\nPath(\"pipeline_graph.png\").write_bytes(response.image_data)\nprint(\"Pipeline visualization saved to pipeline_graph.png\")\n</code></pre></p> <p>Python Example - Mermaid: <pre><code>response = stub.GetPipelineVisualization(\n    cuvis_ai_pb2.GetPipelineVisualizationRequest(\n        session_id=session_id,\n        format=\"mermaid\",\n    )\n)\n\nmermaid_text = response.image_data.decode(\"utf-8\")\nprint(\"Pipeline in Mermaid format:\")\nprint(mermaid_text)\n</code></pre></p> <p>Supported Formats: - <code>\"png\"</code> - PNG image (binary) - <code>\"svg\"</code> - SVG image (text/XML) - <code>\"dot\"</code> - Graphviz DOT format (text) - <code>\"mermaid\"</code> - Mermaid diagram (text)</p> <p>Use Cases: - Documentation generation - Debugging pipeline structure - Presenting architecture to stakeholders - Automated diagram generation in CI/CD</p>"},{"location":"grpc/api-reference/#listavailablepipelines","title":"ListAvailablePipelines","text":"<p>Purpose: Discover registered pipelines available for loading.</p> <p>Request: <pre><code>message ListAvailablePipelinesRequest {\n  string filter_tag = 1;  // Optional tag filter\n}\n</code></pre></p> <p>Response: <pre><code>message ListAvailablePipelinesResponse {\n  repeated PipelineInfo pipelines = 1;\n}\n\nmessage PipelineInfo {\n  string name = 1;\n  PipelineMetadata metadata = 2;\n  repeated string tags = 3;\n}\n</code></pre></p> <p>Python Example: <pre><code>response = stub.ListAvailablePipelines(\n    cuvis_ai_pb2.ListAvailablePipelinesRequest(\n        filter_tag=\"anomaly_detection\",  # Optional filter\n    )\n)\n\nprint(\"Available pipelines:\")\nfor pipeline in response.pipelines:\n    print(f\"  - {pipeline.name}\")\n    print(f\"    Description: {pipeline.metadata.description}\")\n    print(f\"    Tags: {', '.join(pipeline.tags)}\")\n</code></pre></p> <p>Use Cases: - Pipeline discovery for users - Dynamic pipeline selection in applications - Catalog generation</p>"},{"location":"grpc/api-reference/#getpipelineinfo","title":"GetPipelineInfo","text":"<p>Purpose: Get detailed metadata for a specific pipeline.</p> <p>Request: <pre><code>message GetPipelineInfoRequest {\n  string pipeline_name = 1;\n}\n</code></pre></p> <p>Response: <pre><code>message GetPipelineInfoResponse {\n  PipelineInfo info = 1;\n  map&lt;string, TensorSpec&gt; required_inputs = 2;\n  map&lt;string, TensorSpec&gt; outputs = 3;\n}\n</code></pre></p> <p>Python Example: <pre><code>response = stub.GetPipelineInfo(\n    cuvis_ai_pb2.GetPipelineInfoRequest(\n        pipeline_name=\"deep_svdd_anomaly\"\n    )\n)\n\nprint(f\"Pipeline: {response.info.name}\")\nprint(f\"Description: {response.info.metadata.description}\")\nprint(f\"Required inputs: {list(response.required_inputs.keys())}\")\nprint(f\"Outputs: {list(response.outputs.keys())}\")\n</code></pre></p>"},{"location":"grpc/api-reference/#error-handling","title":"Error Handling","text":"<p>All gRPC RPCs use standard gRPC status codes for error handling.</p>"},{"location":"grpc/api-reference/#status-codes","title":"Status Codes","text":"<pre><code>import grpc\n\ntry:\n    response = stub.Inference(request)\nexcept grpc.RpcError as e:\n    code = e.code()\n    details = e.details()\n\n    if code == grpc.StatusCode.INVALID_ARGUMENT:\n        print(f\"Invalid request: {details}\")\n    elif code == grpc.StatusCode.NOT_FOUND:\n        print(f\"Resource not found: {details}\")\n    elif code == grpc.StatusCode.FAILED_PRECONDITION:\n        print(f\"Operation not allowed: {details}\")\n    elif code == grpc.StatusCode.RESOURCE_EXHAUSTED:\n        print(f\"Resource exhausted: {details}\")\n    elif code == grpc.StatusCode.INTERNAL:\n        print(f\"Internal server error: {details}\")\n    else:\n        print(f\"gRPC error: {code} - {details}\")\n        raise\n</code></pre>"},{"location":"grpc/api-reference/#common-error-codes","title":"Common Error Codes","text":"Code Meaning Common Causes <code>INVALID_ARGUMENT</code> Malformed request Missing required fields, invalid types <code>NOT_FOUND</code> Resource not found Session ID doesn't exist, expired session <code>FAILED_PRECONDITION</code> Operation not allowed Training before config set, inference before weights loaded <code>RESOURCE_EXHAUSTED</code> Resource limit exceeded Message size exceeded, GPU out of memory <code>INTERNAL</code> Server error Unexpected exception, configuration bug <code>UNAVAILABLE</code> Service unavailable Server down, network issue <code>DEADLINE_EXCEEDED</code> Operation timeout Long training, slow network"},{"location":"grpc/api-reference/#error-examples","title":"Error Examples","text":"<p>Session Not Found: <pre><code>grpc.StatusCode.NOT_FOUND\n\"Session '7f3e4d2c-1a9b-4c8d-9e7f-2b5a6c8d9e0f' not found or expired\"\n</code></pre></p> <p>Message Size Exceeded: <pre><code>grpc.StatusCode.RESOURCE_EXHAUSTED\n\"Received message larger than max (100000000 vs. 4194304)\"\nSolution: Increase client/server message size limits\n</code></pre></p> <p>Missing Config: <pre><code>grpc.StatusCode.FAILED_PRECONDITION\n\"Cannot train: trainrun config not set. Call SetTrainRunConfig first.\"\n</code></pre></p> <p>CUDA Out of Memory: <pre><code>grpc.StatusCode.RESOURCE_EXHAUSTED\n\"CUDA out of memory. Tried to allocate 2.00 GiB\"\nSolution: Reduce batch size, close unused sessions\n</code></pre></p>"},{"location":"grpc/api-reference/#data-types-reference","title":"Data Types Reference","text":""},{"location":"grpc/api-reference/#inputbatch","title":"InputBatch","text":"<p>Complete input specification for inference.</p> <pre><code>message InputBatch {\n  Tensor cube = 1;              // Required: [B, H, W, C] hyperspectral cube\n  Tensor wavelengths = 2;       // Optional: [B, C] wavelengths in nm\n  Tensor mask = 3;              // Optional: [B, H, W] binary mask\n  BoundingBoxes bboxes = 4;     // Optional: Bounding boxes\n  Points points = 5;            // Optional: Point prompts\n  string text_prompt = 6;       // Optional: Text description\n  map&lt;string, Tensor&gt; extra_inputs = 7;  // Optional: Additional inputs\n}\n</code></pre> <p>Example: <pre><code>inputs = cuvis_ai_pb2.InputBatch(\n    cube=helpers.numpy_to_proto(cube),                    # Required\n    wavelengths=helpers.numpy_to_proto(wavelengths),      # Optional\n    mask=helpers.numpy_to_proto(mask),                    # Optional\n    bboxes=cuvis_ai_pb2.BoundingBoxes(boxes=[...]),       # Optional\n    points=cuvis_ai_pb2.Points(points=[...]),             # Optional\n    text_prompt=\"Find anomalies\",                         # Optional\n)\n</code></pre></p>"},{"location":"grpc/api-reference/#tensor","title":"Tensor","text":"<p>Protocol Buffer tensor representation.</p> <pre><code>message Tensor {\n  repeated int64 shape = 1;  // Tensor dimensions\n  DType dtype = 2;           // Data type\n  bytes raw_data = 3;        // Raw binary data\n}\n\nenum DType {\n  D_TYPE_FLOAT32 = 0;\n  D_TYPE_FLOAT64 = 1;\n  D_TYPE_INT32 = 2;\n  D_TYPE_INT64 = 3;\n  D_TYPE_UINT8 = 4;\n  D_TYPE_UINT16 = 5;\n  // ...\n}\n</code></pre> <p>Conversion Helpers: <pre><code>from cuvis_ai_core.grpc import helpers\n\n# NumPy array to Tensor\narray = np.random.rand(1, 32, 32, 61).astype(np.float32)\ntensor_proto = helpers.numpy_to_proto(array)\n\n# Tensor to NumPy array\narray_recovered = helpers.proto_to_numpy(tensor_proto)\n\n# PyTorch tensor to Tensor\ntensor = torch.randn(1, 32, 32, 61)\ntensor_proto = helpers.tensor_to_proto(tensor)\n</code></pre></p>"},{"location":"grpc/api-reference/#tensorspec","title":"TensorSpec","text":"<p>Tensor specification (metadata without data).</p> <pre><code>message TensorSpec {\n  string name = 1;           // Tensor name\n  repeated int64 shape = 2;  // Shape (-1 for dynamic dimensions)\n  DType dtype = 3;           // Data type\n  bool required = 4;         // Required for inference\n}\n</code></pre> <p>Example: <pre><code>name: \"cube\"\nshape: [1, -1, -1, 61]  // Batch=1, Height/Width dynamic, Channels=61\ndtype: D_TYPE_FLOAT32\nrequired: true\n</code></pre></p>"},{"location":"grpc/api-reference/#boundingboxes","title":"BoundingBoxes","text":"<p>Bounding box collection for object detection or SAM integration.</p> <pre><code>message BoundingBoxes {\n  repeated BoundingBox boxes = 1;\n}\n\nmessage BoundingBox {\n  int32 element_id = 1;  // Batch element index\n  float x_min = 2;\n  float y_min = 3;\n  float x_max = 4;\n  float y_max = 5;\n}\n</code></pre> <p>Example: <pre><code>bboxes = cuvis_ai_pb2.BoundingBoxes(\n    boxes=[\n        cuvis_ai_pb2.BoundingBox(\n            element_id=0,\n            x_min=10.0, y_min=10.0,\n            x_max=20.0, y_max=20.0,\n        ),\n        cuvis_ai_pb2.BoundingBox(\n            element_id=0,\n            x_min=30.0, y_min=30.0,\n            x_max=40.0, y_max=40.0,\n        ),\n    ]\n)\n</code></pre></p>"},{"location":"grpc/api-reference/#points","title":"Points","text":"<p>Point prompts for interactive segmentation (SAM-style).</p> <pre><code>message Points {\n  repeated Point points = 1;\n}\n\nmessage Point {\n  int32 element_id = 1;   // Batch element index\n  float x = 2;\n  float y = 3;\n  PointType type = 4;     // POSITIVE or NEGATIVE\n}\n\nenum PointType {\n  POINT_TYPE_POSITIVE = 0;\n  POINT_TYPE_NEGATIVE = 1;\n}\n</code></pre> <p>Example: <pre><code>points = cuvis_ai_pb2.Points(\n    points=[\n        cuvis_ai_pb2.Point(\n            element_id=0,\n            x=15.5, y=15.5,\n            type=cuvis_ai_pb2.POINT_TYPE_POSITIVE,\n        ),\n        cuvis_ai_pb2.Point(\n            element_id=0,\n            x=25.5, y=25.5,\n            type=cuvis_ai_pb2.POINT_TYPE_NEGATIVE,\n        ),\n    ]\n)\n</code></pre></p>"},{"location":"grpc/api-reference/#see-also","title":"See Also","text":""},{"location":"grpc/api-reference/#related-documentation","title":"Related Documentation","text":"<ul> <li>gRPC Overview - Introduction and architecture</li> <li>Client Patterns - Common usage patterns and best practices</li> <li>Sequence Diagrams - Visual workflows</li> </ul>"},{"location":"grpc/api-reference/#tutorials-guides","title":"Tutorials &amp; Guides","text":"<ul> <li>gRPC Workflow Tutorial - Hands-on tutorial</li> <li>Remote gRPC Access - Detailed how-to guide</li> </ul>"},{"location":"grpc/api-reference/#configuration","title":"Configuration","text":"<ul> <li>TrainRun Schema - Trainrun configuration reference</li> <li>Pipeline Schema - Pipeline YAML schema</li> <li>Hydra Composition - Config composition patterns</li> </ul>"},{"location":"grpc/api-reference/#deployment","title":"Deployment","text":"<ul> <li>Deployment Guide - Docker, Kubernetes, production</li> </ul>"},{"location":"grpc/client-patterns/","title":"Client Patterns","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"grpc/client-patterns/#grpc-client-patterns","title":"gRPC Client Patterns","text":"<p>Common patterns and best practices for building robust gRPC clients for CUVIS.AI.</p>"},{"location":"grpc/client-patterns/#overview","title":"Overview","text":"<p>This guide documents proven patterns for CUVIS.AI gRPC clients, extracted from 24+ production examples. These patterns cover connection management, session lifecycle, configuration, training, inference, error handling, and production deployment.</p> <p>Philosophy: Start simple, add complexity only when needed. Most workflows use the standard 5-phase pattern with helper utilities.</p>"},{"location":"grpc/client-patterns/#connection-management","title":"Connection Management","text":""},{"location":"grpc/client-patterns/#basic-connection-pattern","title":"Basic Connection Pattern","text":"<pre><code>from cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\nimport grpc\n\n# Configure message size for hyperspectral data\noptions = [\n    (\"grpc.max_send_message_length\", 300 * 1024 * 1024),  # 300 MB\n    (\"grpc.max_receive_message_length\", 300 * 1024 * 1024),\n]\n\n# Create channel and stub\nchannel = grpc.insecure_channel(\"localhost:50051\", options=options)\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n</code></pre> <p>Using Helper Utility: <pre><code>from examples.grpc.workflow_utils import build_stub\n\n# Simplified connection\nstub = build_stub(\"localhost:50051\", max_msg_size=600*1024*1024)\n</code></pre></p> <p>Notes: - Message size limits are critical for hyperspectral data (typical cubes: 100-600 MB) - Default gRPC limit is 4 MB (too small) - Both client and server must have matching limits</p>"},{"location":"grpc/client-patterns/#tlsssl-connection-production","title":"TLS/SSL Connection (Production)","text":"<pre><code>import grpc\n\n# Load TLS certificates\ncredentials = grpc.ssl_channel_credentials(\n    root_certificates=open(\"ca.crt\", \"rb\").read(),\n)\n\n# Create secure channel\nchannel = grpc.secure_channel(\n    \"production-server:50051\",\n    credentials,\n    options=[\n        (\"grpc.max_send_message_length\", 600 * 1024 * 1024),\n        (\"grpc.max_receive_message_length\", 600 * 1024 * 1024),\n    ],\n)\n\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n</code></pre> <p>With Client Certificate (mTLS): <pre><code>credentials = grpc.ssl_channel_credentials(\n    root_certificates=open(\"ca.crt\", \"rb\").read(),\n    private_key=open(\"client.key\", \"rb\").read(),\n    certificate_chain=open(\"client.crt\", \"rb\").read(),\n)\nchannel = grpc.secure_channel(\"production-server:50051\", credentials, options=options)\n</code></pre></p> <p>Notes: - Always use TLS in production - mTLS provides client authentication - Keep certificates secure (never commit to git)</p>"},{"location":"grpc/client-patterns/#retry-logic-with-exponential-backoff","title":"Retry Logic with Exponential Backoff","text":"<pre><code>import time\nimport grpc\n\ndef train_with_retry(stub, session_id, trainer_type, max_retries=3):\n    \"\"\"Train with automatic retry on transient failures.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            for progress in stub.Train(\n                cuvis_ai_pb2.TrainRequest(\n                    session_id=session_id,\n                    trainer_type=trainer_type,\n                )\n            ):\n                yield progress\n            break  # Success, exit retry loop\n        except grpc.RpcError as e:\n            if e.code() in [grpc.StatusCode.UNAVAILABLE, grpc.StatusCode.DEADLINE_EXCEEDED]:\n                if attempt &lt; max_retries - 1:\n                    wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n                    print(f\"Retry {attempt + 1}/{max_retries} after {wait_time}s (error: {e.code()})\")\n                    time.sleep(wait_time)\n                else:\n                    print(f\"Max retries reached, giving up\")\n                    raise\n            else:\n                # Non-retryable error (e.g., INVALID_ARGUMENT), fail immediately\n                raise\n\n# Usage\nfor progress in train_with_retry(stub, session_id, cuvis_ai_pb2.TRAINER_TYPE_GRADIENT):\n    print(format_progress(progress))\n</code></pre> <p>Retryable vs Non-Retryable Errors: - Retry: <code>UNAVAILABLE</code>, <code>DEADLINE_EXCEEDED</code>, <code>RESOURCE_EXHAUSTED</code> (transient) - Don't Retry: <code>INVALID_ARGUMENT</code>, <code>NOT_FOUND</code>, <code>FAILED_PRECONDITION</code> (permanent)</p>"},{"location":"grpc/client-patterns/#connection-health-check","title":"Connection Health Check","text":"<pre><code>def check_server_health(stub):\n    \"\"\"Check if server is responsive.\"\"\"\n    try:\n        response = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\n        stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(\n            session_id=response.session_id\n        ))\n        return True\n    except grpc.RpcError as e:\n        print(f\"Server health check failed: {e.code()} - {e.details()}\")\n        return False\n\n# Check before starting work\nif not check_server_health(stub):\n    print(\"Server is not available, exiting\")\n    exit(1)\n\nprint(\"Server is healthy, proceeding...\")\n</code></pre>"},{"location":"grpc/client-patterns/#session-management-patterns","title":"Session Management Patterns","text":""},{"location":"grpc/client-patterns/#context-manager-pattern","title":"Context Manager Pattern","text":"<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef grpc_session(stub, search_paths=None):\n    \"\"\"Context manager for safe session lifecycle.\"\"\"\n    from examples.grpc.workflow_utils import create_session_with_search_paths\n\n    session_id = create_session_with_search_paths(stub, search_paths)\n    try:\n        yield session_id\n    finally:\n        stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n        print(f\"Session {session_id} closed\")\n\n# Usage\nwith grpc_session(stub) as session_id:\n    # Training or inference\n    for progress in stub.Train(...):\n        print(format_progress(progress))\n    # Session automatically closed on exit\n</code></pre> <p>Benefits: - Guaranteed cleanup (even on exceptions) - Pythonic resource management - Reduced boilerplate</p>"},{"location":"grpc/client-patterns/#manual-session-management-with-tryfinally","title":"Manual Session Management with try/finally","text":"<pre><code>session_id = None\ntry:\n    # Create session\n    session_id = stub.CreateSession(...).session_id\n\n    # Register search paths\n    stub.SetSessionSearchPaths(...)\n\n    # Training or inference\n    for progress in stub.Train(...):\n        print(progress)\n\n    # Save results\n    stub.SavePipeline(...)\n\nfinally:\n    # Always close session\n    if session_id:\n        stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre>"},{"location":"grpc/client-patterns/#concurrent-sessions-for-ab-testing","title":"Concurrent Sessions for A/B Testing","text":"<pre><code>import concurrent.futures\n\ndef run_experiment(stub, trainrun_name, overrides):\n    \"\"\"Run single training experiment.\"\"\"\n    from examples.grpc.workflow_utils import (\n        create_session_with_search_paths,\n        resolve_trainrun_config,\n        apply_trainrun_config,\n    )\n\n    # Create isolated session\n    session_id = create_session_with_search_paths(stub)\n\n    try:\n        # Resolve and apply config\n        resolved, config_dict = resolve_trainrun_config(\n            stub, session_id, trainrun_name, overrides\n        )\n        apply_trainrun_config(stub, session_id, resolved.config_bytes)\n\n        # Train\n        results = []\n        for progress in stub.Train(\n            cuvis_ai_pb2.TrainRequest(\n                session_id=session_id,\n                trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n            )\n        ):\n            results.append(progress)\n\n        return config_dict, results\n\n    finally:\n        stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n\n# Run multiple experiments concurrently\nexperiments = [\n    (\"deep_svdd\", [\"training.optimizer.lr=0.001\"]),\n    (\"deep_svdd\", [\"training.optimizer.lr=0.0005\"]),\n    (\"deep_svdd\", [\"training.optimizer.lr=0.0001\"]),\n]\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n    futures = [\n        executor.submit(run_experiment, stub, name, overrides)\n        for name, overrides in experiments\n    ]\n\n    for future in concurrent.futures.as_completed(futures):\n        config, results = future.result()\n        final_metrics = results[-1].metrics\n        print(f\"Experiment {config['name']} (lr={config['training']['optimizer']['lr']})\")\n        print(f\"  Final IoU: {final_metrics.get('val_iou', 0):.4f}\")\n</code></pre>"},{"location":"grpc/client-patterns/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"grpc/client-patterns/#standard-config-resolution","title":"Standard Config Resolution","text":"<pre><code>from examples.grpc.workflow_utils import (\n    resolve_trainrun_config,\n    apply_trainrun_config,\n)\n\n# Resolve trainrun config with overrides\nresolved, config_dict = resolve_trainrun_config(\n    stub,\n    session_id,\n    \"deep_svdd\",\n    overrides=[\n        \"training.trainer.max_epochs=50\",\n        \"training.optimizer.lr=0.0005\",\n        \"data.batch_size=8\",\n    ],\n)\n\n# Apply to session\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n\n# Inspect resolved config\nprint(f\"Pipeline: {config_dict['pipeline']['name']}\")\nprint(f\"Optimizer: {config_dict['training']['optimizer']['name']}\")\nprint(f\"Learning rate: {config_dict['training']['optimizer']['lr']}\")\n</code></pre>"},{"location":"grpc/client-patterns/#config-validation-before-training","title":"Config Validation Before Training","text":"<pre><code>import json\n\ndef validate_and_train(stub, session_id, trainrun_name, overrides=None):\n    \"\"\"Resolve, validate, and train with error handling.\"\"\"\n    from examples.grpc.workflow_utils import resolve_trainrun_config\n\n    # Resolve config\n    resolved, config_dict = resolve_trainrun_config(\n        stub, session_id, trainrun_name, overrides\n    )\n\n    # Validate training config\n    training_json = json.dumps(config_dict[\"training\"]).encode(\"utf-8\")\n    validation = stub.ValidateConfig(\n        cuvis_ai_pb2.ValidateConfigRequest(\n            config_type=\"training\",\n            config_bytes=training_json,\n        )\n    )\n\n    if not validation.valid:\n        print(\"Training configuration validation failed:\")\n        for error in validation.errors:\n            print(f\"  ERROR: {error}\")\n        raise ValueError(\"Invalid training configuration\")\n\n    # Show warnings\n    for warning in validation.warnings:\n        print(f\"  WARNING: {warning}\")\n\n    # Apply config and train\n    stub.SetTrainRunConfig(\n        cuvis_ai_pb2.SetTrainRunConfigRequest(\n            session_id=session_id,\n            config=cuvis_ai_pb2.TrainRunConfig(config_bytes=resolved.config_bytes),\n        )\n    )\n\n    # Train with validated config\n    for progress in stub.Train(\n        cuvis_ai_pb2.TrainRequest(\n            session_id=session_id,\n            trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n        )\n    ):\n        yield progress\n\n# Usage\nfor progress in validate_and_train(stub, session_id, \"deep_svdd\"):\n    print(format_progress(progress))\n</code></pre>"},{"location":"grpc/client-patterns/#training-patterns","title":"Training Patterns","text":""},{"location":"grpc/client-patterns/#two-phase-training-pattern","title":"Two-Phase Training Pattern","text":"<pre><code>from examples.grpc.workflow_utils import format_progress\n\ndef two_phase_training(stub, session_id):\n    \"\"\"Standard two-phase training: statistical then gradient.\"\"\"\n\n    # Phase 1: Statistical initialization (fast, no backprop)\n    print(\"=== Phase 1: Statistical Training ===\")\n    for progress in stub.Train(\n        cuvis_ai_pb2.TrainRequest(\n            session_id=session_id,\n            trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n        )\n    ):\n        print(f\"[Statistical] {format_progress(progress)}\")\n\n    # Phase 2: Gradient-based fine-tuning (full training)\n    print(\"\\n=== Phase 2: Gradient Training ===\")\n    for progress in stub.Train(\n        cuvis_ai_pb2.TrainRequest(\n            session_id=session_id,\n            trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n        )\n    ):\n        print(f\"[Gradient] {format_progress(progress)}\")\n\n    print(\"\\n=== Training Complete ===\")\n\n# Usage\ntwo_phase_training(stub, session_id)\n</code></pre>"},{"location":"grpc/client-patterns/#progress-monitoring-with-early-stopping","title":"Progress Monitoring with Early Stopping","text":"<pre><code>def train_with_early_stopping(stub, session_id, trainer_type, patience=5):\n    \"\"\"Train with custom early stopping logic.\"\"\"\n    best_metric = float('-inf')\n    patience_counter = 0\n\n    for progress in stub.Train(\n        cuvis_ai_pb2.TrainRequest(\n            session_id=session_id,\n            trainer_type=trainer_type,\n        )\n    ):\n        # Print all progress\n        print(format_progress(progress))\n\n        # Check validation metrics for early stopping\n        if progress.context.stage == cuvis_ai_pb2.EXECUTION_STAGE_VALIDATE:\n            current_metric = progress.metrics.get('val_iou', 0.0)\n\n            if current_metric &gt; best_metric:\n                best_metric = current_metric\n                patience_counter = 0\n                print(f\"  \u2192 New best metric: {best_metric:.4f}\")\n            else:\n                patience_counter += 1\n                print(f\"  \u2192 No improvement (patience: {patience_counter}/{patience})\")\n\n            if patience_counter &gt;= patience:\n                print(f\"\\n=== Early stopping triggered (patience={patience}) ===\")\n                break\n\n    print(f\"\\nBest validation metric: {best_metric:.4f}\")\n\n# Usage\ntrain_with_early_stopping(stub, session_id, cuvis_ai_pb2.TRAINER_TYPE_GRADIENT)\n</code></pre>"},{"location":"grpc/client-patterns/#checkpoint-saving-strategy","title":"Checkpoint Saving Strategy","text":"<pre><code>def train_with_checkpoints(stub, session_id, trainer_type, checkpoint_every_n_epochs=10):\n    \"\"\"Save checkpoints periodically during training.\"\"\"\n    from pathlib import Path\n\n    output_dir = Path(\"outputs/checkpoints\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    epoch_counter = 0\n\n    for progress in stub.Train(\n        cuvis_ai_pb2.TrainRequest(\n            session_id=session_id,\n            trainer_type=trainer_type,\n        )\n    ):\n        print(format_progress(progress))\n\n        # Save checkpoint at end of each N epochs\n        if progress.context.stage == cuvis_ai_pb2.EXECUTION_STAGE_TRAIN:\n            current_epoch = progress.context.epoch\n\n            if current_epoch &gt; epoch_counter:\n                epoch_counter = current_epoch\n\n                if epoch_counter % checkpoint_every_n_epochs == 0:\n                    checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch_counter}.yaml\"\n\n                    print(f\"\\n  \u2192 Saving checkpoint at epoch {epoch_counter}...\")\n                    response = stub.SavePipeline(\n                        cuvis_ai_pb2.SavePipelineRequest(\n                            session_id=session_id,\n                            pipeline_path=str(checkpoint_path),\n                        )\n                    )\n                    print(f\"  \u2192 Saved: {response.pipeline_path}\\n\")\n\n# Usage\ntrain_with_checkpoints(stub, session_id, cuvis_ai_pb2.TRAINER_TYPE_GRADIENT)\n</code></pre>"},{"location":"grpc/client-patterns/#inference-patterns","title":"Inference Patterns","text":""},{"location":"grpc/client-patterns/#single-inference-pattern","title":"Single Inference Pattern","text":"<pre><code>from cuvis_ai_core.grpc import helpers\nimport numpy as np\n\ndef run_inference(stub, session_id, cube, wavelengths):\n    \"\"\"Run inference on single hyperspectral cube.\"\"\"\n\n    # Run inference\n    response = stub.Inference(\n        cuvis_ai_pb2.InferenceRequest(\n            session_id=session_id,\n            inputs=cuvis_ai_pb2.InputBatch(\n                cube=helpers.numpy_to_proto(cube),\n                wavelengths=helpers.numpy_to_proto(wavelengths),\n            ),\n        )\n    )\n\n    # Convert outputs\n    outputs = {\n        name: helpers.proto_to_numpy(tensor_proto)\n        for name, tensor_proto in response.outputs.items()\n    }\n\n    return outputs\n\n# Usage\ncube = np.random.rand(1, 32, 32, 61).astype(np.float32)\nwavelengths = np.linspace(430, 910, 61).reshape(1, -1).astype(np.float32)\n\noutputs = run_inference(stub, session_id, cube, wavelengths)\nprint(f\"Outputs: {list(outputs.keys())}\")\n</code></pre>"},{"location":"grpc/client-patterns/#batch-inference-pattern","title":"Batch Inference Pattern","text":"<pre><code>from torch.utils.data import DataLoader\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\n\ndef batch_inference(stub, session_id, cu3s_path, batch_size=4):\n    \"\"\"Efficient batch inference on CU3S dataset.\"\"\"\n    from cuvis_ai_core.grpc import helpers\n\n    # Create data loader\n    datamodule = SingleCu3sDataModule(\n        cu3s_file_path=cu3s_path,\n        batch_size=batch_size,\n        processing_mode=\"Reflectance\",\n    )\n    datamodule.setup(stage=\"test\")\n    dataloader = datamodule.test_dataloader()\n\n    # Run inference on each batch\n    all_results = []\n    for batch_idx, batch in enumerate(dataloader):\n        print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}\")\n\n        response = stub.Inference(\n            cuvis_ai_pb2.InferenceRequest(\n                session_id=session_id,\n                inputs=cuvis_ai_pb2.InputBatch(\n                    cube=helpers.tensor_to_proto(batch[\"cube\"]),\n                    wavelengths=helpers.tensor_to_proto(batch[\"wavelengths\"]),\n                ),\n            )\n        )\n\n        # Convert and store outputs\n        batch_outputs = {\n            name: helpers.proto_to_numpy(tensor_proto)\n            for name, tensor_proto in response.outputs.items()\n        }\n        all_results.append(batch_outputs)\n\n    return all_results\n\n# Usage\nresults = batch_inference(stub, session_id, \"data/Lentils_000.cu3s\", batch_size=4)\nprint(f\"Processed {len(results)} batches\")\n</code></pre>"},{"location":"grpc/client-patterns/#output-filtering-pattern","title":"Output Filtering Pattern","text":"<pre><code>def filtered_inference(stub, session_id, cube, wavelengths, output_specs):\n    \"\"\"Inference with output filtering (reduces payload).\"\"\"\n    from cuvis_ai_core.grpc import helpers\n\n    response = stub.Inference(\n        cuvis_ai_pb2.InferenceRequest(\n            session_id=session_id,\n            inputs=cuvis_ai_pb2.InputBatch(\n                cube=helpers.numpy_to_proto(cube),\n                wavelengths=helpers.numpy_to_proto(wavelengths),\n            ),\n            output_specs=output_specs,  # Only request specific outputs\n        )\n    )\n\n    outputs = {\n        name: helpers.proto_to_numpy(tensor_proto)\n        for name, tensor_proto in response.outputs.items()\n    }\n\n    return outputs\n\n# Usage - only request final decisions\noutputs = filtered_inference(\n    stub,\n    session_id,\n    cube,\n    wavelengths,\n    output_specs=[\"decider.decisions\", \"detector.scores\"],\n)\n</code></pre>"},{"location":"grpc/client-patterns/#error-handling","title":"Error Handling","text":""},{"location":"grpc/client-patterns/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>import grpc\n\ndef safe_inference(stub, session_id, cube, wavelengths):\n    \"\"\"Inference with comprehensive error handling.\"\"\"\n    from cuvis_ai_core.grpc import helpers\n\n    try:\n        response = stub.Inference(\n            cuvis_ai_pb2.InferenceRequest(\n                session_id=session_id,\n                inputs=cuvis_ai_pb2.InputBatch(\n                    cube=helpers.numpy_to_proto(cube),\n                    wavelengths=helpers.numpy_to_proto(wavelengths),\n                ),\n            )\n        )\n\n        return {\n            name: helpers.proto_to_numpy(tensor_proto)\n            for name, tensor_proto in response.outputs.items()\n        }\n\n    except grpc.RpcError as e:\n        code = e.code()\n        details = e.details()\n\n        if code == grpc.StatusCode.INVALID_ARGUMENT:\n            print(f\"Invalid inference request: {details}\")\n        elif code == grpc.StatusCode.NOT_FOUND:\n            print(f\"Session not found: {details}\")\n        elif code == grpc.StatusCode.FAILED_PRECONDITION:\n            print(f\"Cannot run inference: {details}\")\n        elif code == grpc.StatusCode.RESOURCE_EXHAUSTED:\n            print(f\"Resource exhausted: {details}\")\n        else:\n            print(f\"gRPC error: {code} - {details}\")\n\n        raise\n</code></pre>"},{"location":"grpc/client-patterns/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Always close sessions - Use try/finally or context managers</li> <li>Configure message size - Set 600 MB limits for hyperspectral data</li> <li>Validate configs early - Use <code>ValidateConfig</code> before training</li> <li>Filter outputs - Specify <code>output_specs</code> to reduce payload</li> <li>Handle errors gracefully - Check gRPC status codes</li> <li>Use helper functions - Leverage <code>workflow_utils.py</code></li> <li>Monitor training progress - Process streaming updates</li> <li>Save checkpoints - Periodic <code>SavePipeline</code> during training</li> <li>Reuse channels - Don't create new channels for each request</li> <li>Enable TLS in production - Use <code>ssl_channel_credentials</code></li> </ol>"},{"location":"grpc/client-patterns/#troubleshooting","title":"Troubleshooting","text":""},{"location":"grpc/client-patterns/#connection-refused","title":"Connection Refused","text":"<p>Error: <code>grpc._channel._InactiveRpcError: failed to connect to all addresses</code></p> <p>Solutions: 1. Verify server is running: <code>ps aux | grep production_server</code> 2. Check port: <code>netstat -an | grep 50051</code> 3. Test connectivity: <code>telnet localhost 50051</code> 4. Check firewall (if remote): <code>sudo ufw allow 50051</code></p>"},{"location":"grpc/client-patterns/#message-size-exceeded","title":"Message Size Exceeded","text":"<p>Error: <code>grpc._channel._MultiThreadedRendezvous: Received message larger than max</code></p> <p>Solution: <pre><code>stub = build_stub(\"localhost:50051\", max_msg_size=600*1024*1024)\n</code></pre></p>"},{"location":"grpc/client-patterns/#session-not-found","title":"Session Not Found","text":"<p>Error: <code>Session ID not found</code></p> <p>Solutions: 1. Session expired (1-hour timeout) - create new session 2. Server restarted - sessions are not persisted 3. Verify session ID is correct</p>"},{"location":"grpc/client-patterns/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Error: <code>CUDA out of memory</code></p> <p>Solutions: 1. Reduce batch size in data config 2. Close unused sessions 3. Restart server to clear GPU memory</p>"},{"location":"grpc/client-patterns/#plugin-management","title":"Plugin Management","text":"<p>CUVIS.AI supports a plugin system for extending functionality. Plugins can be loaded dynamically at runtime to add custom nodes, data sources, or processing capabilities.</p> <p>For comprehensive plugin system documentation, see: - Plugin System Overview - Architecture and core concepts - Plugin Registry - Available plugins - Plugin Development Guide - Creating custom plugins</p> <p>gRPC Integration: Plugins loaded on the server side are automatically available to all gRPC clients. Use the discovery RPCs to query available capabilities after plugins are loaded.</p>"},{"location":"grpc/client-patterns/#see-also","title":"See Also","text":"<ul> <li>gRPC API Reference - Complete RPC method documentation</li> <li>gRPC Overview - Architecture and concepts</li> <li>Sequence Diagrams - Visual workflows</li> <li>gRPC Tutorial - Hands-on tutorial</li> <li>Deployment Guide - Production patterns</li> </ul>"},{"location":"grpc/overview/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"grpc/overview/#grpc-services-overview","title":"gRPC Services Overview","text":"<p>Access CUVIS.AI pipelines remotely using gRPC for distributed training, inference, and production deployment.</p>"},{"location":"grpc/overview/#introduction","title":"Introduction","text":"<p>gRPC (Google Remote Procedure Call) is a high-performance RPC framework that enables client-server communication for running CUVIS.AI pipelines on remote hardware. The gRPC service separates training infrastructure from client applications, allowing you to execute workflows from anywhere.</p>"},{"location":"grpc/overview/#why-grpc-for-ml-pipelines","title":"Why gRPC for ML Pipelines?","text":"<p>gRPC is ideal for hyperspectral imaging ML workflows because it provides:</p> <ul> <li>Remote Execution - Train models on GPU servers from local machines or lightweight clients</li> <li>Scalability - Deploy pipelines as independent microservices that scale horizontally</li> <li>Streaming - Real-time progress updates during training without polling</li> <li>Language-Agnostic - Client libraries available for Python, C++, Java, Go, and more</li> <li>Production-Ready - Battle-tested technology used by Google, Netflix, Square, and other major companies</li> <li>Efficient - Protocol Buffers provide compact binary serialization for large hyperspectral cubes</li> </ul>"},{"location":"grpc/overview/#use-cases","title":"Use Cases","text":"<p>Development &amp; Research: - Train models on remote GPU servers while developing on local machines - Share trained pipelines across teams without copying large model files - Run experiments concurrently with isolated sessions</p> <p>Production Deployment: - Deploy pipelines as stateless microservices behind load balancers - Scale inference horizontally across multiple server instances - Integrate with existing microservice architectures - Build web applications or mobile clients that access ML models</p> <p>Distributed Processing: - Process large datasets across multiple machines - Batch inference on remote hardware - A/B testing with concurrent sessions</p>"},{"location":"grpc/overview/#architecture","title":"Architecture","text":"<p>The CUVIS.AI gRPC service follows a server-client architecture with session-based isolation.</p>"},{"location":"grpc/overview/#components","title":"Components","text":"<pre><code>graph TB\n    subgraph \"Client Applications\"\n        C1[Python Client]\n        C2[Web Application]\n        C3[Mobile App]\n    end\n\n    subgraph \"gRPC Server: localhost:50051\"\n        SVC[CuvisAIService&lt;br/&gt;46 RPC Methods]\n        SM[Session Manager]\n        CS[Config Service&lt;br/&gt;Hydra Integration]\n        PM[Pipeline Manager]\n        TE[Training Engine]\n        IE[Inference Engine]\n    end\n\n    subgraph \"Resources\"\n        GPU[GPU/CPU Resources]\n        CFG[Config Files]\n        DATA[Training Data]\n    end\n\n    C1 --&gt;|gRPC/HTTP2| SVC\n    C2 --&gt;|gRPC/HTTP2| SVC\n    C3 --&gt;|gRPC/HTTP2| SVC\n\n    SVC --&gt; SM\n    SVC --&gt; CS\n    SVC --&gt; PM\n    SVC --&gt; TE\n    SVC --&gt; IE\n\n    SM --&gt; GPU\n    PM --&gt; GPU\n    TE --&gt; GPU\n    IE --&gt; GPU\n\n    CS --&gt; CFG\n    TE --&gt; DATA\n\n    style SVC fill:#fff4e1\n    style C1 fill:#e1f5ff\n    style C2 fill:#e1f5ff\n    style C3 fill:#e1f5ff\n    style GPU fill:#ffe1e1</code></pre> <p>1. CuvisAIService - Single unified service with 46 RPC methods - Handles all client requests via Protocol Buffers - Listens on port 50051 by default - Supports concurrent sessions from multiple clients</p> <p>2. Session Manager - Creates isolated execution contexts (sessions) - Each session has independent pipeline, data, and training state - Automatic cleanup after 1 hour of inactivity - Enables concurrent users without interference</p> <p>3. Config Service - Integrates with Hydra configuration framework - Resolves config groups with composition and inheritance - Supports dynamic overrides at request time - Validates configurations before application</p> <p>4. Pipeline Manager - Loads pipelines from YAML configs or Protocol Buffers - Manages pipeline weights and checkpoints - Provides introspection (inputs, outputs, visualization) - Handles save/restore operations</p> <p>5. Training Engine - Executes statistical and gradient training - Streams real-time progress updates to clients - Manages optimizer state and learning rate schedules - Supports two-phase training patterns</p> <p>6. Inference Engine - Runs predictions on trained pipelines - Supports input/output filtering to reduce payload size - Handles batched inference efficiently - Manages GPU memory automatically</p>"},{"location":"grpc/overview/#rpc-methods-46-total","title":"RPC Methods (46 Total)","text":"<p>The CuvisAIService provides 46 RPC methods organized into 6 functional areas:</p> Category Methods Purpose Session Management 3 Create, configure, and close sessions Configuration 4 Resolve, validate, and apply Hydra configs Pipeline Management 5 Load, save, and introspect pipelines Training Operations 3 Execute statistical/gradient training Inference Operations 1 Run predictions on trained models Discovery &amp; Introspection 6+ List pipelines, get capabilities, visualize graphs <p>For complete method documentation, see the gRPC API Reference.</p>"},{"location":"grpc/overview/#quick-start","title":"Quick Start","text":""},{"location":"grpc/overview/#step-1-start-the-server","title":"Step 1: Start the Server","text":"<pre><code># Start gRPC server locally (default port: 50051)\nuv run python -m cuvis_ai.grpc.production_server\n</code></pre> <p>Expected Output: <pre><code>Starting CUVIS.AI gRPC server on [::]:50051\nServer listening...\n</code></pre></p>"},{"location":"grpc/overview/#step-2-connect-from-python-client","title":"Step 2: Connect from Python Client","text":"<pre><code>from cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\nimport grpc\n\n# Connect to server with increased message size for hyperspectral data\noptions = [\n    (\"grpc.max_send_message_length\", 300 * 1024 * 1024),  # 300 MB\n    (\"grpc.max_receive_message_length\", 300 * 1024 * 1024),\n]\nchannel = grpc.insecure_channel(\"localhost:50051\", options=options)\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n\nprint(\"Connected to gRPC server\")\n</code></pre>"},{"location":"grpc/overview/#step-3-create-session","title":"Step 3: Create Session","text":"<pre><code># Create isolated session\nresponse = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\nsession_id = response.session_id\nprint(f\"Session created: {session_id}\")\n</code></pre>"},{"location":"grpc/overview/#step-4-run-simple-inference","title":"Step 4: Run Simple Inference","text":"<pre><code>from cuvis_ai_core.grpc import helpers\nimport numpy as np\n\n# Prepare input data\ncube = np.random.rand(1, 32, 32, 61).astype(np.float32)\nwavelengths = np.linspace(430, 910, 61).reshape(1, -1).astype(np.float32)\n\n# Run inference\nresponse = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(\n            cube=helpers.numpy_to_proto(cube),\n            wavelengths=helpers.numpy_to_proto(wavelengths),\n        ),\n    )\n)\n\n# Process outputs\nfor name, tensor_proto in response.outputs.items():\n    output_array = helpers.proto_to_numpy(tensor_proto)\n    print(f\"{name}: shape={output_array.shape}\")\n</code></pre>"},{"location":"grpc/overview/#step-5-clean-up","title":"Step 5: Clean Up","text":"<pre><code># Always close sessions to free resources\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\nprint(\"Session closed\")\n</code></pre> <p>Time to Complete: 5 minutes</p>"},{"location":"grpc/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"grpc/overview/#sessions-isolated-execution-contexts","title":"Sessions: Isolated Execution Contexts","text":"<p>Sessions provide isolated execution environments for each client:</p> <ul> <li>Isolation - Each session has independent pipeline, weights, and training state</li> <li>Concurrency - Multiple clients can use the server simultaneously</li> <li>Automatic Cleanup - Sessions expire after 1 hour of inactivity</li> <li>Resource Management - Closing a session immediately frees GPU memory</li> </ul> <p>Best Practice: Always close sessions explicitly using <code>try</code>/<code>finally</code> blocks or context managers.</p> <pre><code>try:\n    session_id = stub.CreateSession(...).session_id\n    # ... use session ...\nfinally:\n    stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre>"},{"location":"grpc/overview/#configuration-service-hydra-integration","title":"Configuration Service: Hydra Integration","text":"<p>The gRPC service integrates directly with Hydra for powerful configuration composition:</p> <ul> <li>Config Groups - Organize configs into groups (pipeline, data, training, trainrun)</li> <li>Composition - Combine multiple configs using Hydra's composition system</li> <li>Dynamic Overrides - Change parameters at request time without editing files</li> <li>Validation - Pre-validate configs before applying them</li> </ul> <p>Example: Dynamic Overrides <pre><code># Resolve config with runtime overrides\nresponse = stub.ResolveConfig(\n    cuvis_ai_pb2.ResolveConfigRequest(\n        session_id=session_id,\n        config_type=\"trainrun\",\n        path=\"trainrun/deep_svdd\",\n        overrides=[\n            \"training.trainer.max_epochs=50\",\n            \"training.optimizer.lr=0.001\",\n            \"data.batch_size=8\",\n        ],\n    )\n)\n</code></pre></p> <p>See Hydra Composition Patterns for details.</p>"},{"location":"grpc/overview/#streaming-updates-real-time-progress","title":"Streaming Updates: Real-Time Progress","text":"<p>Training operations use server-side streaming to provide real-time progress updates:</p> <ul> <li>No Polling - Server pushes updates as they occur</li> <li>Fine-Grained - Epoch, step, loss, and metric updates</li> <li>Efficient - Minimal overhead compared to REST polling</li> <li>Standard Pattern - Process stream with for-loop</li> </ul> <p>Example: Streaming Training Progress <pre><code>for progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n    print(f\"[{stage}] {status} | losses={dict(progress.losses)}\")\n</code></pre></p>"},{"location":"grpc/overview/#5-phase-workflow","title":"5-Phase Workflow","text":"<p>All gRPC workflows follow a standardized 5-phase pattern:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    Note over Client,Server: Phase 1: Create Session\n    Client-&gt;&gt;Server: CreateSession()\n    Server--&gt;&gt;Client: session_id\n\n    Note over Client,Server: Phase 2: Configure\n    Client-&gt;&gt;Server: SetSessionSearchPaths(search_paths)\n    Client-&gt;&gt;Server: ResolveConfig(trainrun, overrides)\n    Server--&gt;&gt;Client: config_bytes\n    Client-&gt;&gt;Server: SetTrainRunConfig(config_bytes)\n\n    Note over Client,Server: Phase 3: Execute\n    Client-&gt;&gt;Server: Train() or Inference()\n    Server--&gt;&gt;Client: Streaming progress / Results\n\n    Note over Client,Server: Phase 4: Persist\n    Client-&gt;&gt;Server: SavePipeline() or SaveTrainRun()\n    Server--&gt;&gt;Client: Saved paths\n\n    Note over Client,Server: Phase 5: Close\n    Client-&gt;&gt;Server: CloseSession()\n    Server--&gt;&gt;Client: Success</code></pre>"},{"location":"grpc/overview/#phase-1-create-session","title":"Phase 1: Create Session","text":"<p>Initialize an isolated execution context with unique session ID.</p>"},{"location":"grpc/overview/#phase-2-configure","title":"Phase 2: Configure","text":"<p>Register config search paths, resolve configs with Hydra, apply to session.</p>"},{"location":"grpc/overview/#phase-3-execute","title":"Phase 3: Execute","text":"<p>Run training (with streaming progress) or inference (with results).</p>"},{"location":"grpc/overview/#phase-4-persist","title":"Phase 4: Persist","text":"<p>Save trained pipelines, weights, or complete trainrun configurations.</p>"},{"location":"grpc/overview/#phase-5-close","title":"Phase 5: Close","text":"<p>Release all resources and clean up GPU memory.</p> <p>Helper Utilities:</p> <p>The <code>examples/grpc/workflow_utils.py</code> module provides convenience functions: - <code>build_stub()</code> - Create configured gRPC stub - <code>create_session_with_search_paths()</code> - Combine phases 1-2 - <code>resolve_trainrun_config()</code> - Hydra config resolution - <code>apply_trainrun_config()</code> - Apply resolved config - <code>format_progress()</code> - Pretty-print training updates</p> <p>See gRPC Client Patterns for complete usage examples.</p>"},{"location":"grpc/overview/#message-size-limits","title":"Message Size Limits","text":"<p>Hyperspectral imaging cubes can be very large (100s of MB). Configure message size limits appropriately:</p>"},{"location":"grpc/overview/#default-limits","title":"Default Limits","text":"<ul> <li>gRPC Default: 4 MB (too small for hyperspectral data)</li> <li>CUVIS.AI Default: 300 MB (suitable for most workflows)</li> <li>Recommended for Large Data: 600 MB</li> </ul>"},{"location":"grpc/overview/#configuration","title":"Configuration","text":"<p>Client-Side: <pre><code>options = [\n    (\"grpc.max_send_message_length\", 600 * 1024 * 1024),  # 600 MB\n    (\"grpc.max_receive_message_length\", 600 * 1024 * 1024),\n]\nchannel = grpc.insecure_channel(\"localhost:50051\", options=options)\n</code></pre></p> <p>Server-Side: <pre><code>server = grpc.server(\n    futures.ThreadPoolExecutor(max_workers=10),\n    options=[\n        (\"grpc.max_send_message_length\", 600 * 1024 * 1024),\n        (\"grpc.max_receive_message_length\", 600 * 1024 * 1024),\n    ],\n)\n</code></pre></p> <p>Note: Both client and server must have matching limits. If you see <code>RESOURCE_EXHAUSTED</code> errors, increase these limits.</p>"},{"location":"grpc/overview/#when-to-use-grpc-vs-local-execution","title":"When to Use gRPC vs Local Execution","text":""},{"location":"grpc/overview/#use-grpc-when","title":"Use gRPC When:","text":"<p>\u2705 Remote GPU Access - Your development machine lacks GPU, but you have access to GPU servers</p> <p>\u2705 Production Deployment - You're deploying pipelines as microservices behind load balancers</p> <p>\u2705 Distributed Processing - You're processing large datasets across multiple machines</p> <p>\u2705 Team Collaboration - Multiple team members need to access shared trained models</p> <p>\u2705 Cross-Language Clients - You need to call pipelines from non-Python applications (C++, Java, Go)</p> <p>\u2705 Scalability - You need horizontal scaling with multiple server instances</p>"},{"location":"grpc/overview/#use-local-execution-when","title":"Use Local Execution When:","text":"<p>\u26d4 Single-Machine Workflows - Training and inference happen on the same machine</p> <p>\u26d4 Rapid Prototyping - You're iterating quickly on pipeline designs</p> <p>\u26d4 Debugging - You need direct access to pipeline internals for debugging</p> <p>\u26d4 Small Datasets - Data fits in memory and doesn't require distributed processing</p> <p>\u26d4 Offline Environments - No network connectivity available</p>"},{"location":"grpc/overview/#decision-matrix","title":"Decision Matrix","text":"Scenario Recommended Approach Local development with GPU Local execution Local development without GPU gRPC to remote server Production inference service gRPC deployment Batch processing large datasets gRPC with multiple workers Interactive Jupyter notebook Local execution Web application backend gRPC microservice Mobile application gRPC to cloud server Research experiment automation Local or gRPC (depends on resources)"},{"location":"grpc/overview/#production-deployment","title":"Production Deployment","text":"<p>For production deployment, consider:</p> <ul> <li>TLS/SSL - Enable encryption for secure communication</li> <li>Authentication - Implement token-based auth or mTLS</li> <li>Load Balancing - Deploy multiple servers behind a load balancer</li> <li>Monitoring - Integrate Prometheus metrics and health checks</li> <li>Resource Limits - Configure appropriate message sizes and timeouts</li> <li>Docker/Kubernetes - Containerize for easy deployment and scaling</li> </ul> <p>See the Deployment Guide for complete instructions.</p>"},{"location":"grpc/overview/#see-also","title":"See Also","text":""},{"location":"grpc/overview/#grpc-documentation","title":"gRPC Documentation","text":"<ul> <li>gRPC API Reference - Complete documentation of all 46 RPC methods</li> <li>Client Patterns - Common usage patterns and best practices</li> <li>Sequence Diagrams - Visual workflows for all major operations</li> </ul>"},{"location":"grpc/overview/#tutorials-how-to-guides","title":"Tutorials &amp; How-To Guides","text":"<ul> <li>gRPC Workflow Tutorial - Hands-on tutorial with complete examples</li> <li>Remote gRPC Access - Detailed how-to guide with 5 client examples</li> </ul>"},{"location":"grpc/overview/#configuration_1","title":"Configuration","text":"<ul> <li>TrainRun Schema - Complete trainrun configuration reference</li> <li>Hydra Composition - Config composition patterns and overrides</li> </ul>"},{"location":"grpc/overview/#deployment","title":"Deployment","text":"<ul> <li>Deployment Guide - Docker, Kubernetes, and production patterns</li> </ul>"},{"location":"grpc/sequence-diagrams/","title":"Sequence diagrams","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"grpc/sequence-diagrams/#grpc-sequence-diagrams","title":"gRPC Sequence Diagrams","text":"<p>This document provides visual sequence diagrams for all major cuvis-ai gRPC workflows. All diagrams are verified against the actual Protocol Buffer definitions and production client examples.</p>"},{"location":"grpc/sequence-diagrams/#overview","title":"Overview","text":"<p>The cuvis-ai gRPC service (<code>CuvisAIService</code>) provides 46 RPC methods organized into several functional areas:</p> <ul> <li>Session Management: Create, configure, and close sessions</li> <li>Configuration: Resolve, validate, and apply Hydra configs</li> <li>Training: Statistical and gradient training with streaming progress</li> <li>Pipeline Management: Load, save, and introspect pipelines</li> <li>Inference: Run predictions on trained pipelines</li> <li>Discovery: List available pipelines and capabilities</li> </ul>"},{"location":"grpc/sequence-diagrams/#1-complete-training-workflow","title":"1. Complete Training Workflow","text":"<p>This diagram shows the full end-to-end training workflow including config resolution, two-phase training, and saving artifacts.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: CreateSession()\n  CuvisAIService--&gt;&gt;Client: CreateSessionResponse(session_id)\n\n  Client-&gt;&gt;CuvisAIService: SetSessionSearchPaths(search_paths)\n  CuvisAIService--&gt;&gt;Client: SetSessionSearchPathsResponse()\n\n  Client-&gt;&gt;CuvisAIService: ResolveConfig(config_type=\"trainrun\", path=\"deep_svdd\")\n  CuvisAIService--&gt;&gt;Client: ResolveConfigResponse(config_bytes)\n\n  Client-&gt;&gt;CuvisAIService: SetTrainRunConfig(session_id, config_bytes)\n  CuvisAIService--&gt;&gt;Client: SetTrainRunConfigResponse(success)\n\n  opt Validate Configuration\n    Client-&gt;&gt;CuvisAIService: ValidateConfig(config_type=\"training\", config_bytes)\n    CuvisAIService--&gt;&gt;Client: ValidateConfigResponse(valid, errors, warnings)\n  end\n\n  Client-&gt;&gt;CuvisAIService: Train(session_id, TRAINER_TYPE_STATISTICAL)\n\n  loop Streaming Updates\n    CuvisAIService--&gt;&gt;Client: TrainResponse(context, losses, metrics, status)\n  end\n\n  Client-&gt;&gt;CuvisAIService: SavePipeline(session_id, pipeline_path)\n  CuvisAIService--&gt;&gt;Client: SavePipelineResponse(pipeline_path, weights_path)\n\n  Client-&gt;&gt;CuvisAIService: SaveTrainRun(session_id, trainrun_path)\n  CuvisAIService--&gt;&gt;Client: SaveTrainRunResponse(trainrun_path)\n\n  Client-&gt;&gt;CuvisAIService: CloseSession(session_id)\n  CuvisAIService--&gt;&gt;Client: CloseSessionResponse(success)</code></pre> <p>Key Steps:</p> <ol> <li>CreateSession: Establish a server-side session context</li> <li>SetSessionSearchPaths: Register Hydra config search paths</li> <li>ResolveConfig: Resolve trainrun config with Hydra composition and overrides</li> <li>SetTrainRunConfig: Apply resolved config (builds pipeline, sets data/training configs)</li> <li>ValidateConfig (optional): Pre-validate training configuration</li> <li>Train: Execute training with streaming progress updates</li> <li>SavePipeline: Save trained pipeline YAML + weights (.pt file)</li> <li>SaveTrainRun: Save composed trainrun config for reproducibility</li> <li>CloseSession: Clean up session resources</li> </ol> <p>Example Client: - examples/grpc/complete_workflow_client.py</p>"},{"location":"grpc/sequence-diagrams/#2-two-phase-training-workflow","title":"2. Two-Phase Training Workflow","text":"<p>Demonstrates the two-phase training pattern used by statistical anomaly detectors (Phase 1: initialization, Phase 2: fine-tuning).</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: CreateSession()\n  CuvisAIService--&gt;&gt;Client: CreateSessionResponse(session_id)\n\n  Client-&gt;&gt;CuvisAIService: SetSessionSearchPaths(search_paths)\n  CuvisAIService--&gt;&gt;Client: SetSessionSearchPathsResponse()\n\n  Client-&gt;&gt;CuvisAIService: ResolveConfig(config_type=\"trainrun\", path=\"deep_svdd\")\n  CuvisAIService--&gt;&gt;Client: ResolveConfigResponse(config_bytes)\n\n  Client-&gt;&gt;CuvisAIService: SetTrainRunConfig(session_id, config_bytes)\n  CuvisAIService--&gt;&gt;Client: SetTrainRunConfigResponse(success)\n\n  Note over Client,CuvisAIService: Phase 1: Statistical Training (Calibration)\n  Client-&gt;&gt;CuvisAIService: Train(session_id, TRAINER_TYPE_STATISTICAL)\n  CuvisAIService--&gt;&gt;Client: TrainResponse(status=COMPLETE)\n\n  Note over Client,CuvisAIService: Phase 2: Gradient Training (Fine-tuning)\n  Client-&gt;&gt;CuvisAIService: Train(session_id, TRAINER_TYPE_GRADIENT)\n\n  loop Streaming Updates (Epochs)\n    CuvisAIService--&gt;&gt;Client: TrainResponse(losses, metrics, status=RUNNING)\n  end\n  CuvisAIService--&gt;&gt;Client: TrainResponse(status=COMPLETE)\n\n  par Save Pipeline\n    Client-&gt;&gt;CuvisAIService: SavePipeline(session_id, pipeline_path)\n    CuvisAIService--&gt;&gt;Client: SavePipelineResponse(pipeline_path, weights_path)\n  and Save TrainRun\n    Client-&gt;&gt;CuvisAIService: SaveTrainRun(session_id, trainrun_path)\n    CuvisAIService--&gt;&gt;Client: SaveTrainRunResponse(trainrun_path)\n  end\n\n  opt Test Inference\n    Client-&gt;&gt;CuvisAIService: Inference(session_id, test_inputs)\n    CuvisAIService--&gt;&gt;Client: InferenceResponse(outputs, metrics)\n  end\n\n  Client-&gt;&gt;CuvisAIService: CloseSession(session_id)\n  CuvisAIService--&gt;&gt;Client: CloseSessionResponse(success)</code></pre> <p>Training Phases:</p> <ul> <li>TRAINER_TYPE_STATISTICAL: Single-pass initialization (computes mean, covariance, centers, etc.)</li> <li>No streaming updates (completes immediately)</li> <li> <p>Initializes statistical nodes with background statistics</p> </li> <li> <p>TRAINER_TYPE_GRADIENT: Multi-epoch gradient-based training</p> </li> <li>Streams progress updates for each epoch</li> <li>Reports losses, metrics, and training context</li> </ul> <p>Example Clients: - examples/grpc/statistical_training_client.py - examples/grpc/gradient_training_client.py - examples/grpc/deepsvdd_client.py</p>"},{"location":"grpc/sequence-diagrams/#3-inference-workflow","title":"3. Inference Workflow","text":"<p>Shows how to load a trained pipeline and run inference. Supports two loading patterns: TrainRun restoration or manual pipeline loading.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: CreateSession()\n  CuvisAIService--&gt;&gt;Client: CreateSessionResponse(session_id)\n\n  Client-&gt;&gt;CuvisAIService: SetSessionSearchPaths(search_paths)\n  CuvisAIService--&gt;&gt;Client: SetSessionSearchPathsResponse()\n\n  alt Load from TrainRun (Recommended)\n    Client-&gt;&gt;CuvisAIService: RestoreTrainRun(session_id, trainrun_path)\n    CuvisAIService--&gt;&gt;Client: RestoreTrainRunResponse(success)\n  else Load Pipeline + Weights Separately\n    Client-&gt;&gt;CuvisAIService: ResolveConfig(config_type=\"pipeline\", path)\n    CuvisAIService--&gt;&gt;Client: ResolveConfigResponse(config_bytes)\n    Client-&gt;&gt;CuvisAIService: LoadPipeline(session_id, PipelineConfig)\n    CuvisAIService--&gt;&gt;Client: LoadPipelineResponse(success)\n    Client-&gt;&gt;CuvisAIService: LoadPipelineWeights(session_id, weights_path)\n    CuvisAIService--&gt;&gt;Client: LoadPipelineWeightsResponse(success)\n  end\n\n  loop Multiple Inferences\n    Client-&gt;&gt;CuvisAIService: Inference(session_id, InputBatch)\n    CuvisAIService--&gt;&gt;Client: InferenceResponse(outputs, metrics)\n  end\n\n  Client-&gt;&gt;CuvisAIService: CloseSession(session_id)\n  CuvisAIService--&gt;&gt;Client: CloseSessionResponse(success)</code></pre> <p>Loading Options:</p> <ol> <li>RestoreTrainRun (Recommended):</li> <li>Loads pipeline config, weights, data config, and training config from saved TrainRun</li> <li>Single call, ensures exact reproducibility</li> <li> <p>Example: examples/grpc/restore_trainrun_grpc.py</p> </li> <li> <p>Manual Loading:</p> </li> <li>Step 1: <code>ResolveConfig</code> or provide raw pipeline config</li> <li>Step 2: <code>LoadPipeline</code> builds the pipeline graph</li> <li>Step 3: <code>LoadPipelineWeights</code> loads trained weights</li> <li>Example: examples/grpc/inference_with_pretrained_client.py</li> </ol> <p>InputBatch Fields: <pre><code>inputs=cuvis_ai_pb2.InputBatch(\n    cube=...,              # Hyperspectral cube (B, H, W, C)\n    wavelengths=...,       # Wavelength array (B, C)\n    mask=...,              # Optional ground truth mask\n    bboxes=...,            # Optional bounding boxes\n    points=...,            # Optional point prompts\n    text_prompt=...,       # Optional text prompt\n    extra_inputs={}        # Optional custom inputs\n)\n</code></pre></p>"},{"location":"grpc/sequence-diagrams/#4-pipeline-introspection","title":"4. Pipeline Introspection","text":"<p>Inspect pipeline structure, inputs, outputs, and visualize the computation graph.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: CreateSession()\n  CuvisAIService--&gt;&gt;Client: CreateSessionResponse(session_id)\n\n  Client-&gt;&gt;CuvisAIService: SetSessionSearchPaths(search_paths)\n  CuvisAIService--&gt;&gt;Client: SetSessionSearchPathsResponse()\n\n  Client-&gt;&gt;CuvisAIService: ResolveConfig(config_type=\"pipeline\", path=\"rx_statistical\")\n  CuvisAIService--&gt;&gt;Client: ResolveConfigResponse(config_bytes)\n\n  Client-&gt;&gt;CuvisAIService: LoadPipeline(session_id, PipelineConfig)\n  CuvisAIService--&gt;&gt;Client: LoadPipelineResponse(success)\n\n  Client-&gt;&gt;CuvisAIService: LoadPipelineWeights(session_id, weights_path)\n  CuvisAIService--&gt;&gt;Client: LoadPipelineWeightsResponse(success)\n\n  Client-&gt;&gt;CuvisAIService: GetPipelineInputs(session_id)\n  CuvisAIService--&gt;&gt;Client: GetPipelineInputsResponse(input_names, input_specs)\n\n  Client-&gt;&gt;CuvisAIService: GetPipelineOutputs(session_id)\n  CuvisAIService--&gt;&gt;Client: GetPipelineOutputsResponse(output_names, output_specs)\n\n  Client-&gt;&gt;CuvisAIService: GetPipelineVisualization(session_id, format=\"png\")\n  CuvisAIService--&gt;&gt;Client: GetPipelineVisualizationResponse(image_data)\n\n  Client-&gt;&gt;CuvisAIService: CloseSession(session_id)\n  CuvisAIService--&gt;&gt;Client: CloseSessionResponse(success)</code></pre> <p>Introspection Methods:</p> <ul> <li>GetPipelineInputs: Returns list of input port names and their specifications (shape, dtype, required flag)</li> <li>GetPipelineOutputs: Returns list of output port names and their specifications</li> <li>GetPipelineVisualization: Generates pipeline graph visualization (PNG or SVG format)</li> </ul> <p>TensorSpec Format: <pre><code>spec = TensorSpec(\n    name=\"cube\",\n    shape=[1, -1, -1, 61],  # -1 indicates dynamic dimension\n    dtype=DType.FLOAT32,\n    required=True\n)\n</code></pre></p> <p>Example Client: - examples/grpc/introspection_client.py</p>"},{"location":"grpc/sequence-diagrams/#5-pipeline-discovery-capabilities","title":"5. Pipeline Discovery &amp; Capabilities","text":"<p>Discover available pipelines and query training capabilities before creating a session.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: ListAvailablePipelinees()\n  CuvisAIService--&gt;&gt;Client: ListAvailablePipelineesResponse(pipelines[])\n\n  loop For Each Pipeline\n    Client-&gt;&gt;CuvisAIService: GetPipelineInfo(pipeline_name)\n    CuvisAIService--&gt;&gt;Client: GetPipelineInfoResponse(metadata, requirements)\n  end\n\n  Client-&gt;&gt;CuvisAIService: GetTrainingCapabilities()\n  CuvisAIService--&gt;&gt;Client: GetTrainingCapabilitiesResponse(optimizers, schedulers, callbacks)\n\n  opt Validate Config Before Training\n    Client-&gt;&gt;CuvisAIService: ValidateConfig(config_type, config_bytes)\n    CuvisAIService--&gt;&gt;Client: ValidateConfigResponse(valid, errors, warnings)\n  end</code></pre> <p>Discovery Methods:</p> <ol> <li>ListAvailablePipelinees: Returns list of all registered pipeline types</li> <li>GetPipelineInfo: Returns metadata, description, and requirements for a specific pipeline</li> <li>GetTrainingCapabilities: Returns supported optimizers, schedulers, and callbacks with parameter schemas</li> <li>ValidateConfig: Pre-validates configuration before applying to session</li> </ol> <p>Capabilities Response Example: <pre><code>GetTrainingCapabilitiesResponse(\n    supported_optimizers=[\"Adam\", \"SGD\", \"AdamW\"],\n    supported_schedulers=[\"CosineAnnealingLR\", \"StepLR\", \"ReduceLROnPlateau\"],\n    supported_callbacks=[\n        CallbackInfo(\n            type=\"EarlyStopping\",\n            description=\"Stop training when metric stops improving\",\n            parameters=[\n                ParameterInfo(name=\"patience\", type=\"int\", required=True),\n                ParameterInfo(name=\"min_delta\", type=\"float\", required=False, default_value=\"0.0\")\n            ]\n        )\n    ]\n)\n</code></pre></p> <p>Example Clients: - examples/grpc/pipeline_discovery_client.py - examples/grpc/capabilities_client.py</p>"},{"location":"grpc/sequence-diagrams/#6-resume-training-from-checkpoint","title":"6. Resume Training from Checkpoint","text":"<p>Resume training from a previously saved TrainRun checkpoint.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: CreateSession()\n  CuvisAIService--&gt;&gt;Client: CreateSessionResponse(session_id)\n\n  Client-&gt;&gt;CuvisAIService: SetSessionSearchPaths(search_paths)\n  CuvisAIService--&gt;&gt;Client: SetSessionSearchPathsResponse()\n\n  Note over Client,CuvisAIService: Restore from checkpoint\n  Client-&gt;&gt;CuvisAIService: RestoreTrainRun(session_id, trainrun_path)\n  CuvisAIService--&gt;&gt;Client: RestoreTrainRunResponse(success)\n\n  Note over Client,CuvisAIService: Continue training\n  Client-&gt;&gt;CuvisAIService: Train(session_id, TRAINER_TYPE_GRADIENT)\n\n  loop Streaming Updates\n    CuvisAIService--&gt;&gt;Client: TrainResponse(context, losses, metrics, status)\n  end\n\n  Client-&gt;&gt;CuvisAIService: SavePipeline(session_id, pipeline_path)\n  CuvisAIService--&gt;&gt;Client: SavePipelineResponse(pipeline_path, weights_path)\n\n  Client-&gt;&gt;CuvisAIService: SaveTrainRun(session_id, trainrun_path)\n  CuvisAIService--&gt;&gt;Client: SaveTrainRunResponse(trainrun_path)\n\n  Client-&gt;&gt;CuvisAIService: CloseSession(session_id)\n  CuvisAIService--&gt;&gt;Client: CloseSessionResponse(success)</code></pre> <p>Resume Training Notes:</p> <ul> <li><code>RestoreTrainRun</code> loads:</li> <li>Pipeline configuration and weights</li> <li>Data configuration (datasets, dataloaders)</li> <li> <p>Training configuration (optimizer state, scheduler state, epoch counter)</p> </li> <li> <p>Training resumes from the saved epoch count</p> </li> <li>Optimizer and scheduler states are preserved</li> </ul> <p>Example Client: - examples/grpc/resume_training_client.py</p>"},{"location":"grpc/sequence-diagrams/#7-batch-inference-pattern","title":"7. Batch Inference Pattern","text":"<p>Efficient batch processing for multiple samples.</p> <pre><code>sequenceDiagram\n  participant Client\n  participant CuvisAIService\n\n  Client-&gt;&gt;CuvisAIService: CreateSession()\n  CuvisAIService--&gt;&gt;Client: CreateSessionResponse(session_id)\n\n  Client-&gt;&gt;CuvisAIService: RestoreTrainRun(session_id, trainrun_path)\n  CuvisAIService--&gt;&gt;Client: RestoreTrainRunResponse(success)\n\n  Note over Client,CuvisAIService: Process multiple batches\n  loop For Each Batch\n    Client-&gt;&gt;CuvisAIService: Inference(session_id, InputBatch)\n    Note right of CuvisAIService: Batch size: B samples&lt;br/&gt;Processed in parallel\n    CuvisAIService--&gt;&gt;Client: InferenceResponse(outputs[B], metrics)\n  end\n\n  Client-&gt;&gt;CuvisAIService: CloseSession(session_id)\n  CuvisAIService--&gt;&gt;Client: CloseSessionResponse(success)</code></pre> <p>Batch Processing Tips:</p> <ul> <li>Use appropriate batch size based on GPU memory (typically 4-32)</li> <li>InputBatch supports batched inputs: <code>cube.shape = (B, H, W, C)</code></li> <li>All outputs are returned in batched format</li> <li>Server processes batch in parallel when possible</li> </ul> <p>Example Client: - examples/grpc/run_inference.py</p>"},{"location":"grpc/sequence-diagrams/#common-patterns","title":"Common Patterns","text":""},{"location":"grpc/sequence-diagrams/#error-handling","title":"Error Handling","text":"<p>All RPC methods can raise gRPC exceptions. Handle them appropriately:</p> <pre><code>import grpc\n\ntry:\n    response = stub.Train(request)\nexcept grpc.RpcError as e:\n    if e.code() == grpc.StatusCode.INVALID_ARGUMENT:\n        print(f\"Invalid configuration: {e.details()}\")\n    elif e.code() == grpc.StatusCode.NOT_FOUND:\n        print(f\"Session not found: {e.details()}\")\n    elif e.code() == grpc.StatusCode.INTERNAL:\n        print(f\"Server error: {e.details()}\")\n    else:\n        raise\n</code></pre>"},{"location":"grpc/sequence-diagrams/#streaming-progress-updates","title":"Streaming Progress Updates","text":"<p>The <code>Train</code> RPC uses server streaming to provide real-time progress:</p> <pre><code>for progress in stub.Train(request):\n    print(f\"Epoch {progress.context.epoch}, \"\n          f\"Step {progress.context.global_step}, \"\n          f\"Loss: {progress.losses.get('total_loss', 0.0):.4f}\")\n\n    if progress.status == cuvis_ai_pb2.TRAIN_STATUS_ERROR:\n        print(f\"Training failed: {progress.message}\")\n        break\n</code></pre>"},{"location":"grpc/sequence-diagrams/#session-lifecycle-best-practices","title":"Session Lifecycle Best Practices","text":"<ol> <li>Always close sessions: Use try/finally or context managers</li> <li>Set search paths early: Call <code>SetSessionSearchPaths</code> immediately after <code>CreateSession</code></li> <li>Validate configs: Use <code>ValidateConfig</code> before applying to catch errors early</li> <li>Save checkpoints: Periodically save TrainRuns during long training runs</li> </ol>"},{"location":"grpc/sequence-diagrams/#see-also","title":"See Also","text":""},{"location":"grpc/sequence-diagrams/#grpc-documentation","title":"gRPC Documentation","text":"<ul> <li>gRPC Overview - Architecture, quick start, and core concepts</li> <li>API Reference - Complete RPC method documentation</li> <li>Client Patterns - Common usage patterns and best practices</li> </ul>"},{"location":"grpc/sequence-diagrams/#tutorials-guides","title":"Tutorials &amp; Guides","text":"<ul> <li>gRPC Tutorial - End-to-end hands-on tutorial</li> <li>How-To: Remote gRPC Access - Detailed deployment guide</li> <li>gRPC Client Examples - All example clients</li> </ul>"},{"location":"grpc/sequence-diagrams/#verification-status","title":"Verification Status","text":"<p>\u2705 All diagrams verified against: - Protocol Buffer definitions in <code>cuvis_ai_core.proto</code> - Production client examples in <code>examples/grpc/</code> - 46 RPC methods in <code>CuvisAIService</code></p> <p>Last Updated: 2026-02-04</p>"},{"location":"how-to/","title":"How-To Guides","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/#how-to-guides","title":"How-To Guides","text":"<p>Practical guides for common tasks and workflows in CUVIS.AI.</p>"},{"location":"how-to/#available-guides","title":"Available Guides","text":"<ul> <li> <p> Build Pipeline (Python)</p> <p>Create pipelines programmatically using Python API</p> </li> <li> <p> Build Pipeline (YAML)</p> <p>Define pipelines declaratively using YAML configuration</p> </li> <li> <p> Restore Pipeline</p> <p>Load and restore trained pipelines from checkpoints</p> </li> <li> <p> Add Built-in Node</p> <p>Extend the framework with custom built-in nodes</p> </li> <li> <p> Monitoring &amp; Visualization</p> <p>Track training progress and visualize results</p> </li> <li> <p>:material-gpu: GPU Acceleration</p> <p>Configure and optimize GPU usage for training</p> </li> <li> <p> Remote gRPC</p> <p>Deploy and access pipelines remotely via gRPC</p> </li> </ul>"},{"location":"how-to/add-builtin-node/","title":"Add Built-in Node","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/add-builtin-node/#how-to-add-built-in-nodes-to-cuvisai","title":"How-To: Add Built-in Nodes to CUVIS.AI","text":""},{"location":"how-to/add-builtin-node/#overview","title":"Overview","text":"<p>Learn how to create custom nodes and integrate them into the CUVIS.AI framework. This guide covers node architecture, implementation patterns, testing, and documentation requirements.</p>"},{"location":"how-to/add-builtin-node/#prerequisites","title":"Prerequisites","text":"<ul> <li>cuvis-ai development environment set up</li> <li>Understanding of Node System Deep Dive</li> <li>Familiarity with PyTorch <code>nn.Module</code></li> <li>Python type hints knowledge</li> </ul>"},{"location":"how-to/add-builtin-node/#node-architecture-overview","title":"Node Architecture Overview","text":"<p>All CUVIS.AI nodes inherit from the <code>Node</code> base class, which provides:</p> <ul> <li>Port system: Typed input/output connections</li> <li>Serialization: Automatic config saving/loading</li> <li>Lifecycle hooks: Statistical initialization, freezing/unfreezing</li> <li>PyTorch integration: Full <code>nn.Module</code> compatibility</li> </ul> <pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nimport torch\nimport torch.nn as nn\n\nclass MyNode(Node):\n    INPUT_SPECS = {...}   # Define input ports\n    OUTPUT_SPECS = {...}  # Define output ports\n\n    def __init__(self, ...):\n        # Initialization logic\n        pass\n\n    def forward(self, **inputs) -&gt; dict[str, torch.Tensor]:\n        # Processing logic\n        pass\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-1-define-port-specifications","title":"Step 1: Define Port Specifications","text":"<p>Ports define the data contracts for your node.</p>"},{"location":"how-to/add-builtin-node/#basic-port-definition","title":"Basic Port Definition","text":"<pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nimport torch\n\nclass ThresholdFilter(Node):\n    \"\"\"Filters values below a threshold.\"\"\"\n\n    INPUT_SPECS = {\n        \"data\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),  # BHWC format\n            description=\"Input tensor to filter\"\n        ),\n        \"threshold\": PortSpec(\n            dtype=torch.float32,\n            shape=(),\n            description=\"Threshold value (optional at runtime)\",\n            optional=True\n        )\n    }\n\n    OUTPUT_SPECS = {\n        \"filtered\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),\n            description=\"Filtered output\"\n        ),\n        \"mask\": PortSpec(\n            dtype=torch.bool,\n            shape=(-1, -1, -1, -1),\n            description=\"Binary mask of filtered values\"\n        )\n    }\n</code></pre>"},{"location":"how-to/add-builtin-node/#port-specification-guidelines","title":"Port Specification Guidelines","text":"<p>Shape Dimensions: - <code>-1</code>: Dynamic dimension (batch size, spatial dimensions) - <code>()</code>: Scalar value - <code>(C,)</code>: Fixed-size vector (C channels) - <code>(-1, -1, -1, C)</code>: BHWC format with fixed channels</p> <p>Data Types: - <code>torch.float32</code>: Standard floating point - <code>torch.int32</code>: Integer labels/indices - <code>torch.bool</code>: Binary masks - <code>np.int32</code>: NumPy arrays (e.g., wavelengths)</p> <p>Optional Ports: - <code>optional=True</code>: Connection not required - Useful for conditional inputs or auxiliary outputs</p>"},{"location":"how-to/add-builtin-node/#step-2-implement-initialization","title":"Step 2: Implement Initialization","text":"<p>The <code>__init__</code> method sets up node parameters and registers buffers.</p>"},{"location":"how-to/add-builtin-node/#simple-stateless-node","title":"Simple Stateless Node","text":"<pre><code>class ThresholdFilter(Node):\n    \"\"\"Filters values below a threshold.\"\"\"\n\n    INPUT_SPECS = {...}\n    OUTPUT_SPECS = {...}\n\n    def __init__(\n        self,\n        default_threshold: float = 0.5,\n        invert: bool = False,\n        **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Initialize threshold filter.\n\n        Parameters\n        ----------\n        default_threshold : float, optional\n            Default threshold value. Default is 0.5.\n        invert : bool, optional\n            If True, keep values below threshold. Default is False.\n        \"\"\"\n        self.default_threshold = default_threshold\n        self.invert = invert\n\n        # IMPORTANT: Pass parameters to super().__init__\n        # This enables automatic serialization\n        super().__init__(\n            default_threshold=default_threshold,\n            invert=invert,\n            **kwargs\n        )\n</code></pre>"},{"location":"how-to/add-builtin-node/#stateful-node-with-buffers","title":"Stateful Node with Buffers","text":"<pre><code>class AdaptiveThresholdFilter(Node):\n    \"\"\"Filters using learned adaptive threshold.\"\"\"\n\n    INPUT_SPECS = {...}\n    OUTPUT_SPECS = {...}\n\n    def __init__(\n        self,\n        num_channels: int,\n        init_threshold: float = 0.5,\n        **kwargs\n    ) -&gt; None:\n        \"\"\"Initialize with per-channel thresholds.\"\"\"\n        self.num_channels = num_channels\n        self.init_threshold = init_threshold\n\n        super().__init__(\n            num_channels=num_channels,\n            init_threshold=init_threshold,\n            **kwargs\n        )\n\n        # Register buffers (frozen, non-trainable)\n        self.register_buffer(\n            \"thresholds\",\n            torch.full((num_channels,), init_threshold)\n        )\n\n        # Track initialization state\n        self._statistically_initialized = False\n</code></pre>"},{"location":"how-to/add-builtin-node/#trainable-node-with-parameters","title":"Trainable Node with Parameters","text":"<pre><code>class LearnableFilter(Node):\n    \"\"\"Filters using learnable neural network.\"\"\"\n\n    INPUT_SPECS = {...}\n    OUTPUT_SPECS = {...}\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 64,\n        **kwargs\n    ) -&gt; None:\n        \"\"\"Initialize learnable filter network.\"\"\"\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        super().__init__(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            **kwargs\n        )\n\n        # Define neural network layers\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-3-implement-forward-method","title":"Step 3: Implement Forward Method","text":"<p>The <code>forward</code> method defines node processing logic.</p>"},{"location":"how-to/add-builtin-node/#basic-forward-implementation","title":"Basic Forward Implementation","text":"<pre><code>def forward(\n    self,\n    data: torch.Tensor,\n    threshold: torch.Tensor | None = None,\n    **_\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Filter data based on threshold.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input data in BHWC format\n    threshold : torch.Tensor, optional\n        Override default threshold\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Dictionary with \"filtered\" and \"mask\" keys\n    \"\"\"\n    # Use provided threshold or default\n    thresh = threshold if threshold is not None else self.default_threshold\n\n    # Create mask\n    if self.invert:\n        mask = data &lt; thresh\n    else:\n        mask = data &gt;= thresh\n\n    # Apply filter\n    filtered = data * mask.float()\n\n    return {\n        \"filtered\": filtered,\n        \"mask\": mask\n    }\n</code></pre>"},{"location":"how-to/add-builtin-node/#forward-with-error-handling","title":"Forward with Error Handling","text":"<pre><code>def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Process data with adaptive thresholds.\"\"\"\n    # Validation\n    if not self._statistically_initialized:\n        raise RuntimeError(\n            f\"{self.__class__.__name__} requires statistical_initialization() \"\n            \"before processing. Call node.statistical_initialization(data_stream) first.\"\n        )\n\n    B, H, W, C = data.shape\n    if C != self.num_channels:\n        raise ValueError(\n            f\"Expected {self.num_channels} channels, got {C}. \"\n            f\"Initialize node with correct num_channels parameter.\"\n        )\n\n    # Process with per-channel thresholds\n    # Shape broadcasting: (B,H,W,C) &gt;= (C,) \u2192 (B,H,W,C)\n    mask = data &gt;= self.thresholds.view(1, 1, 1, -1)\n    filtered = data * mask.float()\n\n    return {\"filtered\": filtered, \"mask\": mask}\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-4-add-statistical-initialization-optional","title":"Step 4: Add Statistical Initialization (Optional)","text":"<p>For nodes that learn parameters from data.</p>"},{"location":"how-to/add-builtin-node/#basic-statistical-initialization","title":"Basic Statistical Initialization","text":"<pre><code>def statistical_initialization(self, input_stream) -&gt; None:\n    \"\"\"\n    Learn adaptive thresholds from initialization data.\n\n    Parameters\n    ----------\n    input_stream : iterable\n        Iterator yielding batches with \"data\" key\n    \"\"\"\n    # Accumulate statistics\n    channel_sums = torch.zeros(self.num_channels)\n    channel_counts = torch.zeros(self.num_channels)\n\n    for batch in input_stream:\n        data = batch[\"data\"]  # Shape: (B, H, W, C)\n        B, H, W, C = data.shape\n\n        # Accumulate per-channel statistics\n        channel_sums += data.sum(dim=(0, 1, 2))\n        channel_counts += B * H * W\n\n    # Compute mean as threshold\n    channel_means = channel_sums / channel_counts\n\n    # Update buffers\n    self.thresholds.copy_(channel_means)\n    self._statistically_initialized = True\n</code></pre>"},{"location":"how-to/add-builtin-node/#advanced-welfords-online-algorithm","title":"Advanced: Welford's Online Algorithm","text":"<p>For numerically stable mean/variance computation (reference):</p> <pre><code>def statistical_initialization(self, input_stream) -&gt; None:\n    \"\"\"Initialize using Welford's algorithm for numerical stability.\"\"\"\n    count = 0\n    mean = torch.zeros(self.num_channels)\n    M2 = torch.zeros(self.num_channels)  # Sum of squared differences\n\n    for batch in input_stream:\n        data = batch[\"data\"]\n        B, H, W, C = data.shape\n\n        # Flatten spatial dimensions\n        flat_data = data.reshape(-1, C)  # (B*H*W, C)\n\n        for value in flat_data:\n            count += 1\n            delta = value - mean\n            mean += delta / count\n            delta2 = value - mean\n            M2 += delta * delta2\n\n    # Compute variance and std\n    variance = M2 / count\n    std = torch.sqrt(variance + 1e-8)\n\n    # Set threshold as mean + 2*std\n    self.thresholds.copy_(mean + 2 * std)\n    self._statistically_initialized = True\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-5-add-unfreeze-method-optional","title":"Step 5: Add Unfreeze Method (Optional)","text":"<p>For nodes that transition from frozen statistics to trainable parameters.</p>"},{"location":"how-to/add-builtin-node/#two-phase-training-pattern","title":"Two-Phase Training Pattern","text":"<pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"\n    Convert frozen buffers to trainable parameters.\n\n    Call after statistical_initialization() to enable gradient training.\n    \"\"\"\n    if not self._statistically_initialized:\n        raise RuntimeError(\n            \"Must call statistical_initialization() before unfreeze()\"\n        )\n\n    # Convert buffer to parameter\n    if isinstance(self.thresholds, torch.Tensor) and not isinstance(self.thresholds, nn.Parameter):\n        self.thresholds = nn.Parameter(self.thresholds.clone())\n\n    # Call parent unfreeze (handles other components)\n    super().unfreeze()\n</code></pre>"},{"location":"how-to/add-builtin-node/#usage-pattern","title":"Usage Pattern","text":"<p>For testing/development (direct method call):</p> <pre><code># Phase 1: Statistical initialization\nnode = AdaptiveThresholdFilter(num_channels=61)\nnode.statistical_initialization(initialization_data)  # Direct call for testing\n\n# At this point, thresholds are frozen buffers\nprint(node.thresholds.requires_grad)  # False\n\n# Phase 2: Enable gradient training\nnode.unfreeze()\nprint(node.thresholds.requires_grad)  # True\n\n# Now can train with backpropagation\noptimizer = torch.optim.Adam(node.parameters(), lr=0.001)\n</code></pre> <p>In production pipelines (recommended):</p> <pre><code>from cuvis_ai_core.training import StatisticalTrainer\n\n# Add node to pipeline\nnode = AdaptiveThresholdFilter(num_channels=61)\npipeline.add_node(node)\n\n# Phase 1: Statistical initialization\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes all nodes that need it\n\n# Phase 2: Enable gradient training\nnode.unfreeze()\ngrad_trainer = GradientTrainer(pipeline=pipeline, datamodule=datamodule, ...)\ngrad_trainer.fit()\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-6-add-documentation","title":"Step 6: Add Documentation","text":"<p>Follow NumPy docstring style for consistency with SciPy/scikit-learn.</p>"},{"location":"how-to/add-builtin-node/#complete-docstring-example","title":"Complete Docstring Example","text":"<pre><code>class AdaptiveThresholdFilter(Node):\n    \"\"\"\n    Adaptive threshold filter with per-channel learned thresholds.\n\n    This node learns optimal threshold values from training data and applies\n    them independently to each spectral channel. Useful for hyperspectral\n    anomaly detection where different wavelengths have different baseline\n    intensities.\n\n    Parameters\n    ----------\n    num_channels : int\n        Number of spectral channels in input data\n    init_threshold : float, optional\n        Initial threshold value for all channels. Default is 0.5.\n\n    Attributes\n    ----------\n    thresholds : torch.Tensor\n        Per-channel threshold values (num_channels,)\n\n    Raises\n    ------\n    RuntimeError\n        If forward() called before statistical_initialization()\n    ValueError\n        If input channel count doesn't match num_channels\n\n    See Also\n    --------\n    ThresholdFilter : Fixed threshold filtering\n    MinMaxNormalizer : Normalization before thresholding\n\n    Notes\n    -----\n    The statistical initialization computes per-channel means and sets\n    thresholds to mean + 2*std to capture ~95% of normal variation.\n\n    Memory complexity: O(C) where C is num_channels\n    Time complexity: O(N*C) for initialization, O(B*H*W*C) for forward pass\n\n    Examples\n    --------\n    Statistical initialization:\n\n    &gt;&gt;&gt; node = AdaptiveThresholdFilter(num_channels=61)\n    &gt;&gt;&gt; node.statistical_initialization(initialization_data)\n    &gt;&gt;&gt; result = node.forward(data=test_tensor)\n    &gt;&gt;&gt; result[\"filtered\"].shape\n    torch.Size([1, 256, 256, 61])\n\n    Gradient-based training:\n\n    &gt;&gt;&gt; node = AdaptiveThresholdFilter(num_channels=61)\n    &gt;&gt;&gt; node.statistical_initialization(initialization_data)\n    &gt;&gt;&gt; node.unfreeze()  # Enable gradient descent\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; optimizer = torch.optim.Adam(node.parameters(), lr=0.001)\n    &gt;&gt;&gt; for epoch in range(50):\n    ...     result = node.forward(data=train_data)\n    ...     loss = criterion(result[\"filtered\"], targets)\n    ...     loss.backward()\n    ...     optimizer.step()\n    \"\"\"\n\n    INPUT_SPECS = {...}\n    OUTPUT_SPECS = {...}\n\n    # Implementation...\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-7-register-node","title":"Step 7: Register Node","text":"<p>Make your node discoverable by the framework.</p>"},{"location":"how-to/add-builtin-node/#decorator-registration","title":"Decorator Registration","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n@NodeRegistry.register\nclass AdaptiveThresholdFilter(Node):\n    \"\"\"Your node implementation.\"\"\"\n    pass\n</code></pre>"},{"location":"how-to/add-builtin-node/#manual-registration","title":"Manual Registration","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai.node.filters import AdaptiveThresholdFilter\n\n# Register after class definition\nNodeRegistry.register(AdaptiveThresholdFilter)\n\n# Verify registration\nnode_class = NodeRegistry.get(\"AdaptiveThresholdFilter\")\nassert node_class is AdaptiveThresholdFilter\n</code></pre>"},{"location":"how-to/add-builtin-node/#list-available-nodes","title":"List Available Nodes","text":"<pre><code># List all built-in nodes\nall_nodes = NodeRegistry.list_builtin_nodes()\nprint(f\"Available nodes: {len(all_nodes)}\")\nfor name in sorted(all_nodes.keys()):\n    print(f\"  - {name}\")\n</code></pre>"},{"location":"how-to/add-builtin-node/#step-8-write-tests","title":"Step 8: Write Tests","text":"<p>Comprehensive testing ensures node reliability.</p>"},{"location":"how-to/add-builtin-node/#basic-test-structure","title":"Basic Test Structure","text":"<pre><code>import pytest\nimport torch\nfrom cuvis_ai.node.filters import AdaptiveThresholdFilter\n\nclass TestAdaptiveThresholdFilter:\n    \"\"\"Test suite for AdaptiveThresholdFilter node.\"\"\"\n\n    def test_creation(self):\n        \"\"\"Test node can be created with valid parameters.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=61, init_threshold=0.5)\n        assert node.num_channels == 61\n        assert node.init_threshold == 0.5\n        assert not node._statistically_initialized\n\n    def test_forward_requires_initialization(self):\n        \"\"\"Test forward raises error before initialization.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=61)\n        data = torch.randn(1, 10, 10, 61)\n\n        with pytest.raises(RuntimeError, match=\"requires statistical_initialization\"):\n            node.forward(data=data)\n\n    def test_statistical_initialization(self):\n        \"\"\"Test statistical initialization from data.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=3)\n\n        # Create initialization data\n        data_stream = [\n            {\"data\": torch.tensor([[[[1.0, 2.0, 3.0]]]])},\n            {\"data\": torch.tensor([[[[2.0, 3.0, 4.0]]]])},\n            {\"data\": torch.tensor([[[[3.0, 4.0, 5.0]]]])},\n        ]\n\n        node.statistical_initialization(data_stream)\n\n        assert node._statistically_initialized\n        assert node.thresholds.shape == (3,)\n        # Thresholds should be close to means [2.0, 3.0, 4.0]\n        assert torch.allclose(node.thresholds, torch.tensor([2.0, 3.0, 4.0]), atol=0.5)\n\n    def test_forward_after_initialization(self):\n        \"\"\"Test forward pass after initialization.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=3)\n\n        # Initialize\n        data_stream = [{\"data\": torch.tensor([[[[1.0, 2.0, 3.0]]]]))}]\n        node.statistical_initialization(data_stream)\n\n        # Forward pass\n        test_data = torch.tensor([[[[0.5, 2.0, 4.0]]]])\n        result = node.forward(data=test_data)\n\n        assert \"filtered\" in result\n        assert \"mask\" in result\n        assert result[\"filtered\"].shape == (1, 1, 1, 3)\n        assert result[\"mask\"].dtype == torch.bool\n\n    def test_unfreeze_conversion(self):\n        \"\"\"Test buffer to parameter conversion.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=3)\n\n        # Initialize (creates buffer)\n        data_stream = [{\"data\": torch.ones(1, 1, 1, 3)}]\n        node.statistical_initialization(data_stream)\n        assert not node.thresholds.requires_grad\n\n        # Unfreeze (converts to parameter)\n        node.unfreeze()\n        assert node.thresholds.requires_grad\n        assert isinstance(node.thresholds, torch.nn.Parameter)\n\n    def test_channel_mismatch_error(self):\n        \"\"\"Test error on channel count mismatch.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=61)\n        data_stream = [{\"data\": torch.ones(1, 10, 10, 61)}]\n        node.statistical_initialization(data_stream)\n\n        # Wrong number of channels\n        wrong_data = torch.ones(1, 10, 10, 30)\n        with pytest.raises(ValueError, match=\"Expected 61 channels, got 30\"):\n            node.forward(data=wrong_data)\n\n    def test_serialization(self):\n        \"\"\"Test node can be serialized and deserialized.\"\"\"\n        node = AdaptiveThresholdFilter(num_channels=61, init_threshold=0.7)\n\n        # Serialize to dict\n        config = node.serialize()\n        assert config[\"class\"] == \"AdaptiveThresholdFilter\"\n        assert config[\"params\"][\"num_channels\"] == 61\n        assert config[\"params\"][\"init_threshold\"] == 0.7\n\n        # Deserialize\n        from cuvis_ai_core.node import Node\n        restored = Node.from_config(config)\n        assert isinstance(restored, AdaptiveThresholdFilter)\n        assert restored.num_channels == 61\n</code></pre>"},{"location":"how-to/add-builtin-node/#test-coverage-checklist","title":"Test Coverage Checklist","text":"<ul> <li> Node creation with various parameter combinations</li> <li> Forward pass with valid inputs</li> <li> Forward pass with optional inputs</li> <li> Error handling (wrong shapes, uninitialized, invalid values)</li> <li> Statistical initialization (if applicable)</li> <li> Unfreeze behavior (if applicable)</li> <li> Serialization and deserialization</li> <li> Integration with pipeline</li> </ul>"},{"location":"how-to/add-builtin-node/#node-implementation-patterns","title":"Node Implementation Patterns","text":""},{"location":"how-to/add-builtin-node/#pattern-1-stateless-transformation","title":"Pattern 1: Stateless Transformation","text":"<p>For simple deterministic transforms:</p> <pre><code>class SquareTransform(Node):\n    \"\"\"Squares all input values.\"\"\"\n\n    INPUT_SPECS = {\"data\": PortSpec(torch.float32, (-1, -1, -1, -1))}\n    OUTPUT_SPECS = {\"squared\": PortSpec(torch.float32, (-1, -1, -1, -1))}\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self._statistically_initialized = True  # No initialization needed\n\n    def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n        return {\"squared\": data ** 2}\n</code></pre>"},{"location":"how-to/add-builtin-node/#pattern-2-statistical-node-frozen","title":"Pattern 2: Statistical Node (Frozen)","text":"<p>For nodes with learned statistics:</p> <pre><code>class ZScoreNormalizer(Node):\n    \"\"\"Normalizes using learned mean and std.\"\"\"\n\n    def __init__(self, num_channels: int, eps: float = 1e-8, **kwargs):\n        self.num_channels = num_channels\n        self.eps = eps\n        super().__init__(num_channels=num_channels, eps=eps, **kwargs)\n\n        self.register_buffer(\"mean\", torch.zeros(num_channels))\n        self.register_buffer(\"std\", torch.ones(num_channels))\n        self._statistically_initialized = False\n\n    def statistical_initialization(self, input_stream) -&gt; None:\n        # Compute mean and std from data\n        ...\n        self._statistically_initialized = True\n\n    def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n        if not self._statistically_initialized:\n            raise RuntimeError(\"Call statistical_initialization() first\")\n        normalized = (data - self.mean) / (self.std + self.eps)\n        return {\"normalized\": normalized}\n</code></pre>"},{"location":"how-to/add-builtin-node/#pattern-3-two-phase-trainable","title":"Pattern 3: Two-Phase Trainable","text":"<p>For nodes with statistical init + gradient training:</p> <pre><code>class AdaptiveScaler(Node):\n    \"\"\"Learns adaptive scaling factors.\"\"\"\n\n    def __init__(self, num_channels: int, **kwargs):\n        self.num_channels = num_channels\n        super().__init__(num_channels=num_channels, **kwargs)\n\n        self.register_buffer(\"scale_factors\", torch.ones(num_channels))\n        self._statistically_initialized = False\n\n    def statistical_initialization(self, input_stream) -&gt; None:\n        # Initialize scale_factors from data statistics\n        ...\n        self._statistically_initialized = True\n\n    def unfreeze(self) -&gt; None:\n        # Convert buffer to parameter\n        self.scale_factors = nn.Parameter(self.scale_factors.clone())\n        super().unfreeze()\n\n    def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n        scaled = data * self.scale_factors.view(1, 1, 1, -1)\n        return {\"scaled\": scaled}\n</code></pre>"},{"location":"how-to/add-builtin-node/#pattern-4-deep-learning-node","title":"Pattern 4: Deep Learning Node","text":"<p>For neural network-based nodes:</p> <pre><code>class CNNFeatureExtractor(Node):\n    \"\"\"Extracts features using CNN.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_dims: list[int] = [64, 32],\n        **kwargs\n    ):\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_dims = hidden_dims\n\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            hidden_dims=hidden_dims,\n            **kwargs\n        )\n\n        # Build CNN layers\n        layers = []\n        prev_dim = in_channels\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Conv2d(prev_dim, hidden_dim, kernel_size=3, padding=1),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU()\n            ])\n            prev_dim = hidden_dim\n\n        layers.append(nn.Conv2d(prev_dim, out_channels, kernel_size=1))\n        self.encoder = nn.Sequential(*layers)\n\n        # No statistical initialization needed\n        self._statistically_initialized = True\n\n    def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n        # BHWC \u2192 BCHW\n        data_chw = data.permute(0, 3, 1, 2)\n        features = self.encoder(data_chw)\n        # BCHW \u2192 BHWC\n        features_hwc = features.permute(0, 2, 3, 1)\n        return {\"features\": features_hwc}\n</code></pre>"},{"location":"how-to/add-builtin-node/#integration-with-pipeline","title":"Integration with Pipeline","text":""},{"location":"how-to/add-builtin-node/#add-node-to-yaml-pipeline","title":"Add Node to YAML Pipeline","text":"<pre><code>nodes:\n  - name: adaptive_filter\n    class: cuvis_ai.node.filters.AdaptiveThresholdFilter\n    params:\n      num_channels: 61\n      init_threshold: 0.5\n\nconnections:\n  - from: normalizer.outputs.normalized\n    to: adaptive_filter.inputs.data\n  - from: adaptive_filter.outputs.filtered\n    to: detector.inputs.data\n</code></pre>"},{"location":"how-to/add-builtin-node/#add-node-in-python","title":"Add Node in Python","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.filters import AdaptiveThresholdFilter\n\npipeline = CuvisPipeline(\"my_pipeline\")\n\n# Add node\nfilter_node = pipeline.add_node(\n    \"adaptive_filter\",\n    AdaptiveThresholdFilter,\n    num_channels=61,\n    init_threshold=0.5\n)\n\n# Connect to pipeline\npipeline.connect(\"normalizer.outputs.normalized\", \"adaptive_filter.inputs.data\")\npipeline.connect(\"adaptive_filter.outputs.filtered\", \"detector.inputs.data\")\n</code></pre>"},{"location":"how-to/add-builtin-node/#contributing-to-cuvisai","title":"Contributing to CUVIS.AI","text":""},{"location":"how-to/add-builtin-node/#pre-contribution-checklist","title":"Pre-contribution Checklist","text":"<ul> <li> Node implements focused, single-responsibility functionality</li> <li> Complete docstrings (NumPy style) with examples</li> <li> Full type hints on all methods</li> <li> Comprehensive test suite (&gt;90% coverage)</li> <li> Follows existing code style (ruff formatting)</li> <li> Added to appropriate module (data, normalization, selectors, etc.)</li> <li> Registered with NodeRegistry</li> <li> Documentation added to Node Catalog</li> </ul>"},{"location":"how-to/add-builtin-node/#file-organization","title":"File Organization","text":"<pre><code>cuvis_ai/\n\u2514\u2500\u2500 node/\n    \u251c\u2500\u2500 __init__.py           # Export your node here\n    \u251c\u2500\u2500 data.py               # Data loading nodes\n    \u251c\u2500\u2500 normalization.py      # Normalization nodes\n    \u251c\u2500\u2500 filters.py            # \u2190 Add filtering nodes here\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"how-to/add-builtin-node/#export-node","title":"Export Node","text":"<p>In <code>cuvis_ai/node/__init__.py</code>:</p> <pre><code>from cuvis_ai.node.filters import (\n    ThresholdFilter,\n    AdaptiveThresholdFilter,\n)\n\n__all__ = [\n    ...,\n    \"ThresholdFilter\",\n    \"AdaptiveThresholdFilter\",\n]\n</code></pre>"},{"location":"how-to/add-builtin-node/#add-to-documentation","title":"Add to Documentation","text":"<p>Create entry in Node Catalog:</p> <pre><code>## Filtering Nodes\n\n### AdaptiveThresholdFilter\n\n**Module:** `cuvis_ai.node.filters`\n\nAdaptive threshold filter with per-channel learned thresholds.\n\n**When to use:**\n- Need channel-specific filtering\n- Want data-driven threshold selection\n- Require gradient-based threshold optimization\n\n**Parameters:**\n- `num_channels` (int): Number of spectral channels\n- `init_threshold` (float): Initial threshold value\n\n**Ports:**\n- Input: `data` (BHWC float32 tensor)\n- Output: `filtered` (BHWC float32 tensor), `mask` (BHWC bool tensor)\n\n**Example:**\n\\`\\`\\`python\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nnode = AdaptiveThresholdFilter(num_channels=61)\npipeline.add_node(node)\n\n# Initialize using trainer\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()\n\nresult = node.forward(data=test_data)\n\\`\\`\\`\n\n**See also:** ThresholdFilter, MinMaxNormalizer\n</code></pre>"},{"location":"how-to/add-builtin-node/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/add-builtin-node/#issue-port-connection-error","title":"Issue: Port Connection Error","text":"<p><pre><code>ValueError: Port 'output' not found on node 'my_node'\n</code></pre> Solution: Check port names in INPUT_SPECS/OUTPUT_SPECS match connection strings.</p>"},{"location":"how-to/add-builtin-node/#issue-serialization-fails","title":"Issue: Serialization Fails","text":"<p><pre><code>TypeError: __init__() got unexpected keyword argument\n</code></pre> Solution: Ensure all <code>__init__</code> parameters are passed to <code>super().__init__()</code>: <pre><code>def __init__(self, param1, param2, **kwargs):\n    self.param1 = param1\n    self.param2 = param2\n    # MUST pass all params to super\n    super().__init__(param1=param1, param2=param2, **kwargs)\n</code></pre></p>"},{"location":"how-to/add-builtin-node/#issue-gradient-not-flowing","title":"Issue: Gradient Not Flowing","text":"<p><pre><code>RuntimeError: element 0 of tensors does not require grad\n</code></pre> Solution: Call <code>unfreeze()</code> to convert buffers to parameters: <pre><code>node.statistical_initialization(data)\nnode.unfreeze()  # Enable gradients\noptimizer = torch.optim.Adam(node.parameters())\n</code></pre></p>"},{"location":"how-to/add-builtin-node/#issue-shape-mismatch","title":"Issue: Shape Mismatch","text":"<p><pre><code>RuntimeError: shape '[1, 256, 256, 61]' is invalid for input of size 3932160\n</code></pre> Solution: Check tensor format (BHWC vs BCHW). CUVIS.AI uses BHWC: <pre><code># Wrong: BCHW format\ndata = torch.randn(1, 61, 256, 256)\n\n# Correct: BHWC format\ndata = torch.randn(1, 256, 256, 61)\n</code></pre></p>"},{"location":"how-to/add-builtin-node/#best-practices","title":"Best Practices","text":""},{"location":"how-to/add-builtin-node/#1-keep-nodes-focused","title":"1. Keep Nodes Focused","text":"<pre><code># Good: Single responsibility\nclass ChannelMeanCalculator(Node):\n    \"\"\"Computes mean across channels.\"\"\"\n    pass\n\n# Avoid: Multiple responsibilities\nclass ChannelMeanAndVarianceAndSkewnessCalculator(Node):\n    \"\"\"Computes many statistics.\"\"\"  # Too complex, split into multiple nodes\n    pass\n</code></pre>"},{"location":"how-to/add-builtin-node/#2-validate-inputs-thoroughly","title":"2. Validate Inputs Thoroughly","text":"<pre><code>def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n    # Check initialization\n    if not self._statistically_initialized:\n        raise RuntimeError(f\"{self.__class__.__name__} requires initialization\")\n\n    # Check shape\n    if data.ndim != 4:\n        raise ValueError(f\"Expected 4D tensor (BHWC), got {data.ndim}D\")\n\n    B, H, W, C = data.shape\n    if C != self.num_channels:\n        raise ValueError(\n            f\"Expected {self.num_channels} channels, got {C}. \"\n            f\"Reinitialize node with num_channels={C}\"\n        )\n\n    # Check value range\n    if data.min() &lt; 0 or data.max() &gt; 1:\n        raise ValueError(\"Data must be in [0, 1] range. Apply normalization first.\")\n\n    # Process data\n    ...\n</code></pre>"},{"location":"how-to/add-builtin-node/#3-use-type-hints-consistently","title":"3. Use Type Hints Consistently","text":"<pre><code>from typing import Any\nimport torch\n\ndef forward(\n    self,\n    data: torch.Tensor,\n    mask: torch.Tensor | None = None,\n    **_: Any\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Type hints improve IDE support and catch errors early.\"\"\"\n    pass\n</code></pre>"},{"location":"how-to/add-builtin-node/#4-document-edge-cases","title":"4. Document Edge Cases","text":"<pre><code>def forward(self, data: torch.Tensor, **_) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Process input data.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        Input in BHWC format\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Processed output\n\n    Notes\n    -----\n    - Returns zeros for all-zero input (graceful degradation)\n    - Handles NaN values by replacing with zero\n    - Preserves gradient flow even when mask is all-False\n    \"\"\"\n    # Handle edge cases explicitly\n    if torch.isnan(data).any():\n        data = torch.nan_to_num(data, nan=0.0)\n\n    ...\n</code></pre>"},{"location":"how-to/add-builtin-node/#5-test-edge-cases","title":"5. Test Edge Cases","text":"<pre><code>def test_edge_cases():\n    \"\"\"Test boundary conditions.\"\"\"\n    node = MyNode(num_channels=3)\n    node.statistical_initialization([{\"data\": torch.ones(1, 1, 1, 3)}])\n\n    # Test zero input\n    result = node.forward(data=torch.zeros(1, 10, 10, 3))\n    assert not torch.isnan(result[\"output\"]).any()\n\n    # Test near-zero values\n    result = node.forward(data=torch.full((1, 10, 10, 3), 1e-10))\n    assert torch.isfinite(result[\"output\"]).all()\n\n    # Test large values\n    result = node.forward(data=torch.full((1, 10, 10, 3), 1e6))\n    assert torch.isfinite(result[\"output\"]).all()\n</code></pre>"},{"location":"how-to/add-builtin-node/#see-also","title":"See Also","text":"<ul> <li>Node System Deep Dive</li> <li>Node Catalog</li> <li>Build Pipelines in Python</li> <li>Build Pipelines in YAML</li> <li>Contributing Guide</li> </ul>"},{"location":"how-to/build-pipeline-python/","title":"Build Pipeline (Python)","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/build-pipeline-python/#how-to-build-pipelines-in-python","title":"How-To: Build Pipelines in Python","text":""},{"location":"how-to/build-pipeline-python/#overview","title":"Overview","text":"<p>Learn how to build pipelines programmatically in Python using the cuvis-ai framework. This guide demonstrates the recommended pattern used in all cuvis-ai examples.</p>"},{"location":"how-to/build-pipeline-python/#prerequisites","title":"Prerequisites","text":"<ul> <li>cuvis-ai installed</li> <li>Basic understanding of Pipeline Lifecycle</li> <li>Familiarity with Nodes</li> </ul>"},{"location":"how-to/build-pipeline-python/#recommended-approach-direct-port-connections","title":"Recommended Approach: Direct Port Connections","text":"<p>This is the pattern used in all cuvis-ai examples. Nodes are instantiated directly and connected using tuples of port references.</p>"},{"location":"how-to/build-pipeline-python/#basic-pipeline-construction","title":"Basic Pipeline Construction","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\n\n# Create pipeline\npipeline = CuvisPipeline(\"my_pipeline\")\n\n# Instantiate nodes directly\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\nrx = RXGlobal(num_channels=61, eps=1.0e-6)\n\n# Connect using tuples of port references\npipeline.connect(\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, rx.data),\n)\n\n# Validate and run\npipeline.validate()\n</code></pre>"},{"location":"how-to/build-pipeline-python/#multi-branch-pipeline","title":"Multi-Branch Pipeline","text":"<p>Group connections by purpose using comments for better readability:</p> <pre><code>from cuvis_ai.node.conversion import ScoreToLogit\nfrom cuvis_ai.deciders.binary_decider import BinaryDecider\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\n\npipeline = CuvisPipeline(\"multi_branch\")\n\n# Instantiate all nodes\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\nrx = RXGlobal(num_channels=61, eps=1.0e-6)\nlogit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\ndecider = BinaryDecider(threshold=0.5)\nmetrics = AnomalyDetectionMetrics(name=\"metrics\")\ntensorboard = TensorBoardMonitorNode(output_dir=\"logs/\", run_name=\"experiment\")\n\n# Connect all branches in one call\npipeline.connect(\n    # Processing flow\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, rx.data),\n    (rx.scores, logit_head.scores),\n    (logit_head.logits, decider.logits),\n    # Metrics flow\n    (decider.decisions, metrics.decisions),\n    (data_node.outputs.mask, metrics.targets),\n    (metrics.metrics, tensorboard.metrics),\n)\n</code></pre>"},{"location":"how-to/build-pipeline-python/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"how-to/build-pipeline-python/#parallel-processing-branches","title":"Parallel Processing Branches","text":"<p>A common pattern from Deep SVDD example showing multiple processing branches:</p> <pre><code>from cuvis_ai.anomaly.deep_svdd import (\n    DeepSVDDProjection,\n    DeepSVDDCenterTracker,\n    DeepSVDDScores,\n    ZScoreNormalizerGlobal\n)\nfrom cuvis_ai.node.preprocessors import BandpassByWavelength\nfrom cuvis_ai.node.normalization import PerPixelUnitNorm\n\npipeline = CuvisPipeline(\"parallel_processing\")\n\n# Data and preprocessing nodes\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nbandpass_node = BandpassByWavelength(min_wavelength_nm=450, max_wavelength_nm=900)\nunit_norm_node = PerPixelUnitNorm(eps=1e-8)\n\n# Processing branches\nencoder = ZScoreNormalizerGlobal(num_channels=50, hidden=128)\nprojection = DeepSVDDProjection(in_channels=128, rep_dim=64, hidden=256)\ncenter_tracker = DeepSVDDCenterTracker(rep_dim=64, alpha=0.1)\nscore_node = DeepSVDDScores()\n\n# Monitoring\nmetrics_node = AnomalyDetectionMetrics(name=\"metrics\")\ntensorboard = TensorBoardMonitorNode(output_dir=\"logs/\", run_name=\"parallel\")\n\n# Connect preprocessing chain\npipeline.connect(\n    (data_node.outputs.cube, bandpass_node.data),\n    (data_node.outputs.wavelengths, bandpass_node.wavelengths),\n    (bandpass_node.filtered, unit_norm_node.data),\n    (unit_norm_node.normalized, encoder.data),\n)\n\n# Connect parallel branches from encoder\npipeline.connect(\n    (encoder.normalized, projection.data),\n    (projection.embeddings, center_tracker.embeddings),\n    (projection.embeddings, score_node.embeddings),\n    (center_tracker.center, score_node.center),\n)\n\n# Connect metrics and monitoring\npipeline.connect(\n    (score_node.scores, metrics_node.logits),\n    (data_node.outputs.mask, metrics_node.targets),\n    (metrics_node.metrics, tensorboard.metrics),\n)\n</code></pre>"},{"location":"how-to/build-pipeline-python/#pipeline-factories","title":"Pipeline Factories","text":"<p>Create reusable factory functions for common pipeline patterns:</p> <pre><code>def create_rx_pipeline(\n    normal_class_ids: list[int],\n    num_channels: int = 61,\n    output_dir: str = \"outputs/\"\n) -&gt; CuvisPipeline:\n    \"\"\"Factory for RX statistical anomaly detection pipelines.\"\"\"\n    pipeline = CuvisPipeline(\"RX_Statistical\")\n\n    # Instantiate nodes\n    data_node = LentilsAnomalyDataNode(normal_class_ids=normal_class_ids)\n    normalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\n    rx = RXGlobal(num_channels=num_channels, eps=1.0e-6)\n    logit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\n    decider = BinaryDecider(threshold=0.5)\n    metrics = AnomalyDetectionMetrics(name=\"metrics\")\n    tensorboard = TensorBoardMonitorNode(output_dir=output_dir, run_name=\"rx\")\n\n    # Connect\n    pipeline.connect(\n        (data_node.outputs.cube, normalizer.data),\n        (normalizer.normalized, rx.data),\n        (rx.scores, logit_head.scores),\n        (logit_head.logits, decider.logits),\n        (decider.decisions, metrics.decisions),\n        (data_node.outputs.mask, metrics.targets),\n        (metrics.metrics, tensorboard.metrics),\n    )\n\n    return pipeline\n\n# Use factory\npipeline1 = create_rx_pipeline(normal_class_ids=[0, 1], output_dir=\"exp1/\")\npipeline2 = create_rx_pipeline(normal_class_ids=[1, 2], output_dir=\"exp2/\")\n</code></pre>"},{"location":"how-to/build-pipeline-python/#saving-and-loading","title":"Saving and Loading","text":""},{"location":"how-to/build-pipeline-python/#save-pipeline","title":"Save Pipeline","text":"<pre><code>from cuvis_ai_core.training.config import PipelineMetadata\n\n# Save without metadata (simplest form)\npipeline.save_to_file(\"pipeline.yaml\")\n# Creates:\n#   - pipeline.yaml (configuration)\n#   - pipeline.pt (weights)\n\n# Save with optional metadata for better organization\npipeline.save_to_file(\n    \"pipeline.yaml\",\n    metadata=PipelineMetadata(\n        name=\"my_pipeline\",\n        description=\"RX anomaly detection pipeline\",\n        tags=[\"statistical\", \"rx\"],\n        author=\"your_name\"\n    )\n)\n</code></pre>"},{"location":"how-to/build-pipeline-python/#load-and-evaluate-pipeline","title":"Load and Evaluate Pipeline","text":"<pre><code>from cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.training import StatisticalTrainer\n\n# Load pipeline from configuration (automatically finds .pt weights)\nloaded_pipeline = CuvisPipeline.load_pipeline(\"pipeline.yaml\")\n\n# Load with custom weights path and device\nloaded_pipeline = CuvisPipeline.load_pipeline(\n    config_path=\"pipeline.yaml\",\n    weights_path=\"custom_weights.pt\",\n    device=\"cuda\",\n    strict_weight_loading=True  # Fail if weights don't match exactly\n)\n\n# Load with config overrides\nloaded_pipeline = CuvisPipeline.load_pipeline(\n    config_path=\"pipeline.yaml\",\n    config_overrides={\"nodes.0.params.threshold\": 0.8}\n)\n\n# To evaluate the loaded pipeline, use a trainer with datamodule\ndatamodule = SingleCu3sDataModule(\n    cu3s_file_path=\"data/test.cu3s\",\n    batch_size=1,\n    processing_mode=\"Reflectance\"\n)\ndatamodule.setup(stage=\"test\")\n\n# For statistical pipelines\ntrainer = StatisticalTrainer(pipeline=loaded_pipeline, datamodule=datamodule)\ntest_results = trainer.test()\n\n# For gradient-trained pipelines\nfrom cuvis_ai_core.training import GradientTrainer\ntrainer = GradientTrainer(\n    pipeline=loaded_pipeline,\n    datamodule=datamodule,\n    loss_nodes=[],  # Empty for inference-only\n    metric_nodes=[metrics_node]\n)\ntest_results = trainer.test()\n</code></pre>"},{"location":"how-to/build-pipeline-python/#best-practices","title":"Best Practices","text":"<ol> <li>Use direct port connections - More readable and type-safe than string-based connections</li> <li>Group related connections with comments - Organize connection tuples by purpose (processing flow, metrics flow, visualization flow)</li> <li>Store nodes in descriptive variables - Use <code>data_node</code>, <code>normalizer</code>, <code>rx</code> instead of generic names</li> <li>Validate early - Call <code>pipeline.validate()</code> before training to catch connection errors</li> <li>Leverage port attributes - Use <code>node.port_name</code> for direct port access (e.g., <code>data_node.outputs.cube</code>)</li> <li>Connect in logical order - Group connections by data flow (processing \u2192 metrics \u2192 visualization)</li> <li>Use factory functions - Create reusable pipeline patterns for common workflows</li> </ol>"},{"location":"how-to/build-pipeline-python/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/build-pipeline-python/#issue-connection-error","title":"Issue: Connection Error","text":"<p><pre><code>ConnectionError: Port 'output' not found on node 'loader'\n</code></pre> Solution: Check available ports using class attributes: <pre><code># Check input port specs\nprint(DataLoaderNode.INPUT_SPECS.keys())\n\n# Check output port specs\nprint(DataLoaderNode.OUTPUT_SPECS.keys())\n\n# Or check on an instance\ndata_node = DataLoaderNode()\nprint(dir(data_node.inputs))   # List available input ports\nprint(dir(data_node.outputs))  # List available output ports\n</code></pre></p>"},{"location":"how-to/build-pipeline-python/#issue-type-mismatch","title":"Issue: Type Mismatch","text":"<p><pre><code>TypeError: Port expects np.ndarray, got torch.Tensor\n</code></pre> Solution: Check port specifications and add conversion if needed: <pre><code># Check port dtype requirements\nprint(RXNode.INPUT_SPECS[\"data\"].dtype)  # Expected dtype\n\n# Add conversion node if types don't match\n</code></pre></p>"},{"location":"how-to/build-pipeline-python/#see-also","title":"See Also","text":"<ul> <li>Build Pipelines in YAML</li> <li>Pipeline Lifecycle</li> <li>Node System</li> <li>RX Statistical Tutorial</li> <li>Deep SVDD Tutorial</li> </ul>"},{"location":"how-to/build-pipeline-yaml/","title":"Build Pipeline (YAML)","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/build-pipeline-yaml/#how-to-build-pipelines-in-yaml","title":"How-To: Build Pipelines in YAML","text":""},{"location":"how-to/build-pipeline-yaml/#overview","title":"Overview","text":"<p>Learn how to define pipelines using YAML configuration files. YAML pipelines enable version control, reproducibility, and easy experimentation through Hydra composition.</p>"},{"location":"how-to/build-pipeline-yaml/#prerequisites","title":"Prerequisites","text":"<ul> <li>cuvis-ai installed</li> <li>Basic understanding of Pipeline Lifecycle</li> <li>Familiarity with YAML syntax</li> <li>Optional: Understanding of Hydra composition</li> </ul>"},{"location":"how-to/build-pipeline-yaml/#pipeline-yaml-structure","title":"Pipeline YAML Structure","text":"<p>A pipeline YAML file has three main sections:</p> <pre><code>metadata:\n  name: My_Pipeline\n  description: Pipeline description\n  tags: [tag1, tag2]\n  author: your_name\n\nnodes:\n  - name: node1\n    class: cuvis_ai.module.NodeClass\n    params:\n      param1: value1\n      param2: value2\n\nconnections:\n  - from: node1.outputs.output_port\n    to: node2.inputs.input_port\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#basic-pipeline-example","title":"Basic Pipeline Example","text":"<p>Here's a simple RX anomaly detection pipeline:</p> <pre><code>metadata:\n  name: RX_Statistical\n  description: RX anomaly detector with statistical training\n  tags:\n    - statistical\n    - rx\n  author: cuvis.ai\n\nnodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: MinMaxNormalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: RXGlobal\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\n  - name: BinaryDecider\n    class: cuvis_ai.deciders.binary_decider.BinaryDecider\n    params:\n      threshold: 0.5\n\n  - name: metrics\n    class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n    params: {}\n\nconnections:\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n\n  - from: MinMaxNormalizer.outputs.normalized\n    to: RXGlobal.inputs.data\n\n  - from: RXGlobal.outputs.scores\n    to: BinaryDecider.inputs.logits\n\n  - from: BinaryDecider.outputs.decisions\n    to: metrics.inputs.decisions\n\n  - from: LentilsAnomalyDataNode.outputs.mask\n    to: metrics.inputs.targets\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#multi-branch-pipeline-example","title":"Multi-Branch Pipeline Example","text":"<p>Complex pipelines with multiple branches (channel selector with losses and metrics):</p> <pre><code>metadata:\n  name: Channel_Selector\n  description: Channel selection with gradient training\n  tags:\n    - gradient\n    - channel_selector\n  author: cuvis.ai\n\nnodes:\n  # Data loading\n  - name: data_node\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  # Preprocessing\n  - name: normalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  # Channel selection\n  - name: selector\n    class: cuvis_ai.node.selector.SoftChannelSelector\n    params:\n      n_select: 3\n      input_channels: 61\n      init_method: variance\n      temperature_init: 5.0\n\n  # Anomaly detection\n  - name: rx\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      num_channels: 61\n      eps: 1.0e-06\n\n  # Losses\n  - name: bce_loss\n    class: cuvis_ai.node.losses.AnomalyBCEWithLogits\n    params:\n      weight: 10.0\n      pos_weight: null\n\n  - name: entropy_loss\n    class: cuvis_ai.node.losses.SelectorEntropyRegularizer\n    params:\n      weight: 0.1\n\n  # Metrics\n  - name: metrics\n    class: cuvis_ai.node.metrics.AnomalyDetectionMetrics\n    params: {}\n\n  # Monitoring\n  - name: tensorboard\n    class: cuvis_ai.node.monitor.TensorBoardMonitorNode\n    params:\n      output_dir: logs/\n      run_name: channel_selector\n\nconnections:\n  # Data \u2192 Preprocessing\n  - from: data_node.outputs.cube\n    to: normalizer.inputs.data\n\n  # Preprocessing \u2192 Selection \u2192 Detection\n  - from: normalizer.outputs.normalized\n    to: selector.inputs.data\n  - from: selector.outputs.selected\n    to: rx.inputs.data\n\n  # Selection weights \u2192 Regularization\n  - from: selector.outputs.weights\n    to: entropy_loss.inputs.weights\n\n  # RX \u2192 Loss\n  - from: rx.outputs.scores\n    to: bce_loss.inputs.predictions\n  - from: data_node.outputs.mask\n    to: bce_loss.inputs.targets\n\n  # RX \u2192 Metrics\n  - from: rx.outputs.scores\n    to: metrics.inputs.decisions\n  - from: data_node.outputs.mask\n    to: metrics.inputs.targets\n\n  # Metrics \u2192 Monitoring\n  - from: metrics.outputs.metrics\n    to: tensorboard.inputs.metrics\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#loading-yaml-pipelines-in-python","title":"Loading YAML Pipelines in Python","text":""},{"location":"how-to/build-pipeline-yaml/#basic-loading","title":"Basic Loading","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\n\n# Load pipeline from YAML\npipeline = CuvisPipeline.load_pipeline(\"configs/pipeline/rx_statistical.yaml\")\n\n# Validate\npipeline.validate()\n\n# Use with trainer\nfrom cuvis_ai_core.training import StatisticalTrainer\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\n\ndatamodule = SingleCu3sDataModule(\n    cu3s_file_path=\"data/train.cu3s\",\n    batch_size=1\n)\ndatamodule.setup(stage=\"fit\")\n\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#loading-with-overrides","title":"Loading with Overrides","text":"<p>Override specific parameters without modifying the YAML file:</p> <pre><code># Override node parameters\npipeline = CuvisPipeline.load_pipeline(\n    config_path=\"configs/pipeline/rx_statistical.yaml\",\n    config_overrides={\n        \"nodes.2.params.threshold\": 0.8,  # Override BinaryDecider threshold\n        \"nodes.3.params.eps\": 1e-8,       # Override RXGlobal eps\n    }\n)\n\n# Or using dot notation (requires Hydra)\npipeline = CuvisPipeline.load_pipeline(\n    config_path=\"configs/pipeline/rx_statistical.yaml\",\n    config_overrides=[\n        \"nodes.2.params.threshold=0.8\",\n        \"metadata.name=RX_Custom\"\n    ]\n)\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#loading-with-custom-weights","title":"Loading with Custom Weights","text":"<pre><code># Load pipeline with specific weights file\npipeline = CuvisPipeline.load_pipeline(\n    config_path=\"configs/pipeline/rx_statistical.yaml\",\n    weights_path=\"outputs/trained_models/rx_custom.pt\",\n    device=\"cuda\",\n    strict_weight_loading=True\n)\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#hydra-composition-for-trainruns","title":"Hydra Composition for TrainRuns","text":"<p>For training experiments, use Hydra composition to combine pipeline, data, and training configs:</p>"},{"location":"how-to/build-pipeline-yaml/#trainrun-yaml-structure","title":"TrainRun YAML Structure","text":"<pre><code># @package _global_\n\nname: my_trainrun\n\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\n# Override data configuration\ndata:\n  train_ids: [0]\n  val_ids: [3, 4]\n  test_ids: [1, 5]\n  batch_size: 1\n\n# Override training configuration\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 50\n    accelerator: auto\n    devices: 1\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n\n# Pipeline-specific overrides\noutput_dir: outputs/my_experiment\nunfreeze_nodes: []\nmetric_nodes: [metrics_anomaly]\nloss_nodes: []\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#using-trainrun-configs","title":"Using TrainRun Configs","text":"<pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\n\n@hydra.main(config_path=\"../configs/\", config_name=\"trainrun/my_trainrun\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    # Access composed configuration\n    print(f\"Pipeline: {cfg.pipeline.metadata.name}\")\n    print(f\"Data: {cfg.data}\")\n    print(f\"Training: {cfg.training}\")\n\n    # Build pipeline from composed config\n    # ... training code ...\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#command-line-overrides-with-hydra","title":"Command-line Overrides with Hydra","text":"<pre><code># Override parameters from command line\nuv run python my_script.py \\\n    training.trainer.max_epochs=100 \\\n    training.optimizer.lr=0.0001 \\\n    data.batch_size=4 \\\n    output_dir=outputs/custom_experiment\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#yaml-best-practices","title":"YAML Best Practices","text":""},{"location":"how-to/build-pipeline-yaml/#1-consistent-naming-conventions","title":"1. Consistent Naming Conventions","text":"<pre><code># Good: descriptive, consistent names\nnodes:\n  - name: data_loader\n  - name: preprocessor\n  - name: anomaly_detector\n  - name: metrics_node\n\n# Avoid: generic or inconsistent names\nnodes:\n  - name: node1\n  - name: n2\n  - name: Detector_Node\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#2-organize-connections-by-flow","title":"2. Organize Connections by Flow","text":"<pre><code>connections:\n  # Data loading \u2192 Preprocessing\n  - from: data_loader.outputs.cube\n    to: preprocessor.inputs.data\n\n  # Preprocessing \u2192 Detection\n  - from: preprocessor.outputs.normalized\n    to: detector.inputs.data\n\n  # Detection \u2192 Metrics\n  - from: detector.outputs.scores\n    to: metrics.inputs.predictions\n  - from: data_loader.outputs.labels\n    to: metrics.inputs.targets\n\n  # Metrics \u2192 Monitoring\n  - from: metrics.outputs.results\n    to: monitor.inputs.metrics\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#3-use-comments-for-clarity","title":"3. Use Comments for Clarity","text":"<pre><code>nodes:\n  # ===== Data Loading =====\n  - name: data_loader\n    class: cuvis_ai.node.data.DataLoaderNode\n    params:\n      path: data/\n\n  # ===== Preprocessing =====\n  - name: normalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06  # Small epsilon for numerical stability\n      use_running_stats: true  # Track running statistics during training\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#4-version-control-metadata","title":"4. Version Control Metadata","text":"<pre><code>metadata:\n  name: My_Pipeline\n  description: Detailed description of pipeline purpose and capabilities\n  created: 2026-02-04\n  tags:\n    - anomaly-detection\n    - hyperspectral\n    - production-ready\n  author: your_name\n  version: 1.2.0\n  cuvis_ai_version: 0.1.5\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#5-use-config-groups-for-reusability","title":"5. Use Config Groups for Reusability","text":"<pre><code>configs/\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 rx_statistical.yaml\n\u2502   \u251c\u2500\u2500 channel_selector.yaml\n\u2502   \u2514\u2500\u2500 deep_svdd.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 lentils.yaml\n\u2502   \u251c\u2500\u2500 concrete.yaml\n\u2502   \u2514\u2500\u2500 custom.yaml\n\u2514\u2500\u2500 training/\n    \u251c\u2500\u2500 default.yaml\n    \u251c\u2500\u2500 fast.yaml\n    \u2514\u2500\u2500 production.yaml\n</code></pre>"},{"location":"how-to/build-pipeline-yaml/#converting-python-to-yaml","title":"Converting Python to YAML","text":"<p>If you have a working Python pipeline, save it to YAML:</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training.config import PipelineMetadata\n\n# Build pipeline in Python (as shown in build-pipeline-python.md)\npipeline = CuvisPipeline(\"my_pipeline\")\n# ... add nodes and connections ...\n\n# Save to YAML\npipeline.save_to_file(\n    \"configs/pipeline/my_pipeline.yaml\",\n    metadata=PipelineMetadata(\n        name=\"my_pipeline\",\n        description=\"Converted from Python\",\n        tags=[\"custom\"],\n        author=\"your_name\"\n    )\n)\n</code></pre> <p>This creates: - <code>my_pipeline.yaml</code> - Pipeline configuration - <code>my_pipeline.pt</code> - Trained weights (if nodes have parameters)</p>"},{"location":"how-to/build-pipeline-yaml/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/build-pipeline-yaml/#issue-invalid-yaml-syntax","title":"Issue: Invalid YAML Syntax","text":"<p><pre><code>yaml.scanner.ScannerError: while scanning a simple key\n</code></pre> Solution: Check indentation (use spaces, not tabs) and ensure colons have spaces: <pre><code># Wrong\nnodes:\n  -name:node1  # Missing space after colon\n\n# Correct\nnodes:\n  - name: node1\n</code></pre></p>"},{"location":"how-to/build-pipeline-yaml/#issue-class-not-found","title":"Issue: Class Not Found","text":"<p><pre><code>ModuleNotFoundError: No module named 'cuvis_ai.node.custom'\n</code></pre> Solution: Verify the class path matches the actual module structure: <pre><code># Check available nodes\nfrom cuvis_ai.node import data\nprint(dir(data))  # List available classes\n</code></pre></p>"},{"location":"how-to/build-pipeline-yaml/#issue-connection-error","title":"Issue: Connection Error","text":"<p><pre><code>ConnectionError: Port 'output' not found on node 'detector'\n</code></pre> Solution: Check node's actual port names using the Node Catalog or: <pre><code>from cuvis_ai.anomaly.rx_detector import RXGlobal\nprint(RXGlobal.INPUT_SPECS.keys())   # \u2192 ['data']\nprint(RXGlobal.OUTPUT_SPECS.keys())  # \u2192 ['scores']\n</code></pre></p>"},{"location":"how-to/build-pipeline-yaml/#issue-parameter-type-mismatch","title":"Issue: Parameter Type Mismatch","text":"<p><pre><code>TypeError: expected int but got str for parameter 'num_channels'\n</code></pre> Solution: Ensure parameter types match node requirements: <pre><code># Wrong\nparams:\n  num_channels: \"61\"  # String\n\n# Correct\nparams:\n  num_channels: 61  # Integer\n</code></pre></p>"},{"location":"how-to/build-pipeline-yaml/#see-also","title":"See Also","text":"<ul> <li>Build Pipelines in Python</li> <li>Pipeline Schema Reference</li> <li>Hydra Composition</li> <li>Config Groups</li> <li>Node Catalog</li> </ul>"},{"location":"how-to/monitoring-and-viz/","title":"Monitoring & Visualization","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/monitoring-and-viz/#monitoring-visualization","title":"Monitoring &amp; Visualization","text":"<p>Monitor training progress and visualize results using TensorBoard integration, metrics tracking, and custom visualization nodes.</p>"},{"location":"how-to/monitoring-and-viz/#overview","title":"Overview","text":"<p>CUVIS.AI provides comprehensive monitoring and visualization capabilities:</p> <ul> <li>TensorBoard Integration: Centralized logging with TensorBoardMonitorNode</li> <li>Metrics Tracking: Performance evaluation with specialized metric nodes</li> <li>Visual Monitoring: Real-time visualization of predictions, scores, and anomalies</li> <li>Execution Stage Control: Automatic filtering to minimize overhead</li> </ul>"},{"location":"how-to/monitoring-and-viz/#quick-start","title":"Quick Start","text":""},{"location":"how-to/monitoring-and-viz/#basic-monitoring-setup","title":"Basic Monitoring Setup","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.visualizations import AnomalyMask, ScoreHeatmapVisualizer\n\n# Create pipeline\npipeline = CuvisPipeline(\"monitored_pipeline\")\n\n# ... create processing nodes ...\n\n# Create monitoring nodes\nmetrics = AnomalyDetectionMetrics()\nviz_mask = AnomalyMask(channel=30, up_to=5)\nscore_viz = ScoreHeatmapVisualizer(up_to=5)\n\nmonitor = TensorBoardMonitorNode(\n    output_dir=\"./runs\",\n    run_name=\"experiment_01\"\n)\n\n# Connect monitoring\npipeline.connect(\n    # Processing flow\n    (detector.scores, decider.scores),\n    (decider.decisions, metrics.decisions),\n    (data.mask, metrics.targets),\n\n    # Visualization flow\n    (decider.decisions, viz_mask.decisions),\n    (data.mask, viz_mask.mask),\n    (data.cube, viz_mask.cube),\n    (detector.scores, score_viz.scores),\n\n    # Monitor connections\n    (metrics.metrics, monitor.metrics),\n    (viz_mask.artifacts, monitor.artifacts),\n    (score_viz.artifacts, monitor.artifacts),\n)\n\n# Pass monitor to trainer\nfrom cuvis_ai.trainers.gradient_trainer import GradientTrainer\n\ntrainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[loss],\n    metric_nodes=[metrics],\n    monitors=[monitor],  # \u2190 Enable monitoring\n)\n\ntrainer.fit()\n</code></pre> <p>View results: <pre><code>tensorboard --logdir=./runs\n# Open: http://localhost:6006\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#tensorboard-integration","title":"TensorBoard Integration","text":""},{"location":"how-to/monitoring-and-viz/#tensorboardmonitornode","title":"TensorBoardMonitorNode","text":"<p>Centralized logging sink that writes metrics and images to TensorBoard.</p> <p>Key characteristics: - Type: Sink node (no outputs) - Execution: All stages (TRAIN, VAL, TEST, INFERENCE) - Auto-increment: Creates <code>run_01</code>, <code>run_02</code>, etc. if <code>run_name</code> is None</p> <p>Parameters:</p> Parameter Type Default Description <code>output_dir</code> str <code>\"./runs\"</code> Base directory for TensorBoard logs <code>run_name</code> str | None <code>None</code> Specific run name (auto-increments if None) <code>comment</code> str <code>\"\"</code> Additional comment appended to run name <code>flush_secs</code> int <code>120</code> Flush events to disk interval (seconds) <p>Input ports:</p> Port Type Required Description <code>artifacts</code> <code>list</code> No List of <code>Artifact</code> objects (images) <code>metrics</code> <code>list</code> No List of <code>Metric</code> objects (scalars)"},{"location":"how-to/monitoring-and-viz/#example-custom-run-organization","title":"Example: Custom Run Organization","text":"<pre><code># Organized directory structure\nmonitor = TensorBoardMonitorNode(\n    output_dir=\"./outputs/my_experiment/tensorboard\",\n    run_name=f\"lr_{learning_rate}_bs_{batch_size}\",\n    comment=\"with_augmentation\",\n)\n\n# Results in: ./outputs/my_experiment/tensorboard/lr_0.001_bs_2_with_augmentation/\n</code></pre>"},{"location":"how-to/monitoring-and-viz/#direct-logging-api","title":"Direct Logging API","text":"<pre><code># Log custom scalars programmatically\nmonitor.log(\"train/custom_metric\", 0.75, step=100)\nmonitor.log(\"val/threshold\", 0.5, step=100)\n</code></pre> <p>Used internally by GradientTrainer for training/validation loss logging.</p>"},{"location":"how-to/monitoring-and-viz/#metrics-tracking","title":"Metrics Tracking","text":""},{"location":"how-to/monitoring-and-viz/#available-metric-nodes","title":"Available Metric Nodes","text":""},{"location":"how-to/monitoring-and-viz/#anomalydetectionmetrics","title":"AnomalyDetectionMetrics","text":"<p>Comprehensive anomaly detection performance metrics.</p> <p>Metrics computed: - Precision: TP / (TP + FP) - Recall: TP / (TP + FN) - F1-Score: Harmonic mean of precision and recall - IoU (Jaccard Index): TP / (TP + FP + FN) - Average Precision (AP): Area under precision-recall curve (if <code>logits</code> provided)</p> <p>Input ports:</p> Port Type Shape Description <code>decisions</code> <code>bool</code> <code>(B,H,W,1)</code> Binary predictions <code>targets</code> <code>bool</code> <code>(B,H,W,1)</code> Ground truth masks <code>logits</code> <code>float32</code> <code>(B,H,W,1)</code> Optional anomaly scores for AP <p>Execution: VAL, TEST stages only</p> <p>Example:</p> <pre><code>from cuvis_ai.node.metrics import AnomalyDetectionMetrics\n\nmetrics = AnomalyDetectionMetrics()\n\n# Connect\npipeline.connect(\n    (decider.decisions, metrics.decisions),\n    (data_node.outputs.mask, metrics.targets),\n    (detector.scores, metrics.logits),  # Optional for AP\n    (metrics.metrics, monitor.metrics),\n)\n</code></pre> <p>TensorBoard tags: <pre><code>val/precision\nval/recall\nval/f1_score\nval/iou\nval/average_precision\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#scorestatisticsmetric","title":"ScoreStatisticsMetric","text":"<p>Track distribution properties of anomaly scores.</p> <p>Metrics computed: - Mean, standard deviation - Min, max, median - Quantiles: q25, q75, q95, q99</p> <p>Input ports:</p> Port Type Shape Description <code>scores</code> <code>float32</code> <code>(B,H,W,1)</code> Anomaly scores <p>Example:</p> <pre><code>from cuvis_ai.node.metrics import ScoreStatisticsMetric\n\nscore_stats = ScoreStatisticsMetric()\n\npipeline.connect(\n    (detector.scores, score_stats.scores),\n    (score_stats.metrics, monitor.metrics),\n)\n</code></pre> <p>TensorBoard tags: <pre><code>scores/mean\nscores/std\nscores/min\nscores/max\nscores/median\nscores/q25\nscores/q75\nscores/q95\nscores/q99\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#explainedvariancemetric","title":"ExplainedVarianceMetric","text":"<p>Monitor PCA variance breakdown.</p> <p>Metrics computed: - Per-component variance - Total explained variance - Cumulative variance ratios</p> <p>Input ports:</p> Port Type Shape Description <code>explained_variance</code> <code>float32</code> <code>(num_components,)</code> Variance per PC <p>Example:</p> <pre><code>from cuvis_ai.node.metrics import ExplainedVarianceMetric\n\nvariance_metric = ExplainedVarianceMetric()\n\npipeline.connect(\n    (pca_node.explained_variance, variance_metric.explained_variance),\n    (variance_metric.metrics, monitor.metrics),\n)\n</code></pre> <p>TensorBoard tags: <pre><code>explained_variance_pc1\nexplained_variance_pc2\n...\ntotal_explained_variance\ncumulative_variance_pc1\ncumulative_variance_pc2\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#selectorentropymetric-selectordiversitymetric","title":"SelectorEntropyMetric &amp; SelectorDiversityMetric","text":"<p>Monitor channel selection diversity.</p> <p>Metrics computed: - Entropy: Shannon entropy of selection weights - Weight Variance: Variance of selection weights - Gini Coefficient: Inequality measure (0 = uniform, 1 = concentrated)</p> <p>Input ports:</p> Port Type Shape Description <code>weights</code> <code>float32</code> <code>(C,)</code> Channel selection weights <p>Example:</p> <pre><code>from cuvis_ai.node.metrics import SelectorEntropyMetric, SelectorDiversityMetric\n\nentropy = SelectorEntropyMetric()\ndiversity = SelectorDiversityMetric()\n\npipeline.connect(\n    (selector.weights, entropy.weights),\n    (selector.weights, diversity.weights),\n    (entropy.metrics, monitor.metrics),\n    (diversity.metrics, monitor.metrics),\n)\n</code></pre> <p>TensorBoard tags: <pre><code>selector/entropy\nweight_variance\ngini_coefficient\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#componentorthogonalitymetric","title":"ComponentOrthogonalityMetric","text":"<p>Monitor PCA component orthonormality.</p> <p>Metrics computed: - Orthogonality Error: Frobenius norm of <code>(G - I)</code> - Avg Off-Diagonal: Mean absolute off-diagonal value - Diagonal Mean/Std: Statistics of diagonal entries</p> <p>Input ports:</p> Port Type Shape Description <code>components</code> <code>float32</code> <code>(num_components, features)</code> PCA components <p>Example:</p> <pre><code>from cuvis_ai.node.metrics import ComponentOrthogonalityMetric\n\northogonality = ComponentOrthogonalityMetric()\n\npipeline.connect(\n    (pca_node.components, orthogonality.components),\n    (orthogonality.metrics, monitor.metrics),\n)\n</code></pre> <p>TensorBoard tags: <pre><code>orthogonality_error\navg_off_diagonal\ndiagonal_mean\ndiagonal_std\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#visualization-nodes","title":"Visualization Nodes","text":""},{"location":"how-to/monitoring-and-viz/#anomalymask","title":"AnomalyMask","text":"<p>Side-by-side ground truth and prediction comparison with overlay.</p> <p>Visualization output: 1. Ground Truth Mask (if available) 2. Predicted Overlay on selected cube channel:    - Green: True Positives (TP)    - Red: False Positives (FP)    - Yellow: False Negatives (FN) 3. Predicted Mask with metrics (Precision, Recall, F1, IoU, AP)</p> <p>Parameters:</p> Parameter Type Default Description <code>channel</code> int <code>30</code> Cube channel index for background <code>up_to</code> int <code>5</code> Max images per batch to visualize <p>Input ports:</p> Port Type Shape Description <code>decisions</code> <code>bool</code> <code>(B,H,W,1)</code> Binary predictions <code>mask</code> <code>bool</code> <code>(B,H,W,1)</code> Ground truth (optional) <code>cube</code> <code>float32</code> <code>(B,H,W,C)</code> Hyperspectral cube <code>scores</code> <code>float32</code> <code>(B,H,W,1)</code> Scores for AP (optional) <p>Execution: VAL, TEST, INFERENCE stages</p> <p>Example:</p> <pre><code>from cuvis_ai.node.visualizations import AnomalyMask\n\nviz_mask = AnomalyMask(channel=30, up_to=5)\n\npipeline.connect(\n    (decider.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (data_node.outputs.cube, viz_mask.cube),\n    (detector.scores, viz_mask.scores),\n    (viz_mask.artifacts, monitor.artifacts),\n)\n</code></pre> <p>TensorBoard images: <pre><code>val/anomaly_mask_img00\nval/anomaly_mask_img01\n...\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#rgbanomalymask","title":"RGBAnomalyMask","text":"<p>Like AnomalyMask but for RGB images.</p> <p>Use cases: - Band selector output visualization - DRCNN mixer evaluation - AdaCLIP RGB-like workflows</p> <p>Input ports:</p> Port Type Shape Description <code>decisions</code> <code>bool</code> <code>(B,H,W,1)</code> Binary predictions <code>mask</code> <code>bool</code> <code>(B,H,W,1)</code> Ground truth (optional) <code>rgb_image</code> <code>float32</code> <code>(B,H,W,3)</code> RGB background image <code>scores</code> <code>float32</code> <code>(B,H,W,1)</code> Scores for AP (optional) <p>Example:</p> <pre><code>from cuvis_ai.node.visualizations import RGBAnomalyMask\n\nrgb_viz = RGBAnomalyMask(up_to=5)\n\npipeline.connect(\n    (decider.decisions, rgb_viz.decisions),\n    (data_node.outputs.mask, rgb_viz.mask),\n    (mixer.output, rgb_viz.rgb_image),  # RGB from mixer\n    (rgb_viz.artifacts, monitor.artifacts),\n)\n</code></pre>"},{"location":"how-to/monitoring-and-viz/#scoreheatmapvisualizer","title":"ScoreHeatmapVisualizer","text":"<p>Heatmap visualization of anomaly scores.</p> <p>Parameters:</p> Parameter Type Default Description <code>normalize_scores</code> bool <code>True</code> Normalize scores to [0,1] <code>cmap</code> str <code>\"inferno\"</code> Matplotlib colormap <code>up_to</code> int <code>5</code> Max heatmaps per batch <p>Input ports:</p> Port Type Shape Description <code>scores</code> <code>float32</code> <code>(B,H,W,1)</code> Anomaly scores <p>Example:</p> <pre><code>from cuvis_ai.node.visualizations import ScoreHeatmapVisualizer\n\nscore_viz = ScoreHeatmapVisualizer(\n    normalize_scores=True,\n    cmap=\"inferno\",\n    up_to=5,\n)\n\npipeline.connect(\n    (detector.scores, score_viz.scores),\n    (score_viz.artifacts, monitor.artifacts),\n)\n</code></pre> <p>TensorBoard images: <pre><code>val/score_heatmap_img00\nval/score_heatmap_img01\n...\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#pcavisualization","title":"PCAVisualization","text":"<p>Visualize first 2 PCA components with spatial encoding.</p> <p>Visualization output: 1. Scatter Plot: PC1 vs PC2, colored by spatial position (HSV encoding) 2. Spatial Reference: HSV color coding guide 3. Image Representation: PC1 in red channel, PC2 in green 4. Statistics Box: Range, shape, point count</p> <p>HSV Encoding: - Hue: x-coordinate (left \u2192 right) - Saturation: y-coordinate (top \u2192 bottom) - Value: constant (brightness)</p> <p>Input ports:</p> Port Type Shape Description <code>projections</code> <code>float32</code> <code>(B,H,W,num_components)</code> PCA projections <p>Example:</p> <pre><code>from cuvis_ai.node.visualizations import PCAVisualization\n\npca_viz = PCAVisualization(up_to=3)\n\npipeline.connect(\n    (pca_node.projections, pca_viz.projections),\n    (pca_viz.artifacts, monitor.artifacts),\n)\n</code></pre> <p>TensorBoard images: <pre><code>val/pca_projection_img00\nval/pca_projection_img01\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#cubergbvisualizer","title":"CubeRGBVisualizer","text":"<p>False-color RGB from selected hyperspectral channels.</p> <p>Visualization output: 1. False-color RGB: Top 3 weighted channels mapped to R, G, B 2. Channel Weights Bar Chart: All channels with top 3 highlighted</p> <p>Input ports:</p> Port Type Shape Description <code>cube</code> <code>float32</code> <code>(B,H,W,C)</code> Hyperspectral cube <code>weights</code> <code>float32</code> <code>(C,)</code> Channel selection weights <code>wavelengths</code> <code>int32</code> <code>(C,)</code> Wavelengths per channel <p>Example:</p> <pre><code>from cuvis_ai.node.visualizations import CubeRGBVisualizer\n\nrgb_viz = CubeRGBVisualizer(up_to=5)\n\npipeline.connect(\n    (data_node.outputs.cube, rgb_viz.cube),\n    (selector.weights, rgb_viz.weights),\n    (data_node.outputs.wavelengths, rgb_viz.wavelengths),\n    (rgb_viz.artifacts, monitor.artifacts),\n)\n</code></pre> <p>TensorBoard images: <pre><code>val/viz_rgb_sample_0\nval/viz_rgb_sample_1\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#drcnntensorboardviz","title":"DRCNNTensorBoardViz","text":"<p>Specialized visualizations for DRCNN pipelines.</p> <p>Visualization output: 1. HSI Input: False-color RGB from selected channels 2. Mixer Output: AdaClip RGB input 3. Ground Truth Mask 4. AdaClip Scores: Anomaly heatmap</p> <p>Parameters:</p> Parameter Type Default Description <code>hsi_channels</code> list[int] <code>[0, 20, 40]</code> Channels for false-color RGB <code>max_samples</code> int <code>4</code> Max samples per batch <code>log_every_n_batches</code> int <code>1</code> Logging frequency <p>Input ports:</p> Port Type Shape Description <code>hsi_cube</code> <code>float32</code> <code>(B,H,W,C)</code> Input hyperspectral cube <code>mixer_output</code> <code>float32</code> <code>(B,H,W,3)</code> Mixed RGB output <code>ground_truth_mask</code> <code>bool</code> <code>(B,H,W,1)</code> Ground truth mask <code>adaclip_scores</code> <code>float32</code> <code>(B,H,W,1)</code> AdaClip anomaly scores <p>Execution: TRAIN, VAL, TEST stages</p> <p>Example:</p> <pre><code>from cuvis_ai.node.drcnn_tensorboard_viz import DRCNNTensorBoardViz\n\ndrcnn_viz = DRCNNTensorBoardViz(\n    hsi_channels=[0, 20, 40],\n    max_samples=4,\n    log_every_n_batches=1,\n)\n\npipeline.connect(\n    (data_node.outputs.cube, drcnn_viz.hsi_cube),\n    (mixer.output, drcnn_viz.mixer_output),\n    (data_node.outputs.mask, drcnn_viz.ground_truth_mask),\n    (adaclip.scores, drcnn_viz.adaclip_scores),\n    (drcnn_viz.artifacts, monitor.artifacts),\n)\n</code></pre> <p>TensorBoard images: <pre><code>train/hsi_input_sample_0\ntrain/mixer_output_adaclip_input_sample_0\ntrain/ground_truth_mask_sample_0\ntrain/adaclip_scores_heatmap_sample_0\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#complete-examples","title":"Complete Examples","text":""},{"location":"how-to/monitoring-and-viz/#example-1-rx-statistical-training","title":"Example 1: RX Statistical Training","text":"<p>Simple monitoring for statistical initialization.</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.conversion import ScoreToLogit\nfrom cuvis_ai.anomaly.binary_decider import BinaryDecider\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.visualizations import AnomalyMask, ScoreHeatmapVisualizer\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\n\n# Create pipeline\npipeline = CuvisPipeline(\"RX_Statistical\")\n\n# Processing nodes\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\nrx = RXGlobal(num_channels=61, eps=1.0e-6)\nlogit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\ndecider = BinaryDecider(threshold=0.5)\n\n# Monitoring nodes\nmetrics = AnomalyDetectionMetrics()\nviz_mask = AnomalyMask(channel=30, up_to=5)\nscore_viz = ScoreHeatmapVisualizer(up_to=5)\nmonitor = TensorBoardMonitorNode(\n    output_dir=\"./outputs/rx_statistical/tensorboard\",\n    run_name=\"rx_baseline\",\n)\n\n# Connect processing\npipeline.connect(\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, rx.data),\n    (rx.scores, logit_head.scores),\n    (logit_head.logits, decider.logits),\n)\n\n# Connect monitoring\npipeline.connect(\n    # Metrics\n    (decider.decisions, metrics.decisions),\n    (data_node.outputs.mask, metrics.targets),\n    (rx.scores, metrics.logits),\n\n    # Visualizations\n    (decider.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (data_node.outputs.cube, viz_mask.cube),\n    (rx.scores, viz_mask.scores),\n    (rx.scores, score_viz.scores),\n\n    # Monitor sink\n    (metrics.metrics, monitor.metrics),\n    (viz_mask.artifacts, monitor.artifacts),\n    (score_viz.artifacts, monitor.artifacts),\n)\n\n# Validate\npipeline.validate()\n\n# Statistical training (no gradients)\nfrom cuvis_ai.trainers.statistical_trainer import StatisticalTrainer\nfrom cuvis_ai.datamodule.cu3s_datamodule import Cu3sDataModule\n\ndatamodule = Cu3sDataModule(\n    cu3s_file_path=\"data/Lentils/Lentils_000.cu3s\",\n    train_ids=[0, 2, 3],\n    val_ids=[1, 5],\n    test_ids=[1, 5],\n    batch_size=1,\n)\n\ntrainer = StatisticalTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    metric_nodes=[metrics],\n    monitors=[monitor],\n)\n\ntrainer.fit()\ntrainer.test()\n</code></pre> <p>View results: <pre><code>tensorboard --logdir=./outputs/rx_statistical/tensorboard\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#example-2-drcnn-adaclip-gradient-training","title":"Example 2: DRCNN + AdaClip Gradient Training","text":"<p>Comprehensive monitoring with multiple visualizations and losses.</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.anomaly.learnable_channel_mixer import LearnableChannelMixer\nfrom cuvis_ai.anomaly.adaclip_anomaly_detector import AdaClipAnomalyDetector\nfrom cuvis_ai.anomaly.binary_decider import BinaryDecider\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.visualizations import AnomalyMask, ScoreHeatmapVisualizer\nfrom cuvis_ai.node.drcnn_tensorboard_viz import DRCNNTensorBoardViz\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.anomaly.iou_loss import IoULoss\n\n# Create pipeline\npipeline = CuvisPipeline(\"DRCNN_AdaClip_Gradient\")\n\n# Processing nodes\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\nchannel_mixer = LearnableChannelMixer(num_channels=61)\nadaclip = AdaClipAnomalyDetector()\ndecider = BinaryDecider(threshold=0.5)\niou_loss = IoULoss(weight=1.0, normalize_method=\"minmax\")\n\n# Monitoring nodes\nmetrics = AnomalyDetectionMetrics()\nviz_mask = AnomalyMask(channel=30, up_to=5)\nscore_viz = ScoreHeatmapVisualizer(up_to=5)\ndrcnn_viz = DRCNNTensorBoardViz(\n    hsi_channels=[0, 20, 40],\n    max_samples=4,\n    log_every_n_batches=1,\n)\nmonitor = TensorBoardMonitorNode(\n    output_dir=\"./outputs/drcnn_adaclip/tensorboard\",\n    run_name=\"drcnn_baseline\",\n)\n\n# Connect processing\npipeline.connect(\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, channel_mixer.data),\n    (channel_mixer.output, adaclip.image),\n    (adaclip.scores, decider.scores),\n\n    # Loss\n    (decider.decisions, iou_loss.predictions),\n    (data_node.outputs.mask, iou_loss.targets),\n)\n\n# Connect monitoring\npipeline.connect(\n    # Metrics\n    (decider.decisions, metrics.decisions),\n    (data_node.outputs.mask, metrics.targets),\n    (adaclip.scores, metrics.logits),\n\n    # Visualizations\n    (decider.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (normalizer.normalized, viz_mask.cube),\n    (adaclip.scores, viz_mask.scores),\n    (adaclip.scores, score_viz.scores),\n\n    # DRCNN-specific visualization\n    (data_node.outputs.cube, drcnn_viz.hsi_cube),\n    (channel_mixer.output, drcnn_viz.mixer_output),\n    (data_node.outputs.mask, drcnn_viz.ground_truth_mask),\n    (adaclip.scores, drcnn_viz.adaclip_scores),\n\n    # Monitor sink\n    (metrics.metrics, monitor.metrics),\n    (viz_mask.artifacts, monitor.artifacts),\n    (score_viz.artifacts, monitor.artifacts),\n    (drcnn_viz.artifacts, monitor.artifacts),\n)\n\n# Validate\npipeline.validate()\n\n# Gradient training\nfrom cuvis_ai.trainers.gradient_trainer import GradientTrainer\nfrom cuvis_ai.datamodule.cu3s_datamodule import Cu3sDataModule\n\ndatamodule = Cu3sDataModule(\n    cu3s_file_path=\"data/Lentils/Lentils_000.cu3s\",\n    train_ids=[0],\n    val_ids=[3, 4],\n    test_ids=[1, 5],\n    batch_size=1,\n)\n\ntrainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[iou_loss],\n    metric_nodes=[metrics],\n    trainer_config={\n        \"max_epochs\": 20,\n        \"log_every_n_steps\": 10,\n        \"val_check_interval\": 1.0,\n    },\n    optimizer_config={\n        \"name\": \"adamw\",\n        \"lr\": 0.001,\n        \"weight_decay\": 0.01,\n    },\n    monitors=[monitor],\n)\n\ntrainer.fit()\ntrainer.test()\n</code></pre> <p>View results: <pre><code>tensorboard --logdir=./outputs/drcnn_adaclip/tensorboard\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#example-3-multi-loss-training-with-concrete-band-selector","title":"Example 3: Multi-Loss Training with Concrete Band Selector","text":"<p>Monitor multiple loss components and selector diversity.</p> <pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.anomaly.concrete_band_selector import ConcreteBandSelector\nfrom cuvis_ai.anomaly.adaclip_anomaly_detector import AdaClipAnomalyDetector\nfrom cuvis_ai.anomaly.binary_decider import BinaryDecider\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics, SelectorEntropyMetric\nfrom cuvis_ai.node.visualizations import AnomalyMask, CubeRGBVisualizer\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.anomaly.iou_loss import IoULoss\nfrom cuvis_ai.anomaly.distinctness_loss import DistinctnessLoss\n\n# Create pipeline\npipeline = CuvisPipeline(\"Concrete_AdaClip\")\n\n# Processing nodes\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\nselector = ConcreteBandSelector(\n    num_channels=61,\n    num_bands_to_select=3,\n    temperature=0.5,\n)\nadaclip = AdaClipAnomalyDetector()\ndecider = BinaryDecider(threshold=0.5)\n\n# Loss nodes\niou_loss = IoULoss(weight=1.0, normalize_method=\"minmax\")\ndistinctness_loss = DistinctnessLoss(weight=0.1)\n\n# Monitoring nodes\nmetrics = AnomalyDetectionMetrics()\nselector_entropy = SelectorEntropyMetric()\nviz_mask = AnomalyMask(channel=30, up_to=5)\nrgb_viz = CubeRGBVisualizer(up_to=5)\nmonitor = TensorBoardMonitorNode(\n    output_dir=\"./outputs/concrete_adaclip/tensorboard\",\n    run_name=\"concrete_baseline\",\n)\n\n# Connect processing\npipeline.connect(\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, selector.data),\n    (selector.selected_bands, adaclip.image),\n    (adaclip.scores, decider.scores),\n\n    # Losses\n    (decider.decisions, iou_loss.predictions),\n    (data_node.outputs.mask, iou_loss.targets),\n    (selector.weights, distinctness_loss.weights),\n)\n\n# Connect monitoring\npipeline.connect(\n    # Metrics\n    (decider.decisions, metrics.decisions),\n    (data_node.outputs.mask, metrics.targets),\n    (adaclip.scores, metrics.logits),\n    (selector.weights, selector_entropy.weights),\n\n    # Visualizations\n    (decider.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (normalizer.normalized, viz_mask.cube),\n    (normalizer.normalized, rgb_viz.cube),\n    (selector.weights, rgb_viz.weights),\n    (data_node.outputs.wavelengths, rgb_viz.wavelengths),\n\n    # Monitor sink\n    (metrics.metrics, monitor.metrics),\n    (selector_entropy.metrics, monitor.metrics),\n    (viz_mask.artifacts, monitor.artifacts),\n    (rgb_viz.artifacts, monitor.artifacts),\n)\n\n# Validate\npipeline.validate()\n\n# Gradient training with multiple losses\nfrom cuvis_ai.trainers.gradient_trainer import GradientTrainer\nfrom cuvis_ai.datamodule.cu3s_datamodule import Cu3sDataModule\n\ndatamodule = Cu3sDataModule(\n    cu3s_file_path=\"data/Lentils/Lentils_000.cu3s\",\n    train_ids=[0],\n    val_ids=[3, 4],\n    test_ids=[1, 5],\n    batch_size=1,\n)\n\ntrainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[iou_loss, distinctness_loss],  # Multiple losses\n    metric_nodes=[metrics],\n    trainer_config={\n        \"max_epochs\": 20,\n        \"log_every_n_steps\": 10,\n    },\n    optimizer_config={\n        \"name\": \"adamw\",\n        \"lr\": 0.001,\n    },\n    monitors=[monitor],\n)\n\ntrainer.fit()\n</code></pre> <p>Monitor selector entropy and loss components in TensorBoard to ensure balanced learning.</p>"},{"location":"how-to/monitoring-and-viz/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"how-to/monitoring-and-viz/#trainrun-yaml-configuration","title":"TrainRun YAML Configuration","text":"<pre><code># @package _global_\n\nname: drcnn_adaclip\noutput_dir: ./outputs/${name}\n\ndefaults:\n  - /pipeline@pipeline: drcnn_adaclip\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n\ntraining:\n  seed: 42\n  trainer:\n    max_epochs: 20\n    accelerator: auto\n    devices: 1\n    log_every_n_steps: 10        # Log metrics every 10 steps\n    val_check_interval: 1.0      # Validate every epoch\n    enable_checkpointing: true\n\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n\n  scheduler:\n    name: reduce_on_plateau\n    monitor: metrics_anomaly/iou  # Monitor IoU for scheduler\n    mode: max\n    factor: 0.5\n    patience: 5\n\nloss_nodes:\n  - iou_loss\n\nmetric_nodes:\n  - metrics_anomaly\n\nunfreeze_nodes:\n  - channel_mixer\n</code></pre> <p>TensorBoard logging: - Logs are written to <code>./outputs/drcnn_adaclip/tensorboard/</code> - <code>log_every_n_steps: 10</code> logs metrics every 10 training steps - <code>val_check_interval: 1.0</code> runs validation (and visualizations) every epoch</p>"},{"location":"how-to/monitoring-and-viz/#output-directory-structure","title":"Output Directory Structure","text":"<pre><code>outputs/drcnn_adaclip/\n\u251c\u2500\u2500 tensorboard/\n\u2502   \u251c\u2500\u2500 drcnn_baseline/\n\u2502   \u2502   \u2514\u2500\u2500 events.out.tfevents.TIMESTAMP.hostname\n\u2502   \u251c\u2500\u2500 drcnn_baseline_run_02/\n\u2502   \u2514\u2500\u2500 drcnn_baseline_run_03/\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u251c\u2500\u2500 epoch=00.ckpt\n\u2502   \u251c\u2500\u2500 epoch=01.ckpt\n\u2502   \u2514\u2500\u2500 last.ckpt\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 DRCNN_AdaClip_Gradient.png\n\u2502   \u2514\u2500\u2500 DRCNN_AdaClip_Gradient.md\n\u2514\u2500\u2500 trained_models/\n    \u251c\u2500\u2500 DRCNN_AdaClip_Gradient.yaml\n    \u251c\u2500\u2500 DRCNN_AdaClip_Gradient.pt\n    \u2514\u2500\u2500 drcnn_adaclip_trainrun.yaml\n</code></pre>"},{"location":"how-to/monitoring-and-viz/#best-practices","title":"Best Practices","text":""},{"location":"how-to/monitoring-and-viz/#1-visualization-performance","title":"1. Visualization Performance","text":"<p>Limit visualizations to reduce overhead:</p> <pre><code># Good: Limit images per batch\nviz_mask = AnomalyMask(channel=30, up_to=5)  # Max 5 images\nscore_viz = ScoreHeatmapVisualizer(up_to=5)\n\n# Good: Log less frequently for large datasets\ndrcnn_viz = DRCNNTensorBoardViz(log_every_n_batches=5)  # Every 5th batch\n</code></pre> <p>Avoid: <pre><code># Bad: Visualize all images (slow, large disk usage)\nviz_mask = AnomalyMask(up_to=1000)\n</code></pre></p> <p>Execution stages: - Visualization nodes run during VAL, TEST, INFERENCE by default - This avoids training slowdown - TensorBoardMonitorNode runs during all stages to accept any inputs</p>"},{"location":"how-to/monitoring-and-viz/#2-tensorboard-organization","title":"2. TensorBoard Organization","text":"<p>Use descriptive run names:</p> <pre><code># Good: Clear experiment identification\nmonitor = TensorBoardMonitorNode(\n    output_dir=\"./outputs/my_experiment/tensorboard\",\n    run_name=f\"lr_{lr}_bs_{batch_size}_aug\",\n)\n</code></pre> <p>Group related experiments:</p> <pre><code># Directory structure for hyperparameter sweeps\noutputs/\n\u2514\u2500\u2500 channel_selector/\n    \u2514\u2500\u2500 tensorboard/\n        \u251c\u2500\u2500 lr_0.001_bs_1/\n        \u251c\u2500\u2500 lr_0.0001_bs_1/\n        \u251c\u2500\u2500 lr_0.001_bs_2/\n        \u2514\u2500\u2500 lr_0.0001_bs_2/\n\n# View all runs together\ntensorboard --logdir=outputs/channel_selector/tensorboard\n</code></pre> <p>Monitor disk usage: - TensorBoard event files can grow large (100s of MB) - Set <code>flush_secs</code> appropriately (default: 120 seconds) - Clean old runs periodically</p>"},{"location":"how-to/monitoring-and-viz/#3-monitoring-strategy","title":"3. Monitoring Strategy","text":"<p>Core metrics always:</p> <pre><code># Always include basic performance metrics\nmetrics = AnomalyDetectionMetrics()\npipeline.connect(\n    (decider.decisions, metrics.decisions),\n    (data_node.outputs.mask, metrics.targets),\n    (metrics.metrics, monitor.metrics),\n)\n</code></pre> <p>Add visualizations for validation:</p> <pre><code># Visualizations for VAL/TEST only (no training overhead)\nviz_mask = AnomalyMask(channel=30, up_to=5)\nscore_viz = ScoreHeatmapVisualizer(up_to=5)\n</code></pre> <p>Specialized metrics for debugging:</p> <pre><code># Add selector/PCA metrics when debugging specific components\nselector_entropy = SelectorEntropyMetric()\northogonality = ComponentOrthogonalityMetric()\nscore_stats = ScoreStatisticsMetric()\n</code></pre>"},{"location":"how-to/monitoring-and-viz/#4-metric-selection-for-scheduler","title":"4. Metric Selection for Scheduler","text":"<p>Monitor appropriate metrics:</p> <pre><code># Good: Monitor IoU for anomaly detection\nscheduler:\n  name: reduce_on_plateau\n  monitor: metrics_anomaly/iou\n  mode: max\n\n# Good: Monitor loss for early stopping\ncallbacks:\n  early_stopping:\n    monitor: val/loss\n    mode: min\n</code></pre> <p>Verify metric names: - Check TensorBoard to see exact metric names - Metric tags: <code>val/precision</code>, <code>val/iou</code>, <code>scores/mean</code>, etc. - Loss tags: <code>train/loss</code>, <code>val/loss</code></p>"},{"location":"how-to/monitoring-and-viz/#5-checkpoint-strategy","title":"5. Checkpoint Strategy","text":"<p>Enable checkpointing with metric monitoring:</p> <pre><code>training:\n  trainer:\n    enable_checkpointing: true\n    callbacks:\n      model_checkpoint:\n        dirpath: outputs/${name}/checkpoints\n        monitor: metrics_anomaly/iou\n        mode: max\n        save_top_k: 3\n        save_last: true\n</code></pre> <p>Benefits: - Save best models based on validation metrics - Resume training from checkpoints - Compare multiple checkpoints in TensorBoard</p>"},{"location":"how-to/monitoring-and-viz/#6-logging-intervals","title":"6. Logging Intervals","text":"<p>Balance between detail and performance:</p> <pre><code>training:\n  trainer:\n    log_every_n_steps: 10      # Log metrics every 10 steps\n    val_check_interval: 1.0    # Validate every epoch (1.0 = 100%)\n</code></pre> <p>For large datasets: <pre><code>training:\n  trainer:\n    log_every_n_steps: 50      # Less frequent logging\n    val_check_interval: 0.5    # Validate every half-epoch\n</code></pre></p> <p>For small datasets: <pre><code>training:\n  trainer:\n    log_every_n_steps: 1       # Log every step\n    val_check_interval: 1.0    # Validate every epoch\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/monitoring-and-viz/#tensorboard-not-showing-images","title":"TensorBoard Not Showing Images","text":"<p>Problem: Metrics appear but images are missing.</p> <p>Solution: Verify artifact connections:</p> <pre><code># Ensure artifacts are connected to monitor\npipeline.connect(\n    (viz_mask.artifacts, monitor.artifacts),\n    (score_viz.artifacts, monitor.artifacts),\n)\n\n# Check execution stages\nprint(viz_mask.execution_stages)  # Should include VAL/TEST\n</code></pre>"},{"location":"how-to/monitoring-and-viz/#metrics-not-logging","title":"Metrics Not Logging","text":"<p>Problem: No metrics in TensorBoard.</p> <p>Solution: Check metric node connections:</p> <pre><code># Connect metrics to monitor\npipeline.connect(\n    (metrics.metrics, monitor.metrics),\n)\n\n# Verify metric nodes are passed to trainer\ntrainer = GradientTrainer(\n    metric_nodes=[metrics],  # \u2190 Must specify\n    monitors=[monitor],\n)\n</code></pre>"},{"location":"how-to/monitoring-and-viz/#visualizations-slow-training","title":"Visualizations Slow Training","text":"<p>Problem: Training is slow with visualizations enabled.</p> <p>Solution: Limit visualization frequency:</p> <pre><code># Reduce images per batch\nviz_mask = AnomalyMask(up_to=3)  # Instead of up_to=10\n\n# Log less frequently\ndrcnn_viz = DRCNNTensorBoardViz(log_every_n_batches=10)  # Every 10th batch\n</code></pre> <p>Verify execution stages: <pre><code># Visualizations should NOT run during training\nassert ExecutionStage.TRAIN not in viz_mask.execution_stages\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#scheduler-not-responding","title":"Scheduler Not Responding","text":"<p>Problem: Learning rate scheduler doesn't reduce loss.</p> <p>Solution: Verify monitored metric name:</p> <pre><code># Check exact metric name in TensorBoard\nscheduler:\n  name: reduce_on_plateau\n  monitor: metrics_anomaly/iou  # Must match TensorBoard tag exactly\n  mode: max\n</code></pre> <p>Debug: <pre><code># Print available metrics\nfor metric in metrics.metrics:\n    print(f\"{metric.name}: {metric.value}\")\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#disk-space-issues","title":"Disk Space Issues","text":"<p>Problem: TensorBoard logs consuming too much disk space.</p> <p>Solution: Clean old runs and adjust logging:</p> <pre><code># Remove old runs\nrm -rf outputs/*/tensorboard/old_run_*\n\n# Or compress old runs\ntar -czf old_runs.tar.gz outputs/*/tensorboard/run_01/\nrm -rf outputs/*/tensorboard/run_01/\n</code></pre> <p>Reduce logging frequency: <pre><code>monitor = TensorBoardMonitorNode(\n    flush_secs=300,  # Flush less frequently (5 minutes)\n)\n\nviz = DRCNNTensorBoardViz(\n    log_every_n_batches=10,  # Log less often\n    max_samples=2,  # Fewer images\n)\n</code></pre></p>"},{"location":"how-to/monitoring-and-viz/#see-also","title":"See Also","text":"<ul> <li>Node Catalog:</li> <li>Visualization Nodes - TensorBoardMonitorNode and visual monitoring</li> <li>Metrics Nodes - AnomalyDetectionMetrics, ScoreStatistics, etc.</li> <li>Visualization Nodes - All visualization node details</li> <li>Guides:</li> <li>Build Pipelines in Python - Pipeline construction basics</li> <li>Configuration Guide - TrainRun configuration</li> <li>Examples:</li> <li><code>examples/adaclip/drcnn_adaclip_gradient_training.py</code> - Full DRCNN monitoring example</li> <li><code>examples/adaclip/concrete_adaclip_gradient_training.py</code> - Multi-loss monitoring example</li> </ul>"},{"location":"how-to/remote-grpc/","title":"Remote gRPC","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/remote-grpc/#remote-grpc-access","title":"Remote gRPC Access","text":"<p>Access CUVIS.AI pipelines remotely using gRPC for distributed training, inference, and deployment.</p> <p>Comprehensive gRPC Documentation</p> <p>For complete gRPC documentation, see:</p> <ul> <li>gRPC Overview - Introduction and architecture</li> <li>gRPC API Reference - Complete documentation of all 46 RPC methods</li> <li>Client Patterns - Best practices and common patterns</li> </ul> <p>This guide focuses on detailed examples and deployment scenarios.</p>"},{"location":"how-to/remote-grpc/#overview","title":"Overview","text":"<p>CUVIS.AI provides comprehensive gRPC infrastructure for remote pipeline execution:</p> <ul> <li>Server Deployment: Production-ready server with Docker/Kubernetes support</li> <li>Client SDK: Simple API for training, inference, and configuration</li> <li>Session Management: Isolated execution contexts with automatic cleanup</li> <li>Streaming Updates: Real-time training progress without polling</li> <li>Configuration Composition: Hydra integration with dynamic overrides</li> <li>Error Handling: Standard gRPC error codes with retry logic</li> </ul>"},{"location":"how-to/remote-grpc/#quick-start","title":"Quick Start","text":""},{"location":"how-to/remote-grpc/#server-setup","title":"Server Setup","text":"<p>Start server locally: <pre><code>uv run python -m cuvis_ai.grpc.production_server\n</code></pre></p> <p>Start server with Docker: <pre><code>docker-compose up\n</code></pre></p> <p>Default connection: <code>localhost:50051</code></p>"},{"location":"how-to/remote-grpc/#basic-client-usage","title":"Basic Client Usage","text":"<pre><code>from cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\nimport grpc\n\n# Connect to server\nchannel = grpc.insecure_channel(\"localhost:50051\")\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n\n# Create session\nresponse = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\nsession_id = response.session_id\n\n# Run inference (example)\ninference_response = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(cube=...),\n    )\n)\n\n# Clean up\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre>"},{"location":"how-to/remote-grpc/#grpc-server-setup","title":"gRPC Server Setup","text":""},{"location":"how-to/remote-grpc/#local-development","title":"Local Development","text":"<p>Launch server: <pre><code>uv run python -m cuvis_ai.grpc.production_server\n</code></pre></p> <p>Default configuration: - Host: <code>0.0.0.0</code> (all network interfaces) - Port: <code>50051</code> - Max workers: <code>10</code> (thread pool) - Max message size: <code>300 MB</code></p> <p>Environment variables: <pre><code>export GRPC_PORT=50051\nexport GRPC_MAX_WORKERS=10\nexport LOG_LEVEL=INFO\nexport DATA_DIR=/path/to/data\n</code></pre></p>"},{"location":"how-to/remote-grpc/#docker-deployment","title":"Docker Deployment","text":"<p>Dockerfile (production-ready): <pre><code>FROM cubertgmbh/cuvis_python:3.4.1-ubuntu24.04\n\nWORKDIR /app\n\n# Install dependencies\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\n# Copy application\nCOPY cuvis_ai/ cuvis_ai/\nCOPY configs/ configs/\n\nEXPOSE 50051\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s \\\n    CMD python -c \"import grpc; ...\"\n\nCMD [\"python\", \"-m\", \"cuvis_ai.grpc.production_server\"]\n</code></pre></p> <p>Build and run: <pre><code>docker build -t cuvis-ai-server .\ndocker run -p 50051:50051 --gpus all cuvis-ai-server\n</code></pre></p>"},{"location":"how-to/remote-grpc/#docker-compose","title":"Docker Compose","text":"<p>docker-compose.yml: <pre><code>version: \"3.8\"\nservices:\n  cuvis-ai-server:\n    build: .\n    ports:\n      - \"50051:50051\"\n    environment:\n      - GRPC_PORT=50051\n      - GRPC_MAX_WORKERS=10\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./data:/app/data\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre></p> <p>Start services: <pre><code>docker-compose up\n</code></pre></p>"},{"location":"how-to/remote-grpc/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>deployment.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cuvis-ai-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cuvis-ai-server\n  template:\n    metadata:\n      labels:\n        app: cuvis-ai-server\n    spec:\n      containers:\n      - name: cuvis-ai-server\n        image: cuvis-ai-server:latest\n        ports:\n        - containerPort: 50051\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: 8Gi\n          requests:\n            memory: 4Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cuvis-ai-service\nspec:\n  selector:\n    app: cuvis-ai-server\n  ports:\n  - protocol: TCP\n    port: 50051\n    targetPort: 50051\n  type: LoadBalancer\n</code></pre></p> <p>Deploy: <pre><code>kubectl apply -f deployment.yaml\nkubectl get service cuvis-ai-service\n</code></pre></p>"},{"location":"how-to/remote-grpc/#production-configuration","title":"Production Configuration","text":"<p>TLS/SSL (secure channel): <pre><code>import grpc\n\n# Server-side\ncredentials = grpc.ssl_server_credentials([\n    (open(\"server.key\", \"rb\").read(),\n     open(\"server.crt\", \"rb\").read())\n])\nserver.add_secure_port(\"[::]:50051\", credentials)\n\n# Client-side\nchannel = grpc.secure_channel(\n    \"production-server:50051\",\n    grpc.ssl_channel_credentials(),\n)\n</code></pre></p> <p>Message size limits: <pre><code>import grpc\nfrom concurrent import futures\n\n# Server\nserver = grpc.server(\n    futures.ThreadPoolExecutor(max_workers=10),\n    options=[\n        (\"grpc.max_send_message_length\", 1024 * 1024 * 1024),  # 1 GB\n        (\"grpc.max_receive_message_length\", 1024 * 1024 * 1024),\n    ],\n)\n\n# Client\noptions = [\n    (\"grpc.max_send_message_length\", 1024 * 1024 * 1024),\n    (\"grpc.max_receive_message_length\", 1024 * 1024 * 1024),\n]\nchannel = grpc.insecure_channel(\"localhost:50051\", options=options)\n</code></pre></p>"},{"location":"how-to/remote-grpc/#grpc-client-usage","title":"gRPC Client Usage","text":""},{"location":"how-to/remote-grpc/#helper-utilities","title":"Helper Utilities","text":"<p>Recommended approach: Use helper functions from <code>workflow_utils.py</code>:</p> <p>File: <code>examples/grpc/workflow_utils.py</code></p> <pre><code>from cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\nimport grpc\nfrom pathlib import Path\n\ndef build_stub(\n    server_address: str = \"localhost:50051\",\n    max_msg_size: int = 300 * 1024 * 1024\n) -&gt; cuvis_ai_pb2_grpc.CuvisAIServiceStub:\n    \"\"\"Create a gRPC stub with configured message limits.\"\"\"\n    options = [\n        (\"grpc.max_send_message_length\", max_msg_size),\n        (\"grpc.max_receive_message_length\", max_msg_size),\n    ]\n    channel = grpc.insecure_channel(server_address, options=options)\n    return cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n\ndef config_search_paths(\n    extra_paths: list[str] | None = None\n) -&gt; list[str]:\n    \"\"\"Return absolute search paths for Hydra composition.\"\"\"\n    config_root = Path(__file__).parent.parent / \"configs\"\n    seeds = [\n        config_root,\n        config_root / \"trainrun\",\n        config_root / \"pipeline\",\n        config_root / \"data\",\n        config_root / \"training\",\n    ]\n    paths = [str(p.resolve()) for p in seeds]\n    if extra_paths:\n        paths.extend(extra_paths)\n    return paths\n\ndef create_session_with_search_paths(\n    stub: cuvis_ai_pb2_grpc.CuvisAIServiceStub,\n    search_paths: list[str] | None = None\n) -&gt; str:\n    \"\"\"Create session and register config search paths.\"\"\"\n    response = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\n    session_id = response.session_id\n\n    paths = search_paths or config_search_paths()\n    stub.SetSessionSearchPaths(\n        cuvis_ai_pb2.SetSessionSearchPathsRequest(\n            session_id=session_id,\n            search_paths=paths,\n            append=False,\n        )\n    )\n    return session_id\n\ndef resolve_trainrun_config(\n    stub: cuvis_ai_pb2_grpc.CuvisAIServiceStub,\n    session_id: str,\n    name: str,\n    overrides: list[str] | None = None,\n):\n    \"\"\"Resolve trainrun config via Hydra composition.\"\"\"\n    import json\n    response = stub.ResolveConfig(\n        cuvis_ai_pb2.ResolveConfigRequest(\n            session_id=session_id,\n            config_type=\"trainrun\",\n            path=f\"trainrun/{name}\",\n            overrides=overrides or [],\n        )\n    )\n    config_dict = json.loads(response.config_bytes.decode(\"utf-8\"))\n    return response, config_dict\n\ndef apply_trainrun_config(\n    stub: cuvis_ai_pb2_grpc.CuvisAIServiceStub,\n    session_id: str,\n    config_bytes: bytes,\n):\n    \"\"\"Apply resolved trainrun config to session.\"\"\"\n    stub.SetTrainRunConfig(\n        cuvis_ai_pb2.SetTrainRunConfigRequest(\n            session_id=session_id,\n            config=cuvis_ai_pb2.TrainRunConfig(config_bytes=config_bytes),\n        )\n    )\n</code></pre>"},{"location":"how-to/remote-grpc/#session-management","title":"Session Management","text":"<p>Create session: <pre><code>from workflow_utils import build_stub, create_session_with_search_paths\n\nstub = build_stub(\"localhost:50051\")\nsession_id = create_session_with_search_paths(stub)\n</code></pre></p> <p>Close session: <pre><code>stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre></p> <p>Session lifecycle: - Sessions are isolated (separate pipelines, configs, weights) - Sessions expire after 1 hour of inactivity - Always close sessions when done to free resources</p>"},{"location":"how-to/remote-grpc/#configuration-resolution","title":"Configuration Resolution","text":"<p>Resolve config with overrides: <pre><code>from workflow_utils import resolve_trainrun_config, apply_trainrun_config\n\n# Resolve trainrun config\nresolved, config_dict = resolve_trainrun_config(\n    stub,\n    session_id,\n    \"rx_statistical\",  # trainrun name\n    overrides=[\n        \"data.batch_size=4\",\n        \"training.trainer.max_epochs=10\",\n        \"training.optimizer.lr=0.001\",\n    ],\n)\n\n# Apply resolved config\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n</code></pre></p> <p>Available override patterns: <pre><code>overrides = [\n    # Training config\n    \"training.trainer.max_epochs=100\",\n    \"training.optimizer.lr=0.001\",\n    \"training.optimizer.weight_decay=0.01\",\n    \"training.scheduler.patience=10\",\n\n    # Data config\n    \"data.batch_size=16\",\n    \"data.train_ids=[0,1,2]\",\n    \"data.val_ids=[3,4]\",\n\n    # Pipeline node params\n    \"pipeline.nodes.channel_selector.params.tau_start=8.0\",\n    \"pipeline.nodes.rx_detector.params.eps=1e-6\",\n]\n</code></pre></p>"},{"location":"how-to/remote-grpc/#training-operations","title":"Training Operations","text":""},{"location":"how-to/remote-grpc/#statistical-training","title":"Statistical Training","text":"<pre><code># Run statistical training (streaming progress)\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n    epoch = progress.context.epoch\n    batch = progress.context.batch_idx\n\n    print(f\"[{stage}] {status} | epoch={epoch} batch={batch}\")\n\n    if progress.metrics:\n        metrics = dict(progress.metrics)\n        print(f\"  Metrics: {metrics}\")\n</code></pre>"},{"location":"how-to/remote-grpc/#gradient-training","title":"Gradient Training","text":"<pre><code># Run gradient training\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n\n    losses = dict(progress.losses) if progress.losses else {}\n    metrics = dict(progress.metrics) if progress.metrics else {}\n\n    print(f\"[{stage}] {status} | losses={losses} | metrics={metrics}\")\n</code></pre>"},{"location":"how-to/remote-grpc/#two-phase-training","title":"Two-Phase Training","text":"<pre><code># Phase 1: Statistical initialization\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    print(f\"[Statistical Init] {format_progress(progress)}\")\n\n# Phase 2: Gradient training\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    print(f\"[Gradient Training] {format_progress(progress)}\")\n</code></pre>"},{"location":"how-to/remote-grpc/#inference-operations","title":"Inference Operations","text":"<p>Basic inference: <pre><code>from cuvis_ai_core.grpc import helpers\nimport numpy as np\n\n# Prepare inputs\ncube = np.random.rand(1, 32, 32, 61).astype(np.float32)\nwavelengths = np.linspace(430, 910, 61).reshape(1, -1).astype(np.int32)\n\n# Run inference\nresponse = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(\n            cube=helpers.numpy_to_proto(cube),\n            wavelengths=helpers.numpy_to_proto(wavelengths),\n        ),\n    )\n)\n\n# Convert outputs\nresults = {\n    name: helpers.proto_to_numpy(tensor_proto)\n    for name, tensor_proto in response.outputs.items()\n}\n\nprint(f\"Outputs: {list(results.keys())}\")\nprint(f\"Decisions shape: {results['decider.decisions'].shape}\")\n</code></pre></p> <p>Inference with output filtering: <pre><code># Request specific outputs only\nresponse = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(cube=..., wavelengths=...),\n        output_specs=[\n            \"selector.selected\",\n            \"detector.scores\",\n            \"decider.decisions\",\n        ],\n    )\n)\n</code></pre></p> <p>Batch inference: <pre><code>from torch.utils.data import DataLoader\nfrom cuvis_ai.datamodule.cu3s_dataset import SingleCu3sDataset\n\ndataset = SingleCu3sDataset(\n    cu3s_file_path=\"data/Lentils/Lentils_000.cu3s\",\n    processing_mode=\"Reflectance\",\n)\ndataloader = DataLoader(dataset, batch_size=1)\n\nfor batch in dataloader:\n    inference_response = stub.Inference(\n        cuvis_ai_pb2.InferenceRequest(\n            session_id=session_id,\n            inputs=cuvis_ai_pb2.InputBatch(\n                cube=helpers.tensor_to_proto(batch[\"cube\"]),\n                wavelengths=helpers.tensor_to_proto(batch[\"wavelengths\"]),\n            ),\n        )\n    )\n\n    # Process results\n    decisions = helpers.proto_to_numpy(\n        inference_response.outputs[\"decider.decisions\"]\n    )\n</code></pre></p>"},{"location":"how-to/remote-grpc/#pipeline-management","title":"Pipeline Management","text":"<p>Load pipeline from config: <pre><code>import json\n\n# Resolve pipeline config\npipeline_config = stub.ResolveConfig(\n    cuvis_ai_pb2.ResolveConfigRequest(\n        session_id=session_id,\n        config_type=\"pipeline\",\n        path=\"pipeline/rx_statistical\",\n        overrides=[],\n    )\n)\n\n# Load pipeline\nstub.LoadPipeline(\n    cuvis_ai_pb2.LoadPipelineRequest(\n        session_id=session_id,\n        pipeline=cuvis_ai_pb2.PipelineConfig(\n            config_bytes=pipeline_config.config_bytes\n        ),\n    )\n)\n</code></pre></p> <p>Load trained weights: <pre><code>stub.LoadPipelineWeights(\n    cuvis_ai_pb2.LoadPipelineWeightsRequest(\n        session_id=session_id,\n        weights_path=\"outputs/my_experiment/weights.pt\",\n        strict=True,\n    )\n)\n</code></pre></p> <p>Save pipeline and weights: <pre><code>save_response = stub.SavePipeline(\n    cuvis_ai_pb2.SavePipelineRequest(\n        session_id=session_id,\n        pipeline_path=\"outputs/my_pipeline.yaml\",\n        metadata=cuvis_ai_pb2.PipelineMetadata(\n            name=\"My Pipeline\",\n            description=\"Trained anomaly detection pipeline\",\n        ),\n    )\n)\n\nprint(f\"Pipeline saved: {save_response.pipeline_path}\")\nprint(f\"Weights saved: {save_response.weights_path}\")\n</code></pre></p> <p>Get pipeline specifications: <pre><code># Get input specs\ninputs_response = stub.GetPipelineInputs(\n    cuvis_ai_pb2.GetPipelineInputsRequest(session_id=session_id)\n)\n\nfor name, spec in inputs_response.inputs.items():\n    dtype = cuvis_ai_pb2.DType.Name(spec.dtype)\n    print(f\"Input: {name} | dtype={dtype} | shape={spec.shape}\")\n\n# Get output specs\noutputs_response = stub.GetPipelineOutputs(\n    cuvis_ai_pb2.GetPipelineOutputsRequest(session_id=session_id)\n)\n\nfor name, spec in outputs_response.outputs.items():\n    dtype = cuvis_ai_pb2.DType.Name(spec.dtype)\n    print(f\"Output: {name} | dtype={dtype} | shape={spec.shape}\")\n</code></pre></p>"},{"location":"how-to/remote-grpc/#restore-trainrun","title":"Restore TrainRun","text":"<p>Restore from trainrun file: <pre><code>restore_response = stub.RestoreTrainRun(\n    cuvis_ai_pb2.RestoreTrainRunRequest(\n        trainrun_path=\"outputs/my_experiment/trainrun.yaml\",\n        weights_path=\"outputs/my_experiment/weights.pt\",  # optional\n        strict=True,\n    )\n)\n\nprint(f\"Restored session: {restore_response.session_id}\")\n</code></pre></p> <p>Continue training from checkpoint: <pre><code># Restore trainrun\nrestore_response = stub.RestoreTrainRun(\n    cuvis_ai_pb2.RestoreTrainRunRequest(\n        trainrun_path=\"outputs/my_experiment/trainrun.yaml\",\n        weights_path=\"outputs/my_experiment/checkpoints/epoch=10.ckpt\",\n    )\n)\n\n# Continue training\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=restore_response.session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    print(format_progress(progress))\n</code></pre></p>"},{"location":"how-to/remote-grpc/#complete-examples","title":"Complete Examples","text":""},{"location":"how-to/remote-grpc/#example-1-statistical-training","title":"Example 1: Statistical Training","text":"<p>File: <code>examples/grpc/statistical_training_client.py</code></p> <pre><code>from workflow_utils import (\n    build_stub, create_session_with_search_paths,\n    resolve_trainrun_config, apply_trainrun_config, format_progress\n)\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2\n\n# Connect to server\nstub = build_stub(\"localhost:50051\")\n\n# Create session\nsession_id = create_session_with_search_paths(stub)\n\n# Resolve trainrun config\nresolved, config_dict = resolve_trainrun_config(\n    stub,\n    session_id,\n    \"rx_statistical\",\n    overrides=[\n        \"data.batch_size=4\",\n        \"training.trainer.max_epochs=1\",\n        \"training.seed=42\",\n    ],\n)\n\n# Apply config\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n\n# Run statistical training\nprint(\"Running statistical training...\")\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    print(format_progress(progress))\n\n# Clean up\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\nprint(\"Done!\")\n</code></pre>"},{"location":"how-to/remote-grpc/#example-2-gradient-training","title":"Example 2: Gradient Training","text":"<p>File: <code>examples/grpc/gradient_training_client.py</code></p> <pre><code>from workflow_utils import (\n    build_stub, create_session_with_search_paths,\n    resolve_trainrun_config, apply_trainrun_config, format_progress\n)\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2\n\nstub = build_stub(\"localhost:50051\")\nsession_id = create_session_with_search_paths(stub)\n\n# Resolve gradient trainrun\nresolved, config_dict = resolve_trainrun_config(\n    stub,\n    session_id,\n    \"deep_svdd\",\n    overrides=[\n        \"training.trainer.max_epochs=5\",\n        \"training.optimizer.lr=0.0005\",\n        \"training.optimizer.weight_decay=0.005\",\n    ],\n)\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n\n# Statistical initialization\nprint(\"Statistical initialization...\")\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    print(format_progress(progress))\n\n# Gradient training\nprint(\"Gradient training...\")\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    print(format_progress(progress))\n\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre>"},{"location":"how-to/remote-grpc/#example-3-complete-workflow","title":"Example 3: Complete Workflow","text":"<p>File: <code>examples/grpc/complete_workflow_client.py</code></p> <pre><code>from workflow_utils import (\n    build_stub, create_session_with_search_paths,\n    resolve_trainrun_config, apply_trainrun_config, format_progress\n)\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2, helpers\nimport numpy as np\n\nstub = build_stub(\"localhost:50051\")\nsession_id = create_session_with_search_paths(stub)\n\n# 1. Resolve and apply config\nresolved, config_dict = resolve_trainrun_config(\n    stub, session_id, \"channel_selector\",\n    overrides=[\"training.trainer.max_epochs=3\"],\n)\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n\n# 2. Train pipeline\nprint(\"Training pipeline...\")\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    print(format_progress(progress))\n\n# 3. Save pipeline\nsave_response = stub.SavePipeline(\n    cuvis_ai_pb2.SavePipelineRequest(\n        session_id=session_id,\n        pipeline_path=\"outputs/my_pipeline.yaml\",\n    )\n)\nprint(f\"Saved: {save_response.pipeline_path}, {save_response.weights_path}\")\n\n# 4. Run inference\ncube = np.random.rand(1, 32, 32, 61).astype(np.float32)\nwavelengths = np.linspace(430, 910, 61).reshape(1, -1).astype(np.int32)\n\ninference = stub.Inference(\n    cuvis_ai_pb2.InferenceRequest(\n        session_id=session_id,\n        inputs=cuvis_ai_pb2.InputBatch(\n            cube=helpers.numpy_to_proto(cube),\n            wavelengths=helpers.numpy_to_proto(wavelengths),\n        ),\n    )\n)\n\nresults = {\n    name: helpers.proto_to_numpy(tensor_proto)\n    for name, tensor_proto in inference.outputs.items()\n}\nprint(f\"Inference outputs: {list(results.keys())}\")\n\n# 5. Clean up\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n</code></pre>"},{"location":"how-to/remote-grpc/#example-4-inference-with-pretrained-model","title":"Example 4: Inference with Pretrained Model","text":"<p>File: <code>examples/grpc/run_inference.py</code></p> <pre><code>from workflow_utils import build_stub, create_session_with_search_paths\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2, helpers\nfrom cuvis_ai.datamodule.cu3s_dataset import SingleCu3sDataset\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\n\ndef run_inference(\n    pipeline_path: Path,\n    weights_path: Path,\n    cu3s_file_path: Path,\n    server_address: str = \"localhost:50051\",\n):\n    stub = build_stub(server_address, max_msg_size=600 * 1024 * 1024)\n    session_id = create_session_with_search_paths(stub)\n\n    # Resolve pipeline config\n    pipeline_config = stub.ResolveConfig(\n        cuvis_ai_pb2.ResolveConfigRequest(\n            session_id=session_id,\n            config_type=\"pipeline\",\n            path=str(pipeline_path),\n            overrides=[],\n        )\n    )\n\n    # Load pipeline and weights\n    stub.LoadPipeline(\n        cuvis_ai_pb2.LoadPipelineRequest(\n            session_id=session_id,\n            pipeline=cuvis_ai_pb2.PipelineConfig(\n                config_bytes=pipeline_config.config_bytes\n            ),\n        )\n    )\n\n    stub.LoadPipelineWeights(\n        cuvis_ai_pb2.LoadPipelineWeightsRequest(\n            session_id=session_id,\n            weights_path=str(weights_path),\n            strict=True,\n        )\n    )\n\n    # Load data\n    dataset = SingleCu3sDataset(\n        cu3s_file_path=str(cu3s_file_path),\n        processing_mode=\"Reflectance\",\n    )\n    dataloader = DataLoader(dataset, batch_size=1)\n\n    # Run inference\n    print(f\"Running inference on {len(dataset)} samples...\")\n    for batch in dataloader:\n        inference_response = stub.Inference(\n            cuvis_ai_pb2.InferenceRequest(\n                session_id=session_id,\n                inputs=cuvis_ai_pb2.InputBatch(\n                    cube=helpers.tensor_to_proto(batch[\"cube\"]),\n                    wavelengths=helpers.tensor_to_proto(batch[\"wavelengths\"]),\n                ),\n            )\n        )\n\n        # Process outputs\n        decisions = helpers.proto_to_numpy(\n            inference_response.outputs[\"decider.decisions\"]\n        )\n        print(f\"Sample decisions: {decisions.shape}\")\n\n    stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n\nif __name__ == \"__main__\":\n    run_inference(\n        pipeline_path=Path(\"outputs/my_experiment/pipeline.yaml\"),\n        weights_path=Path(\"outputs/my_experiment/weights.pt\"),\n        cu3s_file_path=Path(\"data/Lentils/Lentils_000.cu3s\"),\n    )\n</code></pre>"},{"location":"how-to/remote-grpc/#example-5-restore-trainrun","title":"Example 5: Restore TrainRun","text":"<p>File: <code>examples/grpc/restore_trainrun_grpc.py</code></p> <pre><code>from workflow_utils import build_stub, create_session_with_search_paths, format_progress\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2\nfrom pathlib import Path\nfrom typing import Literal\n\ndef restore_trainrun_grpc(\n    trainrun_path: Path,\n    mode: Literal[\"info\", \"train\", \"validate\", \"test\"] = \"info\",\n    weights_path: Path | None = None,\n    server_address: str = \"localhost:50051\",\n):\n    stub = build_stub(server_address)\n    session_id = create_session_with_search_paths(stub)\n\n    # Restore trainrun\n    restore_response = stub.RestoreTrainRun(\n        cuvis_ai_pb2.RestoreTrainRunRequest(\n            trainrun_path=str(trainrun_path),\n            weights_path=str(weights_path) if weights_path else None,\n            strict=True,\n        )\n    )\n\n    if mode == \"info\":\n        # Display pipeline info\n        inputs = stub.GetPipelineInputs(\n            cuvis_ai_pb2.GetPipelineInputsRequest(session_id=session_id)\n        )\n        outputs = stub.GetPipelineOutputs(\n            cuvis_ai_pb2.GetPipelineOutputsRequest(session_id=session_id)\n        )\n\n        print(\"Pipeline Inputs:\")\n        for name, spec in inputs.inputs.items():\n            print(f\"  {name}: {cuvis_ai_pb2.DType.Name(spec.dtype)} {spec.shape}\")\n\n        print(\"Pipeline Outputs:\")\n        for name, spec in outputs.outputs.items():\n            print(f\"  {name}: {cuvis_ai_pb2.DType.Name(spec.dtype)} {spec.shape}\")\n\n    elif mode == \"train\":\n        # Continue training\n        print(\"Continuing training...\")\n        for progress in stub.Train(\n            cuvis_ai_pb2.TrainRequest(\n                session_id=session_id,\n                trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n            )\n        ):\n            print(format_progress(progress))\n\n    stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n\nif __name__ == \"__main__\":\n    restore_trainrun_grpc(\n        trainrun_path=Path(\"outputs/my_experiment/trainrun.yaml\"),\n        mode=\"train\",\n        weights_path=Path(\"outputs/my_experiment/checkpoints/epoch=10.ckpt\"),\n    )\n</code></pre>"},{"location":"how-to/remote-grpc/#error-handling-retries","title":"Error Handling &amp; Retries","text":""},{"location":"how-to/remote-grpc/#grpc-error-codes","title":"gRPC Error Codes","text":"<pre><code>import grpc\n\ntry:\n    response = stub.CreateSession(request)\nexcept grpc.RpcError as exc:\n    code = exc.code()\n    details = exc.details()\n\n    if code == grpc.StatusCode.INVALID_ARGUMENT:\n        print(f\"Invalid request: {details}\")\n    elif code == grpc.StatusCode.NOT_FOUND:\n        print(f\"Resource not found: {details}\")\n    elif code == grpc.StatusCode.FAILED_PRECONDITION:\n        print(f\"Operation not allowed: {details}\")\n    elif code == grpc.StatusCode.UNAVAILABLE:\n        print(f\"Server unavailable: {details}\")\n    elif code == grpc.StatusCode.INTERNAL:\n        print(f\"Internal error: {details}\")\n    else:\n        print(f\"gRPC error [{code}]: {details}\")\n</code></pre> <p>Common error scenarios: - <code>INVALID_ARGUMENT</code>: Malformed inputs (e.g., missing cube, invalid shape) - <code>NOT_FOUND</code>: Unknown session ID or checkpoint path - <code>FAILED_PRECONDITION</code>: Operation not allowed in current state (e.g., inference before loading pipeline) - <code>UNAVAILABLE</code>: Server not running or network issues - <code>RESOURCE_EXHAUSTED</code>: Message size exceeded or server overloaded - <code>INTERNAL</code>: Unexpected server error (check server logs)</p>"},{"location":"how-to/remote-grpc/#retry-logic-with-exponential-backoff","title":"Retry Logic with Exponential Backoff","text":"<pre><code>import time\nimport grpc\n\ndef train_with_retry(\n    stub,\n    session_id: str,\n    trainer_type,\n    max_retries: int = 3,\n):\n    \"\"\"Train with retry logic and exponential backoff.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            for progress in stub.Train(\n                cuvis_ai_pb2.TrainRequest(\n                    session_id=session_id,\n                    trainer_type=trainer_type,\n                )\n            ):\n                yield progress\n            break  # Success\n\n        except grpc.RpcError as e:\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f\"Retry {attempt + 1}/{max_retries} after {wait_time}s...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"Max retries reached. Error: {e.details()}\")\n                raise\n</code></pre>"},{"location":"how-to/remote-grpc/#health-checks","title":"Health Checks","text":"<pre><code>def check_server_health(server_address: str = \"localhost:50051\") -&gt; bool:\n    \"\"\"Check if server is healthy.\"\"\"\n    try:\n        stub = build_stub(server_address)\n        response = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\n        stub.CloseSession(\n            cuvis_ai_pb2.CloseSessionRequest(session_id=response.session_id)\n        )\n        return True\n    except grpc.RpcError:\n        return False\n\n# Usage\nif not check_server_health():\n    print(\"Server is not available!\")\nelse:\n    print(\"Server is healthy\")\n</code></pre>"},{"location":"how-to/remote-grpc/#best-practices","title":"Best Practices","text":""},{"location":"how-to/remote-grpc/#1-session-management","title":"1. Session Management","text":"<p>Always close sessions: <pre><code>session_id = None\ntry:\n    session_id = create_session_with_search_paths(stub)\n    # ... operations ...\nfinally:\n    if session_id:\n        stub.CloseSession(\n            cuvis_ai_pb2.CloseSessionRequest(session_id=session_id)\n        )\n</code></pre></p> <p>Use context managers: <pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef grpc_session(stub):\n    session_id = create_session_with_search_paths(stub)\n    try:\n        yield session_id\n    finally:\n        stub.CloseSession(\n            cuvis_ai_pb2.CloseSessionRequest(session_id=session_id)\n        )\n\n# Usage\nwith grpc_session(stub) as session_id:\n    # ... operations ...\n    pass\n# Session automatically closed\n</code></pre></p>"},{"location":"how-to/remote-grpc/#2-message-size-configuration","title":"2. Message Size Configuration","text":"<p>Client and server must agree: <pre><code># Client\nstub = build_stub(\"localhost:50051\", max_msg_size=1024 * 1024 * 1024)  # 1 GB\n\n# Server must also have 1 GB limits\n</code></pre></p> <p>Guidelines: - Default 300 MB: Suitable for most hyperspectral data - 600 MB: Large cubes or high-resolution data - 1 GB: Very large datasets or batch inference</p>"},{"location":"how-to/remote-grpc/#3-error-handling","title":"3. Error Handling","text":"<p>Catch specific errors: <pre><code>try:\n    response = stub.Inference(request)\nexcept grpc.RpcError as exc:\n    if exc.code() == grpc.StatusCode.RESOURCE_EXHAUSTED:\n        print(\"Message too large. Reduce batch size or increase limits.\")\n    elif exc.code() == grpc.StatusCode.UNAVAILABLE:\n        print(\"Server unavailable. Check connection and retry.\")\n    else:\n        raise\n</code></pre></p>"},{"location":"how-to/remote-grpc/#4-configuration-overrides","title":"4. Configuration Overrides","text":"<p>Use structured overrides: <pre><code># Good: Clear intent\noverrides = [\n    \"training.trainer.max_epochs=100\",\n    \"training.optimizer.lr=0.001\",\n    \"data.batch_size=16\",\n]\n\n# Avoid: Hardcoded strings\noverrides = [\"max_epochs=100\"]  # Ambiguous\n</code></pre></p>"},{"location":"how-to/remote-grpc/#5-streaming-progress","title":"5. Streaming Progress","text":"<p>Handle all progress states: <pre><code>for progress in stub.Train(...):\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n\n    if status == \"TRAIN_STATUS_RUNNING\":\n        # Update progress bar\n        pass\n    elif status == \"TRAIN_STATUS_COMPLETED\":\n        # Training finished\n        break\n    elif status == \"TRAIN_STATUS_FAILED\":\n        # Handle failure\n        print(f\"Training failed: {progress.error_message}\")\n        break\n</code></pre></p>"},{"location":"how-to/remote-grpc/#6-production-deployment","title":"6. Production Deployment","text":"<p>Use TLS in production: <pre><code># Never use insecure channels in production\n# BAD: grpc.insecure_channel(\"production-server:50051\")\n\n# GOOD: Use TLS\nchannel = grpc.secure_channel(\n    \"production-server:50051\",\n    grpc.ssl_channel_credentials(),\n)\n</code></pre></p> <p>Implement health checks: <pre><code># docker-compose.yml\nhealthcheck:\n  test: [\"CMD\", \"python\", \"-c\", \"import grpc; ...\"]\n  interval: 30s\n  timeout: 10s\n  retries: 3\n</code></pre></p> <p>Monitor resource usage: - Track active sessions - Monitor GPU memory - Log training progress - Set session timeout</p>"},{"location":"how-to/remote-grpc/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/remote-grpc/#server-not-running","title":"Server Not Running","text":"<p>Problem: <code>grpc.StatusCode.UNAVAILABLE</code></p> <p>Solution: <pre><code># Check if server is running\nps aux | grep production_server\n\n# Check port availability\nnetstat -an | grep 50051\n\n# Test connection\ntelnet localhost 50051\n\n# Restart server\ndocker-compose restart\n</code></pre></p>"},{"location":"how-to/remote-grpc/#message-size-exceeded","title":"Message Size Exceeded","text":"<p>Problem: <code>grpc.StatusCode.RESOURCE_EXHAUSTED</code></p> <p>Solution: <pre><code># Increase client limits\nstub = build_stub(\"localhost:50051\", max_msg_size=1024 * 1024 * 1024)\n\n# OR reduce batch size\noverrides=[\"data.batch_size=1\"]\n</code></pre></p>"},{"location":"how-to/remote-grpc/#session-expired","title":"Session Expired","text":"<p>Problem: <code>grpc.StatusCode.NOT_FOUND</code> (session not found)</p> <p>Solution: - Sessions expire after 1 hour of inactivity - Create a new session - Implement periodic activity to keep session alive</p> <pre><code>import threading\nimport time\n\ndef keep_alive(stub, session_id, interval=300):\n    \"\"\"Send periodic heartbeat to keep session alive.\"\"\"\n    while True:\n        time.sleep(interval)\n        try:\n            stub.GetPipelineInputs(\n                cuvis_ai_pb2.GetPipelineInputsRequest(session_id=session_id)\n            )\n        except grpc.RpcError:\n            break  # Session closed or expired\n</code></pre>"},{"location":"how-to/remote-grpc/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Problem: Training fails with CUDA OOM error</p> <p>Solution: <pre><code># Reduce batch size\noverrides=[\"data.batch_size=1\"]\n\n# Close unused sessions\nstub.CloseSession(...)\n\n# Monitor GPU memory\n# nvidia-smi\n</code></pre></p>"},{"location":"how-to/remote-grpc/#connection-timeout","title":"Connection Timeout","text":"<p>Problem: Long-running operations timeout</p> <p>Solution: <pre><code># Increase timeout for long operations\nchannel = grpc.insecure_channel(\n    \"localhost:50051\",\n    options=[\n        (\"grpc.max_receive_message_length\", 1024 * 1024 * 1024),\n        (\"grpc.keepalive_time_ms\", 30000),\n        (\"grpc.keepalive_timeout_ms\", 10000),\n    ],\n)\n</code></pre></p>"},{"location":"how-to/remote-grpc/#see-also","title":"See Also","text":"<ul> <li>API Reference:</li> <li>gRPC API Reference - Complete RPC method documentation</li> <li>Protocol Definitions - Message format specifications</li> <li>Deployment:</li> <li>gRPC Deployment Guide - Production deployment patterns</li> <li>Docker Configuration - Container setup</li> <li>Kubernetes Setup - Orchestration</li> <li>Tutorials:</li> <li>gRPC Workflow Tutorial - Comprehensive end-to-end guide</li> <li>Examples:</li> <li><code>examples/grpc/workflow_utils.py</code> - Helper utilities</li> <li><code>examples/grpc/statistical_training_client.py</code> - Statistical training</li> <li><code>examples/grpc/gradient_training_client.py</code> - Gradient training</li> <li><code>examples/grpc/complete_workflow_client.py</code> - End-to-end workflow</li> <li><code>examples/grpc/run_inference.py</code> - Inference with pretrained models</li> <li><code>examples/grpc/restore_trainrun_grpc.py</code> - TrainRun restoration</li> </ul>"},{"location":"how-to/restore-pipeline-trainrun/","title":"Restore Pipeline","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"how-to/restore-pipeline-trainrun/#how-to-restore-pipelines-from-trainruns","title":"How-To: Restore Pipelines from TrainRuns","text":""},{"location":"how-to/restore-pipeline-trainrun/#overview","title":"Overview","text":"<p>Learn how to restore trained pipelines from TrainRun experiments for inference, continued training, or validation. TrainRuns capture complete experiment state including pipeline configuration, data setup, training parameters, and model weights.</p>"},{"location":"how-to/restore-pipeline-trainrun/#prerequisites","title":"Prerequisites","text":"<ul> <li>cuvis-ai installed</li> <li>Completed training run with saved TrainRun configuration</li> <li>Basic understanding of Pipeline Lifecycle</li> <li>Familiarity with TrainRun Schema</li> </ul>"},{"location":"how-to/restore-pipeline-trainrun/#what-is-a-trainrun","title":"What is a TrainRun?","text":"<p>A TrainRun is a complete experiment specification that includes:</p> <pre><code>TrainRunConfig(\n    name=\"my_experiment\",           # Experiment identifier\n    pipeline={...},                  # Full pipeline structure\n    data={...},                      # Data loading configuration\n    training={...},                  # Training settings (optimizer, callbacks)\n    output_dir=\"outputs/my_exp/\",   # Results directory\n    loss_nodes=[\"bce_loss\"],        # Gradient training loss nodes\n    metric_nodes=[\"metrics\"],       # Evaluation metrics\n    freeze_nodes=[],                # Initially frozen nodes\n    unfreeze_nodes=[\"selector\"]     # Nodes to train\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#trainrun-output-structure","title":"TrainRun Output Structure","text":"<p>After training, your experiment directory contains:</p> <pre><code>outputs/my_experiment/\n\u251c\u2500\u2500 trained_models/\n\u2502   \u251c\u2500\u2500 My_Pipeline.yaml              # Pipeline configuration\n\u2502   \u251c\u2500\u2500 My_Pipeline.pt                # Model weights (trainable nodes)\n\u2502   \u2514\u2500\u2500 my_experiment_trainrun.yaml   # Complete TrainRun config\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u251c\u2500\u2500 epoch=00.ckpt                 # Periodic checkpoints\n\u2502   \u251c\u2500\u2500 epoch=05.ckpt\n\u2502   \u251c\u2500\u2500 last.ckpt                     # Latest epoch\n\u2502   \u2514\u2500\u2500 best.ckpt                     # Best metric checkpoint\n\u2514\u2500\u2500 tensorboard/                      # Training logs\n    \u2514\u2500\u2500 events.out.tfevents...\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#restoration-modes","title":"Restoration Modes","text":""},{"location":"how-to/restore-pipeline-trainrun/#mode-1-info-display-only","title":"Mode 1: Info (Display Only)","text":"<p>View experiment details without running:</p> <pre><code>uv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode info\n</code></pre> <p>Output: <pre><code>TrainRun: my_experiment\nPipeline: My_Pipeline (5 nodes, 7 connections)\nLoss nodes: bce_loss\nMetric nodes: metrics\nUnfreeze nodes: selector\nData: LentilsAnomalyDataNode (train/val/test: [0]/[3,4]/[1,5])\nTraining: AdamW (lr=0.001, max_epochs=50)\nOutput directory: outputs/my_experiment/\n</code></pre></p>"},{"location":"how-to/restore-pipeline-trainrun/#mode-2-train-run-training","title":"Mode 2: Train (Run Training)","text":"<p>Re-run or continue training:</p> <pre><code># Train from scratch\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train\n\n# Continue from checkpoint\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train \\\n  --checkpoint-path outputs/my_experiment/checkpoints/epoch=05.ckpt\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#mode-3-validate-validation-set","title":"Mode 3: Validate (Validation Set)","text":"<p>Run evaluation on validation set:</p> <pre><code>uv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode validate\n</code></pre> <p>Output: Validation metrics (IoU, AUC, F1, etc.)</p>"},{"location":"how-to/restore-pipeline-trainrun/#mode-4-test-test-set","title":"Mode 4: Test (Test Set)","text":"<p>Run evaluation on test set:</p> <pre><code>uv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode test\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#quick-inference-pipeline-only","title":"Quick Inference (Pipeline Only)","text":"<p>For simple inference without full TrainRun context:</p> <pre><code># Display pipeline structure\nuv run restore-pipeline \\\n  --pipeline-path outputs/my_experiment/trained_models/My_Pipeline.yaml\n\n# Run inference on .cu3s file\nuv run restore-pipeline \\\n  --pipeline-path outputs/my_experiment/trained_models/My_Pipeline.yaml \\\n  --cu3s-file-path data/test_sample.cu3s \\\n  --processing-mode Reflectance\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#configuration-overrides","title":"Configuration Overrides","text":"<p>Modify experiment settings during restoration without editing YAML files:</p> <pre><code>uv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train \\\n  --override output_dir=outputs/my_experiment_v2 \\\n  --override data.batch_size=32 \\\n  --override training.optimizer.lr=0.0001 \\\n  --override training.trainer.max_epochs=100 \\\n  --override nodes.2.params.threshold=0.8\n</code></pre> <p>Common overrides: - <code>output_dir</code> - Change save location - <code>data.batch_size</code> - Adjust batch size - <code>data.train_ids</code>, <code>data.val_ids</code>, <code>data.test_ids</code> - Change data splits - <code>training.optimizer.lr</code> - Modify learning rate - <code>training.trainer.max_epochs</code> - Adjust training duration - <code>nodes.N.params.*</code> - Override node parameters (N = node index)</p>"},{"location":"how-to/restore-pipeline-trainrun/#python-api-usage","title":"Python API Usage","text":""},{"location":"how-to/restore-pipeline-trainrun/#basic-restoration","title":"Basic Restoration","text":"<pre><code>from cuvis_ai.utils import restore_trainrun\n\n# Display info\nrestore_trainrun(\n    trainrun_path=\"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\",\n    mode=\"info\"\n)\n\n# Run training\nrestore_trainrun(\n    trainrun_path=\"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\",\n    mode=\"train\"\n)\n\n# Validate\nrestore_trainrun(\n    trainrun_path=\"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\",\n    mode=\"validate\"\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#restoration-with-overrides","title":"Restoration with Overrides","text":"<pre><code>from cuvis_ai.utils import restore_trainrun\nfrom omegaconf import OmegaConf\n\n# Load and modify config\nconfig_overrides = OmegaConf.create({\n    \"output_dir\": \"outputs/my_experiment_v2\",\n    \"training\": {\n        \"optimizer\": {\"lr\": 0.0001},\n        \"trainer\": {\"max_epochs\": 100}\n    },\n    \"data\": {\n        \"batch_size\": 32\n    }\n})\n\nrestore_trainrun(\n    trainrun_path=\"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\",\n    mode=\"train\",\n    config_overrides=config_overrides\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#manual-pipeline-loading","title":"Manual Pipeline Loading","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training.config import TrainRunConfig\n\n# Load TrainRun config\ntrainrun_config = TrainRunConfig.load_from_file(\n    \"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\"\n)\n\n# Extract and build pipeline\npipeline_config = trainrun_config.pipeline\npipeline = CuvisPipeline.from_config(pipeline_config)\n\n# Load weights\npipeline.load_weights(\n    \"outputs/my_experiment/trained_models/My_Pipeline.pt\",\n    strict=True\n)\n\n# Run inference\npipeline.validate()\nresult = pipeline.execute()\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#statistical-vs-gradient-training","title":"Statistical vs Gradient Training","text":""},{"location":"how-to/restore-pipeline-trainrun/#statistical-only-pipeline","title":"Statistical-Only Pipeline","text":"<p>Pipelines without gradient training (no loss nodes):</p> <pre><code># Statistical training automatically detected\nuv run restore-trainrun \\\n  --trainrun-path outputs/rx_statistical/trained_models/rx_statistical_trainrun.yaml \\\n  --mode train\n</code></pre> <p>Behavior: 1. Runs statistical initialization (mean, covariance, running stats) 2. No gradient descent applied 3. Fast execution (no backpropagation)</p>"},{"location":"how-to/restore-pipeline-trainrun/#two-phase-training","title":"Two-Phase Training","text":"<p>Pipelines with both statistical and gradient components:</p> <pre><code># Automatically runs both phases\nuv run restore-trainrun \\\n  --trainrun-path outputs/channel_selector/trained_models/channel_selector_trainrun.yaml \\\n  --mode train\n</code></pre> <p>Execution flow: 1. Phase 1: Statistical initialization    - Calibrate statistical parameters    - Fit normalizers on training data    - Initialize detector components 2. Phase 2: Gradient training    - Unfreeze specified nodes    - Apply gradient descent with optimizer    - Monitor metrics and early stopping    - Save best checkpoint</p>"},{"location":"how-to/restore-pipeline-trainrun/#resume-training-from-checkpoint","title":"Resume Training from Checkpoint","text":""},{"location":"how-to/restore-pipeline-trainrun/#continue-interrupted-training","title":"Continue Interrupted Training","text":"<pre><code># Resume from last checkpoint\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train \\\n  --checkpoint-path outputs/my_experiment/checkpoints/last.ckpt\n\n# Resume from best checkpoint\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train \\\n  --checkpoint-path outputs/my_experiment/checkpoints/best.ckpt\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#fine-tune-with-different-settings","title":"Fine-tune with Different Settings","text":"<pre><code># Continue training with reduced learning rate\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train \\\n  --checkpoint-path outputs/my_experiment/checkpoints/epoch=05.ckpt \\\n  --override training.optimizer.lr=0.00001 \\\n  --override training.trainer.max_epochs=100 \\\n  --override output_dir=outputs/my_experiment_finetuned\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#grpc-remote-restoration","title":"gRPC Remote Restoration","text":"<p>For remote server-based restoration:</p> <pre><code>import grpc\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\n\n# Connect to server\nchannel = grpc.insecure_channel(\"localhost:50051\")\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n\n# Restore TrainRun\nrestore_response = stub.RestoreTrainRun(\n    cuvis_ai_pb2.RestoreTrainRunRequest(\n        trainrun_path=\"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\",\n        weights_path=\"outputs/my_experiment/trained_models/My_Pipeline.pt\",\n        strict=True\n    )\n)\n\nsession_id = restore_response.session_id\n\n# Run training\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT\n    )\n):\n    print(f\"Epoch {progress.current_epoch}/{progress.total_epochs} - \"\n          f\"Loss: {progress.train_loss:.4f}\")\n\n# Run validation\nval_response = stub.Validate(\n    cuvis_ai_pb2.ValidateRequest(session_id=session_id)\n)\nprint(f\"Validation metrics: {val_response.metrics}\")\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#weight-loading-behavior","title":"Weight Loading Behavior","text":""},{"location":"how-to/restore-pipeline-trainrun/#with-weights-file-pt","title":"With Weights File (.pt)","text":"<pre><code># Weights automatically loaded if present\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode validate\n</code></pre> <p>Behavior: - Trainable nodes restored from <code>.pt</code> file - Statistical nodes skip initialization (already trained) - Ready for immediate inference/validation</p>"},{"location":"how-to/restore-pipeline-trainrun/#without-weights-file","title":"Without Weights File","text":"<pre><code># Statistical initialization required\nuv run restore-trainrun \\\n  --trainrun-path outputs/my_experiment/trained_models/my_experiment_trainrun.yaml \\\n  --mode train\n</code></pre> <p>Behavior: - Statistical nodes must run initialization first - Trainable nodes start from random initialization - Full training required before inference</p>"},{"location":"how-to/restore-pipeline-trainrun/#explicit-weight-loading","title":"Explicit Weight Loading","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\n\n# Load pipeline\npipeline = CuvisPipeline.load_pipeline(\n    \"outputs/my_experiment/trained_models/My_Pipeline.yaml\"\n)\n\n# Load weights explicitly\npipeline.load_weights(\n    \"outputs/my_experiment/trained_models/My_Pipeline.pt\",\n    strict=True  # Require exact parameter match\n)\n\n# Or load from checkpoint\npipeline.load_from_checkpoint(\n    \"outputs/my_experiment/checkpoints/epoch=09.ckpt\",\n    strict=False  # Allow partial loading\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#checkpoint-configuration","title":"Checkpoint Configuration","text":"<p>Configure checkpoint saving during training:</p> <pre><code>from cuvis_ai_core.training.config import TrainingConfig, ModelCheckpointConfig\n\ntraining_cfg = TrainingConfig(\n    trainer={\n        \"max_epochs\": 50,\n        \"callbacks\": {\n            \"checkpoint\": ModelCheckpointConfig(\n                dirpath=\"outputs/my_experiment/checkpoints\",\n                monitor=\"metrics_anomaly/iou\",  # Metric to track\n                mode=\"max\",                      # Maximize IoU\n                save_top_k=3,                    # Keep best 3 checkpoints\n                save_last=True,                  # Always save last epoch\n                filename=\"{epoch:02d}\",          # Naming pattern\n                verbose=True\n            )\n        }\n    }\n)\n</code></pre> <p>Checkpoint strategies: - <code>save_top_k=1</code> - Save only best checkpoint (minimal storage) - <code>save_top_k=3</code> - Save top 3 checkpoints (moderate storage) - <code>save_top_k=-1</code> - Save all checkpoints (maximum storage) - <code>save_last=True</code> - Always save most recent epoch - <code>monitor</code> - Metric to optimize (<code>metrics_anomaly/iou</code>, <code>metrics_anomaly/auc</code>, etc.) - <code>mode</code> - <code>\"max\"</code> (maximize metric) or <code>\"min\"</code> (minimize loss)</p>"},{"location":"how-to/restore-pipeline-trainrun/#version-compatibility","title":"Version Compatibility","text":""},{"location":"how-to/restore-pipeline-trainrun/#automatic-training-type-detection","title":"Automatic Training Type Detection","text":"<p>The restoration system automatically detects training type:</p> <pre><code># Detects gradient training (has loss_nodes)\ntrainrun_config = TrainRunConfig.load_from_file(\n    \"outputs/channel_selector/trained_models/channel_selector_trainrun.yaml\"\n)\n\nif trainrun_config.loss_nodes:\n    print(\"Gradient training detected\")\n    # Uses GradientTrainer\nelse:\n    print(\"Statistical training detected\")\n    # Uses StatisticalTrainer\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#handling-missing-weights","title":"Handling Missing Weights","text":"<pre><code>from pathlib import Path\n\ntrainrun_path = Path(\"outputs/my_experiment/trained_models/my_experiment_trainrun.yaml\")\nweights_path = trainrun_path.parent / f\"{trainrun_path.stem.replace('_trainrun', '')}.pt\"\n\nif weights_path.exists():\n    print(\"Loading with trained weights\")\n    pipeline.load_weights(str(weights_path), strict=True)\nelse:\n    print(\"No weights found - statistical initialization required\")\n    # Must run training first\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#complete-examples","title":"Complete Examples","text":""},{"location":"how-to/restore-pipeline-trainrun/#example-1-reproduce-training-results","title":"Example 1: Reproduce Training Results","text":"<pre><code># Display original experiment\nuv run restore-trainrun \\\n  --trainrun-path outputs/channel_selector/trained_models/channel_selector_trainrun.yaml \\\n  --mode info\n\n# Re-run training with same settings\nuv run restore-trainrun \\\n  --trainrun-path outputs/channel_selector/trained_models/channel_selector_trainrun.yaml \\\n  --mode train \\\n  --override output_dir=outputs/channel_selector_reproduced\n\n# Validate on original validation set\nuv run restore-trainrun \\\n  --trainrun-path outputs/channel_selector/trained_models/channel_selector_trainrun.yaml \\\n  --mode validate\n\n# Test on original test set\nuv run restore-trainrun \\\n  --trainrun-path outputs/channel_selector/trained_models/channel_selector_trainrun.yaml \\\n  --mode test\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#example-2-transfer-learning","title":"Example 2: Transfer Learning","text":"<pre><code># Start from pre-trained weights, train on new data\nuv run restore-trainrun \\\n  --trainrun-path outputs/lentils_model/trained_models/lentils_trainrun.yaml \\\n  --mode train \\\n  --override output_dir=outputs/beans_model \\\n  --override data.train_ids=[10,11,12] \\\n  --override data.val_ids=[13] \\\n  --override data.test_ids=[14,15] \\\n  --override training.optimizer.lr=0.0001 \\\n  --override training.trainer.max_epochs=30\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#example-3-hyperparameter-search","title":"Example 3: Hyperparameter Search","text":"<pre><code># Try different learning rates\nfor lr in 0.001 0.0001 0.00001; do\n  uv run restore-trainrun \\\n    --trainrun-path outputs/base_model/trained_models/base_trainrun.yaml \\\n    --mode train \\\n    --override output_dir=outputs/hp_search_lr_${lr} \\\n    --override training.optimizer.lr=${lr} \\\n    --override training.trainer.max_epochs=50\ndone\n\n# Compare results\nuv run compare-trainruns \\\n  outputs/hp_search_lr_0.001 \\\n  outputs/hp_search_lr_0.0001 \\\n  outputs/hp_search_lr_0.00001\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#example-4-production-inference","title":"Example 4: Production Inference","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nimport torch\n\n# Load trained pipeline for inference\npipeline = CuvisPipeline.load_pipeline(\n    \"outputs/production_model/trained_models/Production_Pipeline.yaml\",\n    weights_path=\"outputs/production_model/trained_models/Production_Pipeline.pt\",\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n\n# Validate pipeline\npipeline.validate()\n\n# Run inference on new data\nimport cuvis\n\n# Load hyperspectral measurement\nmeasurement = cuvis.Measurement(\"data/new_sample.cu3s\")\ncube = measurement.data[\"Reflectance\"]  # Shape: (H, W, C)\n\n# Prepare input\nimport torch\ncube_tensor = torch.from_numpy(cube).float()\ncube_tensor = cube_tensor.permute(2, 0, 1)  # (C, H, W)\ncube_tensor = cube_tensor.unsqueeze(0)      # (1, C, H, W)\n\n# Run pipeline\nwith torch.no_grad():\n    result = pipeline.execute(cube=cube_tensor)\n\n# Extract outputs\nanomaly_scores = result[\"scores\"]     # Anomaly heatmap\ndecisions = result[\"decisions\"]       # Binary mask\nmetrics = result.get(\"metrics\", {})   # Optional metrics\n\nprint(f\"Detected anomalies: {decisions.sum().item()} pixels\")\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#loading-external-plugins","title":"Loading External Plugins","text":"<p>The cuvis.ai framework supports loading external plugin nodes that extend pipeline capabilities. When loading plugins, dependencies are automatically installed to ensure plugins work out of the box.</p>"},{"location":"how-to/restore-pipeline-trainrun/#plugin-configuration","title":"Plugin Configuration","text":"<p>Plugins are specified in a YAML manifest file (e.g., <code>plugins.yaml</code>):</p> <pre><code>plugins:\n  adaclip:\n    # For local development\n    path: \"D:/code-repos/cuvis-ai-adaclip\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n  # Or for production (Git repository)\n  # adaclip:\n  #   repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n  #   ref: \"v1.2.3\"\n  #   provides:\n  #     - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#automatic-dependency-management","title":"Automatic Dependency Management","text":"<p>When a plugin is loaded:</p> <ol> <li>Dependency Detection: Reads <code>pyproject.toml</code> from plugin directory (PEP 621 compliant)</li> <li>Automatic Installation: Installs missing dependencies via <code>uv pip install</code></li> <li>Conflict Resolution: Delegates version conflict resolution to <code>uv</code></li> <li>Transparent Process: Logs what dependencies are being installed</li> </ol> <p>Requirements: - Plugin must have a <code>pyproject.toml</code> file following PEP 621 - Plugin dependencies specified in <code>project.dependencies</code> section</p>"},{"location":"how-to/restore-pipeline-trainrun/#using-plugins-with-cli","title":"Using Plugins with CLI","text":"<p>Load external plugins when restoring pipelines:</p> <pre><code>uv run restore-pipeline \\\n  --pipeline-path configs/pipeline/adaclip_baseline.yaml \\\n  --plugins-path examples/adaclip/plugins_local.yaml\n</code></pre> <p>With inference:</p> <pre><code>uv run restore-pipeline \\\n  --pipeline-path configs/pipeline/adaclip_baseline.yaml \\\n  --plugins-path examples/adaclip/plugins.yaml \\\n  --cu3s-file-path data/Lentils/Lentils_000.cu3s\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#using-plugins-with-python-api","title":"Using Plugins with Python API","text":"<pre><code>from cuvis_ai_core.utils import restore_pipeline\n\n# Load pipeline with plugins\npipeline = restore_pipeline(\n    pipeline_path=\"configs/pipeline/adaclip_baseline.yaml\",\n    plugins_path=\"examples/adaclip/plugins.yaml\"\n)\n\n# Or with inference\npipeline = restore_pipeline(\n    pipeline_path=\"configs/pipeline/adaclip_baseline.yaml\",\n    plugins_path=\"examples/adaclip/plugins.yaml\",\n    cu3s_file_path=\"data/Lentils/Lentils_000.cu3s\"\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#manual-plugin-loading-advanced","title":"Manual Plugin Loading (Advanced)","text":"<p>For more control, load plugins manually with NodeRegistry:</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\n\n# Create registry instance\nregistry = NodeRegistry()\n\n# Load plugins (automatically installs dependencies)\nregistry.load_plugins(\"examples/adaclip/plugins.yaml\")\n\n# Load pipeline with plugin-aware registry\npipeline = CuvisPipeline.load_pipeline(\n    \"configs/pipeline/adaclip_baseline.yaml\",\n    node_registry=registry\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#example-output","title":"Example Output","text":"<p>When loading a plugin with dependencies:</p> <pre><code>INFO | Using local plugin 'adaclip' at D:\\code-repos\\cuvis-ai-adaclip\nDEBUG | Extracted 6 dependencies from pyproject.toml\nINFO | Installing 6 dependencies for plugin 'adaclip'...\nINFO | Dependencies to install: cuvis==3.5.0, cuvis-ai, cuvis-ai-core, ftfy, seaborn, click\nINFO | \u2713 Plugin 'adaclip' dependencies installed successfully\nDEBUG | Registered plugin node 'AdaCLIPDetector' from 'adaclip'\nINFO | Loaded plugin 'adaclip' with 1 nodes\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#plugin-development-guidelines","title":"Plugin Development Guidelines","text":"<p>For your plugin to work with automatic dependency management:</p> <ol> <li>Include <code>pyproject.toml</code> in plugin root directory</li> <li>Specify dependencies in <code>project.dependencies</code>:    <pre><code>[project]\nname = \"my-plugin\"\ndependencies = [\n    \"numpy&gt;=1.20.0\",\n    \"pandas&gt;=1.5.0\",\n    \"scikit-learn&gt;=1.0.0\",\n]\n</code></pre></li> <li>Follow PEP 621 standard for project metadata</li> <li>Test locally before deploying to Git</li> </ol> <p>For complete plugin development documentation, see Plugin Development Guide.</p>"},{"location":"how-to/restore-pipeline-trainrun/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/restore-pipeline-trainrun/#issue-missing-weights-file","title":"Issue: Missing Weights File","text":"<p><pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'My_Pipeline.pt'\n</code></pre> Solution: Train the pipeline first or ensure weights were saved during training: <pre><code># Re-train to generate weights\nuv run restore-trainrun --trainrun-path ... --mode train\n</code></pre></p>"},{"location":"how-to/restore-pipeline-trainrun/#issue-weight-shape-mismatch","title":"Issue: Weight Shape Mismatch","text":"<p><pre><code>RuntimeError: Error(s) in loading state_dict: size mismatch for selector.weights\n</code></pre> Solution: Pipeline structure changed. Options: 1. Use <code>strict=False</code> to load partial weights 2. Retrain from scratch 3. Manually adapt weights</p> <pre><code># Load with strict=False\npipeline.load_weights(\"My_Pipeline.pt\", strict=False)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#issue-checkpoint-not-found","title":"Issue: Checkpoint Not Found","text":"<p><pre><code>FileNotFoundError: Checkpoint 'epoch=10.ckpt' not found\n</code></pre> Solution: Check available checkpoints: <pre><code>ls outputs/my_experiment/checkpoints/\n# Use last.ckpt or adjust epoch number\n</code></pre></p>"},{"location":"how-to/restore-pipeline-trainrun/#issue-cuda-out-of-memory","title":"Issue: CUDA Out of Memory","text":"<p><pre><code>RuntimeError: CUDA out of memory\n</code></pre> Solution: Reduce batch size or use CPU: <pre><code>uv run restore-trainrun \\\n  --trainrun-path ... \\\n  --override data.batch_size=1 \\\n  --override training.trainer.accelerator=cpu\n</code></pre></p>"},{"location":"how-to/restore-pipeline-trainrun/#issue-statistical-node-not-initialized","title":"Issue: Statistical Node Not Initialized","text":"<p><pre><code>RuntimeError: RXGlobal requires statistical initialization before use\n</code></pre> Solution: Run training mode first (not test/validate): <pre><code># Train to initialize statistical nodes\nuv run restore-trainrun --trainrun-path ... --mode train\n\n# Then validate/test\nuv run restore-trainrun --trainrun-path ... --mode validate\n</code></pre></p>"},{"location":"how-to/restore-pipeline-trainrun/#issue-missing-pyprojecttoml-in-plugin","title":"Issue: Missing pyproject.toml in Plugin","text":"<p><pre><code>FileNotFoundError: Plugin 'my-plugin' must have a pyproject.toml file.\nPEP 621 (https://peps.python.org/pep-0621/) specifies pyproject.toml\nas the standard for Python project metadata and dependencies.\n</code></pre> Solution: Add a <code>pyproject.toml</code> file to your plugin root directory</p>"},{"location":"how-to/restore-pipeline-trainrun/#issue-plugin-dependency-conflicts","title":"Issue: Plugin Dependency Conflicts","text":"<p><pre><code>RuntimeError: Failed to install dependencies for plugin 'my-plugin'.\nThis may indicate version conflicts or missing packages.\nuv could not resolve the dependency tree.\n</code></pre> Solution: - Review dependency version constraints in your <code>pyproject.toml</code> - Check for conflicts with main environment dependencies</p>"},{"location":"how-to/restore-pipeline-trainrun/#issue-plugin-import-errors","title":"Issue: Plugin Import Errors","text":"<p><pre><code>ImportError: Failed to import module for 'my_plugin.node.MyNode': No module named 'some_package'\n</code></pre> Solution: - Ensure the package is listed in your plugin's <code>pyproject.toml</code> dependencies - Check that <code>uv pip install</code> completed successfully</p>"},{"location":"how-to/restore-pipeline-trainrun/#best-practices","title":"Best Practices","text":""},{"location":"how-to/restore-pipeline-trainrun/#1-always-save-trainrun-configs","title":"1. Always Save TrainRun Configs","text":"<pre><code># At end of training script\ntrainrun_config.save_to_file(\n    str(output_dir / \"trained_models\" / f\"{experiment_name}_trainrun.yaml\")\n)\n\n# Save weights separately\npipeline.save_weights(\n    str(output_dir / \"trained_models\" / f\"{pipeline_name}.pt\")\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#2-use-descriptive-experiment-names","title":"2. Use Descriptive Experiment Names","text":"<pre><code># Good: descriptive, unique names\ntrainrun_config = TrainRunConfig(\n    name=\"channel_selector_lentils_3bands_v2\",\n    ...\n)\n\n# Avoid: generic names\ntrainrun_config = TrainRunConfig(\n    name=\"experiment1\",  # Not descriptive\n    ...\n)\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#3-version-control-yaml-files","title":"3. Version Control YAML Files","text":"<pre><code># Track TrainRun configs in git\ngit add outputs/*/trained_models/*_trainrun.yaml\ngit add outputs/*/trained_models/*.yaml  # Pipeline YAMLs\ngit commit -m \"Add trained model configs for channel selector experiment\"\n\n# Use .gitignore for large binary files\necho \"*.pt\" &gt;&gt; .gitignore\necho \"*.ckpt\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#4-document-experiment-purpose","title":"4. Document Experiment Purpose","text":"<pre><code>metadata:\n  name: Channel_Selector_Lentils\n  description: &gt;\n    Channel selection for lentil anomaly detection.\n    Selects optimal 3 spectral bands from 61 channels using\n    gradient-based optimization with entropy regularization.\n    Trained on lentils dataset with stone anomalies.\n  tags:\n    - channel-selection\n    - lentils\n    - production-ready\n  author: your_name\n  created: 2026-02-04\n  version: 2.1.0\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#5-test-restoration-before-deployment","title":"5. Test Restoration Before Deployment","text":"<pre><code># Always verify restored pipeline works\nuv run restore-trainrun --trainrun-path ... --mode info\nuv run restore-trainrun --trainrun-path ... --mode validate\n\n# Check metrics match original training\n</code></pre>"},{"location":"how-to/restore-pipeline-trainrun/#see-also","title":"See Also","text":"<ul> <li>Build Pipelines in Python</li> <li>Build Pipelines in YAML</li> <li>TrainRun Schema Reference</li> <li>Pipeline Schema Reference</li> <li>Remote gRPC Usage</li> <li>Monitoring and Visualization</li> </ul>"},{"location":"node-catalog/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/#node-catalog","title":"Node Catalog","text":"<p>Complete reference of all 30+ built-in nodes in CUVIS.AI, organized by category for easy navigation.</p>"},{"location":"node-catalog/#overview","title":"Overview","text":"<p>Nodes are the building blocks of CUVIS.AI pipelines. Each node performs a specific operation (data loading, preprocessing, detection, visualization) and connects to other nodes via typed ports.</p> <p>What You'll Find Here: - \ud83d\udcda Complete catalog of all built-in nodes - \ud83d\udd0d Quick reference tables for finding the right node - \ud83d\udcd6 Usage examples and configuration patterns - \ud83d\udd17 Links to tutorials demonstrating each node</p>"},{"location":"node-catalog/#node-categories","title":"Node Categories","text":"<ul> <li> <p> Data Nodes</p> <p>Data loading and dataset management nodes</p> <p>Nodes: LentilsAnomalyDataNode (1)</p> </li> <li> <p> Preprocessing</p> <p>Data transformation and preprocessing nodes</p> <p>Nodes: MinMaxNormalizer, ZScoreNormalizer, PerPixelUnitNorm, and more (7)</p> </li> <li> <p> Selectors</p> <p>Channel and feature selection nodes</p> <p>Nodes: SoftChannelSelector, BaselineFalseRGBSelector, SupervisedSelectors (8)</p> </li> <li> <p> Statistical</p> <p>Statistical analysis and anomaly detection nodes</p> <p>Nodes: RXGlobal, ScoreToLogit (2)</p> </li> <li> <p> Deep Learning</p> <p>Neural network and gradient-based training nodes</p> <p>Nodes: DeepSVDD*, TrainablePCA, LearnableChannelMixer, ConcreteBandSelector (8)</p> </li> <li> <p> Loss &amp; Metrics</p> <p>Loss functions and evaluation metrics</p> <p>Nodes: AnomalyBCEWithLogits, DeepSVDDSoftBoundaryLoss, AnomalyDetectionMetrics (7)</p> </li> <li> <p> Visualization</p> <p>Results visualization and monitoring nodes</p> <p>Nodes: AnomalyMask, CubeRGBVisualizer, ScoreHeatmapVisualizer, TensorBoardMonitor (4)</p> </li> <li> <p> Utility Nodes</p> <p>Helper nodes for decisions and transformations</p> <p>Nodes: BinaryDecider, QuantileBinaryDecider, BinaryAnomalyLabelMapper (4)</p> </li> </ul>"},{"location":"node-catalog/#quick-reference-table","title":"Quick Reference Table","text":"<p>All Built-in Nodes at a Glance:</p> Node Category Statistical Init Trainable Primary Use Case LentilsAnomalyDataNode Data \u274c \u274c Load hyperspectral data with anomaly labels BandpassByWavelength Preprocessing \u274c \u274c Filter channels by wavelength range MinMaxNormalizer Preprocessing \u2705 \u274c Min-max scaling with running statistics ZScoreNormalizer Preprocessing \u274c \u274c Z-score normalization ZScoreNormalizerGlobal Preprocessing/DL \u2705 \u2705 Learnable z-score encoder for Deep SVDD PerPixelUnitNorm Preprocessing \u274c \u274c Per-pixel L2 normalization SigmoidNormalizer Preprocessing \u274c \u274c Median-centered sigmoid squashing IdentityNormalizer Preprocessing \u274c \u274c Pass-through (no-op) SoftChannelSelector Selectors \u2705 \u2705 Learnable soft channel selection (Gumbel-Softmax) TopKIndices Selectors \u274c \u274c Extract top-k channel indices from weights BaselineFalseRGBSelector Selectors \u274c \u274c Fixed RGB wavelength selection CIRFalseColorSelector Selectors \u274c \u274c Color Infrared false color HighContrastBandSelector Selectors \u274c \u274c Data-driven high-contrast selection SupervisedCIRBandSelector Selectors \u2705 \u274c Supervised NIR/Red/Green selection with mRMR SupervisedWindowedFalseRGBSelector Selectors \u2705 \u274c Supervised RGB selection with windowing SupervisedFullSpectrumBandSelector Selectors \u2705 \u274c Supervised global band selection RXGlobal Statistical \u2705 \u274c Reed-Xiaoli anomaly detector ScoreToLogit Utility \u274c \u2705 Transform RX scores to logits DeepSVDDProjection Deep Learning \u274c \u2705 Deep SVDD projection network DeepSVDDCenterTracker Deep Learning \u2705 \u2705 Track hypersphere center with EMA DeepSVDDScores Deep Learning \u274c \u274c Compute distance-based anomaly scores TrainablePCA Deep Learning \u2705 \u2705 PCA with optional gradient training LearnableChannelMixer Deep Learning \u2705 \u2705 DRCNN-style 1x1 convolution mixer ConcreteBandSelector Deep Learning \u274c \u2705 Concrete/Gumbel-Softmax discrete selector AnomalyBCEWithLogits Loss \u274c \u274c Binary cross-entropy loss DeepSVDDSoftBoundaryLoss Loss \u274c \u274c Soft boundary loss for Deep SVDD OrthogonalityLoss Loss \u274c \u274c Orthogonality regularization for PCA SelectorEntropyRegularizer Loss \u274c \u274c Entropy regularization for selectors SelectorDiversityRegularizer Loss \u274c \u274c Diversity regularization for selectors AnomalyDetectionMetrics Metrics \u274c \u274c IoU, precision, recall, F1, AP ExplainedVarianceMetric Metrics \u274c \u274c Track PCA explained variance AnomalyMask Visualization \u274c \u274c Visualize anomaly masks CubeRGBVisualizer Visualization \u274c \u274c False-color RGB from selected channels PCAVisualization Visualization \u274c \u274c Visualize PCA projections ScoreHeatmapVisualizer Visualization \u274c \u274c Anomaly score heatmaps TensorBoardMonitorNode Visualization \u274c \u274c TensorBoard logging (sink node) BinaryDecider Utility \u274c \u274c Fixed threshold binary decisions QuantileBinaryDecider Utility \u274c \u274c Quantile-based binary decisions BinaryAnomalyLabelMapper Utility \u274c \u274c Convert multi-class to binary anomaly labels"},{"location":"node-catalog/#finding-the-right-node","title":"Finding the Right Node","text":""},{"location":"node-catalog/#by-task","title":"By Task","text":"<p>Anomaly Detection: - Statistical: RXGlobal - Fast, interpretable baseline - Deep Learning: DeepSVDD* - One-class learning with neural networks</p> <p>Band/Channel Selection: - Fixed Selection: BaselineFalseRGBSelector, CIRFalseColorSelector - Learnable Selection: SoftChannelSelector, ConcreteBandSelector - Supervised Selection: SupervisedCIRBandSelector, SupervisedWindowedFalseRGBSelector</p> <p>Preprocessing: - Normalization: MinMaxNormalizer, ZScoreNormalizer - Wavelength Filtering: BandpassByWavelength - Per-Pixel Operations: PerPixelUnitNorm</p> <p>Dimensionality Reduction: - PCA: TrainablePCA - Channel Mixing: LearnableChannelMixer</p> <p>Visualization &amp; Monitoring: - TensorBoard: TensorBoardMonitorNode - Anomaly Masks: AnomalyMask - Score Heatmaps: ScoreHeatmapVisualizer</p>"},{"location":"node-catalog/#by-training-paradigm","title":"By Training Paradigm","text":"<p>Statistical-Only (No Gradients): - RXGlobal - MinMaxNormalizer (with <code>use_running_stats=True</code>) - SupervisedCIRBandSelector - SupervisedWindowedFalseRGBSelector</p> <p>Two-Phase (Statistical \u2192 Gradient): - SoftChannelSelector - TrainablePCA - LearnableChannelMixer - DeepSVDD* nodes</p> <p>Gradient-Only: - ConcreteBandSelector - DeepSVDDProjection</p> <p>No Training Required: - ZScoreNormalizer - BandpassByWavelength - BinaryDecider - All visualization nodes</p>"},{"location":"node-catalog/#by-data-type","title":"By Data Type","text":"<p>Hyperspectral Data: - LentilsAnomalyDataNode - BandpassByWavelength - SoftChannelSelector - RXGlobal</p> <p>RGB/False-Color: - BaselineFalseRGBSelector - CIRFalseColorSelector - CubeRGBVisualizer</p> <p>Feature Embeddings: - DeepSVDDProjection - DeepSVDDCenterTracker - DeepSVDDScores</p>"},{"location":"node-catalog/#common-pipeline-patterns","title":"Common Pipeline Patterns","text":""},{"location":"node-catalog/#pattern-1-statistical-detection-pipeline","title":"Pattern 1: Statistical Detection Pipeline","text":"<p>Use Case: Fast baseline anomaly detection with RX</p> <pre><code>graph LR\n    A[LentilsAnomalyDataNode] --&gt;|cube| B[MinMaxNormalizer]\n    B --&gt;|normalized| C[RXGlobal]\n    C --&gt;|scores| D[ScoreToLogit]\n    D --&gt;|logits| E[BinaryDecider]\n    E --&gt;|decisions| F[AnomalyDetectionMetrics]\n    A --&gt;|mask| F\n    F --&gt;|metrics| G[TensorBoard]</code></pre> <p>Training: Statistical-only (Phase 1) Tutorial: RX Statistical Detection</p>"},{"location":"node-catalog/#pattern-2-two-phase-training-with-channel-selection","title":"Pattern 2: Two-Phase Training with Channel Selection","text":"<p>Use Case: Learnable channel selection with gradient optimization</p> <pre><code>graph LR\n    A[Data] --&gt;|cube| B[MinMaxNormalizer]\n    B --&gt;|normalized| C[SoftChannelSelector]\n    C --&gt;|selected| D[RXGlobal]\n    D --&gt;|scores| E[ScoreToLogit]\n    E --&gt;|logits| F[BCELoss]\n    E --&gt;|logits| G[BinaryDecider]\n    G --&gt;|decisions| H[Metrics]\n    C --&gt;|weights| I[EntropyLoss]</code></pre> <p>Training: Phase 1 (Statistical) \u2192 Phase 2 (Unfreeze selector + RX) \u2192 Phase 3 (Gradient) Tutorial: Channel Selector</p>"},{"location":"node-catalog/#pattern-3-deep-learning-pipeline-deep-svdd","title":"Pattern 3: Deep Learning Pipeline (Deep SVDD)","text":"<p>Use Case: Deep one-class anomaly detection</p> <pre><code>graph LR\n    A[Data] --&gt;|cube| B[BandpassByWavelength]\n    B --&gt;|filtered| C[PerPixelUnitNorm]\n    C --&gt;|normalized| D[ZScoreNormalizerGlobal]\n    D --&gt;|encoded| E[DeepSVDDProjection]\n    E --&gt;|embeddings| F[CenterTracker]\n    F --&gt;|center| G[DeepSVDDScores]\n    E --&gt;|embeddings| G\n    G --&gt;|scores| H[QuantileDecider]\n    H --&gt;|decisions| I[Metrics]</code></pre> <p>Training: Phase 1 (Statistical encoder init) \u2192 Phase 2 (Unfreeze encoder) \u2192 Phase 3 (Gradient) Tutorial: Deep SVDD Gradient</p>"},{"location":"node-catalog/#pattern-4-plugin-based-pipeline","title":"Pattern 4: Plugin-Based Pipeline","text":"<p>Use Case: Using external plugin nodes (e.g., AdaCLIP)</p> <pre><code>graph LR\n    A[Data] --&gt;|cube| B[MinMaxNormalizer]\n    B --&gt;|normalized| C[LearnableChannelMixer]\n    C --&gt;|rgb 3ch| D[AdaCLIPDetector Plugin]\n    D --&gt;|scores| E[QuantileDecider]\n    E --&gt;|decisions| F[Metrics]</code></pre> <p>Training: Phase 1 (Statistical) \u2192 Phase 2 (Unfreeze mixer) \u2192 Phase 3 (Gradient with IoU loss) Tutorial: AdaCLIP Workflow</p>"},{"location":"node-catalog/#plugin-nodes","title":"Plugin Nodes","text":"<p>CUVIS.AI supports plugin nodes loaded from external repositories. Plugin nodes extend the built-in catalog with specialized functionality.</p> <p>Key Difference: - Built-in nodes: Included in <code>cuvis-ai</code> package - Plugin nodes: Loaded dynamically from Git repositories</p> <p>Example Plugin: <code>cuvis-ai-adaclip</code> - AdaCLIPDetector: CLIP-based anomaly detection for hyperspectral data - Repository: https://github.com/cubert-hyperspectral/cuvis-ai-adaclip</p> <p>Learn More: - Plugin System Overview - Architecture and concepts - Plugin Usage - How to use plugin nodes - Plugin Development - Create your own plugins</p>"},{"location":"node-catalog/#node-development","title":"Node Development","text":"<p>Want to create custom nodes?</p> <ol> <li>For built-in nodes: Follow the Add Built-in Node guide</li> <li>For external plugins: See Plugin Development</li> </ol> <p>Resources: - Node System Deep Dive - Node architecture and lifecycle - Port System - Typed I/O connections - API Reference - Node base classes and utilities</p>"},{"location":"node-catalog/#related-pages","title":"Related Pages","text":"<p>Concepts: - Node System Deep Dive - Understanding nodes and their lifecycle - Port System Deep Dive - Typed port connections - Two-Phase Training - Training strategies for learnable nodes</p> <p>Tutorials: - RX Statistical Detection - Statistical-only pipeline - Channel Selector - Two-phase training example - Deep SVDD Gradient - Deep learning pipeline</p> <p>API Reference: - Node API - Base node classes - Port API - Port specifications - Pipeline API - Pipeline construction</p> <p>Next Steps: Browse the category pages above to explore detailed documentation for each node, including port specifications, parameters, usage examples, and configuration patterns.</p>"},{"location":"node-catalog/clustering/","title":"Clustering","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/clustering/#clustering-nodes","title":"Clustering Nodes","text":"<p>Status: Under Development (Phase 5) Expected Completion: Week 5</p>"},{"location":"node-catalog/clustering/#coming-soon","title":"Coming Soon","text":"<p>This page will document: - KMeansNode - DBSCANNode - ClusteringNode - Configuration and usage examples - Unsupervised learning pipelines</p> <p>Related Pages: - Node Catalog - Statistical Nodes</p>"},{"location":"node-catalog/data-nodes/","title":"Data Nodes","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/data-nodes/#data-nodes","title":"Data Nodes","text":""},{"location":"node-catalog/data-nodes/#overview","title":"Overview","text":"<p>Data nodes serve as the entry point for hyperspectral datasets into CUVIS.AI pipelines. They handle the crucial tasks of loading raw measurements, converting data types for processing, and transforming multi-class segmentation masks into binary anomaly labels suitable for anomaly detection workflows.</p> <p>When to use Data Nodes:</p> <ul> <li>At the very beginning of your pipeline as the first node</li> <li>When loading hyperspectral cubes from datasets (CU3S files, numpy arrays, etc.)</li> <li>When you need to convert multi-class segmentation masks to binary anomaly labels</li> <li>For standardizing data formats before preprocessing and model training</li> </ul> <p>Data nodes perform three critical transformations:</p> <ol> <li>Type conversion: Convert raw <code>uint16</code> hyperspectral data to <code>float32</code> for numerical stability</li> <li>Label mapping: Transform multi-class segmentation masks into binary anomaly targets</li> <li>Wavelength normalization: Flatten batch-wise wavelength tensors to a single wavelength array</li> </ol>"},{"location":"node-catalog/data-nodes/#nodes-in-this-category","title":"Nodes in This Category","text":""},{"location":"node-catalog/data-nodes/#lentilsanomalydatanode","title":"LentilsAnomalyDataNode","text":"<p>Description: Loads hyperspectral data and converts multi-class segmentation masks to binary anomaly labels</p> <p>Perfect for:</p> <ul> <li>Loading hyperspectral cubes with semantic segmentation masks</li> <li>Defining normal vs anomalous classes for anomaly detection tasks</li> <li>Preprocessing Lentils dataset or similar HSI datasets with class labels</li> <li>Converting uint16 raw measurements to float32 for downstream processing</li> <li>Standardizing wavelength information across batches</li> </ul> <p>Training Paradigm: None (data loading and transformation only)</p>"},{"location":"node-catalog/data-nodes/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional cube uint16 (B, H, W, C) Raw hyperspectral cube [Batch, Height, Width, Channels] No mask int32 (B, H, W) Multi-class segmentation mask with class IDs Yes wavelengths int32 (B, C) Wavelength values (nm) for each spectral channel No <p>Output Ports:</p> Port Type Shape Description cube float32 (B, H, W, C) Type-converted hyperspectral cube mask bool (B, H, W, 1) Binary anomaly mask (0=normal, 1=anomaly) wavelengths int32 (C,) Flattened wavelength array (assumes consistent across batch)"},{"location":"node-catalog/data-nodes/#parameters","title":"Parameters","text":"Parameter Type Default Description normal_class_ids list[int] Required List of class IDs to be considered \"normal\" (non-anomalous) anomaly_class_ids list[int] | None None Explicit anomaly class IDs. If None, all IDs not in <code>normal_class_ids</code> are treated as anomalies"},{"location":"node-catalog/data-nodes/#label-mapping-logic","title":"Label Mapping Logic","text":"<p>The node uses BinaryAnomalyLabelMapper internally to convert multi-class masks:</p> <p>Mode 1: Implicit Anomalies (<code>anomaly_class_ids=None</code>)</p> <ul> <li>Classes in <code>normal_class_ids</code> \u2192 0 (normal)</li> <li>All other classes \u2192 1 (anomaly)</li> </ul> <p>Mode 2: Explicit Anomalies (<code>anomaly_class_ids</code> provided)</p> <ul> <li>Classes in <code>normal_class_ids</code> \u2192 0 (normal)</li> <li>Classes in <code>anomaly_class_ids</code> \u2192 1 (anomaly)</li> <li>All other classes \u2192 0 (normal, by default)</li> </ul> <p>Validation:</p> <ul> <li>No overlap allowed between <code>normal_class_ids</code> and <code>anomaly_class_ids</code> (raises <code>ValueError</code>)</li> <li>Gaps in class ID coverage trigger a warning and are treated as normal classes</li> </ul>"},{"location":"node-catalog/data-nodes/#example-usage-python","title":"Example Usage (Python)","text":"<p>Scenario 1: Implicit anomaly definition (common for binary tasks)</p> <pre><code>from cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai_core.pipeline import Pipeline\n\n# Define background (class 0) and lentils (class 2) as normal\n# Everything else (e.g., class 1 = anomalies) becomes anomaly automatically\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 2],  # Background and lentils\n    anomaly_class_ids=None,   # Implicit: class 1 becomes anomaly\n)\n\n# Add to pipeline as entry point\npipeline = Pipeline()\npipeline.add_node(\"data\", data_node)\n</code></pre> <p>Scenario 2: Explicit anomaly definition (for multi-class datasets)</p> <pre><code># Explicitly define which classes are anomalies\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 2, 3],     # Background, lentils, stems\n    anomaly_class_ids=[1, 4, 5],    # Diseased, broken, foreign objects\n)\n\n# Any class IDs not listed (e.g., 6, 7) default to normal\n</code></pre> <p>Scenario 3: Integration with dataloader</p> <pre><code>from cuvis_ai.data.datasets import LentilsAnomalyDataset\nfrom torch.utils.data import DataLoader\n\n# Create dataset (provides cube, mask, wavelengths)\ndataset = LentilsAnomalyDataset(\n    root_dir=\"data/lentils\",\n    processing_mode=\"Reflectance\",\n)\n\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# Data node processes batches from dataloader\nfor batch in dataloader:\n    outputs = data_node.forward(\n        cube=batch[\"cube\"],           # uint16 [4, 256, 256, 61]\n        mask=batch[\"mask\"],           # int32 [4, 256, 256]\n        wavelengths=batch[\"wavelengths\"],  # int32 [4, 61]\n    )\n    # outputs[\"cube\"]: float32 [4, 256, 256, 61]\n    # outputs[\"mask\"]: bool [4, 256, 256, 1]\n    # outputs[\"wavelengths\"]: int32 [61]\n</code></pre>"},{"location":"node-catalog/data-nodes/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  data:\n    type: LentilsAnomalyDataNode\n    config:\n      normal_class_ids: [0, 2]      # Background and lentils\n      anomaly_class_ids: null       # Everything else is anomaly\n\n  # Connect to preprocessing\n  normalizer:\n    type: MinMaxNormalizer\n    config:\n      epsilon: 1e-6\n\nconnections:\n  - [data.cube, normalizer.data]\n  - [data.mask, metrics.targets]  # Binary mask to metrics node\n  - [data.wavelengths, band_selector.wavelengths]\n</code></pre>"},{"location":"node-catalog/data-nodes/#workflow-integration","title":"Workflow Integration","text":"<pre><code>graph LR\n    A[DataLoader] --&gt;|cube: uint16| B[LentilsAnomalyDataNode]\n    A --&gt;|mask: int32| B\n    A --&gt;|wavelengths: int32| B\n\n    B --&gt;|cube: float32| C[Preprocessing]\n    B --&gt;|mask: bool| D[Metrics Node]\n    B --&gt;|wavelengths: int32| E[Band Selector]\n\n    style B fill:#e1f5ff,stroke:#01579b,stroke-width:2px\n    style A fill:#f3e5f5,stroke:#4a148c</code></pre> <p>Typical Pipeline Position:</p> <pre><code>DataLoader \u2192 [LentilsAnomalyDataNode] \u2192 MinMaxNormalizer \u2192 BandpassByWavelength \u2192 ...\n</code></pre>"},{"location":"node-catalog/data-nodes/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Issue 1: Inconsistent wavelengths across batch</p> <pre><code># Problem: Each sample has different wavelengths\nbatch[\"wavelengths\"].shape  # [4, 61] with different values per sample\n\n# Solution: LentilsAnomalyDataNode assumes consistent wavelengths\n# and extracts only the first sample's wavelengths\n# Ensure your dataset provides consistent wavelengths across all samples\n</code></pre> <p>Issue 2: Class ID gaps causing unintended normal classification</p> <pre><code># Problem: Class IDs 0, 2, 5 exist, but you only specify:\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0],\n    anomaly_class_ids=[5],\n)\n# Class 2 defaults to normal (triggers warning)\n\n# Solution: Be explicit about all class IDs\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 2],  # Include class 2 explicitly\n    anomaly_class_ids=[5],\n)\n</code></pre> <p>Issue 3: Overlapping class IDs</p> <pre><code># Problem: Class appears in both lists\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 1, 2],\n    anomaly_class_ids=[1, 3],  # Class 1 appears in both!\n)\n# Raises ValueError: \"Overlap detected between normal_class_ids and anomaly_class_ids: {1}\"\n\n# Solution: Ensure no overlap\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 2],\n    anomaly_class_ids=[1, 3],\n)\n</code></pre>"},{"location":"node-catalog/data-nodes/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Type conversion overhead: Minimal (uint16 \u2192 float32 is efficient on GPU)</li> <li>Label mapping cost: O(B \u00d7 H \u00d7 W) lookup per batch (negligible for typical image sizes)</li> <li>Wavelength flattening: Assumes all samples in batch share wavelengths (reduces memory)</li> </ul> <p>Optimization tips:</p> <ol> <li>Use this node only once at pipeline entry (avoid redundant conversions)</li> <li>Pre-validate class IDs before training to avoid runtime warnings</li> <li>Ensure wavelengths are consistent across your dataset for correct flattening</li> </ol>"},{"location":"node-catalog/data-nodes/#relationship-to-other-nodes","title":"Relationship to Other Nodes","text":"<p>Downstream nodes that typically follow:</p> <ul> <li>MinMaxNormalizer - Normalize cube to [0, 1]</li> <li>BandpassByWavelength - Filter spectral bands</li> <li>SoftChannelSelector - Learnable band selection</li> </ul> <p>Alternative approaches:</p> <ul> <li>If you don't need label mapping, you can manually preprocess data in your dataset class</li> <li>For non-anomaly tasks, consider using raw data nodes without label transformation</li> </ul>"},{"location":"node-catalog/data-nodes/#see-also","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical Detection - Complete usage example</li> <li>Tutorial 2: Channel Selector - Two-phase training example</li> <li>BinaryAnomalyLabelMapper - Internal label mapper node</li> <li>Concept: Port System - Understanding port connections</li> <li>API Reference: ::: cuvis_ai.node.data.LentilsAnomalyDataNode</li> </ul>"},{"location":"node-catalog/data-nodes/#creating-custom-data-nodes","title":"Creating Custom Data Nodes","text":"<p>If you need to load data from a custom source or format, you can create your own data node:</p> <pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nimport torch\n\nclass CustomDataNode(Node):\n    INPUT_SPECS = {\n        \"file_path\": PortSpec(\n            dtype=str,\n            shape=(),\n            description=\"Path to data file\"\n        ),\n    }\n\n    OUTPUT_SPECS = {\n        \"cube\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),\n            description=\"Loaded cube [B, H, W, C]\"\n        ),\n    }\n\n    def forward(self, file_path: str, **kwargs):\n        # Your custom loading logic\n        cube = load_from_custom_format(file_path)\n        return {\"cube\": torch.tensor(cube, dtype=torch.float32)}\n</code></pre> <p>Learn more:</p> <ul> <li>Plugin System Development - Creating installable custom nodes</li> <li>Node System Deep Dive - Node implementation patterns</li> </ul> <p>Next Steps:</p> <ul> <li>Explore Preprocessing Nodes for normalization and filtering</li> <li>Learn about Selector Nodes for band selection strategies</li> <li>Review Tutorial 1 for a complete pipeline example</li> </ul>"},{"location":"node-catalog/deep-learning/","title":"Deep Learning","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/deep-learning/#deep-learning-nodes","title":"Deep Learning Nodes","text":""},{"location":"node-catalog/deep-learning/#overview","title":"Overview","text":"<p>Deep learning nodes use neural networks for feature learning, dimensionality reduction, and gradient-based optimization. These nodes typically require:</p> <ol> <li>Statistical initialization (Phase 1) - Initialize network parameters from data statistics</li> <li>Gradient training (Phase 2) - Fine-tune via backpropagation with trainable parameters</li> </ol> <p>Key characteristics: - Support two-phase training workflow - Learnable parameters optimized via gradient descent - Can be frozen after statistical init or unfrozen for end-to-end training - Often used for learned representations and adaptive feature extraction</p> <p>When to use: - Need learnable feature extraction beyond fixed transforms - Want end-to-end optimization with task-specific loss - Require adaptive dimensionality reduction - Deep learning-based anomaly detection (Deep SVDD)</p>"},{"location":"node-catalog/deep-learning/#nodes-in-this-category","title":"Nodes in This Category","text":""},{"location":"node-catalog/deep-learning/#deepsvdd-nodes","title":"DeepSVDD Nodes","text":"<p>Deep Support Vector Data Description (Deep SVDD) nodes for one-class anomaly detection using learned hypersphere embeddings.</p>"},{"location":"node-catalog/deep-learning/#zscorenormalizerglobal","title":"ZScoreNormalizerGlobal","text":"<p>Description: Per-pixel z-score normalizer estimating global statistics for Deep SVDD preprocessing</p> <p>Perfect for: - Deep SVDD encoder initialization - Per-pixel normalization across spectral dimension - Statistical preprocessing before neural feature extraction</p> <p>Training Paradigm: Statistical initialization only (frozen after init)</p> <p>Background:</p> <p>Computes per-band z-score normalization using global statistics estimated from initialization data:</p> \\[ \\text{normalized}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\] <p>where \\(\\mu, \\sigma\\) are estimated per-channel means/stds from a random sample of training pixels.</p>"},{"location":"node-catalog/deep-learning/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Input hyperspectral cube BHWC No <p>Output Ports:</p> Port Type Shape Description normalized float32 (B,H,W,C) Z-score normalized cube"},{"location":"node-catalog/deep-learning/#parameters","title":"Parameters","text":"Parameter Type Default Description num_channels int required Number of spectral channels sample_n int 500000 Max pixels to sample for statistics seed int 0 Random seed for sampling eps float 1e-8 Stability constant added to std"},{"location":"node-catalog/deep-learning/#example-usage-python","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.anomaly.deep_svdd import ZScoreNormalizerGlobal\n\n# Create encoder\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nencoder = ZScoreNormalizerGlobal(num_channels=61, sample_n=500_000)\npipeline.add_node(encoder)\n\n# Statistical initialization\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes encoder\n\n# Use in pipeline (frozen)\npipeline.connect(\n    (bandpass.output, encoder.data),\n    (encoder.normalized, projection.data)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  encoder:\n    type: ZScoreNormalizerGlobal\n    config:\n      num_channels: 61\n      sample_n: 500000\n      seed: 0\n      eps: 1e-8\n\nconnections:\n  - [preprocessor.output, encoder.data]\n  - [encoder.normalized, projection.data]\n</code></pre>"},{"location":"node-catalog/deep-learning/#common-issues","title":"Common Issues","text":"<p>1. Channel mismatch error</p> <pre><code># Problem: num_channels doesn't match data\nencoder = ZScoreNormalizerGlobal(num_channels=50)\nencoder.forward(data_61_channels)  # ValueError!\n\n# Solution: Match num_channels to data\nencoder = ZScoreNormalizerGlobal(num_channels=61)\n</code></pre> <p>2. Numerical instability with small std</p> <pre><code># Problem: Some channels have very low variance\nencoder = ZScoreNormalizerGlobal(num_channels=61, eps=1e-10)  # Too small\n\n# Solution: Increase eps for stability\nencoder = ZScoreNormalizerGlobal(num_channels=61, eps=1e-6)\n</code></pre>"},{"location":"node-catalog/deep-learning/#see-also","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>DeepSVDDProjection - Next stage after encoding</li> <li>Two-Phase Training</li> <li>API Reference: ::: cuvis_ai.anomaly.deep_svdd.ZScoreNormalizerGlobal</li> </ul>"},{"location":"node-catalog/deep-learning/#deepsvddprojection","title":"DeepSVDDProjection","text":"<p>Description: Neural projection head mapping per-pixel features to Deep SVDD embeddings</p> <p>Perfect for: - Deep SVDD feature extraction - Learned hyperspectral feature representations - One-class anomaly detection with deep learning</p> <p>Training Paradigm: Two-phase (statistical init \u2192 unfreeze \u2192 gradient training)</p> <p>Architecture:</p> <p>Supports two kernel types:</p> <ol> <li>Linear kernel (default):</li> <li>2-layer MLP: <code>Linear(C \u2192 hidden) \u2192 ReLU \u2192 Linear(hidden \u2192 rep_dim)</code></li> <li> <p>Fast, suitable for linearly separable data</p> </li> <li> <p>RBF kernel:</p> </li> <li>Random Fourier Features (RFF) \u2192 MLP</li> <li><code>RFF(C \u2192 n_rff) \u2192 Linear(n_rff \u2192 hidden) \u2192 ReLU \u2192 Linear(hidden \u2192 rep_dim)</code></li> <li>Better for non-linear patterns</li> </ol>"},{"location":"node-catalog/deep-learning/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Per-pixel features BHWC No <p>Output Ports:</p> Port Type Shape Description embeddings float32 (B,H,W,D) Deep SVDD embeddings (D=rep_dim)"},{"location":"node-catalog/deep-learning/#parameters_1","title":"Parameters","text":"Parameter Type Default Description in_channels int required Input feature dimension (C) rep_dim int 32 Output embedding dimension hidden int 128 Hidden layer size kernel str \"linear\" Kernel type: \"linear\" or \"rbf\" n_rff int 2048 RFF features (if kernel=\"rbf\") gamma float None RBF bandwidth (if kernel=\"rbf\", default=1/C) mlp_forward_batch_size int 65536 Batch size for MLP forward (memory control)"},{"location":"node-catalog/deep-learning/#example-usage-python_1","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.anomaly.deep_svdd import DeepSVDDProjection\n\n# Create projection head (linear kernel)\nprojection = DeepSVDDProjection(\n    in_channels=61,\n    rep_dim=32,\n    hidden=128,\n    kernel=\"linear\"\n)\n\n# Enable gradient training after statistical init\nprojection.unfreeze()\n\n# Use in pipeline\npipeline.add_nodes(projection=projection)\npipeline.connect(\n    (encoder.normalized, projection.data),\n    (projection.embeddings, center_tracker.embeddings)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  projection:\n    type: DeepSVDDProjection\n    config:\n      in_channels: 61\n      rep_dim: 32\n      hidden: 128\n      kernel: \"linear\"\n      mlp_forward_batch_size: 65536\n\nconnections:\n  - [encoder.normalized, projection.data]\n  - [projection.embeddings, center_tracker.embeddings]\n</code></pre>"},{"location":"node-catalog/deep-learning/#kernel-selection-guide","title":"Kernel Selection Guide","text":"Kernel Speed Memory Use Case linear Fast Low Linearly separable anomalies rbf Slower High (n_rff) Complex non-linear patterns <p>Recommendation: Start with <code>kernel=\"linear\"</code>. Switch to <code>kernel=\"rbf\"</code> only if linear performance is insufficient.</p>"},{"location":"node-catalog/deep-learning/#see-also_1","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>DeepSVDDCenterTracker - Next stage</li> <li>ZScoreNormalizerGlobal - Preprocessing</li> <li>API Reference: ::: cuvis_ai.anomaly.deep_svdd.DeepSVDDProjection</li> </ul>"},{"location":"node-catalog/deep-learning/#deepsvddcentertracker","title":"DeepSVDDCenterTracker","text":"<p>Description: Tracks Deep SVDD hypersphere center using exponential moving average (EMA)</p> <p>Perfect for: - Deep SVDD center initialization and tracking - Monitoring center stability during training - Providing center vector for anomaly scoring</p> <p>Training Paradigm: Statistical initialization + EMA updates during training</p> <p>Algorithm:</p> <ol> <li> <p>Phase 1 (Statistical Init): Estimate initial center as mean of embeddings:    $\\(c_0 = \\frac{1}{N} \\sum_{i=1}^{N} z_i\\)$</p> </li> <li> <p>Phase 2 (Training): Update center via EMA:    $\\(c_t = (1 - \\alpha) c_{t-1} + \\alpha \\cdot \\text{mean}(z_{\\text{batch}})\\)$</p> </li> </ol> <p>where \\(\\alpha\\) is the EMA decay rate (default 0.1).</p>"},{"location":"node-catalog/deep-learning/#port-specifications_2","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional embeddings float32 (B,H,W,D) Deep SVDD embeddings No <p>Output Ports:</p> Port Type Shape Description center float32 (D,) Current center vector metrics list () Center norm metric for logging"},{"location":"node-catalog/deep-learning/#parameters_2","title":"Parameters","text":"Parameter Type Default Description rep_dim int required Embedding dimension (D) alpha float 0.1 EMA decay rate (0 &lt; alpha \u2264 1) update_in_eval bool False Whether to update center during val/test"},{"location":"node-catalog/deep-learning/#example-usage-python_2","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.anomaly.deep_svdd import DeepSVDDCenterTracker\n\n# Create center tracker\nfrom cuvis_ai_core.training import StatisticalTrainer\n\ncenter_tracker = DeepSVDDCenterTracker(\n    rep_dim=32,\n    alpha=0.1,\n    update_in_eval=False\n)\npipeline.add_node(center_tracker)\n\n# Statistical initialization\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes center_tracker\n\n# Use in pipeline\npipeline.connect(\n    (projection.embeddings, center_tracker.embeddings),\n    (center_tracker.center, scores.center)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml_2","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  center_tracker:\n    type: DeepSVDDCenterTracker\n    config:\n      rep_dim: 32\n      alpha: 0.1\n      update_in_eval: false\n\nconnections:\n  - [projection.embeddings, center_tracker.embeddings]\n  - [center_tracker.center, scores.center]\n</code></pre>"},{"location":"node-catalog/deep-learning/#alpha-selection-guide","title":"Alpha Selection Guide","text":"Alpha Behavior Use Case 0.01 Slow adaptation Stable datasets, long training 0.1 Balanced (recommended) General use 0.5 Fast adaptation Quickly changing distributions"},{"location":"node-catalog/deep-learning/#see-also_2","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>DeepSVDDScores - Uses center for scoring</li> <li>API Reference: ::: cuvis_ai.anomaly.deep_svdd.DeepSVDDCenterTracker</li> </ul>"},{"location":"node-catalog/deep-learning/#deepsvddscores","title":"DeepSVDDScores","text":"<p>Description: Computes anomaly scores as squared distance from embeddings to Deep SVDD center</p> <p>Perfect for: - Deep SVDD anomaly scoring - Converting embeddings to anomaly maps - One-class classification decision scores</p> <p>Training Paradigm: None (stateless transform)</p> <p>Formula:</p> \\[ \\text{score}(z) = \\| z - c \\|^2 = \\sum_{d=1}^{D} (z_d - c_d)^2 \\] <p>Higher scores indicate points further from the center (potential anomalies).</p>"},{"location":"node-catalog/deep-learning/#port-specifications_3","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional embeddings float32 (B,H,W,D) Deep SVDD embeddings No center float32 (D,) Center vector from tracker No <p>Output Ports:</p> Port Type Shape Description scores float32 (B,H,W,1) Squared distance scores"},{"location":"node-catalog/deep-learning/#example-usage-python_3","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.anomaly.deep_svdd import DeepSVDDScores\n\n# Create scorer (no parameters)\nscorer = DeepSVDDScores()\n\n# Use in pipeline\npipeline.add_nodes(scorer=scorer)\npipeline.connect(\n    (projection.embeddings, scorer.embeddings),\n    (center_tracker.center, scorer.center),\n    (scorer.scores, decider.scores)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml_3","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  scorer:\n    type: DeepSVDDScores\n\nconnections:\n  - [projection.embeddings, scorer.embeddings]\n  - [center_tracker.center, scorer.center]\n  - [scorer.scores, decider.scores]\n</code></pre>"},{"location":"node-catalog/deep-learning/#see-also_3","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>QuantileBinaryDecider - Threshold scores</li> <li>DeepSVDDSoftBoundaryLoss - Training loss</li> <li>API Reference: ::: cuvis_ai.anomaly.deep_svdd.DeepSVDDScores</li> </ul>"},{"location":"node-catalog/deep-learning/#trainablepca","title":"TrainablePCA","text":"<p>Description: Trainable PCA with SVD initialization and gradient-based fine-tuning</p> <p>Perfect for: - Dimensionality reduction with end-to-end training - Baseline channel reduction (61 \u2192 3 for AdaCLIP) - PCA-initialized learnable projection</p> <p>Training Paradigm: Two-phase (SVD init \u2192 unfreeze \u2192 gradient fine-tuning)</p> <p>Algorithm:</p> <ol> <li>Phase 1 (Statistical Init): Compute PCA via SVD on centered data</li> <li>Phase 2 (Optional): Unfreeze components, optimize with gradient descent</li> </ol>"},{"location":"node-catalog/deep-learning/#port-specifications_4","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Input hyperspectral cube BHWC No <p>Output Ports:</p> Port Type Shape Description projected float32 (B,H,W,K) PCA-projected data (K components) explained_variance_ratio float32 (K,) Variance explained per component components float32 (K,C) Principal components matrix"},{"location":"node-catalog/deep-learning/#parameters_3","title":"Parameters","text":"Parameter Type Default Description num_channels int required Input channels (C) n_components int required Number of components to retain (K) whiten bool False Scale components by explained variance init_method str \"svd\" Initialization: \"svd\", \"random\" eps float 1e-6 Stability constant"},{"location":"node-catalog/deep-learning/#example-usage-python_4","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.pca import TrainablePCA\n\n# Create PCA node: 61 \u2192 3 channels\nfrom cuvis_ai_core.training import StatisticalTrainer\n\npca = TrainablePCA(num_channels=61, n_components=3, whiten=False)\npipeline.add_node(pca)\n\n# Phase 1: Statistical initialization via SVD\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes pca via SVD\n\n# Phase 2 (optional): Enable gradient training\npca.unfreeze()  # Convert components to nn.Parameter\n\n# Use in pipeline\npipeline.connect(\n    (normalizer.output, pca.data),\n    (pca.projected, detector.data)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml_4","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  pca:\n    type: TrainablePCA\n    config:\n      num_channels: 61\n      n_components: 3\n      whiten: false\n      init_method: \"svd\"\n\nconnections:\n  - [normalizer.output, pca.data]\n  - [pca.projected, detector.rgb]\n</code></pre>"},{"location":"node-catalog/deep-learning/#whiten-parameter","title":"Whiten Parameter","text":"whiten Effect Use Case False Preserve variance scaling AdaCLIP (recommended) True Equal variance per component Statistical analysis"},{"location":"node-catalog/deep-learning/#see-also_4","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>OrthogonalityLoss - Regularize components</li> <li>API Reference: ::: cuvis_ai.node.pca.TrainablePCA</li> </ul>"},{"location":"node-catalog/deep-learning/#learnablechannelmixer","title":"LearnableChannelMixer","text":"<p>Description: DRCNN-style learnable channel mixer using 1x1 convolutions with multi-layer reduction</p> <p>Perfect for: - AdaCLIP channel reduction with learned linear combinations - End-to-end trainable spectral mixing (61 \u2192 3 channels) - Task-driven hyperspectral data reduction</p> <p>Training Paradigm: Gradient-only (no statistical init required, but PCA init supported)</p> <p>Architecture:</p> <p>Multi-layer reduction with 1x1 convolutions + Leaky ReLU:</p> <pre><code>reduction_scheme = [61, 16, 8, 3]\nLayer 1: Conv1x1(61 \u2192 16) \u2192 LeakyReLU(0.01)\nLayer 2: Conv1x1(16 \u2192 8) \u2192 LeakyReLU(0.01)\nLayer 3: Conv1x1(8 \u2192 3) \u2192 Per-channel MinMax normalization\n</code></pre> <p>Based on Zeegers et al. (2020) DRCNN approach.</p>"},{"location":"node-catalog/deep-learning/#port-specifications_5","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Input hyperspectral cube BHWC No <p>Output Ports:</p> Port Type Shape Description rgb float32 (B,H,W,K) Mixed output (K channels, normalized to [0,1])"},{"location":"node-catalog/deep-learning/#parameters_4","title":"Parameters","text":"Parameter Type Default Description input_channels int required Input channels (C) output_channels int required Output channels (K) leaky_relu_negative_slope float 0.01 Leaky ReLU slope use_bias bool True Use bias in convolutions use_activation bool True Apply Leaky ReLU normalize_output bool True Per-channel MinMax normalization to [0,1] init_method str \"xavier\" Init: \"xavier\", \"kaiming\", \"pca\", \"zeros\" eps float 1e-6 Stability constant for normalization reduction_scheme list[int] None Multi-layer scheme (e.g., [61,16,8,3])"},{"location":"node-catalog/deep-learning/#example-usage-python_5","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.channel_mixer import LearnableChannelMixer\n\n# Create multi-layer mixer (DRCNN-style)\nmixer = LearnableChannelMixer(\n    input_channels=61,\n    output_channels=3,\n    reduction_scheme=[61, 16, 8, 3],  # 3-layer gradual reduction\n    leaky_relu_negative_slope=0.01,\n    normalize_output=True,\n    init_method=\"xavier\"\n)\n\n# Enable gradient training (frozen by default)\nmixer.unfreeze()\n\n# Use in pipeline\npipeline.add_nodes(mixer=mixer)\npipeline.connect(\n    (normalizer.output, mixer.data),\n    (mixer.rgb, adaclip.rgb)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml_5","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  mixer:\n    type: LearnableChannelMixer\n    config:\n      input_channels: 61\n      output_channels: 3\n      reduction_scheme: [61, 16, 8, 3]\n      leaky_relu_negative_slope: 0.01\n      normalize_output: true\n      init_method: \"xavier\"\n\nconnections:\n  - [normalizer.output, mixer.data]\n  - [mixer.rgb, adaclip.rgb]\n</code></pre>"},{"location":"node-catalog/deep-learning/#reduction-scheme-strategies","title":"Reduction Scheme Strategies","text":"Scheme Layers Optimization Use Case <code>[61, 3]</code> 1 Fast Quick baseline <code>[61, 16, 8, 3]</code> 3 Better Recommended (DRCNN paper) <code>[61, 32, 16, 8, 3]</code> 4 Best Complex tasks <p>Recommendation: Use <code>[61, 16, 8, 3]</code> for AdaCLIP (matches DRCNN paper).</p>"},{"location":"node-catalog/deep-learning/#see-also_5","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>IoULoss - End-to-end loss</li> <li>API Reference: ::: cuvis_ai.node.channel_mixer.LearnableChannelMixer</li> </ul>"},{"location":"node-catalog/deep-learning/#concretebandselector","title":"ConcreteBandSelector","text":"<p>Description: Gumbel-Softmax learnable band selector with temperature annealing</p> <p>Perfect for: - Discrete band selection with gradient optimization - AdaCLIP learnable band selection - Temperature-annealed differentiable sampling</p> <p>Training Paradigm: Gradient-only (learns categorical distributions over bands)</p> <p>Algorithm:</p> <p>For each output channel \\(c \\in \\{1, \\ldots, K\\}\\), learns logits \\(L_c \\in \\mathbb{R}^T\\) and samples:</p> \\[ w_c = \\text{softmax}\\left( \\frac{L_c + g}{\\tau} \\right), \\quad g \\sim \\text{Gumbel}(0, 1) \\] <p>Temperature \\(\\tau\\) is annealed: \\(\\tau(e) = \\tau_{\\text{start}} \\cdot \\left(\\frac{\\tau_{\\text{end}}}{\\tau_{\\text{start}}}\\right)^{e / E}\\)</p> <p>Output: \\(Y[:, :, c] = \\sum_{t=1}^T w_c[t] \\cdot X[:, :, t]\\)</p>"},{"location":"node-catalog/deep-learning/#port-specifications_6","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Input hyperspectral cube BHWC No <p>Output Ports:</p> Port Type Shape Description rgb float32 (B,H,W,K) Selected-band RGB-like output selection_weights float32 (K,C) Current selection weights (for loss)"},{"location":"node-catalog/deep-learning/#parameters_5","title":"Parameters","text":"Parameter Type Default Description input_channels int required Number of input bands (T) output_channels int 3 Number of output channels (K) tau_start float 10.0 Initial temperature (high = soft) tau_end float 0.1 Final temperature (low = peaked) max_epochs int 20 Epochs for annealing schedule use_hard_inference bool True Use argmax at inference (one-hot) eps float 1e-6 Stability constant"},{"location":"node-catalog/deep-learning/#example-usage-python_6","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.concrete_selector import ConcreteBandSelector\n\n# Create Concrete selector\nselector = ConcreteBandSelector(\n    input_channels=61,\n    output_channels=3,\n    tau_start=10.0,\n    tau_end=0.1,\n    max_epochs=50,\n    use_hard_inference=True\n)\n\n# Enable gradient training\nselector.unfreeze()\n\n# Use in pipeline\npipeline.add_nodes(selector=selector)\npipeline.connect(\n    (normalizer.output, selector.data),\n    (selector.rgb, adaclip.rgb),\n    (selector.selection_weights, distinctness_loss.weights)\n)\n</code></pre>"},{"location":"node-catalog/deep-learning/#example-configuration-yaml_6","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  selector:\n    type: ConcreteBandSelector\n    config:\n      input_channels: 61\n      output_channels: 3\n      tau_start: 10.0\n      tau_end: 0.1\n      max_epochs: 50\n      use_hard_inference: true\n\nconnections:\n  - [normalizer.output, selector.data]\n  - [selector.rgb, adaclip.rgb]\n  - [selector.selection_weights, distinctness_loss.weights]\n</code></pre>"},{"location":"node-catalog/deep-learning/#temperature-annealing","title":"Temperature Annealing","text":"Epoch Temperature Effect 0 10.0 Soft, exploration 25 ~1.0 Balanced 50 0.1 Peaked, near-discrete <p>Recommended: <code>tau_start=10.0</code>, <code>tau_end=0.1</code>, <code>max_epochs=50</code> for AdaCLIP.</p>"},{"location":"node-catalog/deep-learning/#see-also_6","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>DistinctnessLoss - Encourage diverse bands</li> <li>API Reference: ::: cuvis_ai.node.concrete_selector.ConcreteBandSelector</li> </ul>"},{"location":"node-catalog/deep-learning/#node-comparison","title":"Node Comparison","text":""},{"location":"node-catalog/deep-learning/#dimensionality-reduction-nodes","title":"Dimensionality Reduction Nodes","text":"Node Method Training Output Range Use Case TrainablePCA SVD \u2192 gradient Two-phase Unbounded Statistical baseline LearnableChannelMixer 1x1 Conv Gradient-only [0, 1] normalized DRCNN-style mixing ConcreteBandSelector Gumbel-Softmax Gradient-only [0, 1] (weighted sum) Discrete band selection"},{"location":"node-catalog/deep-learning/#deep-svdd-pipeline","title":"Deep SVDD Pipeline","text":"<p>Complete Deep SVDD workflow:</p> <pre><code>graph LR\n    A[HSI Cube] --&gt; B[ZScoreNormalizerGlobal]\n    B --&gt; C[DeepSVDDProjection]\n    C --&gt; D[DeepSVDDCenterTracker]\n    D --&gt; E[DeepSVDDScores]\n    C --&gt; E\n    E --&gt; F[QuantileBinaryDecider]\n\n    style B fill:#e1f5ff\n    style C fill:#e1f5ff\n    style D fill:#e1f5ff\n    style E fill:#e1f5ff</code></pre>"},{"location":"node-catalog/deep-learning/#adaclip-variants-comparison","title":"AdaCLIP Variants Comparison","text":"Variant Node Trainable Performance PCA Baseline TrainablePCA (frozen) No Fast, statistical DRCNN Mixer LearnableChannelMixer Yes Better, learned Concrete Selector ConcreteBandSelector Yes Best, discrete"},{"location":"node-catalog/deep-learning/#training-workflow-example","title":"Training Workflow Example","text":"<p>Two-phase training with Deep SVDD:</p> <pre><code># Phase 1: Statistical Initialization\ntrainer:\n  type: StatisticalTrainer\n  max_steps: 1000\n\nnodes:\n  encoder:\n    type: ZScoreNormalizerGlobal\n    config:\n      num_channels: 61\n\n  projection:\n    type: DeepSVDDProjection\n    config:\n      in_channels: 61\n      rep_dim: 32\n\n  center_tracker:\n    type: DeepSVDDCenterTracker\n    config:\n      rep_dim: 32\n      alpha: 0.1\n\n# Phase 2: Gradient Training\n# Unfreeze projection.unfreeze()\n# Train with DeepSVDDSoftBoundaryLoss\n</code></pre>"},{"location":"node-catalog/deep-learning/#additional-resources","title":"Additional Resources","text":"<ul> <li>Tutorial: Deep SVDD Gradient Training</li> <li>Tutorial: AdaCLIP Workflow</li> <li>Concepts: Two-Phase Training</li> <li>Concepts: Execution Stages</li> <li>API Reference: cuvis_ai.anomaly.deep_svdd</li> <li>API Reference: cuvis_ai.node.pca</li> <li>API Reference: cuvis_ai.node.channel_mixer</li> <li>API Reference: cuvis_ai.node.concrete_selector</li> </ul>"},{"location":"node-catalog/loss-metrics/","title":"Loss metrics","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/loss-metrics/#loss-metrics-nodes","title":"Loss &amp; Metrics Nodes","text":""},{"location":"node-catalog/loss-metrics/#overview","title":"Overview","text":"<p>Loss and metric nodes enable training supervision and performance evaluation. These nodes:</p> <ul> <li>Loss Nodes: Compute differentiable objectives for gradient-based optimization</li> <li>Metric Nodes: Track non-differentiable performance indicators for monitoring</li> </ul> <p>Key characteristics: - Execute only during training/validation/test (not inference) - Support multi-loss training (combine multiple objectives) - Provide real-time feedback for model tuning</p>"},{"location":"node-catalog/loss-metrics/#loss-nodes","title":"Loss Nodes","text":"<p>Loss nodes compute differentiable objectives for backpropagation. All inherit from <code>LossNode</code> base class.</p>"},{"location":"node-catalog/loss-metrics/#anomalybcewithlogits","title":"AnomalyBCEWithLogits","text":"<p>Description: Binary cross-entropy loss for anomaly detection with numerical stability</p> <p>Perfect for: - Pixel-wise anomaly detection supervision - Handling class imbalance with <code>pos_weight</code> - Standard binary classification loss</p> <p>Training Paradigm: Requires labeled anomaly masks</p>"},{"location":"node-catalog/loss-metrics/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional predictions float32 (B,H,W,1) Predicted logits No targets bool (B,H,W,1) Ground truth binary masks No <p>Output Ports:</p> Port Type Shape Description loss float32 () Scalar BCE loss"},{"location":"node-catalog/loss-metrics/#parameters","title":"Parameters","text":"Parameter Type Default Description weight float 1.0 Overall loss weight pos_weight float None Positive class weight (for imbalance) reduction str \"mean\" Reduction: \"mean\", \"sum\", or \"none\""},{"location":"node-catalog/loss-metrics/#example-usage-python","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.losses import AnomalyBCEWithLogits\n\n# Create loss with class imbalance handling\nloss = AnomalyBCEWithLogits(\n    weight=1.0,\n    pos_weight=10.0,  # 10x weight for anomaly pixels\n    reduction=\"mean\"\n)\n\n# Use in pipeline\npipeline.add_nodes(bce_loss=loss)\npipeline.connect(\n    (logit_head.logits, bce_loss.predictions),\n    (data.mask, bce_loss.targets)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  bce_loss:\n    type: AnomalyBCEWithLogits\n    config:\n      weight: 1.0\n      pos_weight: 10.0  # Handle class imbalance\n      reduction: \"mean\"\n\nconnections:\n  - [model.logits, bce_loss.predictions]\n  - [data.mask, bce_loss.targets]\n</code></pre>"},{"location":"node-catalog/loss-metrics/#see-also","title":"See Also","text":"<ul> <li>Tutorial 2: Channel Selector</li> <li>API Reference: ::: cuvis_ai.node.losses.AnomalyBCEWithLogits</li> </ul>"},{"location":"node-catalog/loss-metrics/#deepsvddsoftboundaryloss","title":"DeepSVDDSoftBoundaryLoss","text":"<p>Description: Soft-boundary Deep SVDD objective with learnable radius</p> <p>Perfect for: - One-class anomaly detection - Deep SVDD training - Hypersphere boundary learning</p> <p>Training Paradigm: Unsupervised (no labels required)</p> <p>Algorithm:</p> \\[ \\mathcal{L} = R^2 + \\frac{1}{\\nu} \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, \\|z_i - c\\|^2 - R^2) \\] <p>where \\(R\\) is the learnable hypersphere radius, \\(\\nu \\in (0,1)\\) controls outlier tolerance.</p>"},{"location":"node-catalog/loss-metrics/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional embeddings float32 (B,H,W,D) Deep SVDD embeddings No center float32 (D,) Center vector from tracker No <p>Output Ports:</p> Port Type Shape Description loss float32 () Deep SVDD soft boundary loss"},{"location":"node-catalog/loss-metrics/#parameters_1","title":"Parameters","text":"Parameter Type Default Description nu float 0.05 Outlier fraction (0 &lt; nu &lt; 1) weight float 1.0 Overall loss weight"},{"location":"node-catalog/loss-metrics/#example-usage-python_1","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.losses import DeepSVDDSoftBoundaryLoss\n\n# Create Deep SVDD loss\nloss = DeepSVDDSoftBoundaryLoss(nu=0.05, weight=1.0)\n\n# Use in pipeline\npipeline.add_nodes(deep_svdd_loss=loss)\npipeline.connect(\n    (projection.embeddings, deep_svdd_loss.embeddings),\n    (center_tracker.center, deep_svdd_loss.center)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  deep_svdd_loss:\n    type: DeepSVDDSoftBoundaryLoss\n    config:\n      nu: 0.05  # Expect 5% outliers\n      weight: 1.0\n\nconnections:\n  - [projection.embeddings, deep_svdd_loss.embeddings]\n  - [center_tracker.center, deep_svdd_loss.center]\n</code></pre>"},{"location":"node-catalog/loss-metrics/#nu-parameter-guide","title":"Nu Parameter Guide","text":"nu Behavior Use Case 0.01 Tight boundary Clean data, few outliers 0.05 Balanced (recommended) General use 0.1 Loose boundary Noisy data, many outliers"},{"location":"node-catalog/loss-metrics/#see-also_1","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>DeepSVDDCenterTracker</li> <li>API Reference: ::: cuvis_ai.node.losses.DeepSVDDSoftBoundaryLoss</li> </ul>"},{"location":"node-catalog/loss-metrics/#orthogonalityloss","title":"OrthogonalityLoss","text":"<p>Description: Regularization enforcing orthonormality of PCA components</p> <p>Perfect for: - Trainable PCA gradient training - Preventing component collapse - Maintaining PCA properties during optimization</p> <p>Formula:</p> \\[ \\mathcal{L}_{\\text{orth}} = \\|W W^T - I\\|_F^2 \\] <p>where \\(W\\) is the components matrix, \\(I\\) is identity.</p>"},{"location":"node-catalog/loss-metrics/#port-specifications_2","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional components float32 (K,C) PCA components matrix No <p>Output Ports:</p> Port Type Shape Description loss float32 () Weighted orthogonality loss"},{"location":"node-catalog/loss-metrics/#parameters_2","title":"Parameters","text":"Parameter Type Default Description weight float 1.0 Regularization strength"},{"location":"node-catalog/loss-metrics/#example-usage-python_2","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.losses import OrthogonalityLoss\n\n# Create orthogonality regularizer\north_loss = OrthogonalityLoss(weight=0.01)\n\n# Use in pipeline\npipeline.add_nodes(orth_loss=orth_loss)\npipeline.connect(\n    (pca.components, orth_loss.components)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#see-also_2","title":"See Also","text":"<ul> <li>TrainablePCA</li> <li>Tutorial 4: AdaCLIP Workflow</li> <li>API Reference: ::: cuvis_ai.node.losses.OrthogonalityLoss</li> </ul>"},{"location":"node-catalog/loss-metrics/#iouloss","title":"IoULoss","text":"<p>Description: Differentiable IoU (Intersection over Union) loss for segmentation</p> <p>Perfect for: - End-to-end AdaCLIP training - Segmentation-based anomaly detection - Direct IoU optimization</p> <p>Formula:</p> \\[ \\mathcal{L}_{\\text{IoU}} = 1 - \\frac{|\\text{pred} \\cap \\text{target}|}{|\\text{pred} \\cup \\text{target}|} \\]"},{"location":"node-catalog/loss-metrics/#port-specifications_3","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional predictions float32 (B,H,W,1) Continuous anomaly scores No targets bool (B,H,W,1) Ground truth binary masks No <p>Output Ports:</p> Port Type Shape Description loss float32 () IoU loss (1 - IoU)"},{"location":"node-catalog/loss-metrics/#parameters_3","title":"Parameters","text":"Parameter Type Default Description weight float 1.0 Overall loss weight smooth float 1e-6 Numerical stability constant normalize_method str \"sigmoid\" Score normalization: \"sigmoid\", \"clamp\", \"minmax\""},{"location":"node-catalog/loss-metrics/#example-usage-python_3","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.losses import IoULoss\n\n# Create IoU loss\niou_loss = IoULoss(\n    weight=1.0,\n    smooth=1e-6,\n    normalize_method=\"sigmoid\"  # For logit inputs\n)\n\n# Use in pipeline\npipeline.add_nodes(iou_loss=iou_loss)\npipeline.connect(\n    (adaclip.scores, iou_loss.predictions),\n    (data.mask, iou_loss.targets)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#example-configuration-yaml_2","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  iou_loss:\n    type: IoULoss\n    config:\n      weight: 1.0\n      smooth: 1e-6\n      normalize_method: \"sigmoid\"\n\nconnections:\n  - [adaclip.scores, iou_loss.predictions]\n  - [data.mask, iou_loss.targets]\n</code></pre>"},{"location":"node-catalog/loss-metrics/#normalize-method-guide","title":"Normalize Method Guide","text":"Method Use Case Input Range sigmoid Logits from model Unbounded clamp Scores already in [0,1] Near [0,1] minmax Varying score ranges Any"},{"location":"node-catalog/loss-metrics/#see-also_3","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>API Reference: ::: cuvis_ai.node.losses.IoULoss</li> </ul>"},{"location":"node-catalog/loss-metrics/#distinctnessloss","title":"DistinctnessLoss","text":"<p>Description: Repulsion loss encouraging selector diversity (prevents band collapse)</p> <p>Perfect for: - ConcreteBandSelector training - Preventing channel collapse to same band - Encouraging diverse band selection</p> <p>Formula:</p> \\[ \\mathcal{L}_{\\text{distinct}} = \\frac{1}{N_{\\text{pairs}}} \\sum_{i &lt; j} \\cos(w_i, w_j) \\] <p>Minimizing this encourages low cosine similarity between selector vectors.</p>"},{"location":"node-catalog/loss-metrics/#port-specifications_4","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional selection_weights float32 (K,C) Selector weight matrix No <p>Output Ports:</p> Port Type Shape Description loss float32 () Repulsion loss"},{"location":"node-catalog/loss-metrics/#parameters_4","title":"Parameters","text":"Parameter Type Default Description weight float 0.1 Regularization strength eps float 1e-6 Stability constant"},{"location":"node-catalog/loss-metrics/#example-usage-python_4","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.losses import DistinctnessLoss\n\n# Create distinctness regularizer\ndistinct_loss = DistinctnessLoss(weight=0.1)\n\n# Use in pipeline\npipeline.add_nodes(distinct_loss=distinct_loss)\npipeline.connect(\n    (selector.selection_weights, distinct_loss.selection_weights)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#see-also_4","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>ConcreteBandSelector</li> <li>API Reference: ::: cuvis_ai.node.losses.DistinctnessLoss</li> </ul>"},{"location":"node-catalog/loss-metrics/#selectorentropyregularizer","title":"SelectorEntropyRegularizer","text":"<p>Description: Entropy regularization encouraging exploration in channel selection</p> <p>Perfect for: - SoftChannelSelector training - Preventing premature selection convergence - Balancing exploration vs exploitation</p> <p>Formula:</p> \\[ \\mathcal{L}_{\\text{entropy}} = -\\sum_{i=1}^{C} p_i \\log(p_i) \\] <p>Positive weight maximizes entropy (exploration), negative minimizes (exploitation).</p>"},{"location":"node-catalog/loss-metrics/#parameters_5","title":"Parameters","text":"Parameter Type Default Description weight float 0.01 Regularization strength target_entropy float None Target entropy (if set, uses squared error) eps float 1e-6 Stability constant"},{"location":"node-catalog/loss-metrics/#see-also_5","title":"See Also","text":"<ul> <li>Tutorial 2: Channel Selector</li> <li>SoftChannelSelector</li> <li>API Reference: ::: cuvis_ai.node.losses.SelectorEntropyRegularizer</li> </ul>"},{"location":"node-catalog/loss-metrics/#selectordiversityregularizer","title":"SelectorDiversityRegularizer","text":"<p>Description: Diversity regularization via negative variance maximization</p> <p>Perfect for: - SoftChannelSelector training - Encouraging spread across channels - Preventing concentration on few channels</p> <p>Formula:</p> \\[ \\mathcal{L}_{\\text{diversity}} = -\\text{Var}(w) = -\\frac{1}{C} \\sum_{i=1}^{C} (w_i - \\bar{w})^2 \\] <p>Minimizing loss = maximizing variance = maximizing diversity.</p>"},{"location":"node-catalog/loss-metrics/#parameters_6","title":"Parameters","text":"Parameter Type Default Description weight float 0.01 Regularization strength"},{"location":"node-catalog/loss-metrics/#see-also_6","title":"See Also","text":"<ul> <li>Tutorial 2: Channel Selector</li> <li>SoftChannelSelector</li> <li>API Reference: ::: cuvis_ai.node.losses.SelectorDiversityRegularizer</li> </ul>"},{"location":"node-catalog/loss-metrics/#metrics-nodes","title":"Metrics Nodes","text":"<p>Metric nodes compute non-differentiable performance indicators for monitoring. Execute only during validation/test.</p>"},{"location":"node-catalog/loss-metrics/#anomalydetectionmetrics","title":"AnomalyDetectionMetrics","text":"<p>Description: Comprehensive anomaly detection metrics (precision, recall, F1, IoU, AP)</p> <p>Perfect for: - Evaluating anomaly detection performance - Model selection and hyperparameter tuning - Performance reporting</p> <p>Metrics Computed: - Precision: TP / (TP + FP) - Recall: TP / (TP + FN) - F1 Score: Harmonic mean of precision and recall - IoU: Intersection over Union (Jaccard index) - Average Precision: Area under precision-recall curve (if logits provided)</p>"},{"location":"node-catalog/loss-metrics/#port-specifications_5","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional decisions bool (B,H,W,1) Binary anomaly decisions No targets bool (B,H,W,1) Ground truth binary masks No logits float32 (B,H,W,1) Optional logits for AP Yes <p>Output Ports:</p> Port Type Shape Description metrics list () List of Metric objects"},{"location":"node-catalog/loss-metrics/#example-usage-python_5","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.metrics import AnomalyDetectionMetrics\n\n# Create metrics node\nmetrics = AnomalyDetectionMetrics()\n\n# Use in pipeline\npipeline.add_nodes(metrics=metrics)\npipeline.connect(\n    (decider.decisions, metrics.decisions),\n    (data.mask, metrics.targets),\n    (logit_head.logits, metrics.logits)  # Optional for AP\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#example-configuration-yaml_3","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  metrics:\n    type: AnomalyDetectionMetrics\n\nconnections:\n  - [decider.decisions, metrics.decisions]\n  - [data.mask, metrics.targets]\n  - [logit_head.logits, metrics.logits]  # Optional\n</code></pre>"},{"location":"node-catalog/loss-metrics/#see-also_7","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical</li> <li>BinaryDecider</li> <li>API Reference: ::: cuvis_ai.node.metrics.AnomalyDetectionMetrics</li> </ul>"},{"location":"node-catalog/loss-metrics/#explainedvariancemetric","title":"ExplainedVarianceMetric","text":"<p>Description: Tracks PCA explained variance ratios and cumulative variance</p> <p>Perfect for: - Monitoring PCA component quality - Determining optimal number of components - Validating dimensionality reduction</p> <p>Metrics Computed: - Per-component variance: <code>explained_variance_pc1</code>, <code>explained_variance_pc2</code>, ... - Total variance: Sum of all components - Cumulative variance: Running sum of components</p>"},{"location":"node-catalog/loss-metrics/#port-specifications_6","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional explained_variance_ratio float32 (K,) Variance ratios from PCA No <p>Output Ports:</p> Port Type Shape Description metrics list () List of Metric objects"},{"location":"node-catalog/loss-metrics/#example-usage-python_6","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.metrics import ExplainedVarianceMetric\n\n# Create variance metric\nvar_metric = ExplainedVarianceMetric()\n\n# Use in pipeline\npipeline.add_nodes(var_metric=var_metric)\npipeline.connect(\n    (pca.explained_variance_ratio, var_metric.explained_variance_ratio)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#see-also_8","title":"See Also","text":"<ul> <li>TrainablePCA</li> <li>Tutorial 4: AdaCLIP Workflow</li> <li>API Reference: ::: cuvis_ai.node.metrics.ExplainedVarianceMetric</li> </ul>"},{"location":"node-catalog/loss-metrics/#anomalypixelstatisticsmetric","title":"AnomalyPixelStatisticsMetric","text":"<p>Description: Computes pixel-level statistics for anomaly detection results</p> <p>Perfect for: - Monitoring anomaly detection behavior - Tracking proportion of detected anomalies per batch - Quick sanity checks on detection outputs</p> <p>Metrics Computed: - Total Pixels: <code>anomaly/total_pixels</code> - Total number of pixels in the batch - Anomalous Pixels: <code>anomaly/anomalous_pixels</code> - Count of pixels classified as anomalies - Anomaly Percentage: <code>anomaly/anomaly_percentage</code> - Percentage of anomalous pixels</p>"},{"location":"node-catalog/loss-metrics/#port-specifications_7","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional decisions bool (B,H,W,1) Binary anomaly decisions No <p>Output Ports:</p> Port Type Shape Description metrics list () List of Metric objects"},{"location":"node-catalog/loss-metrics/#example-usage-python_7","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.metrics import AnomalyPixelStatisticsMetric\n\n# Create pixel statistics metric\npixel_stats = AnomalyPixelStatisticsMetric()\n\n# Use in pipeline\npipeline.add_nodes(pixel_stats=pixel_stats)\npipeline.connect(\n    (decider.decisions, pixel_stats.decisions),\n    (pixel_stats.metrics, monitor.metrics)\n)\n</code></pre>"},{"location":"node-catalog/loss-metrics/#example-configuration-yaml_4","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  pixel_stats:\n    type: AnomalyPixelStatisticsMetric\n\nconnections:\n  - [decider.decisions, pixel_stats.decisions]\n  - [pixel_stats.metrics, monitor.metrics]\n</code></pre>"},{"location":"node-catalog/loss-metrics/#see-also_9","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical</li> <li>BinaryDecider</li> <li>AnomalyDetectionMetrics</li> <li>API Reference: ::: cuvis_ai.node.metrics.AnomalyPixelStatisticsMetric</li> </ul>"},{"location":"node-catalog/loss-metrics/#multi-loss-training","title":"Multi-Loss Training","text":"<p>Combine multiple loss objectives for complex training scenarios:</p>"},{"location":"node-catalog/loss-metrics/#example-channel-selector-with-regularization","title":"Example: Channel Selector with Regularization","text":"<pre><code>nodes:\n  # Primary supervision\n  bce_loss:\n    type: AnomalyBCEWithLogits\n    config:\n      weight: 1.0\n      pos_weight: 10.0\n\n  # Selector regularizers\n  entropy_reg:\n    type: SelectorEntropyRegularizer\n    config:\n      weight: 0.01  # Encourage exploration\n\n  diversity_reg:\n    type: SelectorDiversityRegularizer\n    config:\n      weight: 0.01  # Encourage spread\n\nconnections:\n  - [logit_head.logits, bce_loss.predictions]\n  - [data.mask, bce_loss.targets]\n  - [selector.weights, entropy_reg.weights]\n  - [selector.weights, diversity_reg.weights]\n\n# Combined loss = 1.0*BCE + 0.01*entropy + 0.01*diversity\n</code></pre>"},{"location":"node-catalog/loss-metrics/#example-adaclip-with-iou-distinctness","title":"Example: AdaCLIP with IoU + Distinctness","text":"<pre><code>nodes:\n  # Primary loss\n  iou_loss:\n    type: IoULoss\n    config:\n      weight: 1.0\n\n  # Regularizer for band diversity\n  distinct_loss:\n    type: DistinctnessLoss\n    config:\n      weight: 0.1\n\nconnections:\n  - [adaclip.scores, iou_loss.predictions]\n  - [data.mask, iou_loss.targets]\n  - [selector.selection_weights, distinct_loss.selection_weights]\n\n# Combined loss = 1.0*IoU + 0.1*distinctness\n</code></pre>"},{"location":"node-catalog/loss-metrics/#loss-weight-tuning-guide","title":"Loss Weight Tuning Guide","text":"Loss Component Typical Range Effect Primary loss (BCE, IoU) 1.0 Main supervision signal Orthogonality 0.001 - 0.1 Prevent PCA collapse Entropy reg 0.001 - 0.05 Balance exploration Diversity reg 0.001 - 0.05 Encourage spread Distinctness 0.01 - 0.5 Prevent band collapse <p>Tuning strategy: 1. Start with primary loss only 2. Add regularizers with low weights (0.01) 3. Increase regularizer weights if needed 4. Monitor metrics to validate improvement</p>"},{"location":"node-catalog/loss-metrics/#additional-resources","title":"Additional Resources","text":"<ul> <li>Tutorial: Channel Selector - Multi-loss training</li> <li>Tutorial: Deep SVDD Gradient - Deep SVDD loss</li> <li>Tutorial: AdaCLIP Workflow - IoU + distinctness</li> <li>Concepts: Two-Phase Training</li> <li>API Reference: cuvis_ai.node.losses</li> <li>API Reference: cuvis_ai.node.metrics</li> </ul>"},{"location":"node-catalog/output/","title":"Output Nodes","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/output/#output-nodes","title":"Output Nodes","text":"<p>Status: Under Development (Phase 5) Expected Completion: Week 5</p>"},{"location":"node-catalog/output/#coming-soon","title":"Coming Soon","text":"<p>This page will document: - OutputNode - VisualizationNode - ExportNode - Configuration and usage examples - Result handling and storage</p> <p>Related Pages: - Node Catalog - Monitoring &amp; Visualization</p>"},{"location":"node-catalog/preprocessing/","title":"Preprocessing","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/preprocessing/#preprocessing-nodes","title":"Preprocessing Nodes","text":""},{"location":"node-catalog/preprocessing/#overview","title":"Overview","text":"<p>Preprocessing nodes prepare hyperspectral data for downstream processing by normalizing values, filtering spectral bands, and standardizing formats. These transformations are critical for numerical stability, gradient flow, and feature extraction in both statistical and deep learning pipelines.</p> <p>When to use Preprocessing Nodes:</p> <ul> <li>Normalization: Scale data to standard ranges ([0,1], z-scores) for gradient stability</li> <li>Band filtering: Reduce spectral dimensionality by selecting wavelength ranges</li> <li>Standardization: Remove mean/scale by variance for statistical methods</li> <li>Transformation: Apply nonlinear transformations (sigmoid) for bounded outputs</li> </ul> <p>Key concepts:</p> <ul> <li>Per-sample vs global normalization: Some nodes compute statistics per batch (ZScoreNormalizer), others use running statistics from initialization (MinMaxNormalizer with <code>use_running_stats=True</code>)</li> <li>Differentiability: All nodes preserve gradients for end-to-end training</li> <li>Statistical initialization: Some nodes (MinMaxNormalizer) can be initialized with dataset statistics for consistent normalization</li> </ul>"},{"location":"node-catalog/preprocessing/#nodes-in-this-category","title":"Nodes in This Category","text":""},{"location":"node-catalog/preprocessing/#bandpassbywavelength","title":"BandpassByWavelength","text":"<p>Description: Filters hyperspectral cubes to a specified wavelength range</p> <p>Perfect for:</p> <ul> <li>Removing noisy spectral bands at edges (e.g., &lt;450nm, &gt;900nm)</li> <li>Focusing on specific absorption features (e.g., 650-750nm for chlorophyll)</li> <li>Reducing dimensionality before band selection</li> <li>Standardizing spectral ranges across different sensors</li> </ul> <p>Training Paradigm: None (fixed wavelength filtering)</p>"},{"location":"node-catalog/preprocessing/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input hyperspectral cube No wavelengths int32 (C,) Wavelength values (nm) for each channel No <p>Output Ports:</p> Port Type Shape Description filtered float32 (B, H, W, C_filtered) Cube with selected channels only"},{"location":"node-catalog/preprocessing/#parameters","title":"Parameters","text":"Parameter Type Default Description min_wavelength_nm float Required Minimum wavelength (inclusive) to keep max_wavelength_nm float | None None Maximum wavelength (inclusive). If None, keeps all wavelengths &gt;= min"},{"location":"node-catalog/preprocessing/#example-usage-python","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.preprocessors import BandpassByWavelength\n\n# Filter to visible-NIR range (500-800nm)\nbandpass = BandpassByWavelength(\n    min_wavelength_nm=500.0,\n    max_wavelength_nm=800.0,\n)\n\n# Use in pipeline (wavelengths from data node)\npipeline.connect(\n    (data_node.cube, bandpass.data),\n    (data_node.wavelengths, bandpass.wavelengths),\n    (bandpass.filtered, normalizer.data),\n)\n</code></pre>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  bandpass:\n    type: BandpassByWavelength\n    config:\n      min_wavelength_nm: 500.0\n      max_wavelength_nm: 800.0\n\nconnections:\n  - [data.cube, bandpass.data]\n  - [data.wavelengths, bandpass.wavelengths]\n  - [bandpass.filtered, normalizer.data]\n</code></pre>"},{"location":"node-catalog/preprocessing/#common-use-cases","title":"Common Use Cases","text":"<p>1. Remove atmospheric absorption bands</p> <pre><code># Filter out water absorption at 1400nm and 1900nm\nbandpass = BandpassByWavelength(\n    min_wavelength_nm=430.0,\n    max_wavelength_nm=1300.0,  # Stop before 1400nm water absorption\n)\n</code></pre> <p>2. Focus on specific spectral features</p> <pre><code># Extract red-edge region for vegetation analysis\nred_edge = BandpassByWavelength(\n    min_wavelength_nm=680.0,\n    max_wavelength_nm=750.0,\n)\n</code></pre> <p>3. Progressive filtering</p> <pre><code># Chain multiple bandpasses for complex filtering\n# First: Remove edges\nstage1 = BandpassByWavelength(min_wavelength_nm=450.0, max_wavelength_nm=900.0)\n# Second: Extract specific region from filtered data\nstage2 = BandpassByWavelength(min_wavelength_nm=600.0, max_wavelength_nm=700.0)\n</code></pre>"},{"location":"node-catalog/preprocessing/#see-also","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient - Uses bandpass filtering</li> <li>Concept: Port System - Port connections</li> <li>API Reference: ::: cuvis_ai.node.preprocessors.BandpassByWavelength</li> </ul>"},{"location":"node-catalog/preprocessing/#minmaxnormalizer","title":"MinMaxNormalizer","text":"<p>Description: Scales data to [0, 1] range using min-max normalization</p> <p>Perfect for:</p> <ul> <li>Normalizing hyperspectral cubes for visualization</li> <li>Preparing data for sigmoid-based activations</li> <li>Ensuring bounded inputs for certain loss functions</li> <li>Providing consistent scale for statistical methods</li> </ul> <p>Training Paradigm: Statistical initialization (optional, if <code>use_running_stats=True</code>)</p>"},{"location":"node-catalog/preprocessing/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input tensor to normalize No <p>Output Ports:</p> Port Type Shape Description normalized float32 (B, H, W, C) Min-max normalized tensor in [0, 1]"},{"location":"node-catalog/preprocessing/#parameters_1","title":"Parameters","text":"Parameter Type Default Description eps float 1e-6 Small constant to prevent division by zero use_running_stats bool True If True, use global min/max from statistical initialization. If False, compute per-sample min/max"},{"location":"node-catalog/preprocessing/#normalization-modes","title":"Normalization Modes","text":"<p>Mode 1: Global normalization (<code>use_running_stats=True</code>, default)</p> <ul> <li>Requires statistical initialization to compute global min/max</li> <li>Normalizes all samples consistently: <code>(x - global_min) / (global_max - global_min)</code></li> <li>Best for consistent scaling across train/val/test</li> </ul> <p>Mode 2: Per-sample normalization (<code>use_running_stats=False</code>)</p> <ul> <li>No initialization required</li> <li>Computes min/max per batch: <code>(x - batch_min) / (batch_max - batch_min)</code></li> <li>Best for visualization or when each sample has different dynamic range</li> </ul>"},{"location":"node-catalog/preprocessing/#example-usage-python_1","title":"Example Usage (Python)","text":"<p>Global normalization with statistical initialization</p> <pre><code>from cuvis_ai.node.normalization import MinMaxNormalizer\n\n# Create normalizer (requires initialization)\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nnormalizer = MinMaxNormalizer(\n    eps=1e-6,\n    use_running_stats=True,  # Use global statistics\n)\npipeline.add_node(normalizer)\n\n# Statistical initialization (run once)\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes normalizer\n\n# Now use in pipeline\noutputs = normalizer.forward(data=cube)  # Normalized to [0, 1] using global min/max\n</code></pre> <p>Per-sample normalization (no initialization)</p> <pre><code># Create normalizer without statistical init\nnormalizer = MinMaxNormalizer(\n    eps=1e-6,\n    use_running_stats=False,  # Per-sample normalization\n)\n\n# Use directly (no initialization needed)\noutputs = normalizer.forward(data=cube)  # Each sample normalized to its own [0, 1]\n</code></pre>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  normalizer:\n    type: MinMaxNormalizer\n    config:\n      eps: 1e-6\n      use_running_stats: true  # Requires statistical initialization\n</code></pre>"},{"location":"node-catalog/preprocessing/#statistical-initialization","title":"Statistical Initialization","text":"<pre><code># Create initialization stream\nfrom cuvis_ai_core.pipeline.stream import DataLoaderInputStream\n\nfrom cuvis_ai_core.training import StatisticalTrainer\n\n# Initialize normalizer\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes normalizer\n\n# Running statistics are now stored in normalizer.running_min and normalizer.running_max\n</code></pre>"},{"location":"node-catalog/preprocessing/#common-issues","title":"Common Issues","text":"<p>Issue: Normalized values outside [0, 1]</p> <pre><code># Problem: Test data has values outside training range\nnormalizer.running_min = 0.0  # From training data\nnormalizer.running_max = 1.0\ntest_data = torch.tensor([[[[-0.5, 1.5]]] ])  # Values outside [0, 1]\nnormalized = normalizer.forward(data=test_data)[\"normalized\"]\n# Output: [[[[-0.5, 1.5]]]] (no clamping)\n\n# Solution: Clip values if needed\nnormalized_clipped = torch.clamp(normalized, 0.0, 1.0)\n</code></pre>"},{"location":"node-catalog/preprocessing/#see-also_1","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical - Uses MinMaxNormalizer</li> <li>Tutorial 2: Channel Selector - Statistical initialization example</li> <li>Concept: Two-Phase Training - Statistical init workflow</li> <li>API Reference: ::: cuvis_ai.node.normalization.MinMaxNormalizer</li> </ul>"},{"location":"node-catalog/preprocessing/#zscorenormalizer","title":"ZScoreNormalizer","text":"<p>Description: Applies z-score (standardization) normalization: (x - mean) / std</p> <p>Perfect for:</p> <ul> <li>Preparing data for deep learning (zero mean, unit variance)</li> <li>Statistical anomaly detection (RX, Mahalanobis distance)</li> <li>Removing per-sample brightness variations</li> <li>Gradient-based optimization (improved convergence)</li> </ul> <p>Training Paradigm: None (per-sample normalization)</p>"},{"location":"node-catalog/preprocessing/#port-specifications_2","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input tensor No <p>Output Ports:</p> Port Type Shape Description normalized float32 (B, H, W, C) Z-score normalized tensor"},{"location":"node-catalog/preprocessing/#parameters_2","title":"Parameters","text":"Parameter Type Default Description dims list[int] [1, 2] Dimensions to compute statistics over (1=H, 2=W in BHWC format) eps float 1e-6 Stability constant for division keepdim bool True Whether to keep reduced dimensions for broadcasting"},{"location":"node-catalog/preprocessing/#normalization-behavior","title":"Normalization Behavior","text":"<p>Default (dims=[1, 2]): Standardize over spatial dimensions (H, W)</p> <ul> <li>Computes mean and std across height and width</li> <li>Keeps batch and channel dimensions separate</li> <li>Result: Each channel per sample has zero mean and unit variance</li> </ul> <p>Alternative (dims=[1, 2, 3]): Standardize over spatial + channel dimensions</p> <ul> <li>Computes mean and std across H, W, and C</li> <li>Result: Each sample has zero mean and unit variance globally</li> </ul>"},{"location":"node-catalog/preprocessing/#example-usage-python_2","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.normalization import ZScoreNormalizer\n\n# Default: Normalize over spatial dimensions (H, W)\nzscore = ZScoreNormalizer(\n    dims=[1, 2],  # Height and width\n    eps=1e-6,\n)\n\n# Use in pipeline\noutputs = zscore.forward(data=cube)  # Shape: [B, H, W, C]\n# Each (B, C) slice has mean\u22480, std\u22481\n</code></pre> <p>Global standardization</p> <pre><code># Normalize over all spatial and spectral dimensions\nzscore_global = ZScoreNormalizer(\n    dims=[1, 2, 3],  # H, W, C\n    eps=1e-6,\n)\n\noutputs = zscore_global.forward(data=cube)\n# Each batch element has global mean\u22480, std\u22481\n</code></pre>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml_2","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  zscore:\n    type: ZScoreNormalizer\n    config:\n      dims: [1, 2]  # Normalize over spatial dimensions\n      eps: 1e-6\n      keepdim: true\n</code></pre>"},{"location":"node-catalog/preprocessing/#comparison-with-other-normalizers","title":"Comparison with Other Normalizers","text":"Normalizer Output Range Statistics Use Case MinMaxNormalizer [0, 1] Min/max Bounded outputs, visualization ZScoreNormalizer (-\u221e, \u221e) Mean/std Deep learning, gradient optimization SigmoidNormalizer [0, 1] Median/std Robust to outliers PerPixelUnitNorm [-1, 1] (approx) Per-pixel L2 Unit-norm features"},{"location":"node-catalog/preprocessing/#zscorenormalizerglobal","title":"ZScoreNormalizerGlobal","text":"<p>A specialized variant of <code>ZScoreNormalizer</code> for Deep SVDD training. This normalizer computes statistics globally across all dimensions for consistent encoding initialization. See Deep SVDD Tutorial for usage examples.</p>"},{"location":"node-catalog/preprocessing/#see-also_2","title":"See Also","text":"<ul> <li>Tutorial 2: Channel Selector - Uses ZScoreNormalizer</li> <li>Concept: Two-Phase Training - When to use z-score</li> <li>API Reference: ::: cuvis_ai.node.normalization.ZScoreNormalizer</li> </ul>"},{"location":"node-catalog/preprocessing/#sigmoidnormalizer","title":"SigmoidNormalizer","text":"<p>Description: Median-centered sigmoid squashing to [0, 1] range</p> <p>Perfect for:</p> <ul> <li>Robust normalization when data has outliers</li> <li>Converting unbounded scores to probabilities</li> <li>Visualization of anomaly scores</li> <li>Preprocessing for loss functions expecting [0, 1] inputs</li> </ul> <p>Training Paradigm: None (per-sample normalization)</p>"},{"location":"node-catalog/preprocessing/#port-specifications_3","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input tensor No <p>Output Ports:</p> Port Type Shape Description normalized float32 (B, H, W, C) Sigmoid-normalized tensor in [0, 1]"},{"location":"node-catalog/preprocessing/#parameters_3","title":"Parameters","text":"Parameter Type Default Description std_floor float 1e-6 Minimum standard deviation (prevents division by zero)"},{"location":"node-catalog/preprocessing/#normalization-formula","title":"Normalization Formula","text":"<pre><code>output = sigmoid((x - median) / std)\n</code></pre> <p>Where:</p> <ul> <li><code>median</code>: Per-sample median across spatial dimensions</li> <li><code>std</code>: Per-sample standard deviation (clamped to std_floor)</li> <li><code>sigmoid(z) = 1 / (1 + exp(-z))</code></li> </ul> <p>Properties:</p> <ul> <li>Median maps to 0.5</li> <li>Values &gt; median \u2192 (0.5, 1.0)</li> <li>Values &lt; median \u2192 (0.0, 0.5)</li> <li>Robust to extreme outliers (sigmoid saturation)</li> </ul>"},{"location":"node-catalog/preprocessing/#example-usage-python_3","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.normalization import SigmoidNormalizer\n\n# Create sigmoid normalizer\nsigmoid_norm = SigmoidNormalizer(std_floor=1e-6)\n\n# Use in pipeline\noutputs = sigmoid_norm.forward(data=scores)  # Scores \u2192 [0, 1]\n</code></pre> <p>Comparison with MinMaxNormalizer</p> <pre><code># MinMaxNormalizer: Sensitive to outliers\ndata = torch.tensor([[[[1.0, 2.0, 3.0, 100.0]]]])  # Outlier: 100.0\nminmax = MinMaxNormalizer(use_running_stats=False)\nminmax_out = minmax.forward(data=data)[\"normalized\"]\n# Output: [0.0, 0.01, 0.02, 1.0] - outlier dominates\n\n# SigmoidNormalizer: Robust to outliers\nsigmoid = SigmoidNormalizer()\nsigmoid_out = sigmoid.forward(data=data)[\"normalized\"]\n# Output: [0.27, 0.5, 0.73, ~1.0] - better distribution\n</code></pre>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml_3","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  sigmoid_norm:\n    type: SigmoidNormalizer\n    config:\n      std_floor: 1e-6\n</code></pre>"},{"location":"node-catalog/preprocessing/#when-to-use","title":"When to Use","text":"<p>Use SigmoidNormalizer when:</p> <ul> <li>Data contains outliers that skew min/max normalization</li> <li>You need smooth, bounded outputs for visualization</li> <li>Inputs are unbounded (e.g., RX scores, z-scores)</li> </ul> <p>Use MinMaxNormalizer when:</p> <ul> <li>Data is clean without outliers</li> <li>You need exact [0, 1] mapping of min/max values</li> <li>Consistent scaling across train/test is critical</li> </ul>"},{"location":"node-catalog/preprocessing/#see-also_3","title":"See Also","text":"<ul> <li>SigmoidTransform - Simple sigmoid without centering</li> <li>API Reference: ::: cuvis_ai.node.normalization.SigmoidNormalizer</li> </ul>"},{"location":"node-catalog/preprocessing/#perpixelunitnorm","title":"PerPixelUnitNorm","text":"<p>Description: Per-pixel mean-centering and L2 normalization across spectral channels</p> <p>Perfect for:</p> <ul> <li>Creating unit-norm feature vectors for cosine similarity</li> <li>Removing per-pixel brightness variations</li> <li>Preprocessing for Deep SVDD (hypersphere embedding)</li> <li>Spectral signature normalization</li> </ul> <p>Training Paradigm: None (per-sample normalization)</p>"},{"location":"node-catalog/preprocessing/#port-specifications_4","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input cube No <p>Output Ports:</p> Port Type Shape Description normalized float32 (B, H, W, C) L2-normalized cube (unit norm per pixel)"},{"location":"node-catalog/preprocessing/#parameters_4","title":"Parameters","text":"Parameter Type Default Description eps float 1e-8 Stability constant for L2 norm (prevents division by zero)"},{"location":"node-catalog/preprocessing/#normalization-formula_1","title":"Normalization Formula","text":"<p>For each pixel (h, w) in the spatial grid:</p> <pre><code>1. centered = x[h, w, :] - mean(x[h, w, :])\n2. norm = ||centered||_2\n3. output[h, w, :] = centered / max(norm, eps)\n</code></pre> <p>Result: Each pixel's spectral vector has zero mean and unit L2 norm.</p>"},{"location":"node-catalog/preprocessing/#example-usage-python_4","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.normalization import PerPixelUnitNorm\n\n# Create per-pixel normalizer\nunit_norm = PerPixelUnitNorm(eps=1e-8)\n\n# Use in pipeline\noutputs = unit_norm.forward(data=cube)  # Shape: [B, H, W, C]\n\n# Verify unit norm\nimport torch\nnorms = outputs[\"normalized\"].norm(dim=-1)  # Should be \u22481.0 for all pixels\n</code></pre> <p>Integration with Deep SVDD</p> <pre><code># Deep SVDD preprocessing chain\nfrom cuvis_ai.node.preprocessors import BandpassByWavelength\nfrom cuvis_ai.node.normalization import PerPixelUnitNorm\n\n# Step 1: Filter to relevant bands\nbandpass = BandpassByWavelength(min_wavelength_nm=500.0, max_wavelength_nm=800.0)\n\n# Step 2: Unit-norm per pixel\nunit_norm = PerPixelUnitNorm(eps=1e-8)\n\n# Connect in pipeline\npipeline.connect(\n    (data.cube, bandpass.data),\n    (bandpass.filtered, unit_norm.data),\n    (unit_norm.normalized, encoder.data),  # To Deep SVDD encoder\n)\n</code></pre>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml_4","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  unit_norm:\n    type: PerPixelUnitNorm\n    config:\n      eps: 1e-8\n\nconnections:\n  - [bandpass.filtered, unit_norm.data]\n  - [unit_norm.normalized, encoder.data]\n</code></pre>"},{"location":"node-catalog/preprocessing/#why-unit-normalization","title":"Why Unit Normalization?","text":"<p>Benefits:</p> <ol> <li>Scale invariance: Removes absolute intensity, focuses on spectral shape</li> <li>Cosine similarity: Unit vectors enable efficient similarity computation</li> <li>Hypersphere embedding: Natural for Deep SVDD's hypersphere boundary</li> <li>Gradient flow: Bounded gradients improve training stability</li> </ol> <p>When to avoid:</p> <ul> <li>Absolute intensity is important (e.g., detection of faint anomalies)</li> <li>Data is already zero-mean, unit-variance (z-score normalized)</li> </ul>"},{"location":"node-catalog/preprocessing/#see-also_4","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient - Complete usage</li> <li>Deep SVDD Projection - Downstream encoder</li> <li>API Reference: ::: cuvis_ai.node.normalization.PerPixelUnitNorm</li> </ul>"},{"location":"node-catalog/preprocessing/#sigmoidtransform","title":"SigmoidTransform","text":"<p>Description: Applies sigmoid function to convert logits to probabilities [0, 1]</p> <p>Perfect for:</p> <ul> <li>Converting raw model outputs (logits) to probabilities</li> <li>Visualization of anomaly scores</li> <li>Preparing unbounded scores for bounded loss functions</li> <li>Routing logits to both loss (raw) and visualization (sigmoid) ports</li> </ul> <p>Training Paradigm: None (simple transformation)</p>"},{"location":"node-catalog/preprocessing/#port-specifications_5","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input tensor (logits or scores) No <p>Output Ports:</p> Port Type Shape Description transformed float32 (B, H, W, C) Sigmoid-transformed tensor in [0, 1]"},{"location":"node-catalog/preprocessing/#parameters_5","title":"Parameters","text":"<p>None (applies <code>torch.sigmoid</code> directly)</p>"},{"location":"node-catalog/preprocessing/#example-usage-python_5","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.normalization import SigmoidTransform\n\n# Create sigmoid transform\nsigmoid = SigmoidTransform()\n\n# Use for visualization routing\npipeline.connect(\n    (rx.scores, loss_node.predictions),    # Raw logits to loss\n    (rx.scores, sigmoid.data),             # Logits to sigmoid\n    (sigmoid.transformed, viz.scores),     # Probabilities to viz\n)\n</code></pre> <p>Typical pattern: Dual routing</p> <pre><code>nodes:\n  rx_detector:\n    type: RXGlobal\n    # Outputs: scores (logits)\n\n  logit_head:\n    type: ScoreToLogit\n    # Outputs: logits\n\n  sigmoid:\n    type: SigmoidTransform\n\n  loss:\n    type: AnomalyBCEWithLogits  # Expects raw logits\n\n  viz:\n    type: ScoreHeatmapVisualizer  # Expects [0, 1] probabilities\n\nconnections:\n  - [logit_head.logits, loss.predictions]      # Raw to loss\n  - [logit_head.logits, sigmoid.data]          # Raw to sigmoid\n  - [sigmoid.transformed, viz.scores]          # Probs to viz\n</code></pre>"},{"location":"node-catalog/preprocessing/#comparison-with-sigmoidnormalizer","title":"Comparison with SigmoidNormalizer","text":"Node Transformation Use Case SigmoidTransform <code>sigmoid(x)</code> Simple logit \u2192 probability conversion SigmoidNormalizer <code>sigmoid((x - median) / std)</code> Robust normalization with centering <p>When to use which:</p> <ul> <li>SigmoidTransform: Logits already well-scaled (e.g., from ScoreToLogit)</li> <li>SigmoidNormalizer: Unbounded scores need robust normalization</li> </ul>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml_5","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  sigmoid:\n    type: SigmoidTransform  # No config parameters\n\nconnections:\n  - [model.logits, sigmoid.data]\n  - [sigmoid.transformed, visualizer.scores]\n</code></pre>"},{"location":"node-catalog/preprocessing/#see-also_5","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical - Routing example</li> <li>ScoreToLogit - Produces logits for sigmoid</li> <li>API Reference: ::: cuvis_ai.node.normalization.SigmoidTransform</li> </ul>"},{"location":"node-catalog/preprocessing/#identitynormalizer","title":"IdentityNormalizer","text":"<p>Description: No-op normalizer that passes data through unchanged</p> <p>Perfect for:</p> <ul> <li>Placeholder in modular pipeline configurations</li> <li>A/B testing different normalization strategies</li> <li>Debugging pipelines (bypass normalization)</li> <li>Config-driven architecture selection</li> </ul> <p>Training Paradigm: None (pass-through)</p>"},{"location":"node-catalog/preprocessing/#port-specifications_6","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input tensor No <p>Output Ports:</p> Port Type Shape Description normalized float32 (B, H, W, C) Unchanged output (identity)"},{"location":"node-catalog/preprocessing/#parameters_6","title":"Parameters","text":"<p>None</p>"},{"location":"node-catalog/preprocessing/#example-usage-python_6","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.normalization import IdentityNormalizer\n\n# Create identity normalizer (pass-through)\nidentity = IdentityNormalizer()\n\n# Use as placeholder\noutputs = identity.forward(data=cube)  # outputs[\"normalized\"] == cube\n</code></pre> <p>Config-driven selection</p> <pre><code># Select normalizer based on config\nnormalizer_type = config.get(\"normalizer\", \"identity\")\n\nif normalizer_type == \"minmax\":\n    normalizer = MinMaxNormalizer()\nelif normalizer_type == \"zscore\":\n    normalizer = ZScoreNormalizer()\nelse:\n    normalizer = IdentityNormalizer()  # No normalization\n</code></pre>"},{"location":"node-catalog/preprocessing/#example-configuration-yaml_6","title":"Example Configuration (YAML)","text":"<pre><code># Baseline: No normalization\nnodes:\n  normalizer:\n    type: IdentityNormalizer  # No-op\n\n# Experiment: Try z-score\nnodes:\n  normalizer:\n    type: ZScoreNormalizer\n    config:\n      dims: [1, 2]\n</code></pre>"},{"location":"node-catalog/preprocessing/#when-to-use_1","title":"When to Use","text":"<p>Valid use cases:</p> <ol> <li>Ablation studies: Test impact of normalization</li> <li>Pre-normalized data: Input already in correct range</li> <li>Modular configs: Swap normalizers without code changes</li> </ol> <p>Anti-pattern (avoid):</p> <pre><code># Bad: Unnecessary identity node\npipeline.connect(\n    (data.cube, identity.data),\n    (identity.normalized, next_node.input),\n)\n\n# Better: Direct connection\npipeline.connect(\n    (data.cube, next_node.input),\n)\n</code></pre> <p>Only use IdentityNormalizer when you need a normalizer placeholder in a modular architecture.</p>"},{"location":"node-catalog/preprocessing/#see-also_6","title":"See Also","text":"<ul> <li>Concept: Pipeline Architecture - Modular design patterns</li> <li>API Reference: ::: cuvis_ai.node.normalization.IdentityNormalizer</li> </ul>"},{"location":"node-catalog/preprocessing/#choosing-the-right-normalizer","title":"Choosing the Right Normalizer","text":""},{"location":"node-catalog/preprocessing/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    A[Need Normalization?] --&gt;|Yes| B{Data Range?}\n    A --&gt;|No| I[IdentityNormalizer]\n\n    B --&gt;|Bounded to [0,1]| C{Outliers?}\n    B --&gt;|Unbounded| D{Use Case?}\n\n    C --&gt;|No| E[MinMaxNormalizer]\n    C --&gt;|Yes| F[SigmoidNormalizer]\n\n    D --&gt;|Deep Learning| G[ZScoreNormalizer]\n    D --&gt;|Unit Features| H[PerPixelUnitNorm]\n    D --&gt;|Logits \u2192 Probs| J[SigmoidTransform]</code></pre>"},{"location":"node-catalog/preprocessing/#normalization-strategy-by-task","title":"Normalization Strategy by Task","text":"Task Recommended Normalizer Reason Statistical detection (RX) MinMaxNormalizer Bounded [0,1] for visualization Deep learning training ZScoreNormalizer Zero mean, unit variance for gradients Deep SVDD PerPixelUnitNorm Unit-norm for hypersphere embedding Anomaly visualization SigmoidTransform Convert logits to probabilities Robust to outliers SigmoidNormalizer Median-based, sigmoid saturation Pre-normalized data IdentityNormalizer No transformation needed"},{"location":"node-catalog/preprocessing/#chaining-preprocessors","title":"Chaining Preprocessors","text":"<p>Common pattern 1: Band filtering + normalization</p> <pre><code>nodes:\n  bandpass:\n    type: BandpassByWavelength\n    config:\n      min_wavelength_nm: 500.0\n      max_wavelength_nm: 800.0\n\n  normalizer:\n    type: MinMaxNormalizer\n    config:\n      use_running_stats: true\n\nconnections:\n  - [data.cube, bandpass.data]\n  - [bandpass.filtered, normalizer.data]\n</code></pre> <p>Common pattern 2: Deep SVDD preprocessing chain</p> <pre><code>nodes:\n  bandpass:\n    type: BandpassByWavelength\n    config:\n      min_wavelength_nm: 500.0\n      max_wavelength_nm: 800.0\n\n  unit_norm:\n    type: PerPixelUnitNorm\n    config:\n      eps: 1e-8\n\n  encoder:\n    type: ZScoreNormalizerGlobal  # Statistical encoder\n    # ...\n\nconnections:\n  - [data.cube, bandpass.data]\n  - [bandpass.filtered, unit_norm.data]\n  - [unit_norm.normalized, encoder.data]\n</code></pre>"},{"location":"node-catalog/preprocessing/#performance-considerations","title":"Performance Considerations","text":"<p>Computational Cost (per batch):</p> Node Complexity Memory Overhead BandpassByWavelength O(C) indexing None (views) MinMaxNormalizer O(B\u00d7H\u00d7W\u00d7C) 2 scalars (running stats) ZScoreNormalizer O(B\u00d7H\u00d7W\u00d7C) None (per-sample) SigmoidNormalizer O(B\u00d7H\u00d7W\u00d7C) None (per-sample) PerPixelUnitNorm O(B\u00d7H\u00d7W\u00d7C) None (per-sample) SigmoidTransform O(B\u00d7H\u00d7W\u00d7C) None (elementwise) IdentityNormalizer O(1) None (pass-through) <p>Optimization Tips:</p> <ol> <li>Fuse operations: Combine bandpass + normalization in a single custom node if performance-critical</li> <li>Use running stats: Enable <code>use_running_stats=True</code> for consistent normalization across batches</li> <li>Avoid redundant normalization: Don't normalize twice (e.g., MinMax \u2192 ZScore)</li> <li>GPU-friendly: All nodes are GPU-accelerated via PyTorch operations</li> </ol>"},{"location":"node-catalog/preprocessing/#creating-custom-preprocessors","title":"Creating Custom Preprocessors","text":"<pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nimport torch\n\nclass CustomPreprocessor(Node):\n    INPUT_SPECS = {\n        \"data\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),\n            description=\"Input cube [B, H, W, C]\"\n        ),\n    }\n\n    OUTPUT_SPECS = {\n        \"processed\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),\n            description=\"Processed cube\"\n        ),\n    }\n\n    def __init__(self, custom_param: float = 1.0, **kwargs):\n        self.custom_param = custom_param\n        super().__init__(custom_param=custom_param, **kwargs)\n\n    def forward(self, data: torch.Tensor, **kwargs):\n        # Your custom preprocessing logic\n        processed = data * self.custom_param\n        return {\"processed\": processed}\n</code></pre> <p>Learn more:</p> <ul> <li>Plugin System Development</li> <li>Node System Deep Dive</li> </ul> <p>Next Steps:</p> <ul> <li>Explore Selector Nodes for learnable band selection</li> <li>Learn about Statistical Nodes for anomaly detection</li> <li>Review Tutorial 1: RX Statistical for complete preprocessing examples</li> </ul>"},{"location":"node-catalog/selectors/","title":"Selectors","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/selectors/#selector-nodes","title":"Selector Nodes","text":""},{"location":"node-catalog/selectors/#overview","title":"Overview","text":"<p>Selector nodes reduce hyperspectral dimensionality by selecting or weighting subsets of spectral bands. They address the \"curse of dimensionality\" by extracting discriminative channels for downstream processing, particularly important for models like CLIP that expect 3-channel RGB inputs.</p> <p>When to use Selector Nodes:</p> <ul> <li>Dimensionality reduction: Reduce 61+ channels to manageable subsets (3-10 channels)</li> <li>Band selection: Learn which wavelengths are most discriminative for your task</li> <li>HSI to RGB conversion: Create RGB composites for pretrained vision models (AdaCLIP, CLIP)</li> <li>Interpretability: Understand which spectral regions matter for anomaly detection</li> </ul> <p>Key concepts:</p> <ul> <li>Learnable vs fixed selection: SoftChannelSelector learns weights via gradients, while band selectors use fixed or data-driven strategies</li> <li>Temperature annealing: Gumbel-Softmax temperature controls soft (smooth) vs hard (discrete) selection</li> <li>mRMR (maximum Relevance Minimum Redundancy): Supervised selectors balance discriminative power and band diversity</li> <li>Two-phase training: Statistical initialization \u2192 unfreeze \u2192 gradient training</li> </ul>"},{"location":"node-catalog/selectors/#category-learnable-channel-selection","title":"Category: Learnable Channel Selection","text":""},{"location":"node-catalog/selectors/#softchannelselector","title":"SoftChannelSelector","text":"<p>Description: Differentiable channel selector using Gumbel-Softmax with temperature annealing</p> <p>Perfect for:</p> <ul> <li>Learning optimal band combinations for your specific task</li> <li>Two-phase training with statistical initialization + gradient optimization</li> <li>End-to-end learnable pipelines</li> <li>Interpretable channel importance visualization</li> </ul> <p>Training Paradigm: Two-phase (statistical initialization \u2192 gradient training)</p>"},{"location":"node-catalog/selectors/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B, H, W, C) Input hyperspectral cube No <p>Output Ports:</p> Port Type Shape Description selected float32 (B, H, W, C) Channel-weighted output (same shape as input) weights float32 (C,) Current channel selection weights (for regularization)"},{"location":"node-catalog/selectors/#parameters","title":"Parameters","text":"Parameter Type Default Description n_select int Required Number of channels to select (effective selection budget) input_channels int Required Total number of input channels init_method \"uniform\" | \"variance\" \"uniform\" Initialization: uniform (zeros) or variance-based temperature_init float 5.0 Initial temperature for Gumbel-Softmax temperature_min float 0.1 Minimum temperature (annealing floor) temperature_decay float 0.9 Temperature decay factor per epoch hard bool False If True, use hard (top-k) selection at inference eps float 1e-6 Numerical stability constant"},{"location":"node-catalog/selectors/#how-it-works","title":"How It Works","text":"<p>Soft selection (training):</p> <pre><code>weights = softmax(logits / temperature) * n_select\nselected = cube * weights  # [B, H, W, C]\n</code></pre> <ul> <li>Low temperature \u2192 more discrete (closer to hard selection)</li> <li>High temperature \u2192 more uniform (explores all channels)</li> </ul> <p>Hard selection (inference):</p> <pre><code>top_k_indices = topk(logits, n_select)\nweights[top_k_indices] = 1.0\nweights[other_indices] = 0.0\n</code></pre> <p>Temperature annealing schedule:</p> <pre><code>temp(epoch) = max(temp_min, temp_init * (decay ** epoch))\n</code></pre>"},{"location":"node-catalog/selectors/#example-usage-python","title":"Example Usage (Python)","text":"<p>Scenario 1: Two-phase training (statistical init \u2192 gradient optimization)</p> <pre><code>from cuvis_ai.node.selector import SoftChannelSelector\n\n# Phase 1: Statistical initialization\nselector = SoftChannelSelector(\n    n_select=3,  # Reduce 61 channels to 3\n    input_channels=61,\n    init_method=\"variance\",  # Importance-based initialization\n    temperature_init=5.0,\n)\n\n# Initialize from training data\nfrom cuvis_ai_core.training import StatisticalTrainer\n\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes selector\n\n# Phase 2: Unfreeze and train\nselector.unfreeze()  # Convert logits buffer \u2192 nn.Parameter\n\n# During training, update temperature per epoch\nfor epoch in range(max_epochs):\n    selector.update_temperature(epoch=epoch)\n    # Train with gradient descent...\n</code></pre> <p>Scenario 2: Inspect selected channels</p> <pre><code># Get current selection weights\nweights = selector.get_selection_weights(hard=False)  # Soft weights\nprint(f\"Channel weights: {weights}\")  # [0.05, 0.12, ..., 1.82, ...] sums to n_select\n\n# Get hard selection (top-k)\nhard_weights = selector.get_selection_weights(hard=True)\ntop_k_indices = torch.where(hard_weights &gt; 0)[0]\nprint(f\"Selected channels: {top_k_indices}\")  # [15, 28, 41]\n</code></pre>"},{"location":"node-catalog/selectors/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  selector:\n    type: SoftChannelSelector\n    config:\n      n_select: 3\n      input_channels: 61\n      init_method: variance\n      temperature_init: 5.0\n      temperature_min: 0.1\n      temperature_decay: 0.9\n      hard: false  # Soft during training, hard at inference\n\n  # Entropy regularizer (encourages diversity)\n  entropy_loss:\n    type: SelectorEntropyRegularizer\n    config:\n      weight: 0.01\n\n  # Diversity regularizer (prevents collapse)\n  diversity_loss:\n    type: SelectorDiversityRegularizer\n    config:\n      weight: 0.05\n\nconnections:\n  - [data.cube, selector.data]\n  - [selector.selected, rx.data]  # To downstream node\n  - [selector.weights, entropy_loss.weights]  # Regularization\n  - [selector.weights, diversity_loss.weights]\n</code></pre>"},{"location":"node-catalog/selectors/#statistical-initialization","title":"Statistical Initialization","text":"<pre><code># Variance-based initialization (init_method=\"variance\")\nfrom cuvis_ai_core.training import StatisticalTrainer\n\n# Create trainer with pipeline and data\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes selector\n\n# Result: channel_logits = log(variance_per_channel)\n# High-variance channels get higher initial weights\n</code></pre>"},{"location":"node-catalog/selectors/#temperature-annealing","title":"Temperature Annealing","text":"<pre><code># Manual temperature control\nselector.temperature = 5.0  # High temp: explore all channels\ntrain_epoch(...)\nselector.update_temperature(epoch=0)  # temp = 5.0 * (0.9 ** 0) = 5.0\n\nselector.update_temperature(epoch=1)  # temp = 5.0 * (0.9 ** 1) = 4.5\ntrain_epoch(...)\n\n# ... after many epochs\nselector.update_temperature(epoch=20)  # temp = max(0.1, 5.0 * 0.9^20) \u2248 0.608\nselector.update_temperature(epoch=50)  # temp = 0.1 (floor reached)\n</code></pre>"},{"location":"node-catalog/selectors/#common-issues","title":"Common Issues","text":"<p>Issue 1: Channel collapse (all weight on few channels)</p> <pre><code># Problem: Selector collapses to 1-2 channels\nweights = selector.get_selection_weights()\n# Output: [0.0, 0.0, 2.98, 0.0, ...]  # Almost all weight on channel 2\n\n# Solution: Add diversity regularizer\nfrom cuvis_ai.node.losses import SelectorDiversityRegularizer\ndiversity_loss = SelectorDiversityRegularizer(weight=0.05)\n</code></pre> <p>Issue 2: Temperature not annealing</p> <pre><code># Problem: Forgot to call update_temperature\nfor epoch in range(50):\n    train_epoch(...)\n    # selector.temperature still 5.0!\n\n# Solution: Add callback or manual update\nfor epoch in range(50):\n    selector.update_temperature(epoch=epoch)\n    train_epoch(...)\n</code></pre> <p>Issue 3: Unfreeze not called</p> <pre><code># Problem: channel_logits not trainable\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nselector = SoftChannelSelector(n_select=3, input_channels=61)\npipeline.add_node(selector)\n\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Initializes selector\n# selector.channel_logits is a buffer, not a parameter!\n\noptimizer = torch.optim.Adam(selector.parameters())  # Empty!\n\n# Solution: Unfreeze before creating optimizer\nselector.unfreeze()  # Converts buffer \u2192 nn.Parameter\noptimizer = torch.optim.Adam(selector.parameters())  # Now includes channel_logits\n</code></pre>"},{"location":"node-catalog/selectors/#see-also","title":"See Also","text":"<ul> <li>Tutorial 2: Channel Selector - Complete two-phase training example</li> <li>Tutorial 4: AdaCLIP Workflow - Alternative selector: ConcreteBandSelector</li> <li>SelectorEntropyRegularizer - Entropy regularization loss</li> <li>SelectorDiversityRegularizer - Diversity regularization loss</li> <li>Concept: Two-Phase Training</li> <li>API Reference: ::: cuvis_ai.node.selector.SoftChannelSelector</li> </ul>"},{"location":"node-catalog/selectors/#topkindices","title":"TopKIndices","text":"<p>Description: Utility node that extracts top-k channel indices from selector weights</p> <p>Perfect for:</p> <ul> <li>Introspecting which channels were selected</li> <li>Reporting selected wavelengths</li> <li>Debugging selector behavior</li> <li>Visualization of channel importance</li> </ul> <p>Training Paradigm: None (utility node)</p>"},{"location":"node-catalog/selectors/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional weights float32 (C,) Channel selection weights No <p>Output Ports:</p> Port Type Shape Description indices int64 (k,) Top-k channel indices (sorted by weight)"},{"location":"node-catalog/selectors/#parameters_1","title":"Parameters","text":"Parameter Type Default Description k int Required Number of top indices to return"},{"location":"node-catalog/selectors/#example-usage-python_1","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.selector import SoftChannelSelector, TopKIndices\n\n# Create selector and top-k extractor\nselector = SoftChannelSelector(n_select=3, input_channels=61)\ntop_k = TopKIndices(k=3)\n\n# Connect in pipeline\npipeline.connect(\n    (selector.weights, top_k.weights),\n)\n\n# After training, get selected indices\noutputs = top_k.forward(weights=selector.get_selection_weights(hard=True))\nprint(f\"Top 3 channels: {outputs['indices']}\")  # tensor([15, 28, 41])\n</code></pre>"},{"location":"node-catalog/selectors/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  selector:\n    type: SoftChannelSelector\n    config:\n      n_select: 3\n      input_channels: 61\n\n  top_k:\n    type: TopKIndices\n    config:\n      k: 3\n\nconnections:\n  - [selector.weights, top_k.weights]\n</code></pre>"},{"location":"node-catalog/selectors/#see-also_1","title":"See Also","text":"<ul> <li>SoftChannelSelector - Produces weights for this node</li> <li>API Reference: ::: cuvis_ai.node.selector.TopKIndices</li> </ul>"},{"location":"node-catalog/selectors/#category-fixed-band-selection-unsupervised","title":"Category: Fixed Band Selection (Unsupervised)","text":"<p>These nodes select bands using fixed wavelengths or data-driven heuristics (no labels required).</p>"},{"location":"node-catalog/selectors/#baselinefalsergbselector","title":"BaselineFalseRGBSelector","text":"<p>Description: Fixed wavelength band selection (e.g., 650, 550, 450 nm for R, G, B)</p> <p>Perfect for:</p> <ul> <li>Creating \"true color-ish\" RGB images for visualization</li> <li>Baseline comparison for supervised methods</li> <li>Quick HSI \u2192 RGB conversion without training</li> <li>Pretrained vision models expecting natural RGB</li> </ul> <p>Training Paradigm: None (fixed wavelengths)</p>"},{"location":"node-catalog/selectors/#port-specifications_2","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional cube float32 (B, H, W, C) Hyperspectral cube No wavelengths int32 (C,) Wavelength array (nm) No <p>Output Ports:</p> Port Type Shape Description rgb_image float32 (B, H, W, 3) RGB image (0-1 range, per-sample normalized) band_info dict () Selected band metadata"},{"location":"node-catalog/selectors/#parameters_2","title":"Parameters","text":"Parameter Type Default Description target_wavelengths tuple[float, float, float] (650.0, 550.0, 450.0) Target wavelengths for R, G, B (nm)"},{"location":"node-catalog/selectors/#example-usage-python_2","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.band_selection import BaselineFalseRGBSelector\n\n# Default: Red=650nm, Green=550nm, Blue=450nm\nbaseline = BaselineFalseRGBSelector(\n    target_wavelengths=(650.0, 550.0, 450.0)\n)\n\n# Use in pipeline\noutputs = baseline.forward(cube=cube, wavelengths=wavelengths)\nrgb = outputs[\"rgb_image\"]  # [B, H, W, 3] in [0, 1]\nprint(outputs[\"band_info\"])\n# {'strategy': 'baseline_false_rgb',\n#  'band_indices': [28, 15, 3],\n#  'band_wavelengths_nm': [648.5, 551.2, 447.8],\n#  'target_wavelengths_nm': [650.0, 550.0, 450.0]}\n</code></pre>"},{"location":"node-catalog/selectors/#example-configuration-yaml_2","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  baseline_rgb:\n    type: BaselineFalseRGBSelector\n    config:\n      target_wavelengths: [650.0, 550.0, 450.0]\n\nconnections:\n  - [data.cube, baseline_rgb.cube]\n  - [data.wavelengths, baseline_rgb.wavelengths]\n  - [baseline_rgb.rgb_image, adaclip.rgb_input]\n</code></pre>"},{"location":"node-catalog/selectors/#see-also_2","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow - Uses band selectors</li> <li>HighContrastBandSelector - Data-driven alternative</li> <li>API Reference: ::: cuvis_ai.node.band_selection.BaselineFalseRGBSelector</li> </ul>"},{"location":"node-catalog/selectors/#cirfalsecolorselector","title":"CIRFalseColorSelector","text":"<p>Description: Color Infrared (CIR) false color composition: NIR \u2192 R, Red \u2192 G, Green \u2192 B</p> <p>Perfect for:</p> <ul> <li>Highlighting vegetation and plant health</li> <li>Detecting certain anomalies invisible in true color</li> <li>Remote sensing applications</li> <li>Creating false-color composites</li> </ul> <p>Training Paradigm: None (fixed wavelengths)</p>"},{"location":"node-catalog/selectors/#port-specifications_3","title":"Port Specifications","text":"<p>Same as BaselineFalseRGBSelector</p>"},{"location":"node-catalog/selectors/#parameters_3","title":"Parameters","text":"Parameter Type Default Description nir_nm float 860.0 Near-infrared wavelength (\u2192 Red channel) red_nm float 670.0 Red wavelength (\u2192 Green channel) green_nm float 560.0 Green wavelength (\u2192 Blue channel)"},{"location":"node-catalog/selectors/#example-usage-python_3","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.band_selection import CIRFalseColorSelector\n\n# CIR mapping: NIR=860nm \u2192 R, Red=670nm \u2192 G, Green=560nm \u2192 B\ncir = CIRFalseColorSelector(\n    nir_nm=860.0,\n    red_nm=670.0,\n    green_nm=560.0,\n)\n\noutputs = cir.forward(cube=cube, wavelengths=wavelengths)\nprint(outputs[\"band_info\"][\"channel_mapping\"])\n# {'R': 'NIR', 'G': 'Red', 'B': 'Green'}\n</code></pre>"},{"location":"node-catalog/selectors/#see-also_3","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow - CIR variants</li> <li>API Reference: ::: cuvis_ai.node.band_selection.CIRFalseColorSelector</li> </ul>"},{"location":"node-catalog/selectors/#highcontrastbandselector","title":"HighContrastBandSelector","text":"<p>Description: Data-driven band selection using spatial variance + Laplacian energy</p> <p>Perfect for:</p> <ul> <li>Creating high-contrast RGB images for visual anomaly detection</li> <li>Unsupervised band selection (no labels needed)</li> <li>Emphasizing spatial features</li> <li>Alternative to fixed wavelength selection</li> </ul> <p>Training Paradigm: None (data-driven, computed per forward pass)</p>"},{"location":"node-catalog/selectors/#port-specifications_4","title":"Port Specifications","text":"<p>Same as BaselineFalseRGBSelector</p>"},{"location":"node-catalog/selectors/#parameters_4","title":"Parameters","text":"Parameter Type Default Description windows Sequence[tuple[float, float]] ((440, 500), (500, 580), (610, 700)) Wavelength windows for Blue, Green, Red alpha float 0.1 Weight for Laplacian energy term"},{"location":"node-catalog/selectors/#selection-formula","title":"Selection Formula","text":"<p>For each wavelength window, select the band with highest score:</p> <pre><code>score(band) = variance(band) + alpha * laplacian_energy(band)\n</code></pre> <p>Where:</p> <ul> <li><code>variance(band)</code>: Spatial variance across pixels</li> <li><code>laplacian_energy(band)</code>: Mean absolute Laplacian (edge strength)</li> <li>High-variance, high-edge bands are selected</li> </ul>"},{"location":"node-catalog/selectors/#example-usage-python_4","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.band_selection import HighContrastBandSelector\n\n# Select high-contrast bands in visible spectrum windows\nhigh_contrast = HighContrastBandSelector(\n    windows=((440, 500), (500, 580), (610, 700)),  # B, G, R\n    alpha=0.1,  # Laplacian weight\n)\n\noutputs = high_contrast.forward(cube=cube, wavelengths=wavelengths)\n# Selects bands with max(variance + 0.1 * laplacian_energy) per window\n</code></pre>"},{"location":"node-catalog/selectors/#see-also_4","title":"See Also","text":"<ul> <li>API Reference: ::: cuvis_ai.node.band_selection.HighContrastBandSelector</li> </ul>"},{"location":"node-catalog/selectors/#category-supervised-band-selection","title":"Category: Supervised Band Selection","text":"<p>These nodes require labeled training data and use mRMR (maximum Relevance Minimum Redundancy) for supervised band selection.</p>"},{"location":"node-catalog/selectors/#mrmr-algorithm-overview","title":"mRMR Algorithm Overview","text":"<p>mRMR (maximum Relevance, Minimum Redundancy) balances two objectives:</p> <ol> <li>Relevance: Select bands discriminative for the task (Fisher score + AUC + MI)</li> <li>Redundancy: Avoid highly correlated bands</li> </ol> <p>Scoring formula:</p> <pre><code>combined_score = w_fisher * Fisher + w_auc * AUC + w_mi * MI\nadjusted_score(band) = combined_score - lambda * max_correlation_with_selected\n</code></pre> <p>Metrics:</p> <ul> <li>Fisher score: (\u03bc\u2081 - \u03bc\u2080)\u00b2 / (\u03c3\u2081\u00b2 + \u03c3\u2080\u00b2) \u2014 class separation</li> <li>AUC: ROC AUC using band intensity as classifier</li> <li>MI: Mutual information between band and labels</li> </ul>"},{"location":"node-catalog/selectors/#supervisedcirbandselector","title":"SupervisedCIRBandSelector","text":"<p>Description: Supervised CIR/NIR band selection with window constraints</p> <p>Perfect for:</p> <ul> <li>Supervised band selection in NIR, Red, Green windows</li> <li>Maximizing class separation for anomaly detection</li> <li>CIR-like composites optimized for your specific task</li> <li>Interpretable band selection with mRMR</li> </ul> <p>Training Paradigm: Statistical initialization (requires labels)</p>"},{"location":"node-catalog/selectors/#port-specifications_5","title":"Port Specifications","text":"<p>Same as BaselineFalseRGBSelector, plus:</p> <p>Input Ports (additional):</p> Port Type Shape Description Optional mask bool (B, H, W, 1) Binary mask (1=positive, 0=negative) Yes (required for initialization)"},{"location":"node-catalog/selectors/#parameters_5","title":"Parameters","text":"Parameter Type Default Description num_spectral_bands int Required Total number of spectral bands windows Sequence[tuple[float, float]] ((840.0, 910.0), (650.0, 720.0), (500.0, 570.0)) NIR, Red, Green windows score_weights tuple[float, float, float] (1.0, 1.0, 1.0) Weights for (Fisher, AUC, MI) lambda_penalty float 0.5 Redundancy penalty for mRMR"},{"location":"node-catalog/selectors/#example-usage-python_5","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.band_selection import SupervisedCIRBandSelector\n\n# Create supervised CIR selector\nsupervised_cir = SupervisedCIRBandSelector(\n    num_spectral_bands=61,\n    windows=((840.0, 910.0), (650.0, 720.0), (500.0, 570.0)),  # NIR, R, G\n    score_weights=(1.0, 1.0, 1.0),  # Equal weight to Fisher, AUC, MI\n    lambda_penalty=0.5,  # Balance relevance and redundancy\n)\n\n# Statistical initialization with labeled data\nfrom cuvis_ai_core.training import StatisticalTrainer\n\n# Create trainer with pipeline and data\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes supervised_cir\n\n# Check selected bands\nprint(f\"Selected indices: {supervised_cir.selected_indices}\")  # e.g., [52, 28, 10]\nprint(f\"Fisher scores: {supervised_cir.fisher_scores}\")  # [0.12, 0.08, ...]\nprint(f\"AUC scores: {supervised_cir.auc_scores}\")  # [0.73, 0.68, ...]\n</code></pre>"},{"location":"node-catalog/selectors/#example-configuration-yaml_3","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  supervised_cir:\n    type: SupervisedCIRBandSelector\n    config:\n      num_spectral_bands: 61\n      windows: [[840.0, 910.0], [650.0, 720.0], [500.0, 570.0]]\n      score_weights: [1.0, 1.0, 1.0]\n      lambda_penalty: 0.5\n\nconnections:\n  - [data.cube, supervised_cir.cube]\n  - [data.mask, supervised_cir.mask]\n  - [data.wavelengths, supervised_cir.wavelengths]\n</code></pre>"},{"location":"node-catalog/selectors/#see-also_5","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow - Supervised band selectors</li> <li>API Reference: ::: cuvis_ai.node.band_selection.SupervisedCIRBandSelector</li> </ul>"},{"location":"node-catalog/selectors/#supervisedwindowedfalsergbselector","title":"SupervisedWindowedFalseRGBSelector","text":"<p>Description: Supervised band selection constrained to visible RGB windows</p> <p>Perfect for:</p> <ul> <li>True-color-like composites optimized for your task</li> <li>Supervised selection in Blue, Green, Red windows</li> <li>Maximizing discriminative power while maintaining interpretability</li> </ul> <p>Training Paradigm: Statistical initialization (requires labels)</p>"},{"location":"node-catalog/selectors/#port-specifications_6","title":"Port Specifications","text":"<p>Same as SupervisedCIRBandSelector</p>"},{"location":"node-catalog/selectors/#parameters_6","title":"Parameters","text":"Parameter Type Default Description num_spectral_bands int Required Total number of spectral bands windows Sequence[tuple[float, float]] ((440.0, 500.0), (500.0, 580.0), (610.0, 700.0)) Blue, Green, Red windows score_weights tuple[float, float, float] (1.0, 1.0, 1.0) Weights for (Fisher, AUC, MI) lambda_penalty float 0.5 Redundancy penalty"},{"location":"node-catalog/selectors/#see-also_6","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow - Complete example</li> <li>API Reference: ::: cuvis_ai.node.band_selection.SupervisedWindowedFalseRGBSelector</li> </ul>"},{"location":"node-catalog/selectors/#supervisedfullspectrumbandselector","title":"SupervisedFullSpectrumBandSelector","text":"<p>Description: Supervised selection without window constraints (top-k globally)</p> <p>Perfect for:</p> <ul> <li>Discovering discriminative bands anywhere in the spectrum</li> <li>Unconstrained band selection</li> <li>Exploring unexpected spectral regions for anomaly detection</li> </ul> <p>Training Paradigm: Statistical initialization (requires labels)</p>"},{"location":"node-catalog/selectors/#port-specifications_7","title":"Port Specifications","text":"<p>Same as SupervisedCIRBandSelector</p>"},{"location":"node-catalog/selectors/#parameters_7","title":"Parameters","text":"Parameter Type Default Description num_spectral_bands int Required Total number of spectral bands score_weights tuple[float, float, float] (1.0, 1.0, 1.0) Weights for (Fisher, AUC, MI) lambda_penalty float 0.5 Redundancy penalty"},{"location":"node-catalog/selectors/#example-usage-python_6","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.band_selection import SupervisedFullSpectrumBandSelector\n\n# Select top-3 bands globally (no window constraints)\nfull_spectrum = SupervisedFullSpectrumBandSelector(\n    num_spectral_bands=61,\n    score_weights=(1.0, 1.0, 1.0),\n    lambda_penalty=0.5,\n)\n\nfrom cuvis_ai_core.training import StatisticalTrainer\n\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes full_spectrum\n\n# May select bands anywhere: e.g., [7, 28, 52] (440nm, 650nm, 860nm)\nprint(full_spectrum.selected_indices)\n</code></pre>"},{"location":"node-catalog/selectors/#see-also_7","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow - Comparison of selectors</li> <li>API Reference: ::: cuvis_ai.node.band_selection.SupervisedFullSpectrumBandSelector</li> </ul>"},{"location":"node-catalog/selectors/#comparison-tables","title":"Comparison Tables","text":""},{"location":"node-catalog/selectors/#learnable-vs-fixed-selection","title":"Learnable vs Fixed Selection","text":"Selector Learnable Requires Labels Training Paradigm Best For SoftChannelSelector Yes No Two-phase End-to-end learning, interpretability BaselineFalseRGBSelector No No None Quick RGB conversion CIRFalseColorSelector No No None Vegetation/NIR analysis HighContrastBandSelector No No None High-contrast visualization SupervisedCIRBandSelector No Yes Statistical init CIR windows, supervised SupervisedWindowedFalseRGBSelector No Yes Statistical init RGB windows, supervised SupervisedFullSpectrumBandSelector No Yes Statistical init Unconstrained, supervised"},{"location":"node-catalog/selectors/#supervised-selector-comparison","title":"Supervised Selector Comparison","text":"Selector Windows Selection Constraint Best For SupervisedCIRBandSelector NIR + Red + Green 3 windows (CIR) Vegetation, NIR-focused tasks SupervisedWindowedFalseRGBSelector Blue + Green + Red 3 windows (RGB) Natural color, interpretable SupervisedFullSpectrumBandSelector None Top-k globally Discovering unexpected spectral regions"},{"location":"node-catalog/selectors/#choosing-the-right-selector","title":"Choosing the Right Selector","text":""},{"location":"node-catalog/selectors/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    A[Need Band Selection?] --&gt;|Yes| B{Labels Available?}\n    A --&gt;|No| Z[Use full spectrum]\n\n    B --&gt;|No| C{Learnable?}\n    B --&gt;|Yes| D{Window Constraint?}\n\n    C --&gt;|Yes| E[SoftChannelSelector]\n    C --&gt;|No| F{Use Case?}\n\n    F --&gt;|True Color| G[BaselineFalseRGBSelector]\n    F --&gt;|NIR/Vegetation| H[CIRFalseColorSelector]\n    F --&gt;|High Contrast| I[HighContrastBandSelector]\n\n    D --&gt;|CIR Windows| J[SupervisedCIRBandSelector]\n    D --&gt;|RGB Windows| K[SupervisedWindowedFalseRGBSelector]\n    D --&gt;|No Constraint| L[SupervisedFullSpectrumBandSelector]</code></pre>"},{"location":"node-catalog/selectors/#recommendation-by-use-case","title":"Recommendation by Use Case","text":"Use Case Recommended Selector Reason Two-phase learnable pipeline SoftChannelSelector End-to-end gradient optimization Quick RGB for CLIP/AdaCLIP BaselineFalseRGBSelector Fast, no training needed Vegetation anomaly detection CIRFalseColorSelector or SupervisedCIRBandSelector NIR emphasis Supervised with labels SupervisedWindowedFalseRGBSelector Optimal discriminative bands Exploration/discovery SupervisedFullSpectrumBandSelector Finds unexpected bands"},{"location":"node-catalog/selectors/#performance-considerations","title":"Performance Considerations","text":"<p>Computational Cost:</p> Selector Forward Pass Initialization Notes SoftChannelSelector O(B\u00d7H\u00d7W\u00d7C) O(N\u00d7H\u00d7W\u00d7C) N = initialization batches Fixed selectors (Baseline, CIR) O(C) lookup None Nearest wavelength search HighContrastBandSelector O(C\u00d7H\u00d7W) None Variance + Laplacian per band Supervised selectors O(C) indexing O(N\u00d7H\u00d7W\u00d7C) mRMR scoring across dataset <p>Optimization Tips:</p> <ol> <li>Use supervised selectors for small datasets: mRMR initialization is efficient</li> <li>Use SoftChannelSelector for large datasets: Gradient-based learning scales better</li> <li>Cache band_info: Fixed selectors compute same indices per sample (can cache)</li> <li>GPU-accelerate statistical init: Move data to GPU for faster initialization</li> </ol>"},{"location":"node-catalog/selectors/#creating-custom-selectors","title":"Creating Custom Selectors","text":"<pre><code>from cuvis_ai.node.band_selection import BandSelectorBase\n\nclass CustomBandSelector(BandSelectorBase):\n    def __init__(self, custom_param: float = 1.0, **kwargs):\n        super().__init__(custom_param=custom_param, **kwargs)\n        self.custom_param = custom_param\n\n    def forward(self, cube, wavelengths, **kwargs):\n        # Your custom selection logic\n        indices = your_selection_algorithm(cube, wavelengths)\n\n        # Compose RGB using base class helper\n        rgb = self._compose_rgb(cube, indices)\n\n        band_info = {\n            \"strategy\": \"custom\",\n            \"band_indices\": indices,\n            # ... additional metadata\n        }\n\n        return {\"rgb_image\": rgb, \"band_info\": band_info}\n</code></pre> <p>Learn more:</p> <ul> <li>Plugin System Development</li> <li>Node System Deep Dive</li> </ul> <p>Next Steps:</p> <ul> <li>Explore Tutorial 2: Channel Selector for SoftChannelSelector</li> <li>Review Tutorial 4: AdaCLIP Workflow for supervised band selectors</li> <li>Learn about Loss &amp; Metrics Nodes for selector regularization</li> <li>Understand Two-Phase Training for learnable selectors</li> </ul>"},{"location":"node-catalog/statistical/","title":"Statistical","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/statistical/#statistical-nodes","title":"Statistical Nodes","text":""},{"location":"node-catalog/statistical/#overview","title":"Overview","text":"<p>Statistical nodes perform anomaly detection using classical statistical methods without neural networks. These nodes estimate background statistics (mean, covariance) and compute anomaly scores based on statistical distance metrics.</p> <p>Key characteristics: - No gradient-based training required (statistical initialization only) - Fast inference with closed-form solutions - Interpretable outputs based on statistical theory - Can optionally be unfrozen for gradient-based fine-tuning (two-phase training)</p> <p>When to use: - Baseline anomaly detection without deep learning - Limited training data scenarios - Real-time inference requirements - Interpretable statistical methods preferred</p>"},{"location":"node-catalog/statistical/#nodes-in-this-category","title":"Nodes in This Category","text":""},{"location":"node-catalog/statistical/#rxglobal","title":"RXGlobal","text":"<p>Description: Global RX (Reed-Xiaoli) anomaly detector using Mahalanobis distance with global background statistics</p> <p>Perfect for: - Classical anomaly detection baseline - Hyperspectral anomaly detection without deep learning - Two-phase training (statistical init \u2192 gradient fine-tuning) - Real-time inference with precomputed covariance inverse</p> <p>Training Paradigm: Statistical initialization (optionally two-phase with <code>unfreeze()</code>)</p> <p>Background Theory:</p> <p>RX detection computes the Mahalanobis distance between each pixel and the background distribution:</p> \\[ \\text{score}(x) = (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\] <p>where: - \\(\\mu\\) is the global mean vector (estimated from training data) - \\(\\Sigma\\) is the global covariance matrix (estimated from training data) - Higher scores indicate pixels further from the background distribution (potential anomalies)</p> <p>Statistical Estimation:</p> <p>Uses Welford's online algorithm for numerically stable streaming statistics:</p> <pre><code># Parallel batched updates with Welford's algorithm\ndelta = mean_batch - mean_global\nmean_global = mean_global + delta * (n_batch / n_total)\nM2_global = M2_global + M2_batch + outer(delta, delta) * (n_global * n_batch / n_total)\ncov = M2 / (n - 1)\n</code></pre> <p>This allows processing large datasets that don't fit in memory.</p>"},{"location":"node-catalog/statistical/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Input hyperspectral cube BHWC No <p>Output Ports:</p> Port Type Shape Description scores float32 (B,H,W,1) Anomaly scores (Mahalanobis distances)"},{"location":"node-catalog/statistical/#parameters","title":"Parameters","text":"Parameter Type Default Description num_channels int required Number of spectral channels (C dimension) eps float 1e-6 Regularization for covariance matrix (added to diagonal) cache_inverse bool True Whether to cache covariance inverse for faster inference"},{"location":"node-catalog/statistical/#example-usage-python","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai_schemas.execution import InputStream\n\n# Create RX detector\nrx = RXGlobal(num_channels=61, eps=1e-6, cache_inverse=True)\n\n# Statistical initialization from initialization data\ndef initialization_stream() -&gt; InputStream:\n    for batch in initialization_loader:\n        yield {\"data\": batch[\"cube\"]}  # BHWC format\n\nrx.statistical_initialization(initialization_stream())\n\n# Use in pipeline\npipeline.add_nodes(rx=rx)\npipeline.connect(\n    (normalizer.output, rx.data),\n    (rx.scores, logit_head.scores)\n)\n\n# Optional: Enable gradient-based fine-tuning (two-phase training)\nrx.unfreeze()  # Convert buffers to nn.Parameters\n# Now rx.mu and rx.cov can be optimized with gradient descent\n</code></pre>"},{"location":"node-catalog/statistical/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  rx_detector:\n    type: RXGlobal\n    config:\n      num_channels: 61\n      eps: 1e-6\n      cache_inverse: true\n    # Statistical init handled by StatisticalTrainer\n\nconnections:\n  - [normalizer.output, rx_detector.data]\n  - [rx_detector.scores, logit_head.scores]\n</code></pre>"},{"location":"node-catalog/statistical/#two-phase-training-workflow","title":"Two-Phase Training Workflow","text":"<pre><code>graph LR\n    A[Phase 1: Statistical Init] --&gt; B[Estimate \u03bc, \u03a3]\n    B --&gt; C[Freeze as buffers]\n    C --&gt; D{Gradient training?}\n    D --&gt;|No| E[Inference with frozen stats]\n    D --&gt;|Yes| F[rx.unfreeze]\n    F --&gt; G[Convert to nn.Parameters]\n    G --&gt; H[Phase 2: Gradient training]\n    H --&gt; I[Fine-tune \u03bc, \u03a3 with backprop]</code></pre> <p>Phase 1 (Statistical): - <code>statistical_initialization()</code> estimates \u03bc and \u03a3 from data stream - Statistics stored as buffers (frozen, not trainable) - Typical use: Baseline RX detection</p> <p>Phase 2 (Optional Gradient): - Call <code>unfreeze()</code> to convert buffers to <code>nn.Parameters</code> - Enables gradient-based optimization of \u03bc and \u03a3 - Use case: Fine-tune statistics with end-to-end loss</p>"},{"location":"node-catalog/statistical/#performance-considerations","title":"Performance Considerations","text":"Mode Inference Speed Memory Accuracy <code>cache_inverse=True</code> Fast (einsum) O(C\u00b2) Standard <code>cache_inverse=False</code> Slower (solve) O(C\u00b2) Numerically stable <p>Recommendation: Use <code>cache_inverse=True</code> for production inference unless numerical stability issues arise.</p>"},{"location":"node-catalog/statistical/#common-issues","title":"Common Issues","text":"<p>1. \"RXGlobal not initialized\" error</p> <pre><code># Problem: Forgot statistical initialization\nrx = RXGlobal(num_channels=61)\npipeline.run(data)  # RuntimeError!\n\n# Solution: Always call statistical_initialization() before inference\nrx.statistical_initialization(initialization_stream())\n</code></pre> <p>2. Singular covariance matrix</p> <pre><code># Problem: Not enough samples or low-rank data\nrx = RXGlobal(num_channels=61, eps=1e-10)  # eps too small\n\n# Solution: Increase regularization\nrx = RXGlobal(num_channels=61, eps=1e-4)  # Larger eps stabilizes\n</code></pre> <p>3. High scores for normal pixels</p> <pre><code># Problem: Training data not representative of normal background\n# Solution: Ensure initialization data contains only normal pixels\ndef initialization_stream():\n    for batch in loader:\n        # Filter to normal pixels only\n        if batch[\"mask\"].sum() == 0:  # No anomalies\n            yield {\"data\": batch[\"cube\"]}\n</code></pre>"},{"location":"node-catalog/statistical/#workflow-integration","title":"Workflow Integration","text":"<p>Typical RX detection pipeline:</p> <pre><code>graph LR\n    A[Data Loader] --&gt; B[Bandpass Filter]\n    B --&gt; C[MinMax Normalizer]\n    C --&gt; D[RXGlobal]\n    D --&gt; E[ScoreToLogit]\n    E --&gt; F[Binary Decider]\n    F --&gt; G[Metrics]\n\n    style D fill:#e1f5ff</code></pre>"},{"location":"node-catalog/statistical/#see-also","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical Detection - Complete workflow</li> <li>ScoreToLogit - Convert scores to logits</li> <li>Two-Phase Training Concept</li> <li>BinaryDecider - Threshold RX scores</li> <li>API Reference: ::: cuvis_ai.anomaly.rx_detector.RXGlobal</li> </ul>"},{"location":"node-catalog/statistical/#rxperbatch","title":"RXPerBatch","text":"<p>Description: Per-batch RX detector computing statistics independently for each image (no training required)</p> <p>Perfect for: - Stateless anomaly detection - Each image has different background characteristics - No initialization data available - Quick prototyping without training</p> <p>Training Paradigm: None (no initialization required)</p> <p>Key Difference from RXGlobal:</p> Aspect RXGlobal RXPerBatch Statistics Global (from training) Per-image (computed on-the-fly) Requires training Yes (statistical init) No Assumes Consistent background Varying background per image Speed Faster (precomputed \u03a3\u207b\u00b9) Slower (computes \u03a3\u207b\u00b9 each forward) Memory Stores \u03bc, \u03a3 No stored statistics"},{"location":"node-catalog/statistical/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,C) Input hyperspectral cube BHWC No <p>Output Ports:</p> Port Type Shape Description scores float32 (B,H,W,1) Per-batch anomaly scores"},{"location":"node-catalog/statistical/#parameters_1","title":"Parameters","text":"Parameter Type Default Description eps float 1e-6 Regularization for covariance matrix"},{"location":"node-catalog/statistical/#example-usage-python_1","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.anomaly.rx_detector import RXPerBatch\n\n# Create per-batch RX detector (no initialization needed!)\nrx = RXPerBatch(eps=1e-6)\n\n# Use directly in pipeline (stateless)\npipeline.add_nodes(rx_per_batch=rx)\npipeline.connect(\n    (normalizer.output, rx_per_batch.data),\n    (rx_per_batch.scores, visualizer.scores)\n)\n\n# No statistical_initialization() required - works immediately\nresults = pipeline.run(test_data)\n</code></pre>"},{"location":"node-catalog/statistical/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  rx_per_batch:\n    type: RXPerBatch\n    config:\n      eps: 1e-6\n\nconnections:\n  - [preprocessor.output, rx_per_batch.data]\n  - [rx_per_batch.scores, heatmap.scores]\n</code></pre>"},{"location":"node-catalog/statistical/#when-to-use","title":"When to Use","text":"<p>Use RXPerBatch when: - Each image has a unique background distribution - No initialization/training data available - Need quick anomaly detection without setup - Inference speed is not critical</p> <p>Use RXGlobal when: - Background is consistent across images - Calibration data available - Need faster inference (precomputed inverse) - Better anomaly detection accuracy with learned statistics</p>"},{"location":"node-catalog/statistical/#common-issues_1","title":"Common Issues","text":"<p>1. Poor detection when background varies</p> <pre><code># Problem: Using RXGlobal with varying backgrounds\nrx_global = RXGlobal(num_channels=61)\nrx_global.statistical_initialization(mixed_background_data)\n# May perform poorly on images with different backgrounds\n\n# Solution: Use RXPerBatch for adaptive detection\nrx_per_batch = RXPerBatch(eps=1e-6)\n# Automatically adapts to each image's background\n</code></pre> <p>2. Anomalies affect per-batch statistics</p> <pre><code># Problem: Large anomalies skew the per-image mean/covariance\n# This is inherent to per-batch RX and reduces detection sensitivity\n\n# Solution: Use RXGlobal with clean training data\nfrom cuvis_ai_core.training import StatisticalTrainer\n\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Initializes rx_global with clean training data\n# Global statistics not affected by test-time anomalies\n</code></pre>"},{"location":"node-catalog/statistical/#see-also_1","title":"See Also","text":"<ul> <li>RXGlobal - Global statistics variant</li> <li>Tutorial 1: RX Statistical Detection</li> <li>API Reference: ::: cuvis_ai.anomaly.rx_detector.RXPerBatch</li> </ul>"},{"location":"node-catalog/statistical/#comparison-rxglobal-vs-rxperbatch","title":"Comparison: RXGlobal vs RXPerBatch","text":"Feature RXGlobal RXPerBatch Training Required Yes (statistical init) No Background Assumption Consistent across dataset Varies per image Inference Speed Fast (cached \u03a3\u207b\u00b9) Slower (compute per batch) Memory Footprint O(C\u00b2) stored O(C\u00b2) temporary Detection Quality Better with good initialization Adaptive but less sensitive Two-Phase Training Supported (unfreeze) Not applicable Use Case Production with training data Quick prototyping, no training"},{"location":"node-catalog/statistical/#statistical-anomaly-detection-workflow","title":"Statistical Anomaly Detection Workflow","text":"<p>Complete example pipeline with statistical initialization:</p> <pre><code># Phase 1: Statistical Initialization\ntrainer:\n  type: StatisticalTrainer\n  max_steps: 1000  # Process initialization data\n\nnodes:\n  data:\n    type: LentilsAnomalyDataNode\n    config:\n      normal_ids: [0]\n      anomaly_ids: [1, 2, 3]\n\n  normalizer:\n    type: MinMaxNormalizer\n    config:\n      feature_range: [0.0, 1.0]\n      dims: [1, 2]  # Normalize spatially\n\n  rx:\n    type: RXGlobal\n    config:\n      num_channels: 61\n      eps: 1e-6\n      cache_inverse: true\n\n  logit_head:\n    type: ScoreToLogit  # See Utility Nodes\n    config:\n      init_scale: 1.0\n      init_bias: 0.0\n\n  decider:\n    type: BinaryDecider\n    config:\n      threshold: 0.5\n\nconnections:\n  - [data.cube, normalizer.data]\n  - [normalizer.output, rx.data]\n  - [rx.scores, logit_head.scores]\n  - [logit_head.logits, decider.scores]\n</code></pre> <p>Training workflow:</p> <pre><code># 1. Build pipeline from config\npipeline = build_pipeline_from_config(config)\n\n# 2. Statistical initialization (Phase 1)\nstatistical_trainer = StatisticalTrainer(max_steps=1000)\nstatistical_trainer.fit(pipeline, train_datamodule)\n\n# 3. Inference\ntest_results = pipeline.run(test_data)\n</code></pre>"},{"location":"node-catalog/statistical/#additional-resources","title":"Additional Resources","text":"<ul> <li>Tutorial: RX Statistical Detection Tutorial</li> <li>Concepts: Two-Phase Training</li> <li>Concepts: Statistical Initialization</li> <li>Related Nodes: ScoreToLogit, BinaryDecider</li> <li>API Reference: cuvis_ai.anomaly.rx_detector</li> </ul>"},{"location":"node-catalog/utility/","title":"Utility Nodes","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/utility/#utility-nodes","title":"Utility Nodes","text":""},{"location":"node-catalog/utility/#overview","title":"Overview","text":"<p>Utility nodes provide essential supporting functions like decision thresholding, score transformation, and label mapping. These nodes:</p> <ul> <li>Enable flexible decision boundaries (fixed threshold vs adaptive quantile)</li> <li>Transform raw scores to logit space for loss functions</li> <li>Convert multi-class labels to binary anomaly masks</li> <li>Support two-stage decision strategies</li> </ul>"},{"location":"node-catalog/utility/#nodes-in-this-category","title":"Nodes in This Category","text":""},{"location":"node-catalog/utility/#binarydecider","title":"BinaryDecider","text":"<p>Description: Simple fixed-threshold decision node with sigmoid transformation</p> <p>Perfect for: - Basic binary classification - Fixed threshold experiments - Production inference with known threshold</p> <p>Training Paradigm: None (stateless transform)</p> <p>Algorithm:</p> <ol> <li>Apply sigmoid: \\(p = \\sigma(\\text{logits})\\)</li> <li>Threshold: \\(\\text{decision} = (p \\geq \\text{threshold})\\)</li> </ol>"},{"location":"node-catalog/utility/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional logits float32 (B,H,W,C) Input logits No <p>Output Ports:</p> Port Type Shape Description decisions bool (B,H,W,1) Binary decision mask"},{"location":"node-catalog/utility/#parameters","title":"Parameters","text":"Parameter Type Default Description threshold float 0.5 Decision threshold (after sigmoid)"},{"location":"node-catalog/utility/#example-usage-python","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.deciders.binary_decider import BinaryDecider\n\n# Create decider\ndecider = BinaryDecider(threshold=0.5)\n\n# Use in pipeline\npipeline.add_nodes(decider=decider)\npipeline.connect(\n    (logit_head.logits, decider.logits),\n    (decider.decisions, metrics.decisions)\n)\n</code></pre>"},{"location":"node-catalog/utility/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  decider:\n    type: BinaryDecider\n    config:\n      threshold: 0.5  # Adjust based on precision/recall trade-off\n\nconnections:\n  - [logit_head.logits, decider.logits]\n  - [decider.decisions, metrics.decisions]\n</code></pre>"},{"location":"node-catalog/utility/#threshold-selection-guide","title":"Threshold Selection Guide","text":"Threshold Behavior Use Case 0.3 High recall, low precision Minimize false negatives 0.5 Balanced (default) General use 0.7 Low recall, high precision Minimize false positives <p>Recommendation: Tune threshold on validation set to optimize target metric (F1, IoU, etc.).</p>"},{"location":"node-catalog/utility/#see-also","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical</li> <li>QuantileBinaryDecider - Adaptive threshold</li> <li>API Reference: ::: cuvis_ai.deciders.binary_decider.BinaryDecider</li> </ul>"},{"location":"node-catalog/utility/#quantilebinarydecider","title":"QuantileBinaryDecider","text":"<p>Description: Adaptive quantile-based thresholding computed per batch</p> <p>Perfect for: - Adaptive anomaly detection - Deep SVDD with variable score distributions - Unknown optimal threshold scenarios</p> <p>Training Paradigm: None (stateless transform)</p> <p>Algorithm:</p> <ol> <li>Compute quantile threshold per batch: \\(t = \\text{quantile}(\\text{logits}, q)\\)</li> <li>Threshold: \\(\\text{decision} = (\\text{logits} \\geq t)\\)</li> </ol> <p>Default: Reduce over (H, W, C) dimensions for (B, H, W, C) input.</p>"},{"location":"node-catalog/utility/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional logits float32 (B,H,W,C) Input logits/scores No <p>Output Ports:</p> Port Type Shape Description decisions bool (B,H,W,1) Binary decision mask"},{"location":"node-catalog/utility/#parameters_1","title":"Parameters","text":"Parameter Type Default Description quantile float 0.995 Quantile threshold in [0,1] reduce_dims Sequence[int] None Dims to reduce (None = all non-batch)"},{"location":"node-catalog/utility/#example-usage-python_1","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.deciders.binary_decider import QuantileBinaryDecider\n\n# Create quantile decider\ndecider = QuantileBinaryDecider(\n    quantile=0.995,  # Top 0.5% marked as anomalies\n    reduce_dims=None  # Default: reduce over (H, W, C)\n)\n\n# Use in pipeline\npipeline.add_nodes(decider=decider)\npipeline.connect(\n    (scorer.scores, decider.logits),\n    (decider.decisions, metrics.decisions)\n)\n</code></pre>"},{"location":"node-catalog/utility/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  decider:\n    type: QuantileBinaryDecider\n    config:\n      quantile: 0.995  # Top 0.5% pixels\n      reduce_dims: null  # Reduce over all non-batch dims\n\nconnections:\n  - [deep_svdd_scores.scores, decider.logits]\n  - [decider.decisions, metrics.decisions]\n</code></pre>"},{"location":"node-catalog/utility/#quantile-selection-guide","title":"Quantile Selection Guide","text":"Quantile % Anomalies Use Case 0.99 ~1% High anomaly ratio 0.995 ~0.5% (recommended) Balanced 0.999 ~0.1% Low anomaly ratio <p>Recommendation: Match quantile to expected anomaly frequency in your data.</p>"},{"location":"node-catalog/utility/#see-also_1","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>DeepSVDDScores</li> <li>API Reference: ::: cuvis_ai.deciders.binary_decider.QuantileBinaryDecider</li> </ul>"},{"location":"node-catalog/utility/#twostagebinarydecider","title":"TwoStageBinaryDecider","text":"<p>Description: Two-stage decision: image-level gate \u2192 pixel-level quantile threshold</p> <p>Perfect for: - Reducing false positives on normal images - Hierarchical anomaly detection - High-precision applications</p> <p>Training Paradigm: None (stateless transform)</p> <p>Algorithm:</p> <p>Stage 1 (Image-level gate): 1. Compute image score: Mean of top-k% pixels 2. If image score &lt; threshold \u2192 Return blank mask (no anomalies)</p> <p>Stage 2 (Pixel-level): 3. If gate passed \u2192 Apply quantile thresholding per image</p> <p>This prevents false positives on normal images while allowing fine-grained detection on anomalous images.</p>"},{"location":"node-catalog/utility/#port-specifications_2","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional logits float32 (B,H,W,C) Input logits/scores No <p>Output Ports:</p> Port Type Shape Description decisions bool (B,H,W,1) Binary decision mask"},{"location":"node-catalog/utility/#parameters_2","title":"Parameters","text":"Parameter Type Default Description image_threshold float 0.5 Stage 1 gate threshold top_k_fraction float 0.001 Fraction of pixels for image score quantile float 0.995 Stage 2 quantile threshold reduce_dims Sequence[int] None Dims to reduce (Stage 2)"},{"location":"node-catalog/utility/#example-usage-python_2","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.deciders.two_stage_decider import TwoStageBinaryDecider\n\n# Create two-stage decider\ndecider = TwoStageBinaryDecider(\n    image_threshold=0.5,  # Gate: requires high max score\n    top_k_fraction=0.001,  # Use top 0.1% for image score\n    quantile=0.995  # Pixel threshold if gate passes\n)\n\n# Use in pipeline\npipeline.add_nodes(decider=decider)\npipeline.connect(\n    (scorer.scores, decider.logits),\n    (decider.decisions, metrics.decisions)\n)\n</code></pre>"},{"location":"node-catalog/utility/#example-configuration-yaml_2","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  two_stage_decider:\n    type: TwoStageBinaryDecider\n    config:\n      image_threshold: 0.5\n      top_k_fraction: 0.001\n      quantile: 0.995\n\nconnections:\n  - [scorer.scores, two_stage_decider.logits]\n  - [two_stage_decider.decisions, metrics.decisions]\n</code></pre>"},{"location":"node-catalog/utility/#when-to-use","title":"When to Use","text":"<p>Use TwoStageBinaryDecider when: - Need high precision (minimize false positives) - Most images are normal (sparse anomalies) - Can tolerate missing subtle anomalies</p> <p>Use QuantileBinaryDecider when: - Need balanced precision/recall - Anomalies present in most images - Want simpler single-stage thresholding</p>"},{"location":"node-catalog/utility/#see-also_2","title":"See Also","text":"<ul> <li>QuantileBinaryDecider</li> <li>API Reference: ::: cuvis_ai.deciders.two_stage_decider.TwoStageBinaryDecider</li> </ul>"},{"location":"node-catalog/utility/#scoretologit","title":"ScoreToLogit","text":"<p>Description: Trainable affine transformation converting RX scores to logits</p> <p>Perfect for: - Two-phase RX training (statistical init \u2192 gradient fine-tuning) - Calibrating RX scores for BCE loss - Learning optimal score scaling</p> <p>Training Paradigm: Two-phase (statistical init \u2192 unfreeze \u2192 gradient training)</p> <p>Algorithm:</p> \\[ \\text{logit} = \\text{scale} \\cdot (\\text{score} - \\text{bias}) \\] <p>Statistical initialization: - <code>bias = mean(scores) + 2*std(scores)</code> (anomaly threshold estimate) - <code>scale = 1.0</code> (initial scaling)</p> <p>Gradient training: Unfreeze to optimize <code>scale</code> and <code>bias</code> via backprop.</p>"},{"location":"node-catalog/utility/#port-specifications_3","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional scores float32 (B,H,W,K) RX anomaly scores No <p>Output Ports:</p> Port Type Shape Description logits float32 (B,H,W,K) Calibrated logits"},{"location":"node-catalog/utility/#parameters_3","title":"Parameters","text":"Parameter Type Default Description init_scale float 1.0 Initial value for the scale parameter init_bias float 0.0 Initial value for the bias parameter (threshold)"},{"location":"node-catalog/utility/#example-usage-python_3","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.conversion import ScoreToLogit\n\n# Create logit head\nfrom cuvis_ai_core.training import StatisticalTrainer\n\nlogit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\npipeline.add_node(logit_head)\n\n# Phase 1: Statistical initialization\ntrainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\ntrainer.fit()  # Automatically initializes logit_head\n\n# Phase 2 (optional): Enable gradient training\nlogit_head.unfreeze()  # Convert buffers to nn.Parameters\n\n# Use in pipeline\npipeline.connect(\n    (rx.scores, logit_head.scores),\n    (logit_head.logits, bce_loss.predictions)\n)\n</code></pre>"},{"location":"node-catalog/utility/#example-configuration-yaml_3","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  rx_detector:\n    type: RXGlobal\n    config:\n      num_channels: 61\n\n  logit_head:\n    type: ScoreToLogit\n    config:\n      init_scale: 1.0\n      init_bias: 0.0\n\n  bce_loss:\n    type: AnomalyBCEWithLogits\n\nconnections:\n  - [rx_detector.scores, logit_head.scores]\n  - [logit_head.logits, bce_loss.predictions]\n</code></pre>"},{"location":"node-catalog/utility/#see-also_3","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical</li> <li>ScoreToLogit</li> <li>Two-Phase Training</li> <li>API Reference: ::: cuvis_ai.node.conversion.ScoreToLogit</li> </ul>"},{"location":"node-catalog/utility/#binaryanomalylabelmapper","title":"BinaryAnomalyLabelMapper","text":"<p>Description: Converts multi-class segmentation masks to binary anomaly masks</p> <p>Perfect for: - Preprocessing multi-class datasets - Mapping specific classes to \"anomaly\" category - Flexible normal/anomaly definition</p> <p>Training Paradigm: None (stateless mapping)</p> <p>Mapping Modes:</p> <ol> <li> <p>Explicit anomaly IDs (recommended):    <pre><code>mapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0],      # Class 0 = normal\n    anomaly_ids=[1,2,3]  # Classes 1,2,3 = anomaly\n)\n</code></pre></p> </li> <li> <p>Implicit (all non-normal are anomalies):    <pre><code>mapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0],\n    anomaly_ids=None  # Everything except 0 is anomaly\n)\n</code></pre></p> </li> </ol>"},{"location":"node-catalog/utility/#port-specifications_4","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional cube float32 (B,H,W,C) Input cube (passed through) No mask any (B,H,W,1) Multi-class mask No <p>Output Ports:</p> Port Type Shape Description cube float32 (B,H,W,C) Cube (unchanged) mask bool (B,H,W,1) Binary anomaly mask"},{"location":"node-catalog/utility/#parameters_4","title":"Parameters","text":"Parameter Type Default Description normal_ids list[int] [] Class IDs for normal pixels anomaly_ids list[int] None Class IDs for anomalies (None = implicit)"},{"location":"node-catalog/utility/#example-usage-python_4","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.labels import BinaryAnomalyLabelMapper\n\n# Create mapper\nmapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0],        # Background class\n    anomaly_ids=[1, 2, 3]  # Defect classes\n)\n\n# Used internally by LentilsAnomalyDataNode\n</code></pre>"},{"location":"node-catalog/utility/#example-configuration-yaml_4","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  data:\n    type: LentilsAnomalyDataNode\n    config:\n      normal_ids: [0]      # Class 0 = normal\n      anomaly_ids: [1, 2, 3]  # Classes 1,2,3 = anomaly\n</code></pre>"},{"location":"node-catalog/utility/#common-issues","title":"Common Issues","text":"<p>1. Overlapping normal and anomaly IDs</p> <pre><code># Problem: Same class in both lists\nmapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0],\n    anomaly_ids=[0, 1, 2]  # 0 appears in both!\n)\n# Raises ValueError\n\n# Solution: Ensure disjoint sets\nmapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0],\n    anomaly_ids=[1, 2]  # No overlap\n)\n</code></pre> <p>2. Unassigned classes</p> <pre><code># Data has classes: 0, 1, 2, 3, 4\nmapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0],\n    anomaly_ids=[1, 2]  # Classes 3, 4 unassigned!\n)\n# Warning logged, classes 3, 4 treated as anomalies\n\n# Solution: Explicitly assign all classes\nmapper = BinaryAnomalyLabelMapper(\n    normal_ids=[0, 3, 4],  # All non-anomalies\n    anomaly_ids=[1, 2]     # Actual anomalies\n)\n</code></pre>"},{"location":"node-catalog/utility/#see-also_4","title":"See Also","text":"<ul> <li>LentilsAnomalyDataNode</li> <li>API Reference: ::: cuvis_ai.node.labels.BinaryAnomalyLabelMapper</li> </ul>"},{"location":"node-catalog/utility/#decision-strategy-comparison","title":"Decision Strategy Comparison","text":"Decider Threshold Adaptivity Use Case BinaryDecider Fixed None Known threshold, production QuantileBinaryDecider Adaptive (per-batch) Medium Variable distributions TwoStageBinaryDecider Adaptive (hierarchical) High High-precision applications"},{"location":"node-catalog/utility/#complete-pipeline-example","title":"Complete Pipeline Example","text":"<p>Typical anomaly detection workflow with utilities:</p> <pre><code>nodes:\n  # Data\n  data:\n    type: LentilsAnomalyDataNode\n    config:\n      normal_ids: [0]\n      anomaly_ids: [1, 2, 3]\n\n  # Preprocessing\n  normalizer:\n    type: MinMaxNormalizer\n\n  # Detection\n  rx_detector:\n    type: RXGlobal\n    config:\n      num_channels: 61\n\n  # Utilities\n  logit_head:\n    type: ScoreToLogit\n    config:\n      init_scale: 1.0\n      init_bias: 0.0\n\n  decider:\n    type: QuantileBinaryDecider  # or BinaryDecider, TwoStageBinaryDecider\n    config:\n      quantile: 0.995\n\n  # Evaluation\n  metrics:\n    type: AnomalyDetectionMetrics\n\nconnections:\n  # Data flow\n  - [data.cube, normalizer.data]\n  - [normalizer.output, rx_detector.data]\n\n  # Score \u2192 Logit \u2192 Decision\n  - [rx_detector.scores, logit_head.scores]\n  - [logit_head.logits, decider.logits]\n\n  # Evaluation\n  - [decider.decisions, metrics.decisions]\n  - [data.mask, metrics.targets]  # Binary mask from label mapper\n</code></pre>"},{"location":"node-catalog/utility/#additional-resources","title":"Additional Resources","text":"<ul> <li>Tutorial: RX Statistical Detection - BinaryDecider + ScoreToLogit</li> <li>Tutorial: Deep SVDD Gradient - QuantileBinaryDecider</li> <li>Concepts: Two-Phase Training</li> <li>API Reference: cuvis_ai.deciders</li> <li>API Reference: cuvis_ai.node.conversion</li> <li>API Reference: cuvis_ai.node.labels</li> </ul>"},{"location":"node-catalog/visualization/","title":"Visualization","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"node-catalog/visualization/#visualization-nodes","title":"Visualization Nodes","text":""},{"location":"node-catalog/visualization/#overview","title":"Overview","text":"<p>Visualization nodes create image artifacts for monitoring training progress and model behavior. These are sink nodes that:</p> <ul> <li>Execute during validation, test, and/or inference stages</li> <li>Generate matplotlib figures as numpy arrays (Artifact objects)</li> <li>Feed into TensorBoardMonitorNode for logging</li> <li>No gradient computation (visualization only)</li> </ul> <p>Typical workflow: <pre><code>Model \u2192 Decisions/Scores \u2192 Visualization Node \u2192 TensorBoardMonitorNode\n</code></pre></p>"},{"location":"node-catalog/visualization/#nodes-in-this-category","title":"Nodes in This Category","text":""},{"location":"node-catalog/visualization/#anomalymask","title":"AnomalyMask","text":"<p>Description: Visualize anomaly detection with ground truth comparison and overlay</p> <p>Perfect for: - Evaluating anomaly detection quality - Comparing predictions vs ground truth - Color-coded error analysis (TP/FP/FN)</p> <p>Visualization Components: 1. Ground Truth Mask (if available) 2. Overlay on Cube Image:    - Green: True Positives (correct detections)    - Red: False Positives (false alarms)    - Yellow: False Negatives (missed anomalies) 3. Predicted Mask with metrics</p>"},{"location":"node-catalog/visualization/#port-specifications","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional decisions bool (B,H,W,1) Binary anomaly decisions No mask bool (B,H,W,1) Ground truth mask Yes cube float32 (B,H,W,C) Original cube for background No scores float32 (B,H,W,1) Optional scores for AP Yes <p>Output Ports:</p> Port Type Shape Description artifacts list () List of Artifact objects"},{"location":"node-catalog/visualization/#parameters","title":"Parameters","text":"Parameter Type Default Description channel int required Cube channel index for visualization up_to int None Max images to visualize (None = all)"},{"location":"node-catalog/visualization/#example-usage-python","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.visualizations import AnomalyMask\n\n# Create visualizer\nviz = AnomalyMask(channel=30, up_to=5)\n\n# Use in pipeline\npipeline.add_nodes(\n    viz=viz,\n    tb_monitor=TensorBoardMonitorNode(output_dir=\"./runs\")\n)\npipeline.connect(\n    (decider.decisions, viz.decisions),\n    (data.mask, viz.mask),\n    (data.cube, viz.cube),\n    (logit_head.logits, viz.scores),  # Optional for AP\n    (viz.artifacts, tb_monitor.artifacts)\n)\n</code></pre>"},{"location":"node-catalog/visualization/#example-configuration-yaml","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  viz_mask:\n    type: AnomalyMask\n    config:\n      channel: 30  # Wavelength channel for background\n      up_to: 5\n\n  tensorboard:\n    type: TensorBoardMonitorNode\n    config:\n      output_dir: \"./runs\"\n\nconnections:\n  - [decider.decisions, viz_mask.decisions]\n  - [data.mask, viz_mask.mask]\n  - [data.cube, viz_mask.cube]\n  - [viz_mask.artifacts, tensorboard.artifacts]\n</code></pre>"},{"location":"node-catalog/visualization/#see-also","title":"See Also","text":"<ul> <li>Tutorial 1: RX Statistical</li> <li>TensorBoardMonitorNode</li> <li>API Reference: ::: cuvis_ai.node.visualizations.AnomalyMask</li> </ul>"},{"location":"node-catalog/visualization/#scoreheatmapvisualizer","title":"ScoreHeatmapVisualizer","text":"<p>Description: Creates heatmaps of anomaly scores with colormap</p> <p>Perfect for: - Visualizing spatial score distributions - Monitoring score evolution during training - Debugging score ranges</p>"},{"location":"node-catalog/visualization/#port-specifications_1","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional scores float32 (B,H,W,1) Anomaly scores No <p>Output Ports:</p> Port Type Shape Description artifacts list () List of heatmap artifacts"},{"location":"node-catalog/visualization/#parameters_1","title":"Parameters","text":"Parameter Type Default Description normalize_scores bool True Normalize to [0,1] per image cmap str \"inferno\" Matplotlib colormap up_to int 5 Max heatmaps to generate"},{"location":"node-catalog/visualization/#example-usage-python_1","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.visualizations import ScoreHeatmapVisualizer\n\n# Create heatmap visualizer\nheatmap_viz = ScoreHeatmapVisualizer(\n    normalize_scores=True,\n    cmap=\"inferno\",\n    up_to=10\n)\n\n# Use in pipeline\npipeline.add_nodes(heatmap_viz=heatmap_viz)\npipeline.connect(\n    (scorer.scores, heatmap_viz.scores),\n    (heatmap_viz.artifacts, tb_monitor.artifacts)\n)\n</code></pre>"},{"location":"node-catalog/visualization/#see-also_1","title":"See Also","text":"<ul> <li>Tutorial 3: Deep SVDD Gradient</li> <li>API Reference: ::: cuvis_ai.node.visualizations.ScoreHeatmapVisualizer</li> </ul>"},{"location":"node-catalog/visualization/#cubergbvisualizer","title":"CubeRGBVisualizer","text":"<p>Description: Creates false-color RGB images from hyperspectral cube using channel weights</p> <p>Perfect for: - Visualizing channel selection results - Displaying selected wavelengths for RGB channels - Monitoring selector weight evolution</p> <p>Visualization Components: 1. False-Color RGB: Top 3 weighted channels as R, G, B 2. Weight Plot: Bar chart showing all channel weights with top 3 highlighted</p>"},{"location":"node-catalog/visualization/#port-specifications_2","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional cube float32 (B,H,W,C) Hyperspectral cube No weights float32 (C,) Channel selection weights No wavelengths int32 (C,) Wavelengths for each channel No <p>Output Ports:</p> Port Type Shape Description artifacts list () List of false-color RGB artifacts"},{"location":"node-catalog/visualization/#parameters_2","title":"Parameters","text":"Parameter Type Default Description up_to int 5 Max images to visualize"},{"location":"node-catalog/visualization/#example-usage-python_2","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.visualizations import CubeRGBVisualizer\n\n# Create false-color visualizer\nrgb_viz = CubeRGBVisualizer(up_to=5)\n\n# Use in pipeline\npipeline.add_nodes(rgb_viz=rgb_viz)\npipeline.connect(\n    (data.cube, rgb_viz.cube),\n    (selector.weights, rgb_viz.weights),\n    (data.wavelengths, rgb_viz.wavelengths),\n    (rgb_viz.artifacts, tb_monitor.artifacts)\n)\n</code></pre>"},{"location":"node-catalog/visualization/#see-also_2","title":"See Also","text":"<ul> <li>Tutorial 2: Channel Selector</li> <li>SoftChannelSelector</li> <li>API Reference: ::: cuvis_ai.node.visualizations.CubeRGBVisualizer</li> </ul>"},{"location":"node-catalog/visualization/#pcavisualization","title":"PCAVisualization","text":"<p>Description: Visualizes PCA-projected data with scatter plots and spatial color coding</p> <p>Perfect for: - Monitoring PCA projection quality - Visualizing first 2 principal components - Understanding spatial patterns in PC space</p> <p>Visualization Components: 1. Scatter Plot: H\u00d7W points in 2D PC space (PC1 vs PC2), colored by spatial position 2. Spatial Reference: HSV color coding (Hue from x-coord, Saturation from y-coord) 3. Image Representation: PC1 in red, PC2 in green channels</p>"},{"location":"node-catalog/visualization/#port-specifications_3","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional data float32 (B,H,W,K) PCA-projected data (uses first 2 PCs) No <p>Output Ports:</p> Port Type Shape Description artifacts list () List of PCA visualization artifacts"},{"location":"node-catalog/visualization/#parameters_3","title":"Parameters","text":"Parameter Type Default Description up_to int None Max images to visualize (None = all)"},{"location":"node-catalog/visualization/#example-usage-python_3","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.visualizations import PCAVisualization\n\n# Create PCA visualizer\npca_viz = PCAVisualization(up_to=10)\n\n# Use in pipeline\npipeline.add_nodes(pca_viz=pca_viz)\npipeline.connect(\n    (pca.projected, pca_viz.data),\n    (pca_viz.artifacts, tb_monitor.artifacts)\n)\n</code></pre>"},{"location":"node-catalog/visualization/#see-also_3","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>TrainablePCA</li> <li>API Reference: ::: cuvis_ai.node.visualizations.PCAVisualization</li> </ul>"},{"location":"node-catalog/visualization/#rgbanomalymask","title":"RGBAnomalyMask","text":"<p>Description: Like AnomalyMask but for RGB images (from band selectors)</p> <p>Perfect for: - AdaCLIP workflows with RGB-like inputs - Band selector output visualization - DRCNN mixer evaluation</p> <p>Key Difference: - Expects <code>rgb_image</code> input (3 channels) instead of <code>cube</code> (many channels) - No channel parameter (uses full RGB for background)</p>"},{"location":"node-catalog/visualization/#port-specifications_4","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional decisions bool (B,H,W,1) Binary anomaly decisions No mask bool (B,H,W,1) Ground truth mask Yes rgb_image float32 (B,H,W,3) RGB image for background No scores float32 (B,H,W,1) Optional scores for AP Yes <p>Output Ports:</p> Port Type Shape Description artifacts list () List of RGB anomaly mask artifacts"},{"location":"node-catalog/visualization/#parameters_4","title":"Parameters","text":"Parameter Type Default Description up_to int None Max images to visualize (None = all)"},{"location":"node-catalog/visualization/#example-usage-python_4","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.visualizations import RGBAnomalyMask\n\n# Create RGB visualizer\nrgb_viz = RGBAnomalyMask(up_to=5)\n\n# Use in pipeline\npipeline.add_nodes(rgb_viz=rgb_viz)\npipeline.connect(\n    (decider.decisions, rgb_viz.decisions),\n    (data.mask, rgb_viz.mask),\n    (mixer.rgb, rgb_viz.rgb_image),  # From DRCNN mixer\n    (rgb_viz.artifacts, tb_monitor.artifacts)\n)\n</code></pre>"},{"location":"node-catalog/visualization/#see-also_4","title":"See Also","text":"<ul> <li>Tutorial 4: AdaCLIP Workflow</li> <li>LearnableChannelMixer</li> <li>API Reference: ::: cuvis_ai.node.visualizations.RGBAnomalyMask</li> </ul>"},{"location":"node-catalog/visualization/#tensorboardmonitornode","title":"TensorBoardMonitorNode","text":"<p>Description: Sink node logging artifacts and metrics to TensorBoard</p> <p>Perfect for: - Centralized monitoring of all visualizations - Real-time training progress tracking - Comparing runs and experiments</p> <p>Key Characteristics: - Executes during all stages (ALWAYS) - Accepts multiple artifact/metric inputs (variadic ports) - Auto-increments run directories (<code>run_01</code>, <code>run_02</code>, ...) - Sink node (no outputs)</p>"},{"location":"node-catalog/visualization/#port-specifications_5","title":"Port Specifications","text":"<p>Input Ports:</p> Port Type Shape Description Optional artifacts list () List of Artifact objects Yes metrics list () List of Metric objects Yes <p>Output Ports: None (sink node)</p>"},{"location":"node-catalog/visualization/#parameters_5","title":"Parameters","text":"Parameter Type Default Description output_dir str \"./runs\" Directory for TensorBoard logs run_name str None Run name (auto-increment if None) comment str \"\" Comment appended to log dir flush_secs int 120 Flush interval for disk writes"},{"location":"node-catalog/visualization/#example-usage-python_5","title":"Example Usage (Python)","text":"<pre><code>from cuvis_ai.node.monitor import TensorBoardMonitorNode\n\n# Create TensorBoard monitor\ntb_monitor = TensorBoardMonitorNode(\n    output_dir=\"./runs\",\n    run_name=\"rx_experiment_1\",  # Or None for auto-increment\n    flush_secs=60\n)\n\n# Use in pipeline (sink node)\npipeline.add_nodes(tb_monitor=tb_monitor)\npipeline.connect(\n    (viz_mask.artifacts, tb_monitor.artifacts),\n    (heatmap_viz.artifacts, tb_monitor.artifacts),\n    (metrics_node.metrics, tb_monitor.metrics)\n)\n\n# View logs\n# Run in terminal: tensorboard --logdir=./runs\n</code></pre>"},{"location":"node-catalog/visualization/#example-configuration-yaml_1","title":"Example Configuration (YAML)","text":"<pre><code>nodes:\n  tensorboard:\n    type: TensorBoardMonitorNode\n    config:\n      output_dir: \"./runs\"\n      run_name: null  # Auto-increment run_01, run_02, ...\n      flush_secs: 120\n\nconnections:\n  - [viz_mask.artifacts, tensorboard.artifacts]\n  - [score_viz.artifacts, tensorboard.artifacts]\n  - [metrics.metrics, tensorboard.metrics]\n</code></pre>"},{"location":"node-catalog/visualization/#viewing-logs","title":"Viewing Logs","text":"<pre><code># Start TensorBoard server\ntensorboard --logdir=./runs\n\n# Open in browser: http://localhost:6006\n</code></pre>"},{"location":"node-catalog/visualization/#log-directory-structure","title":"Log Directory Structure","text":"<pre><code>runs/\n\u251c\u2500\u2500 run_01/\n\u2502   \u2514\u2500\u2500 events.out.tfevents...\n\u251c\u2500\u2500 run_02/\n\u2502   \u2514\u2500\u2500 events.out.tfevents...\n\u2514\u2500\u2500 my_experiment_v1/\n    \u2514\u2500\u2500 events.out.tfevents...\n</code></pre>"},{"location":"node-catalog/visualization/#see-also_5","title":"See Also","text":"<ul> <li>All tutorials (TensorBoard monitoring used throughout)</li> <li>AnomalyDetectionMetrics</li> <li>API Reference: ::: cuvis_ai.node.monitor.TensorBoardMonitorNode</li> </ul>"},{"location":"node-catalog/visualization/#visualization-workflow-example","title":"Visualization Workflow Example","text":"<p>Complete monitoring setup:</p> <pre><code>nodes:\n  # Data &amp; Model\n  data:\n    type: LentilsAnomalyDataNode\n\n  rx_detector:\n    type: RXGlobal\n    config:\n      num_channels: 61\n\n  decider:\n    type: BinaryDecider\n    config:\n      threshold: 0.5\n\n  # Visualization Nodes\n  score_heatmap:\n    type: ScoreHeatmapVisualizer\n    config:\n      cmap: \"inferno\"\n      up_to: 5\n\n  anomaly_mask_viz:\n    type: AnomalyMask\n    config:\n      channel: 30\n      up_to: 5\n\n  # Metrics\n  metrics:\n    type: AnomalyDetectionMetrics\n\n  # TensorBoard Monitor (sink)\n  tensorboard:\n    type: TensorBoardMonitorNode\n    config:\n      output_dir: \"./runs\"\n\nconnections:\n  # Model flow\n  - [data.cube, rx_detector.data]\n  - [rx_detector.scores, decider.scores]\n\n  # Visualizations\n  - [rx_detector.scores, score_heatmap.scores]\n  - [decider.decisions, anomaly_mask_viz.decisions]\n  - [data.mask, anomaly_mask_viz.mask]\n  - [data.cube, anomaly_mask_viz.cube]\n\n  # Metrics\n  - [decider.decisions, metrics.decisions]\n  - [data.mask, metrics.targets]\n\n  # Logging to TensorBoard\n  - [score_heatmap.artifacts, tensorboard.artifacts]\n  - [anomaly_mask_viz.artifacts, tensorboard.artifacts]\n  - [metrics.metrics, tensorboard.metrics]\n</code></pre>"},{"location":"node-catalog/visualization/#best-practices","title":"Best Practices","text":""},{"location":"node-catalog/visualization/#visualization-performance","title":"Visualization Performance","text":"<ul> <li> <p>Limit visualization count with <code>up_to</code> parameter to avoid slowdown   <pre><code>viz = AnomalyMask(channel=30, up_to=5)  # Only first 5 images\n</code></pre></p> </li> <li> <p>Execute during val/test only (default) to avoid training overhead   <pre><code># Automatically set: execution_stages={ExecutionStage.VAL, ExecutionStage.TEST}\n</code></pre></p> </li> </ul>"},{"location":"node-catalog/visualization/#tensorboard-organization","title":"TensorBoard Organization","text":"<ul> <li> <p>Use descriptive run names for easy comparison   <pre><code>tb = TensorBoardMonitorNode(\n    output_dir=\"./runs\",\n    run_name=\"rx_ep50_lr0.001\"  # Clear experiment name\n)\n</code></pre></p> </li> <li> <p>Group related experiments in subdirectories   <pre><code>runs/\n\u251c\u2500\u2500 baseline/\n\u2502   \u251c\u2500\u2500 run_01/\n\u2502   \u2514\u2500\u2500 run_02/\n\u2514\u2500\u2500 with_selector/\n    \u251c\u2500\u2500 run_01/\n    \u2514\u2500\u2500 run_02/\n</code></pre></p> </li> </ul>"},{"location":"node-catalog/visualization/#artifact-management","title":"Artifact Management","text":"<ul> <li>Close figures to free memory (done automatically by nodes)</li> <li>Flush frequently for real-time monitoring: <code>flush_secs=30</code></li> <li>Monitor disk usage - TensorBoard logs can grow large</li> </ul>"},{"location":"node-catalog/visualization/#additional-resources","title":"Additional Resources","text":"<ul> <li>Tutorial: RX Statistical Detection</li> <li>Tutorial: Channel Selector</li> <li>Tutorial: Deep SVDD Gradient</li> <li>Concepts: Execution Stages</li> <li>API Reference: cuvis_ai.node.visualizations</li> <li>API Reference: cuvis_ai.node.monitor</li> </ul>"},{"location":"plugin-system/","title":"Overview","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"plugin-system/#plugin-system","title":"Plugin System","text":"<p>The cuvis-ai plugin system enables extending the framework with custom nodes and functionality without modifying the core codebase. Distribute your algorithms via Git, share with the community, and maintain independent versioning.</p>"},{"location":"plugin-system/#quick-start","title":"Quick Start","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Load plugin from Git\nregistry = NodeRegistry()\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Use plugin node\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\ndetector = AdaCLIPDetector(prompt=\"plastic wrapper\", threshold=0.5)\n</code></pre>"},{"location":"plugin-system/#documentation-guides","title":"Documentation Guides","text":"<ul> <li> <p> Plugin System Overview</p> <p>Comprehensive guide to plugin architecture, loading mechanisms, caching, and node registration.</p> <p>Topics: - NodeRegistry architecture - Git and local plugin loading - Plugin manifest format - Cache management - Version management</p> </li> <li> <p> Plugin Development Guide</p> <p>Step-by-step guide to creating your own plugins from scratch to publication.</p> <p>Topics: - Plugin project structure - Node implementation - Testing and validation - Packaging and distribution - Best practices</p> </li> <li> <p> Plugin Usage Guide</p> <p>Practical guide for installing and using plugins in your workflows.</p> <p>Topics: - Finding plugins - Installation methods - Using in pipelines - CLI integration - Troubleshooting</p> </li> </ul>"},{"location":"plugin-system/#why-use-plugins","title":"Why Use Plugins?","text":""},{"location":"plugin-system/#benefits","title":"Benefits","text":"<ul> <li> <p> Modularity</p> <p>Keep the core framework lean by separating specialized nodes into plugins</p> </li> <li> <p> Distribution</p> <p>Share your custom nodes via Git repositories or PyPI packages</p> </li> <li> <p>:material-package-lock: Isolation</p> <p>Manage dependencies independently per plugin without conflicts</p> </li> <li> <p> Versioning</p> <p>Use Git tags for reproducible, deterministic plugin versions</p> </li> <li> <p> Flexibility</p> <p>Add domain-specific functionality without modifying core code</p> </li> <li> <p> Safety</p> <p>Plugin failures don't crash the core framework (session isolation)</p> </li> </ul>"},{"location":"plugin-system/#use-cases","title":"Use Cases","text":"<ul> <li>Custom Algorithms: Implement proprietary anomaly detection methods</li> <li>Research: Experiment with new techniques without polluting core</li> <li>Domain-Specific: Add industry-specific preprocessing nodes</li> <li>Integration: Connect to third-party tools and services</li> <li>Community: Share and reuse community-contributed algorithms</li> </ul>"},{"location":"plugin-system/#plugin-examples","title":"Plugin Examples","text":""},{"location":"plugin-system/#official-plugins","title":"Official Plugins","text":"<ul> <li>cuvis-ai-adaclip - AdaCLIP vision-language anomaly detection using CLIP embeddings</li> </ul>"},{"location":"plugin-system/#central-plugin-registry","title":"Central Plugin Registry","text":"<p>cuvis-ai maintains a central registry of official and community plugins at <code>configs/plugins/registry.yaml</code>.</p> <p>Using the Registry: <pre><code># Load all registered plugins\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_pipeline.yaml \\\n    --plugins-path configs/plugins/registry.yaml\n</code></pre></p> <p>Registered Plugins: - cuvis-ai-adaclip - AdaCLIP vision-language anomaly detection with Fisher band selection and mRMR algorithms</p> <p>Want to add your plugin? See Contributing Guide for submission instructions.</p>"},{"location":"plugin-system/#custom-plugin-manifests","title":"Custom Plugin Manifests","text":"<p>You can also create custom manifests to load specific plugins:</p> <pre><code>plugins:\n  # Reference plugin from registry\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n  # Local development plugin\n  my_plugin:\n    path: \"../my-plugin\"\n    provides:\n      - my_plugin.nodes.CustomNode\n</code></pre>"},{"location":"plugin-system/#key-features","title":"Key Features","text":""},{"location":"plugin-system/#git-based-distribution","title":"Git-Based Distribution","text":"<ul> <li>Automatic cloning from Git repositories (GitHub, GitLab, etc.)</li> <li>Tag-based versioning for reproducibility (no branches)</li> <li>Shallow clones for efficiency (<code>depth=1</code>)</li> <li>Automatic caching in <code>~/.cuvis_plugins/</code></li> </ul>"},{"location":"plugin-system/#local-development","title":"Local Development","text":"<ul> <li>Path-based loading for development workflows</li> <li>Editable installs with <code>pip install -e .</code></li> <li>Hot reloading during development</li> <li>Private plugins without Git hosting</li> </ul>"},{"location":"plugin-system/#dependency-management","title":"Dependency Management","text":"<ul> <li>PEP 621 compliant <code>pyproject.toml</code></li> <li>Automatic dependency installation via <code>uv pip install</code></li> <li>Version constraints enforcement</li> <li>Isolated environments per session</li> </ul>"},{"location":"plugin-system/#cli-integration","title":"CLI Integration","text":"<pre><code># Use plugins with restore-pipeline\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_pipeline.yaml \\\n    --plugins-path plugins.yaml\n\n# Use plugins with restore-trainrun\nuv run restore-trainrun \\\n    --trainrun-path outputs/trainrun.yaml \\\n    --mode train\n</code></pre>"},{"location":"plugin-system/#getting-started","title":"Getting Started","text":""},{"location":"plugin-system/#for-users-installing-plugins","title":"For Users: Installing Plugins","text":"<ol> <li>Find a plugin (GitHub, community, official)</li> <li>Create manifest (<code>plugins.yaml</code>)</li> <li>Load and use:    <pre><code>registry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n</code></pre></li> </ol> <p>See Plugin Usage Guide for details.</p>"},{"location":"plugin-system/#for-developers-creating-plugins","title":"For Developers: Creating Plugins","text":"<ol> <li>Set up structure with <code>pyproject.toml</code></li> <li>Implement nodes inheriting from <code>Node</code></li> <li>Add tests with pytest</li> <li>Publish to Git repository</li> <li>Tag versions using semver</li> </ol> <p>See Plugin Development Guide for step-by-step instructions.</p>"},{"location":"plugin-system/#related-documentation","title":"Related Documentation","text":""},{"location":"plugin-system/#core-concepts","title":"Core Concepts","text":"<ul> <li>Node System Deep Dive - Understanding node architecture</li> <li>Port System Deep Dive - Port specifications and connections</li> <li>Pipeline Lifecycle - Pipeline integration</li> </ul>"},{"location":"plugin-system/#how-to-guides","title":"How-To Guides","text":"<ul> <li>Build Pipeline (Python) - Using plugins in Python pipelines</li> <li>Build Pipeline (YAML) - Using plugins in YAML configs</li> <li>Add Built-in Node - Contributing nodes to core</li> </ul>"},{"location":"plugin-system/#reference","title":"Reference","text":"<ul> <li>Node Catalog - Built-in nodes reference</li> <li>Configuration System - Hydra configuration</li> <li>gRPC API - Remote plugin loading</li> </ul>"},{"location":"plugin-system/development/","title":"Plugin Development","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"plugin-system/development/#plugin-development-guide","title":"Plugin Development Guide","text":""},{"location":"plugin-system/development/#introduction","title":"Introduction","text":"<p>This guide walks you through creating a cuvis-ai plugin from scratch, implementing custom nodes, testing your plugin, and publishing it for others to use. By the end of this guide, you'll have a fully functional plugin that extends cuvis-ai with your own domain-specific algorithms.</p>"},{"location":"plugin-system/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+ installed</li> <li>cuvis-ai and cuvis-ai-core installed</li> <li>Git for version control</li> <li>uv for package management (recommended over pip)</li> <li>Basic understanding of the Node System</li> <li>Familiarity with Port System</li> </ul> <p>Note: This guide uses <code>uv</code> for all package management and script execution. Use <code>uv run python ...</code> instead of <code>python ...</code> for running scripts.</p>"},{"location":"plugin-system/development/#quick-start","title":"Quick Start","text":""},{"location":"plugin-system/development/#step-1-create-plugin-structure","title":"Step 1: Create Plugin Structure","text":"<p>Create your plugin directory structure:</p> <pre><code># Create plugin directory\nmkdir my-cuvis-plugin\ncd my-cuvis-plugin\n\n# Create package structure\nmkdir -p cuvis_ai_plugin/nodes\nmkdir -p cuvis_ai_plugin/configs\nmkdir -p tests\n\n# Create necessary files\ntouch cuvis_ai_plugin/__init__.py\ntouch cuvis_ai_plugin/nodes/__init__.py\ntouch cuvis_ai_plugin/nodes/custom_node.py\ntouch cuvis_ai_plugin/configs/default.yaml\ntouch tests/test_custom_node.py\ntouch pyproject.toml\ntouch README.md\ntouch .gitignore\n</code></pre> <p>Final Structure: <pre><code>my-cuvis-plugin/\n\u251c\u2500\u2500 pyproject.toml          # Project configuration (REQUIRED)\n\u251c\u2500\u2500 cuvis_ai_plugin/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 nodes/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 custom_node.py\n\u2502   \u2514\u2500\u2500 configs/\n\u2502       \u2514\u2500\u2500 default.yaml\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_custom_node.py\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 .gitignore\n</code></pre></p>"},{"location":"plugin-system/development/#step-2-configure-pyprojecttoml","title":"Step 2: Configure pyproject.toml","text":"<p>Create a PEP 621 compliant <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"cuvis-ai-my-plugin\"\nversion = \"0.1.0\"\ndescription = \"Custom anomaly detection plugin for cuvis-ai\"\nauthors = [\n    {name = \"Your Name\", email = \"your.email@example.com\"}\n]\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.9\"\nlicense = {text = \"MIT\"}\n\ndependencies = [\n    \"cuvis-ai-core&gt;=0.1.0\",\n    \"numpy&gt;=1.20.0\",\n    \"torch&gt;=2.0.0\",  # If using deep learning\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-cov&gt;=4.0.0\",\n    \"ruff&gt;=0.1.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py39\"\n</code></pre>"},{"location":"plugin-system/development/#step-3-implement-your-first-node","title":"Step 3: Implement Your First Node","text":"<p>Create <code>cuvis_ai_plugin/nodes/custom_node.py</code>:</p> <pre><code>\"\"\"Custom anomaly detection node.\"\"\"\n\nfrom typing import Any\n\nimport numpy as np\nimport torch\nfrom cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.pipeline import PortSpec\nfrom cuvis_ai_schemas.execution import Context\n\n\nclass CustomAnomalyDetector(Node):\n    \"\"\"\n    Custom anomaly detection node using statistical methods.\n\n    This node implements a simple threshold-based anomaly detection\n    algorithm on hyperspectral data.\n\n    Parameters\n    ----------\n    threshold : float\n        Detection threshold for anomaly scores. Default is 0.95.\n    method : str\n        Detection method ('simple' or 'advanced'). Default is 'simple'.\n    window_size : int\n        Sliding window size for contextual analysis. Default is 5.\n\n    Examples\n    --------\n    &gt;&gt;&gt; detector = CustomAnomalyDetector(threshold=0.95)\n    &gt;&gt;&gt; outputs = detector(data=hyperspectral_cube, context=context)\n    &gt;&gt;&gt; anomaly_map = outputs[\"detections\"]\n    \"\"\"\n\n    # Define input ports (use dict format)\n    INPUT_SPECS = {\n        \"data\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),\n            description=\"Input hyperspectral cube [B, H, W, C]\",\n        )\n    }\n\n    # Define output ports (use dict format)\n    OUTPUT_SPECS = {\n        \"scores\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, 1),\n            description=\"Anomaly scores for each pixel [B, H, W, 1]\",\n        ),\n        \"detections\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, 1),\n            description=\"Binary anomaly detections [B, H, W, 1]\",\n        ),\n    }\n\n    def __init__(\n        self,\n        threshold: float = 0.95,\n        method: str = \"simple\",\n        window_size: int = 5,\n        **kwargs,\n    ):\n        \"\"\"Initialize the custom anomaly detector.\n\n        IMPORTANT: All hyperparameters that need to be serialized must be passed\n        to super().__init__() as keyword arguments for proper serialization.\n        \"\"\"\n        # Store hyperparameters as instance attributes\n        self.threshold = threshold\n        self.method = method\n        self.window_size = window_size\n\n        # Validate parameters\n        if not 0 &lt; threshold &lt; 1:\n            raise ValueError(f\"threshold must be in (0, 1), got {threshold}\")\n        if method not in [\"simple\", \"advanced\"]:\n            raise ValueError(f\"method must be 'simple' or 'advanced', got {method}\")\n\n        # Pass all hyperparameters to super().__init__() for serialization\n        super().__init__(\n            threshold=threshold,\n            method=method,\n            window_size=window_size,\n            **kwargs,\n        )\n\n    def forward(self, data: torch.Tensor, context: Context, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Process input data and detect anomalies.\n\n        IMPORTANT: The forward method must accept a 'context' parameter containing\n        execution metadata (stage, epoch, batch_idx, global_step).\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input hyperspectral cube with shape [B, H, W, C].\n        context : Context\n            Execution context with stage, epoch, batch_idx, global_step.\n\n        Returns\n        -------\n        dict[str, Any]\n            Dictionary with 'scores' and 'detections' keys.\n        \"\"\"\n        # Validate input shape\n        if data.ndim != 4:\n            raise ValueError(f\"Expected 4D input [B, H, W, C], got shape {data.shape}\")\n\n        # Compute anomaly scores\n        scores = self._compute_scores(data)\n\n        # Apply threshold (keep as float tensor, not binary)\n        detections = (scores &gt; self.threshold).float()\n\n        return {\n            \"scores\": scores,\n            \"detections\": detections,\n        }\n\n    def _compute_scores(self, data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute anomaly scores for each pixel.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input data [B, H, W, C].\n\n        Returns\n        -------\n        torch.Tensor\n            Anomaly scores [B, H, W, 1].\n        \"\"\"\n        B, H, W, C = data.shape\n\n        if self.method == \"simple\":\n            # Simple statistical method: Mahalanobis distance per batch\n            # Flatten spatial dimensions\n            pixels = data.reshape(B, H * W, C)  # [B, N, C] where N = H*W\n\n            # Compute per-batch statistics\n            mean = pixels.mean(dim=1, keepdim=True)  # [B, 1, C]\n            centered = pixels - mean  # [B, N, C]\n\n            # Compute covariance matrix [B, C, C]\n            cov = torch.bmm(centered.transpose(1, 2), centered) / (H * W - 1)\n\n            # Add regularization for numerical stability\n            cov = cov + torch.eye(C, device=data.device) * 1e-6\n\n            # Compute Mahalanobis distance\n            cov_inv = torch.linalg.inv(cov)  # [B, C, C]\n            mahal_sq = torch.bmm(\n                torch.bmm(centered, cov_inv),  # [B, N, C]\n                centered.unsqueeze(-1)  # [B, N, C, 1]\n            ).squeeze(-1)  # [B, N]\n\n            scores = torch.sqrt(mahal_sq.clamp(min=0))  # [B, N]\n            scores = scores.reshape(B, H, W, 1)  # [B, H, W, 1]\n\n            # Normalize to [0, 1] per batch\n            scores_min = scores.amin(dim=(1, 2), keepdim=True)\n            scores_max = scores.amax(dim=(1, 2), keepdim=True)\n            scores = (scores - scores_min) / (scores_max - scores_min + 1e-8)\n            return scores\n\n        elif self.method == \"advanced\":\n            # Advanced method: Contextual anomaly detection\n            return self._contextual_detection(data)\n\n        else:\n            raise ValueError(f\"Unknown method: {self.method}\")\n\n    def _contextual_detection(self, data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Context-aware anomaly detection using sliding window.\n\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input data [B, H, W, C].\n\n        Returns\n        -------\n        torch.Tensor\n            Anomaly scores [B, H, W, 1].\n        \"\"\"\n        B, H, W, C = data.shape\n        scores = torch.zeros(B, H, W, 1, device=data.device)\n        half_window = self.window_size // 2\n\n        # Process each batch and spatial location\n        for b in range(B):\n            for i in range(H):\n                for j in range(W):\n                    # Extract local window\n                    i_start = max(0, i - half_window)\n                    i_end = min(H, i + half_window + 1)\n                    j_start = max(0, j - half_window)\n                    j_end = min(W, j + half_window + 1)\n\n                    window = data[b, i_start:i_end, j_start:j_end, :]  # [h, w, C]\n                    window_pixels = window.reshape(-1, C)  # [h*w, C]\n\n                    # Compute local statistics\n                    local_mean = window_pixels.mean(dim=0)  # [C]\n                    local_std = window_pixels.std(dim=0) + 1e-8  # [C]\n\n                    # Compute pixel's deviation from local context\n                    pixel = data[b, i, j, :]  # [C]\n                    deviation = torch.abs(pixel - local_mean) / local_std  # [C]\n                    scores[b, i, j, 0] = deviation.mean()\n\n        # Normalize per batch\n        scores_min = scores.amin(dim=(1, 2), keepdim=True)\n        scores_max = scores.amax(dim=(1, 2), keepdim=True)\n        scores = (scores - scores_min) / (scores_max - scores_min + 1e-8)\n        return scores\n</code></pre>"},{"location":"plugin-system/development/#step-4-export-your-node","title":"Step 4: Export Your Node","text":"<p>Update <code>cuvis_ai_plugin/__init__.py</code>:</p> <pre><code>\"\"\"My Custom CUVIS-AI Plugin.\"\"\"\n\nfrom .nodes.custom_node import CustomAnomalyDetector\n\n__version__ = \"0.1.0\"\n__all__ = [\"CustomAnomalyDetector\"]\n</code></pre> <p>Update <code>cuvis_ai_plugin/nodes/__init__.py</code>:</p> <pre><code>\"\"\"Node implementations.\"\"\"\n\nfrom .custom_node import CustomAnomalyDetector\n\n__all__ = [\"CustomAnomalyDetector\"]\n</code></pre>"},{"location":"plugin-system/development/#step-5-create-tests","title":"Step 5: Create Tests","text":"<p>Create <code>tests/test_custom_node.py</code>:</p> <pre><code>\"\"\"Tests for custom anomaly detector node.\"\"\"\n\nimport pytest\nimport torch\nfrom cuvis_ai_schemas.enums import ExecutionStage\nfrom cuvis_ai_schemas.execution import Context\nfrom cuvis_ai_plugin.nodes import CustomAnomalyDetector\n\n\ndef test_custom_node_initialization():\n    \"\"\"Test node can be initialized.\"\"\"\n    node = CustomAnomalyDetector(threshold=0.95, method=\"simple\")\n    assert node.threshold == 0.95\n    assert node.method == \"simple\"\n\n\ndef test_custom_node_invalid_threshold():\n    \"\"\"Test invalid threshold raises error.\"\"\"\n    with pytest.raises(ValueError, match=\"threshold must be in\"):\n        CustomAnomalyDetector(threshold=1.5)\n\n\ndef test_custom_node_simple_method():\n    \"\"\"Test simple detection method.\"\"\"\n    node = CustomAnomalyDetector(threshold=0.9, method=\"simple\")\n\n    # Create synthetic data [B, H, W, C] format\n    data = torch.randn(2, 10, 10, 50, dtype=torch.float32)\n\n    # Create context\n    context = Context(stage=ExecutionStage.INFERENCE, epoch=0, batch_idx=0, global_step=0)\n\n    # Process\n    outputs = node(data=data, context=context)\n\n    # Check outputs\n    assert \"scores\" in outputs\n    assert \"detections\" in outputs\n    assert outputs[\"scores\"].shape == (2, 10, 10, 1)\n    assert outputs[\"detections\"].shape == (2, 10, 10, 1)\n    assert outputs[\"detections\"].dtype == torch.float32\n\n\ndef test_custom_node_advanced_method():\n    \"\"\"Test advanced detection method.\"\"\"\n    node = CustomAnomalyDetector(threshold=0.9, method=\"advanced\", window_size=3)\n\n    data = torch.randn(2, 10, 10, 50, dtype=torch.float32)\n    context = Context(stage=ExecutionStage.INFERENCE)\n    outputs = node(data=data, context=context)\n\n    assert outputs[\"scores\"].shape == (2, 10, 10, 1)\n    assert outputs[\"detections\"].shape == (2, 10, 10, 1)\n\n\ndef test_custom_node_threshold_behavior():\n    \"\"\"Test threshold parameter works correctly.\"\"\"\n    data = torch.randn(2, 10, 10, 50, dtype=torch.float32)\n    context = Context(stage=ExecutionStage.INFERENCE)\n\n    # High threshold = fewer detections\n    node_high = CustomAnomalyDetector(threshold=0.99)\n    outputs_high = node_high(data=data, context=context)\n\n    # Low threshold = more detections\n    node_low = CustomAnomalyDetector(threshold=0.5)\n    outputs_low = node_low(data=data, context=context)\n\n    assert outputs_low[\"detections\"].sum() &gt;= outputs_high[\"detections\"].sum()\n\n\ndef test_custom_node_invalid_input_shape():\n    \"\"\"Test node rejects invalid input shapes.\"\"\"\n    node = CustomAnomalyDetector()\n    context = Context(stage=ExecutionStage.INFERENCE)\n\n    # 3D input (should be 4D [B, H, W, C])\n    with pytest.raises(ValueError, match=\"Expected 4D input\"):\n        node(data=torch.randn(10, 10, 50), context=context)\n</code></pre>"},{"location":"plugin-system/development/#step-6-test-locally","title":"Step 6: Test Locally","text":"<pre><code># Install plugin with all dependencies using uv (recommended)\nuv sync\n\n# Install with dev dependencies\nuv sync --extra dev\n# Or install all optional dependencies\nuv sync --all-extras\n\n# Alternative: Install in editable mode using uv pip\n# uv pip install -e \".[dev]\"\n\n# Run tests with uv\nuv run pytest tests/ -v\n\n# Run with coverage\nuv run pytest tests/ --cov=cuvis_ai_plugin --cov-report=html\n</code></pre>"},{"location":"plugin-system/development/#step-7-create-plugin-manifest","title":"Step 7: Create Plugin Manifest","text":"<p>Create <code>examples/plugins.yaml</code> to test your plugin:</p> <pre><code>plugins:\n  my_plugin:\n    path: \".\"  # Current directory\n    provides:\n      - cuvis_ai_plugin.nodes.custom_node.CustomAnomalyDetector\n</code></pre>"},{"location":"plugin-system/development/#step-8-test-in-pipeline","title":"Step 8: Test in Pipeline","text":"<p>Create <code>examples/test_plugin.py</code>:</p> <pre><code>\"\"\"Test plugin in a pipeline.\"\"\"\n\nimport torch\nfrom cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai_core.pipeline.pipeline import Pipeline\nfrom cuvis_ai_schemas.enums import ExecutionStage\nfrom cuvis_ai_schemas.execution import Context\n\n\ndef main():\n    # Create registry\n    registry = NodeRegistry()\n\n    # Load plugin from manifest\n    registry.load_plugins(\"examples/plugins.yaml\")\n\n    # Verify node is available\n    CustomAnomalyDetector = registry.get(\"CustomAnomalyDetector\", instance=registry)\n    print(f\"\u2713 Node loaded: {CustomAnomalyDetector}\")\n\n    # Create pipeline\n    pipeline_dict = {\n        \"nodes\": [\n            {\n                \"class_name\": \"CustomAnomalyDetector\",\n                \"name\": \"detector\",\n                \"params\": {\n                    \"threshold\": 0.95,\n                    \"method\": \"simple\"\n                }\n            }\n        ],\n        \"edges\": []\n    }\n\n    pipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\n\n    # Test with synthetic data in [B, H, W, C] format\n    test_data = torch.randn(4, 50, 50, 100, dtype=torch.float32)\n\n    # Create execution context\n    context = Context(stage=ExecutionStage.INFERENCE, epoch=0, batch_idx=0, global_step=0)\n\n    # Execute pipeline\n    outputs = pipeline(data=test_data, context=context)\n\n    print(f\"\u2713 Pipeline executed successfully\")\n    print(f\"  Anomaly scores shape: {outputs['detector']['scores'].shape}\")\n    print(f\"  Detections: {outputs['detector']['detections'].sum().item()} pixels\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run the test:</p> <pre><code>uv run python examples/test_plugin.py\n</code></pre>"},{"location":"plugin-system/development/#critical-requirements-for-node-development","title":"Critical Requirements for Node Development","text":""},{"location":"plugin-system/development/#1-import-context-and-datatypes-from-cuvis-ai-schemas","title":"1. Import Context and Datatypes from cuvis-ai-schemas","text":"<p>All nodes must import the <code>Context</code> class and any required datatypes from <code>cuvis_ai_schemas</code>:</p> <pre><code>from cuvis_ai_schemas.enums import ExecutionStage\nfrom cuvis_ai_schemas.execution import Context, InputStream, Metric\n\n# For commonly used datatypes:\n# - Context: Execution context with stage, epoch, batch_idx, global_step\n# - ExecutionStage: Enum for execution stages (TRAIN, VAL, TEST, INFERENCE)\n# - InputStream: Type hint for input data streams\n# - Metric: Dataclass for metric logging\n# - Artifact: Dataclass for artifact logging (images, etc.)\n# - ArtifactType: Enum for artifact types (IMAGE, etc.)\n</code></pre> <p>Why this matters: The <code>Context</code> parameter provides execution metadata that nodes can use to: - Determine the current execution stage (training vs. inference) - Access the current epoch and batch index for logging - Implement stage-specific behavior (e.g., dropout during training only)</p>"},{"location":"plugin-system/development/#2-forward-method-must-accept-context-parameter","title":"2. Forward Method Must Accept Context Parameter","text":"<p>All node <code>forward()</code> methods must include a <code>context: Context</code> parameter:</p> <pre><code>def forward(self, input_data: torch.Tensor, context: Context, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Process input data.\n\n    Parameters\n    ----------\n    input_data : torch.Tensor\n        Input data tensor.\n    context : Context\n        Execution context with stage, epoch, batch_idx, global_step.\n\n    Returns\n    -------\n    dict[str, Any]\n        Output dictionary.\n    \"\"\"\n    # Use context to access execution metadata\n    if context.stage == ExecutionStage.TRAIN:\n        # Training-specific logic\n        pass\n    elif context.stage == ExecutionStage.INFERENCE:\n        # Inference-specific logic\n        pass\n\n    # Your processing logic here\n    ...\n</code></pre> <p>Context can be optional for nodes that don't need execution metadata: <pre><code>def forward(self, input_data: torch.Tensor, context: Context | None = None, **kwargs: Any) -&gt; dict[str, Any]:\n    # context is optional\n    ...\n</code></pre></p> <p>Execution Stages and Node Filtering:</p> <p>By default, nodes execute in all stages (<code>ExecutionStage.ALWAYS</code>). You can override this to make nodes execute only during specific stages by passing <code>execution_stages</code> directly to <code>super().__init__()</code>:</p> <pre><code>from cuvis_ai_core.node import Node\nfrom cuvis_ai_schemas.enums import ExecutionStage\n\nclass LossNode(Node):\n    \"\"\"Loss computation node - only runs during training.\"\"\"\n\n    def __init__(self, loss_weight: float = 1.0, **kwargs):\n        self.loss_weight = loss_weight\n\n        # Pass execution_stages directly to super().__init__()\n        super().__init__(\n            execution_stages={ExecutionStage.TRAIN},  # Only execute during training\n            loss_weight=loss_weight,\n            **kwargs,\n        )\n\nclass VisualizationNode(Node):\n    \"\"\"Visualization node - runs during training, validation, and testing.\"\"\"\n\n    def __init__(self, max_samples: int = 4, **kwargs):\n        self.max_samples = max_samples\n\n        # Override to run in multiple stages (but not inference)\n        super().__init__(\n            execution_stages={ExecutionStage.TRAIN, ExecutionStage.VAL, ExecutionStage.TEST},\n            max_samples=max_samples,\n            **kwargs,\n        )\n</code></pre> <p>Available execution stages: - <code>ExecutionStage.ALWAYS</code> - Default, runs in all stages - <code>ExecutionStage.TRAIN</code> - Training only - <code>ExecutionStage.VAL</code> (or <code>VALIDATE</code>) - Validation only - <code>ExecutionStage.TEST</code> - Testing only - <code>ExecutionStage.INFERENCE</code> - Inference only</p> <p>Use cases for stage filtering: - Loss nodes: Only needed during training - Metric nodes: Only needed during validation/testing - Dropout/augmentation: Only active during training - Expensive visualizations: Skip during training, show during inference</p>"},{"location":"plugin-system/development/#3-pass-all-hyperparameters-to-superinit","title":"3. Pass All Hyperparameters to super().init()","text":"<p>All hyperparameters that need to be serialized (saved/loaded) must be passed as keyword arguments to <code>super().__init__()</code>:</p> <pre><code>def __init__(\n    self,\n    threshold: float = 0.95,\n    method: str = \"simple\",\n    hidden_dim: int = 128,\n    **kwargs,\n):\n    \"\"\"Initialize node with hyperparameters.\"\"\"\n    # Store as instance attributes\n    self.threshold = threshold\n    self.method = method\n    self.hidden_dim = hidden_dim\n\n    # CRITICAL: Pass ALL hyperparameters to super().__init__()\n    # This enables serialization/deserialization for pipeline saving\n    super().__init__(\n        threshold=threshold,\n        method=method,\n        hidden_dim=hidden_dim,\n        **kwargs,\n    )\n</code></pre> <p>Why this matters: cuvis-ai uses these parameters to: - Serialize node configuration when saving pipelines - Reconstruct nodes when loading pipelines from YAML - Track hyperparameters for experiment logging</p> <p>Examples from cuvis-ai nodes:</p> <pre><code># From BandpassByWavelength node:\ndef __init__(\n    self,\n    min_wavelength_nm: float,\n    max_wavelength_nm: float | None = None,\n    **kwargs,\n) -&gt; None:\n    self.min_wavelength_nm = float(min_wavelength_nm)\n    self.max_wavelength_nm = float(max_wavelength_nm) if max_wavelength_nm is not None else None\n\n    super().__init__(\n        min_wavelength_nm=self.min_wavelength_nm,\n        max_wavelength_nm=self.max_wavelength_nm,\n        **kwargs,\n    )\n\n# From RXGlobal node:\ndef __init__(self, eps: float = 1e-6, **kwargs) -&gt; None:\n    self.eps = eps\n    super().__init__(eps=eps, **kwargs)\n</code></pre>"},{"location":"plugin-system/development/#4-use-port-specifications-from-cuvis-ai-schemas","title":"4. Use Port Specifications from cuvis-ai-schemas","text":"<p>Import <code>PortSpec</code> from <code>cuvis_ai_schemas.pipeline</code>:</p> <pre><code>from cuvis_ai_schemas.pipeline import PortSpec\n\nclass MyNode(Node):\n    INPUT_SPECS = {\n        \"data\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, -1),  # [B, H, W, C]\n            description=\"Input hyperspectral cube\",\n        ),\n        \"wavelengths\": PortSpec(\n            dtype=np.int32,\n            shape=(-1,),  # [C]\n            description=\"Wavelength array in nanometers\",\n        ),\n    }\n\n    OUTPUT_SPECS = {\n        \"result\": PortSpec(\n            dtype=torch.float32,\n            shape=(-1, -1, -1, 1),  # [B, H, W, 1]\n            description=\"Output result\",\n        ),\n    }\n</code></pre>"},{"location":"plugin-system/development/#5-always-use-uv-for-package-management","title":"5. Always Use uv for Package Management","text":"<p>Use <code>uv</code> instead of <code>pip</code> for all package management and script execution:</p> <pre><code># Install dependencies (recommended - creates uv.lock)\nuv sync\nuv sync --extra dev  # With dev dependencies\n\n# Alternative: Use uv pip for editable installs\nuv pip install -e \".[dev]\"\n\n# Run scripts\nuv run python examples/test_plugin.py\nuv run pytest tests/ -v\n\n# NOT: python examples/test_plugin.py\n# NOT: pip install -e .\n</code></pre> <p>Why uv? <code>uv</code> provides: - Faster dependency resolution (10-100x faster than pip) - Automatic lock file generation (<code>uv.lock</code>) for reproducibility - Consistent virtual environment handling - Improved caching and performance</p> <p>uv sync vs uv pip install: - <code>uv sync</code>: Recommended for development, creates lock file, syncs environment to exact versions - <code>uv pip install -e .</code>: Alternative for compatibility, works like traditional pip</p>"},{"location":"plugin-system/development/#advanced-node-development","title":"Advanced Node Development","text":""},{"location":"plugin-system/development/#creating-statistical-nodes","title":"Creating Statistical Nodes","text":"<p>Nodes that require initialization:</p> <pre><code>class StatisticalCustomNode(Node):\n    \"\"\"Node requiring statistical initialization.\"\"\"\n\n    INPUT_SPECS = [\n        PortSpec(name=\"data\", dtype=np.ndarray, required=True)\n    ]\n\n    OUTPUT_SPECS = [\n        PortSpec(name=\"transformed\", dtype=np.ndarray)\n    ]\n\n    def __init__(self, n_components: int = 10):\n        super().__init__()\n        self.n_components = n_components\n        self.is_initialized = False\n        self.statistics = None\n\n    def initialize(self, initialization_data: np.ndarray):\n        \"\"\"\n        Initialize with initialization data.\n\n        Parameters\n        ----------\n        initialization_data : np.ndarray\n            Initialization dataset for computing statistics.\n        \"\"\"\n        # Compute PCA components\n        pixels = initialization_data.reshape(-1, initialization_data.shape[-1])\n        mean = pixels.mean(axis=0)\n        centered = pixels - mean\n\n        # Compute covariance\n        cov = np.cov(centered.T)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n\n        # Store top components\n        idx = np.argsort(eigenvalues)[::-1][:self.n_components]\n        self.statistics = {\n            \"mean\": mean,\n            \"components\": eigenvectors[:, idx],\n            \"eigenvalues\": eigenvalues[idx]\n        }\n        self.is_initialized = True\n\n    def forward(self, data: np.ndarray) -&gt; dict:\n        \"\"\"Transform data using learned statistics.\"\"\"\n        if not self.is_initialized:\n            raise RuntimeError(\"Node not initialized. Call initialize() first.\")\n\n        H, W, C = data.shape\n        pixels = data.reshape(-1, C)\n\n        # Project onto PCA components\n        centered = pixels - self.statistics[\"mean\"]\n        transformed = centered @ self.statistics[\"components\"]\n\n        return {\"transformed\": transformed.reshape(H, W, -1)}\n</code></pre> <p>Two-Phase Training Integration:</p> <p>See Two-Phase Training for details on statistical initialization patterns.</p>"},{"location":"plugin-system/development/#creating-deep-learning-nodes","title":"Creating Deep Learning Nodes","text":"<p>Nodes using PyTorch or other deep learning frameworks:</p> <pre><code>import torch\nimport torch.nn as nn\n\n\nclass DeepLearningDetector(Node):\n    \"\"\"Deep learning-based anomaly detector.\"\"\"\n\n    INPUT_SPECS = [\n        PortSpec(name=\"data\", dtype=np.ndarray, required=True)\n    ]\n\n    OUTPUT_SPECS = [\n        PortSpec(name=\"predictions\", dtype=np.ndarray),\n        PortSpec(name=\"features\", dtype=np.ndarray)\n    ]\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        latent_dim: int = 32,\n        device: str = \"auto\"\n    ):\n        super().__init__()\n\n        # Set device\n        if device == \"auto\":\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Define model architecture\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim),\n            nn.ReLU()\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n\n        self.model = nn.Sequential(self.encoder, self.decoder)\n        self.model.to(self.device)\n\n    def forward(self, data: np.ndarray) -&gt; dict:\n        \"\"\"\n        Forward pass through autoencoder.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Input data (H, W, C).\n\n        Returns\n        -------\n        dict\n            Predictions and latent features.\n        \"\"\"\n        H, W, C = data.shape\n        pixels = data.reshape(-1, C)\n\n        # Convert to tensor\n        x = torch.from_numpy(pixels).float().to(self.device)\n\n        # Forward pass\n        with torch.no_grad():\n            features = self.encoder(x)\n            reconstructed = self.model(x)\n\n        # Compute reconstruction error (anomaly score)\n        error = torch.mean((x - reconstructed) ** 2, dim=1)\n\n        # Convert back to numpy\n        predictions = error.cpu().numpy().reshape(H, W)\n        features_np = features.cpu().numpy().reshape(H, W, -1)\n\n        return {\n            \"predictions\": predictions,\n            \"features\": features_np\n        }\n\n    def train_step(self, data: np.ndarray) -&gt; float:\n        \"\"\"\n        Training step for gradient-based learning.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Training batch.\n\n        Returns\n        -------\n        float\n            Training loss.\n        \"\"\"\n        # Enable training mode\n        self.model.train()\n\n        # Prepare data\n        pixels = data.reshape(-1, data.shape[-1])\n        x = torch.from_numpy(pixels).float().to(self.device)\n\n        # Forward pass\n        reconstructed = self.model(x)\n        loss = nn.functional.mse_loss(reconstructed, x)\n\n        return loss.item()\n</code></pre>"},{"location":"plugin-system/development/#multi-output-nodes","title":"Multi-Output Nodes","text":"<p>Nodes with multiple output ports:</p> <pre><code>class FeatureExtractor(Node):\n    \"\"\"Extract multiple feature types from input.\"\"\"\n\n    INPUT_SPECS = [\n        PortSpec(name=\"data\", dtype=np.ndarray, required=True)\n    ]\n\n    OUTPUT_SPECS = [\n        PortSpec(name=\"spectral_features\", dtype=np.ndarray),\n        PortSpec(name=\"spatial_features\", dtype=np.ndarray),\n        PortSpec(name=\"texture_features\", dtype=np.ndarray)\n    ]\n\n    def __init__(self, feature_dim: int = 32):\n        super().__init__()\n        self.feature_dim = feature_dim\n\n    def forward(self, data: np.ndarray) -&gt; dict:\n        \"\"\"Extract multiple feature types.\"\"\"\n        spectral = self._extract_spectral(data)\n        spatial = self._extract_spatial(data)\n        texture = self._extract_texture(data)\n\n        return {\n            \"spectral_features\": spectral,\n            \"spatial_features\": spatial,\n            \"texture_features\": texture\n        }\n\n    def _extract_spectral(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Extract spectral features via PCA.\"\"\"\n        # Implementation\n        pass\n\n    def _extract_spatial(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Extract spatial features via convolution.\"\"\"\n        # Implementation\n        pass\n\n    def _extract_texture(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Extract texture features.\"\"\"\n        # Implementation\n        pass\n</code></pre>"},{"location":"plugin-system/development/#configuration-support","title":"Configuration Support","text":""},{"location":"plugin-system/development/#add-hydra-configuration","title":"Add Hydra Configuration","text":"<p>Create <code>cuvis_ai_plugin/configs/default.yaml</code>:</p> <pre><code># Default configuration for CustomAnomalyDetector\ncustom_anomaly_detector:\n  threshold: 0.95\n  method: \"simple\"\n  window_size: 5\n\n# Configuration for DeepLearningDetector\ndeep_learning_detector:\n  input_dim: 100\n  hidden_dim: 128\n  latent_dim: 32\n  device: \"auto\"\n  learning_rate: 0.001\n\n# Training configuration\ntraining:\n  batch_size: 32\n  epochs: 50\n  optimizer:\n    _target_: torch.optim.Adam\n    lr: ${deep_learning_detector.learning_rate}\n</code></pre>"},{"location":"plugin-system/development/#use-configuration-in-nodes","title":"Use Configuration in Nodes","text":"<pre><code>from omegaconf import DictConfig\n\n\nclass ConfigurableNode(Node):\n    \"\"\"Node accepting Hydra configuration.\"\"\"\n\n    def __init__(self, cfg: DictConfig):\n        super().__init__()\n        self.threshold = cfg.threshold\n        self.method = cfg.method\n        self.window_size = cfg.window_size\n</code></pre>"},{"location":"plugin-system/development/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"plugin-system/development/#unit-tests","title":"Unit Tests","text":"<p>Test individual node functionality:</p> <pre><code>def test_node_ports():\n    \"\"\"Test node has correct input/output ports.\"\"\"\n    node = CustomAnomalyDetector()\n\n    assert len(node.INPUT_SPECS) == 1\n    assert node.INPUT_SPECS[0].name == \"data\"\n\n    assert len(node.OUTPUT_SPECS) == 2\n    assert node.OUTPUT_SPECS[0].name == \"scores\"\n    assert node.OUTPUT_SPECS[1].name == \"detections\"\n\n\ndef test_node_deterministic():\n    \"\"\"Test node produces deterministic results.\"\"\"\n    node = CustomAnomalyDetector(threshold=0.9, method=\"simple\")\n\n    np.random.seed(42)\n    data = np.random.randn(10, 10, 50).astype(np.float32)\n\n    output1 = node(data=data)\n    output2 = node(data=data)\n\n    np.testing.assert_array_equal(output1[\"scores\"], output2[\"scores\"])\n    np.testing.assert_array_equal(output1[\"detections\"], output2[\"detections\"])\n</code></pre>"},{"location":"plugin-system/development/#integration-tests","title":"Integration Tests","text":"<p>Test node in pipeline context:</p> <pre><code>def test_node_in_pipeline():\n    \"\"\"Test node works in pipeline.\"\"\"\n    from cuvis_ai_core.utils.node_registry import NodeRegistry\n    from cuvis_ai_core.pipeline.pipeline import Pipeline\n\n    registry = NodeRegistry()\n    registry.load_plugins(\"examples/plugins.yaml\")\n\n    pipeline_dict = {\n        \"nodes\": [\n            {\n                \"class_name\": \"CustomAnomalyDetector\",\n                \"name\": \"detector\",\n                \"params\": {\"threshold\": 0.95}\n            }\n        ],\n        \"edges\": []\n    }\n\n    pipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\n\n    data = np.random.randn(20, 20, 50).astype(np.float32)\n    outputs = pipeline(data=data)\n\n    assert \"detector\" in outputs\n    assert \"scores\" in outputs[\"detector\"]\n</code></pre>"},{"location":"plugin-system/development/#performance-tests","title":"Performance Tests","text":"<p>Test node performance:</p> <pre><code>import time\n\n\ndef test_node_performance():\n    \"\"\"Test node processes data within time budget.\"\"\"\n    node = CustomAnomalyDetector()\n    data = np.random.randn(100, 100, 224).astype(np.float32)\n\n    start = time.time()\n    outputs = node(data=data)\n    elapsed = time.time() - start\n\n    # Should process 100x100x224 in &lt; 1 second\n    assert elapsed &lt; 1.0\n</code></pre>"},{"location":"plugin-system/development/#packaging-and-distribution","title":"Packaging and Distribution","text":""},{"location":"plugin-system/development/#create-readmemd","title":"Create README.md","text":"<pre><code># CUVIS-AI Custom Anomaly Detection Plugin\n\nCustom anomaly detection algorithms for cuvis-ai framework.\n\n## Installation\n\n### From Git\n\n```bash\n# Create plugins.yaml\ncat &gt; plugins.yaml &lt;&lt; EOF\nplugins:\n  my_plugin:\n    repo: \"https://github.com/your-org/cuvis-ai-my-plugin.git\"\n    tag: \"v0.1.0\"\n    provides:\n      - cuvis_ai_plugin.nodes.custom_node.CustomAnomalyDetector\nEOF\n\n# Load in Python\nfrom cuvis_ai_core.utils.node_registry import NodeRegistry\nregistry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n</code></pre>"},{"location":"plugin-system/development/#from-local-path","title":"From Local Path","text":"<pre><code># For development (recommended)\nuv sync\n\n# Alternative: editable install\nuv pip install -e .\n</code></pre>"},{"location":"plugin-system/development/#usage","title":"Usage","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\nregistry = NodeRegistry()\nregistry.load_plugin(\n    name=\"my_plugin\",\n    config={\n        \"path\": \"path/to/plugin\",\n        \"provides\": [\"cuvis_ai_plugin.nodes.custom_node.CustomAnomalyDetector\"]\n    }\n)\n\nCustomDetector = registry.get(\"CustomAnomalyDetector\", instance=registry)\ndetector = CustomDetector(threshold=0.95)\n</code></pre>"},{"location":"plugin-system/development/#nodes","title":"Nodes","text":""},{"location":"plugin-system/development/#customanomalydetector","title":"CustomAnomalyDetector","text":"<p>Statistical anomaly detection using Mahalanobis distance or contextual methods.</p> <p>Parameters: - <code>threshold</code> (float): Detection threshold (0-1) - <code>method</code> (str): 'simple' or 'advanced' - <code>window_size</code> (int): Contextual window size</p> <p>Inputs: - <code>data</code> (np.ndarray): Hyperspectral cube (H, W, C)</p> <p>Outputs: - <code>scores</code> (np.ndarray): Anomaly scores (H, W) - <code>detections</code> (np.ndarray): Binary detections (H, W)</p>"},{"location":"plugin-system/development/#development","title":"Development","text":"<pre><code>git clone https://github.com/your-org/cuvis-ai-my-plugin.git\ncd cuvis-ai-my-plugin\nuv sync --extra dev\nuv run pytest tests/\n</code></pre>"},{"location":"plugin-system/development/#license","title":"License","text":"<p>MIT License <pre><code>### Create .gitignore\n</code></pre></p>"},{"location":"plugin-system/development/#python","title":"Python","text":"<p>pycache/ *.py[cod] *$py.class *.so .Python build/ develop-eggs/ dist/ downloads/ eggs/ .eggs/ lib/ lib64/ parts/ sdist/ var/ wheels/ *.egg-info/ .installed.cfg *.egg</p>"},{"location":"plugin-system/development/#virtual-environments","title":"Virtual environments","text":"<p>venv/ ENV/ env/</p>"},{"location":"plugin-system/development/#ides","title":"IDEs","text":"<p>.vscode/ .idea/ *.swp *.swo</p>"},{"location":"plugin-system/development/#testing","title":"Testing","text":"<p>.pytest_cache/ .coverage htmlcov/ .tox/</p>"},{"location":"plugin-system/development/#cuvis-cache","title":"cuvis cache","text":"<p>.cuvis/ .cuvis_plugins/ <pre><code>### Versioning Strategy\n\nFollow semantic versioning (semver):\n\n**Version Format:** `MAJOR.MINOR.PATCH`\n\n- **MAJOR:** Breaking API changes\n- **MINOR:** New features, backwards compatible\n- **PATCH:** Bug fixes\n\n**Git Tags:**\n```bash\n# Create annotated tag\ngit tag -a v0.1.0 -m \"Initial release\"\n\n# Push tags\ngit push origin v0.1.0\n</code></pre></p> <p>Update Version:</p> <ol> <li> <p>Update <code>pyproject.toml</code>:    <pre><code>version = \"0.2.0\"\n</code></pre></p> </li> <li> <p>Update <code>__init__.py</code>:    <pre><code>__version__ = \"0.2.0\"\n</code></pre></p> </li> <li> <p>Create CHANGELOG.md entry:    <pre><code>## [0.2.0] - 2026-02-15\n### Added\n- New advanced detection method\n- GPU acceleration support\n\n### Changed\n- Improved threshold handling\n\n### Fixed\n- Bug in score normalization\n</code></pre></p> </li> <li> <p>Commit and tag:    <pre><code>git add pyproject.toml cuvis_ai_plugin/__init__.py CHANGELOG.md\ngit commit -m \"Bump version to 0.2.0\"\ngit tag -a v0.2.0 -m \"Release v0.2.0\"\ngit push origin main v0.2.0\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/development/#publishing-options","title":"Publishing Options","text":""},{"location":"plugin-system/development/#option-1-git-repository-recommended","title":"Option 1: Git Repository (Recommended)","text":"<p>Host on GitHub/GitLab:</p> <pre><code># Create repository on GitHub\ngh repo create your-org/cuvis-ai-my-plugin --public\n\n# Push code\ngit remote add origin https://github.com/your-org/cuvis-ai-my-plugin.git\ngit push -u origin main\ngit push origin v0.1.0\n</code></pre> <p>Users install via: <pre><code>plugins:\n  my_plugin:\n    repo: \"https://github.com/your-org/cuvis-ai-my-plugin.git\"\n    tag: \"v0.1.0\"\n    provides:\n      - cuvis_ai_plugin.nodes.custom_node.CustomAnomalyDetector\n</code></pre></p>"},{"location":"plugin-system/development/#option-2-pypi-publicprivate","title":"Option 2: PyPI (Public/Private)","text":"<p>Build and upload to PyPI:</p> <pre><code># Install build tools with uv\nuv pip install build twine\n\n# Build package\nuv run python -m build\n\n# Upload to PyPI\nuv run python -m twine upload dist/*\n\n# Or upload to TestPyPI first\nuv run python -m twine upload --repository testpypi dist/*\n</code></pre> <p>Users install via: <pre><code># Using uv (recommended)\nuv add cuvis-ai-my-plugin\n\n# Or using uv pip\nuv pip install cuvis-ai-my-plugin\n</code></pre></p> <p>Then use directly (automatic discovery): <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\nCustomDetector = NodeRegistry.get(\"CustomAnomalyDetector\")\n</code></pre></p>"},{"location":"plugin-system/development/#option-3-private-package-repository","title":"Option 3: Private Package Repository","text":"<p>For enterprise/private plugins:</p> <pre><code># Upload to private PyPI server\ntwine upload --repository-url https://your-pypi.company.com/ dist/*\n\n# Or use artifact repository (e.g., Artifactory, Nexus)\n</code></pre>"},{"location":"plugin-system/development/#best-practices","title":"Best Practices","text":""},{"location":"plugin-system/development/#1-node-design","title":"1. Node Design","text":"<ul> <li>Single Responsibility: Each node should do one thing well</li> <li>Input Validation: Validate all inputs in <code>forward()</code> method</li> <li>Error Handling: Raise clear, actionable exceptions</li> <li>Port Documentation: Document all ports in <code>PortSpec</code> descriptions</li> <li>Docstrings: Follow NumPy-style docstrings (see Docstring Guide)</li> </ul>"},{"location":"plugin-system/development/#2-testing","title":"2. Testing","text":"<ul> <li>Coverage: Aim for &gt;90% test coverage</li> <li>Edge Cases: Test boundary conditions and error cases</li> <li>Integration: Test nodes in pipeline context</li> <li>Performance: Benchmark critical operations</li> <li>Fixtures: Use pytest fixtures for reusable test data</li> </ul>"},{"location":"plugin-system/development/#3-documentation","title":"3. Documentation","text":"<ul> <li>README: Clear installation and usage instructions</li> <li>Examples: Provide working code examples</li> <li>Changelog: Maintain version history</li> <li>API Docs: Document all public APIs</li> <li>Tutorials: Create guides for complex workflows</li> </ul>"},{"location":"plugin-system/development/#4-performance","title":"4. Performance","text":"<ul> <li>Profiling: Profile code to identify bottlenecks</li> <li>Vectorization: Use NumPy vectorized operations</li> <li>GPU Support: Leverage GPUs for compute-heavy operations</li> <li>Memory: Minimize memory allocations</li> <li>Caching: Cache expensive computations when appropriate</li> </ul>"},{"location":"plugin-system/development/#5-dependencies","title":"5. Dependencies","text":"<ul> <li>Minimal: Only include necessary dependencies</li> <li>Pinning: Pin versions for reproducibility</li> <li>Optional: Use optional dependencies for extras</li> <li>Security: Audit dependencies for vulnerabilities</li> </ul>"},{"location":"plugin-system/development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plugin-system/development/#plugin-not-loading","title":"Plugin Not Loading","text":"<p>Issue: Plugin fails to load with import errors.</p> <p>Solution: 1. Verify <code>pyproject.toml</code> exists 2. Check all <code>__init__.py</code> files present 3. Ensure dependencies installed: <code>uv sync</code> (or <code>uv pip install -e .</code>) 4. Test import manually: <code>uv run python -c \"from cuvis_ai_plugin.nodes import CustomAnomalyDetector\"</code></p>"},{"location":"plugin-system/development/#node-not-found","title":"Node Not Found","text":"<p>Issue: Node not found after loading plugin.</p> <p>Solution: 1. Check <code>provides</code> list in manifest includes full class path 2. Verify class name spelling matches exactly 3. Ensure class inherits from <code>Node</code> 4. Check <code>__all__</code> exports in <code>__init__.py</code></p>"},{"location":"plugin-system/development/#test-failures","title":"Test Failures","text":"<p>Issue: Tests fail unexpectedly.</p> <p>Solution: 1. Run with verbose output: <code>uv run pytest tests/ -v</code> 2. Check test data shapes match node expectations 3. Verify NumPy random seeds for reproducibility 4. Isolate failing test: <code>uv run pytest tests/test_custom_node.py::test_name -v</code></p>"},{"location":"plugin-system/development/#performance-issues","title":"Performance Issues","text":"<p>Issue: Node is too slow.</p> <p>Solution: 1. Profile code: <code>uv run python -m cProfile -o profile.stats your_script.py</code> 2. Analyze: <code>uv run python -m pstats profile.stats</code> 3. Vectorize loops using NumPy 4. Use <code>numba</code> JIT compilation for hot loops 5. Consider GPU acceleration with PyTorch/CuPy</p>"},{"location":"plugin-system/development/#example-complete-plugin","title":"Example: Complete Plugin","text":"<p>See the cuvis-ai-adaclip plugin for a complete real-world example.</p>"},{"location":"plugin-system/development/#see-also","title":"See Also","text":"<ul> <li>Plugin System Overview - Plugin architecture and concepts</li> <li>Plugin Usage Guide - Using plugins in workflows</li> <li>Node System Deep Dive - Node architecture details</li> <li>Port System Deep Dive - Port specifications and connections</li> <li>Two-Phase Training - Statistical initialization patterns</li> <li>Docstring Standards - Documentation guidelines</li> </ul>"},{"location":"plugin-system/overview/","title":"Plugin Architecture","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"plugin-system/overview/#plugin-system-overview","title":"Plugin System Overview","text":"<p>The cuvis-ai plugin system enables extensibility by allowing external packages to contribute nodes, configurations, and functionality to the core framework. This architecture keeps the core framework lean while enabling unlimited extensibility for domain-specific use cases.</p>"},{"location":"plugin-system/overview/#why-plugins","title":"Why Plugins?","text":""},{"location":"plugin-system/overview/#benefits","title":"Benefits","text":"<ol> <li>Modularity - Keep core framework lean and focused</li> <li>Extensibility - Add custom nodes without modifying core</li> <li>Distribution - Share implementations via Git or local paths</li> <li>Isolation - Plugin failures don't crash core, session-based isolation</li> <li>Versioning - Independent plugin versioning using Git tags</li> <li>Reproducibility - Git tag-based distribution ensures deterministic builds</li> </ol>"},{"location":"plugin-system/overview/#use-cases","title":"Use Cases","text":"<ul> <li>Custom anomaly detection algorithms (e.g., AdaCLIP)</li> <li>Domain-specific preprocessing nodes</li> <li>Proprietary model integrations</li> <li>Third-party tool connectors</li> <li>Research experiments without polluting core</li> </ul>"},{"location":"plugin-system/overview/#plugin-architecture","title":"Plugin Architecture","text":""},{"location":"plugin-system/overview/#system-overview","title":"System Overview","text":"<pre><code>graph TD\n    A[User Application] --&gt; B[NodeRegistry]\n    B --&gt; C[Built-in Nodes&lt;br/&gt;Class-level Registry]\n    B --&gt; D[Plugin Manager&lt;br/&gt;Instance-level Registry]\n\n    D --&gt; E[Git Plugin Loader]\n    D --&gt; F[Local Plugin Loader]\n\n    E --&gt; G[Plugin Cache&lt;br/&gt;~/.cuvis_plugins/]\n    F --&gt; H[Local Plugin Path]\n\n    G --&gt; I[Plugin Nodes]\n    H --&gt; I\n\n    I --&gt; J[Node Registration]\n    J --&gt; B\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#e8f5e9\n    style D fill:#fff3e0\n    style I fill:#f3e5f5</code></pre>"},{"location":"plugin-system/overview/#core-components","title":"Core Components","text":""},{"location":"plugin-system/overview/#1-noderegistry","title":"1. NodeRegistry","text":"<p>The central registry managing both built-in and plugin nodes.</p> <p>Hybrid Architecture: - Class-level registry: O(1) lookup for built-in nodes via <code>@register</code> decorator - Instance-level registry: Per-session plugin loading for isolation - Benefits: No overhead for built-ins, flexible plugin management</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Class-level access (built-in nodes)\nBuiltinNode = NodeRegistry.get(\"MinMaxNormalizer\")\n\n# Instance-level access (plugin support)\nregistry = NodeRegistry()\nregistry.load_plugin(name=\"adaclip\", config={...})\nAdaCLIPNode = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n</code></pre>"},{"location":"plugin-system/overview/#2-plugin-loaders","title":"2. Plugin Loaders","text":"<p>Two plugin loading mechanisms:</p> <p>Git Plugin Loader: - Clones repositories using shallow clone (<code>depth=1</code>) - Checks out specific Git tags (no branches/commits for reproducibility) - Caches plugins in <code>~/.cuvis_plugins/plugin_name@tag/</code> - Verifies tag matches on subsequent loads</p> <p>Local Plugin Loader: - Loads plugins from local filesystem paths - Supports both absolute and relative paths - Ideal for development and private plugins</p>"},{"location":"plugin-system/overview/#3-plugin-cache","title":"3. Plugin Cache","text":"<p>Location: <code>~/.cuvis_plugins/</code> (Unix/Linux/macOS) or <code>C:\\Users\\{username}\\.cuvis_plugins\\</code> (Windows)</p> <p>Structure: <pre><code>~/.cuvis_plugins/\n\u251c\u2500\u2500 adaclip@v0.1.0/\n\u2502   \u251c\u2500\u2500 .git/\n\u2502   \u251c\u2500\u2500 cuvis_ai_adaclip/\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 adaclip@v0.1.1/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 custom-detector@v2.0.0/\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>Features: - Automatic cache reuse if tag matches - Tag verification prevents cache corruption - Multiple versions can coexist - Manual cache clearing via API</p>"},{"location":"plugin-system/overview/#4-dependency-management","title":"4. Dependency Management","text":"<p>Automatic dependency installation from <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"cuvis-ai-adaclip\"\nversion = \"0.1.1\"\ndependencies = [\n    \"cuvis-ai-core&gt;=0.1.0\",\n    \"torch&gt;=2.9.1\",\n    \"transformers&gt;=4.36.0\",\n    \"pillow&gt;=10.0.0\"\n]\n</code></pre> <p>Installation Process: - Extracts dependencies using <code>tomllib</code> (PEP 621 compliant) - Installs via <code>uv pip install</code> for speed - 300-second timeout with clear error messages - Validates <code>pyproject.toml</code> presence</p>"},{"location":"plugin-system/overview/#plugin-structure","title":"Plugin Structure","text":""},{"location":"plugin-system/overview/#required-files","title":"Required Files","text":"<pre><code>my-cuvis-plugin/\n\u251c\u2500\u2500 pyproject.toml          # PEP 621 compliant project file (REQUIRED)\n\u251c\u2500\u2500 cuvis_ai_plugin/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 nodes/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 custom_node.py  # Your node implementations\n\u2502   \u2514\u2500\u2500 configs/\n\u2502       \u2514\u2500\u2500 default.yaml     # Optional configurations\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_nodes.py\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 .gitignore\n</code></pre>"},{"location":"plugin-system/overview/#plugin-manifest-pluginsyaml","title":"Plugin Manifest (<code>plugins.yaml</code>)","text":"<p>The manifest defines plugin sources and provided nodes:</p> <pre><code>plugins:\n  # Git-based plugin with tagged release\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n  # Pre-release tag support\n  experimental:\n    repo: \"git@github.com:company/experimental-features.git\"\n    tag: \"v2.0.0-beta.1\"\n    provides:\n      - experimental.features.NewFeatureNode\n      - experimental.features.HelperNode\n\n  # Local development plugin (relative path)\n  local_dev:\n    path: \"../my-custom-plugin\"\n    provides:\n      - my_plugin.custom.CustomNode\n\n  # Local plugin (absolute path)\n  production_local:\n    path: \"/absolute/path/to/plugin\"\n    provides:\n      - production_plugin.MainNode\n</code></pre>"},{"location":"plugin-system/overview/#manifest-schema","title":"Manifest Schema","text":"<p>Git Plugin Configuration: <pre><code>{\n    \"repo\": str,           # SSH or HTTPS URL\n    \"tag\": str,            # Git tag (e.g., v1.2.3, v0.1.0-alpha)\n    \"provides\": List[str]  # Fully-qualified class paths\n}\n</code></pre></p> <p>Local Plugin Configuration: <pre><code>{\n    \"path\": str,           # Absolute or relative path\n    \"provides\": List[str]  # Fully-qualified class paths\n}\n</code></pre></p>"},{"location":"plugin-system/overview/#validation-rules","title":"Validation Rules","text":"<ul> <li>Plugin Names: Must be valid Python identifiers</li> <li>Class Paths: Must be fully-qualified (package.module.ClassName)</li> <li>Git URLs: Must start with <code>git@</code>, <code>https://</code>, or <code>http://</code></li> <li>Tags: Git tags only (no branches or commit hashes)</li> <li>Provides: At least one class path required</li> <li>Dependencies: Must have <code>pyproject.toml</code> for Git plugins</li> </ul>"},{"location":"plugin-system/overview/#plugin-loading","title":"Plugin Loading","text":""},{"location":"plugin-system/overview/#loading-flow","title":"Loading Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Registry as NodeRegistry\n    participant Loader as Plugin Loader\n    participant Cache\n    participant Git as Git Repository\n    participant DepMgr as Dependency Manager\n\n    User-&gt;&gt;Registry: load_plugin(name, config)\n    Registry-&gt;&gt;Loader: Parse config\n\n    alt Git Plugin\n        Loader-&gt;&gt;Cache: Check cache for tag\n        alt Cache Hit\n            Cache--&gt;&gt;Loader: Return cached path\n        else Cache Miss\n            Loader-&gt;&gt;Git: Shallow clone (depth=1)\n            Git--&gt;&gt;Loader: Repository cloned\n            Loader-&gt;&gt;Cache: Store at plugin_name@tag/\n        end\n    else Local Plugin\n        Loader-&gt;&gt;Loader: Resolve path (relative/absolute)\n    end\n\n    Loader-&gt;&gt;DepMgr: Check pyproject.toml\n    DepMgr-&gt;&gt;DepMgr: Extract dependencies\n    DepMgr-&gt;&gt;DepMgr: uv pip install (300s timeout)\n\n    Loader-&gt;&gt;Loader: Add to sys.path\n    Loader-&gt;&gt;Loader: Clear module cache\n    Loader-&gt;&gt;Loader: Import node classes\n    Loader-&gt;&gt;Registry: Register nodes\n    Registry--&gt;&gt;User: Plugin loaded</code></pre>"},{"location":"plugin-system/overview/#single-plugin-load","title":"Single Plugin Load","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\nregistry = NodeRegistry()\n\n# Load from Git repository\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Load from local path\nregistry.load_plugin(\n    name=\"custom\",\n    config={\n        \"path\": \"../my-custom-plugin\",\n        \"provides\": [\"my_plugin.nodes.CustomNode\"]\n    }\n)\n</code></pre>"},{"location":"plugin-system/overview/#manifest-based-load","title":"Manifest-Based Load","text":"<p>Load multiple plugins from a YAML manifest:</p> <pre><code>registry = NodeRegistry()\nregistry.load_plugins(\"path/to/plugins.yaml\")\n\n# All plugins from manifest are now loaded\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\nCustomNode = NodeRegistry.get(\"CustomNode\", instance=registry)\n</code></pre>"},{"location":"plugin-system/overview/#cli-integration","title":"CLI Integration","text":""},{"location":"plugin-system/overview/#restore-pipeline","title":"restore-pipeline","text":"<p>Restore and run pipelines with plugin support:</p> <pre><code># Display pipeline information\nuv run restore-pipeline --pipeline-path configs/pipeline/adaclip_baseline.yaml\n\n# Load plugins from manifest\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/adaclip_baseline.yaml \\\n    --plugins-path examples/adaclip/plugins.yaml\n\n# Run inference with plugins\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/adaclip_baseline.yaml \\\n    --plugins-path examples/adaclip/plugins.yaml \\\n    --cu3s-file-path data/test_sample.cu3s\n\n# Export pipeline visualization\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/adaclip_baseline.yaml \\\n    --plugins-path examples/adaclip/plugins.yaml \\\n    --pipeline-vis-ext png\n</code></pre>"},{"location":"plugin-system/overview/#restore-trainrun","title":"restore-trainrun","text":"<p>Restore training runs with plugin support:</p> <pre><code># Display trainrun information\nuv run restore-trainrun \\\n    --trainrun-path outputs/trained_models/trainrun.yaml\n\n# Re-run training with plugins\nuv run restore-trainrun \\\n    --trainrun-path outputs/trained_models/trainrun.yaml \\\n    --mode train \\\n    --override training.optimizer.lr=0.001\n\n# Validation mode\nuv run restore-trainrun \\\n    --trainrun-path outputs/trained_models/trainrun.yaml \\\n    --mode validate\n</code></pre>"},{"location":"plugin-system/overview/#plugin-cache-management","title":"Plugin Cache Management","text":""},{"location":"plugin-system/overview/#set-custom-cache-directory","title":"Set Custom Cache Directory","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Change cache location\nNodeRegistry.set_cache_dir(\"/path/to/custom/cache\")\n</code></pre>"},{"location":"plugin-system/overview/#clear-plugin-cache","title":"Clear Plugin Cache","text":"<pre><code># Clear all plugin caches\nNodeRegistry.clear_plugin_cache()\n\n# Clear specific plugin\nNodeRegistry.clear_plugin_cache(\"adaclip\")\n</code></pre> <p>Manual Cache Cleanup: <pre><code># Unix/Linux/macOS\nrm -rf ~/.cuvis_plugins/\n\n# Windows PowerShell\nRemove-Item -Recurse -Force $env:USERPROFILE\\.cuvis_plugins\\\n</code></pre></p>"},{"location":"plugin-system/overview/#node-registration","title":"Node Registration","text":""},{"location":"plugin-system/overview/#automatic-registration","title":"Automatic Registration","text":"<p>Nodes defined in the manifest's <code>provides</code> list are registered automatically when the plugin loads:</p> <pre><code>registry = NodeRegistry()\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Node automatically registered and retrievable\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n</code></pre>"},{"location":"plugin-system/overview/#node-retrieval-resolution-order","title":"Node Retrieval Resolution Order","text":"<p>Instance Mode (when registry has plugins loaded): 1. Check instance <code>plugin_registry</code> for class name 2. Check instance <code>plugin_registry</code> for last component of full path 3. Check built-in <code>_builtin_registry</code> (O(1) lookup) 4. Try full import path via <code>importlib</code></p> <p>Class Mode (built-ins only): 1. Check built-in registry 2. Try importlib for full paths</p> <pre><code># Short name (class name only)\nNode = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n\n# Full path\nNode = NodeRegistry.get(\n    \"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\",\n    instance=registry\n)\n</code></pre>"},{"location":"plugin-system/overview/#using-plugin-nodes","title":"Using Plugin Nodes","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai_core.pipeline.pipeline import Pipeline\n\n# Load plugins\nregistry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n\n# Get node class\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n\n# Use in pipeline\npipeline_dict = {\n    \"nodes\": [\n        {\n            \"class_name\": \"AdaCLIPDetector\",\n            \"name\": \"adaclip_detector\",\n            \"params\": {\n                \"prompt\": \"plastic wrapper\",\n                \"threshold\": 0.5\n            }\n        }\n    ],\n    \"edges\": [...]\n}\n\npipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\n</code></pre>"},{"location":"plugin-system/overview/#plugin-isolation","title":"Plugin Isolation","text":""},{"location":"plugin-system/overview/#session-based-isolation","title":"Session-Based Isolation","text":"<p>Each <code>NodeRegistry</code> instance maintains independent plugin registries:</p> <pre><code># Session 1\nregistry1 = NodeRegistry()\nregistry1.load_plugin(name=\"adaclip\", config={...})\n\n# Session 2 (independent)\nregistry2 = NodeRegistry()\nregistry2.load_plugin(name=\"custom\", config={...})\n\n# Sessions don't interfere\nassert \"AdaCLIPDetector\" in registry1.plugin_registry\nassert \"AdaCLIPDetector\" not in registry2.plugin_registry\n</code></pre>"},{"location":"plugin-system/overview/#error-handling","title":"Error Handling","text":"<p>Plugin loading errors don't crash the core:</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\nregistry = NodeRegistry()\n\ntry:\n    registry.load_plugin(\n        name=\"broken-plugin\",\n        config={\"repo\": \"...\", \"tag\": \"v1.0.0\", \"provides\": [...]}\n    )\nexcept Exception as e:\n    print(f\"Plugin failed to load: {e}\")\n    # Core continues running\n</code></pre>"},{"location":"plugin-system/overview/#helpful-error-messages","title":"Helpful Error Messages","text":"<p>When a plugin node is not found, the system provides helpful diagnostics:</p> <pre><code>Node 'cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector' not found in registry.\n\n\u26a0\ufe0f  This appears to be an external plugin node!\n   Did you forget to load plugins?\n\n   For CLI usage:\n     uv run restore-pipeline --pipeline-path &lt;path&gt; --plugins-path plugins.yaml\n\n   For Python usage:\n     registry = NodeRegistry()\n     registry.load_plugins('path/to/plugins.yaml')\n</code></pre>"},{"location":"plugin-system/overview/#plugin-versioning","title":"Plugin Versioning","text":""},{"location":"plugin-system/overview/#semantic-versioning","title":"Semantic Versioning","text":"<p>Plugins should follow semver with Git tags: <code>MAJOR.MINOR.PATCH</code></p> <ul> <li>MAJOR: Breaking API changes</li> <li>MINOR: New features, backwards compatible</li> <li>PATCH: Bug fixes</li> </ul> <p>Examples: - <code>v1.0.0</code> - Stable release - <code>v2.1.3</code> - Minor version with patches - <code>v0.1.0-alpha</code> - Pre-release (alpha) - <code>v2.0.0-beta.1</code> - Pre-release (beta)</p>"},{"location":"plugin-system/overview/#version-constraints","title":"Version Constraints","text":"<p>In your plugin's <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-cuvis-plugin\"\nversion = \"1.0.0\"\ndependencies = [\n    \"cuvis-ai-core&gt;=0.1.0,&lt;2.0.0\",  # SemVer range\n    \"numpy&gt;=1.20.0\",\n    \"torch&gt;=2.0.0\"\n]\n</code></pre>"},{"location":"plugin-system/overview/#managing-multiple-versions","title":"Managing Multiple Versions","text":"<p>Multiple plugin versions can coexist in cache:</p> <pre><code>~/.cuvis_plugins/\n\u251c\u2500\u2500 adaclip@v0.1.0/\n\u251c\u2500\u2500 adaclip@v0.1.1/\n\u2514\u2500\u2500 adaclip@v0.2.0/\n</code></pre> <p>Load specific version:</p> <pre><code>registry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.0\",  # Specific version\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n</code></pre>"},{"location":"plugin-system/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"plugin-system/overview/#trust-model","title":"Trust Model","text":"<ul> <li>Official plugins: Verified and maintained by cuvis team</li> <li>Community plugins: User responsibility to vet and audit</li> <li>Private plugins: Under your organization's control</li> </ul>"},{"location":"plugin-system/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Review plugin source before installing from Git</li> <li>Check plugin author and repository reputation</li> <li>Use specific tags (not branches) for reproducibility</li> <li>Audit dependencies in <code>pyproject.toml</code></li> <li>Test plugins in isolation before production use</li> <li>Pin versions in manifests for production</li> </ol>"},{"location":"plugin-system/overview/#dependency-security","title":"Dependency Security","text":"<pre><code># \u2705 Good: Specific versions\n[project]\ndependencies = [\n    \"torch==2.9.1\",\n    \"numpy&gt;=1.20.0,&lt;2.0.0\"\n]\n\n# \u26a0\ufe0f Avoid: Unpinned versions in production\n[project]\ndependencies = [\n    \"torch\",  # Any version - risky!\n    \"numpy&gt;=1.0.0\"  # Too broad\n]\n</code></pre>"},{"location":"plugin-system/overview/#plugin-lifecycle","title":"Plugin Lifecycle","text":""},{"location":"plugin-system/overview/#plugin-methods","title":"Plugin Methods","text":"Method Purpose <code>load_plugin(name, config)</code> Load single plugin into instance <code>load_plugins(manifest_path)</code> Load multiple plugins from YAML <code>unload_plugin(name)</code> Remove plugin and its nodes <code>list_plugins()</code> Get loaded plugin names <code>clear_plugins()</code> Unload all plugins in instance <code>set_cache_dir(path)</code> Change plugin cache location <code>clear_plugin_cache(name)</code> Clear cached plugin files"},{"location":"plugin-system/overview/#plugin-lifecycle-states","title":"Plugin Lifecycle States","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Unloaded\n    Unloaded --&gt; Downloading: load_plugin (Git)\n    Unloaded --&gt; Resolving: load_plugin (Local)\n\n    Downloading --&gt; Cached: Clone &amp; Checkout\n    Resolving --&gt; PathResolved: Path Found\n\n    Cached --&gt; Installing: Extract Dependencies\n    PathResolved --&gt; Installing: Extract Dependencies\n\n    Installing --&gt; Importing: uv pip install\n    Importing --&gt; Registered: Import Classes\n    Registered --&gt; Active: Registration Complete\n\n    Active --&gt; Unloaded: unload_plugin()\n    Active --&gt; [*]: clear_plugins()</code></pre>"},{"location":"plugin-system/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"plugin-system/overview/#git-plugin-caching","title":"Git Plugin Caching","text":"<ul> <li>First load: Full clone (~5-30 seconds depending on repo size)</li> <li>Subsequent loads: Cache hit (~&lt;1 second)</li> <li>Optimization: Shallow clone with <code>depth=1</code> reduces clone time</li> </ul>"},{"location":"plugin-system/overview/#module-cache-clearing","title":"Module Cache Clearing","text":"<p>When loading plugins, the system clears Python's module cache to prevent conflicts:</p> <pre><code># Clears all modules matching plugin package prefixes\nclear_package_modules([\"cuvis_ai_adaclip\", \"my_plugin\"])\n</code></pre> <p>Impact: - Prevents version conflicts - Ensures fresh imports - Minimal overhead (&lt;100ms)</p>"},{"location":"plugin-system/overview/#dependency-installation","title":"Dependency Installation","text":"<ul> <li>Tool: Uses <code>uv pip install</code> for speed (~3-10x faster than pip)</li> <li>Timeout: 300 seconds</li> <li>Caching: Python packages cached by uv/pip</li> </ul>"},{"location":"plugin-system/overview/#grpc-plugin-service","title":"gRPC Plugin Service","text":"<p>For remote execution, plugins can be loaded via gRPC:</p> <pre><code>from cuvis_ai_core.grpc.plugin_service import PluginService\n\n# gRPC session maintains independent NodeRegistry\nsession.node_registry.load_plugins(\"plugins.yaml\")\n\n# Nodes available in session\nnodes = session.list_available_nodes()\n</code></pre> <p>Features: - Session-based isolation - Remote plugin loading - Port spec extraction for clients - Plugin info queries</p> <p>See gRPC Plugin Integration for details.</p>"},{"location":"plugin-system/overview/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plugin-system/overview/#plugin-not-loading","title":"Plugin Not Loading","text":"<p>Symptom: <pre><code>Error: Failed to load plugin 'my-plugin'\n</code></pre></p> <p>Solutions: 1. Check Git URL is correct and accessible 2. Verify tag exists in repository: <code>git ls-remote --tags &lt;repo_url&gt;</code> 3. Ensure <code>pyproject.toml</code> exists in plugin root 4. Check dependencies can be installed 5. Review error logs for specific failure</p>"},{"location":"plugin-system/overview/#node-not-found-after-loading","title":"Node Not Found After Loading","text":"<p>Symptom: <pre><code>NodeNotFoundError: Node 'CustomNode' not found\n</code></pre></p> <p>Solutions: 1. Verify <code>provides</code> list in manifest includes full class path 2. Check class name spelling matches exactly 3. Ensure plugin loaded successfully (no errors) 4. Use <code>registry.list_plugins()</code> to verify plugin is loaded 5. Try full path: <code>NodeRegistry.get(\"pkg.module.CustomNode\", instance=registry)</code></p>"},{"location":"plugin-system/overview/#import-errors","title":"Import Errors","text":"<p>Symptom: <pre><code>ImportError: cannot import name 'CustomNode' from 'my_plugin.nodes'\n</code></pre></p> <p>Solutions: 1. Verify <code>__init__.py</code> files exist in package directories 2. Check class path in <code>provides</code> is correct 3. Ensure all dependencies installed (check <code>pyproject.toml</code>) 4. Try importing manually to diagnose: <code>python -c \"from my_plugin.nodes import CustomNode\"</code></p>"},{"location":"plugin-system/overview/#cache-corruption","title":"Cache Corruption","text":"<p>Symptom: <pre><code>Error: Tag mismatch in cache for plugin 'my-plugin'\n</code></pre></p> <p>Solutions: 1. Clear plugin cache: <code>NodeRegistry.clear_plugin_cache(\"my-plugin\")</code> 2. Or manually delete: <code>rm -rf ~/.cuvis_plugins/my-plugin@*</code> 3. Reload plugin: <code>registry.load_plugin(...)</code></p>"},{"location":"plugin-system/overview/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>Symptom: <pre><code>Error: Dependency conflict: package X requires Y&gt;=2.0, but 1.5 is installed\n</code></pre></p> <p>Solutions: 1. Update conflicting package: <code>pip install --upgrade &lt;package&gt;</code> 2. Use compatible plugin version 3. Create isolated virtual environment 4. Check <code>pyproject.toml</code> for version constraints</p>"},{"location":"plugin-system/overview/#see-also","title":"See Also","text":"<ul> <li>Plugin Development Guide - Create your own plugins</li> <li>Plugin Usage Guide - Use plugins in your workflows</li> <li>Node System Deep Dive - Understand node architecture</li> <li>Pipeline Lifecycle - Pipeline integration</li> <li>gRPC API Reference - Remote plugin loading</li> </ul>"},{"location":"plugin-system/usage/","title":"Plugin Usage","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"plugin-system/usage/#plugin-usage-guide","title":"Plugin Usage Guide","text":""},{"location":"plugin-system/usage/#introduction","title":"Introduction","text":"<p>This guide covers how to find, install, and use plugins in your cuvis-ai workflows. Whether you're using plugins from Git repositories or local development, this guide provides practical examples for integrating plugin nodes into your pipelines.</p>"},{"location":"plugin-system/usage/#quick-start","title":"Quick Start","text":""},{"location":"plugin-system/usage/#install-and-use-a-plugin","title":"Install and Use a Plugin","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Create registry\nregistry = NodeRegistry()\n\n# Load plugin from Git\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Get node class\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n\n# Use in your workflow\ndetector = AdaCLIPDetector(prompt=\"plastic wrapper\", threshold=0.5)\noutputs = detector(data=your_hyperspectral_data)\n</code></pre>"},{"location":"plugin-system/usage/#finding-plugins","title":"Finding Plugins","text":""},{"location":"plugin-system/usage/#official-plugins","title":"Official Plugins","text":"<p>cuvis-ai maintains official plugins:</p> <ul> <li>cuvis-ai-adaclip - AdaCLIP vision-language anomaly detection</li> </ul>"},{"location":"plugin-system/usage/#community-plugins","title":"Community Plugins","text":"<p>Search GitHub topics: - cuvis-ai-plugin topic - Search for \"cuvis-ai\" + domain keywords</p>"},{"location":"plugin-system/usage/#using-the-central-plugin-registry","title":"Using the Central Plugin Registry","text":"<p>cuvis-ai maintains a central plugin registry containing all officially registered plugins. This provides a convenient way to discover and load community plugins without manually configuring each one.</p>"},{"location":"plugin-system/usage/#loading-from-central-registry","title":"Loading from Central Registry","text":"<p>Load all plugins from the central registry:</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Load all registered plugins\nregistry = NodeRegistry()\nregistry.load_plugins(\"configs/plugins/registry.yaml\")\n\n# All registered plugins now available\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n</code></pre> <p>Via CLI: <pre><code># Use central registry with restore-pipeline\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_pipeline.yaml \\\n    --plugins-path configs/plugins/registry.yaml\n</code></pre></p>"},{"location":"plugin-system/usage/#checking-registered-plugins","title":"Checking Registered Plugins","text":"<p>View which plugins are in the registry:</p> <pre><code># View registry contents\ncat configs/plugins/registry.yaml\n\n# Or in Python\nimport yaml\n\nwith open(\"configs/plugins/registry.yaml\") as f:\n    registry_data = yaml.safe_load(f)\n    plugins = registry_data.get(\"plugins\", {})\n\n    print(\"Registered Plugins:\")\n    for name, config in plugins.items():\n        print(f\"  - {name}: {config.get('repo', config.get('path'))}\")\n        print(f\"    Tag: {config.get('tag', 'N/A')}\")\n        print(f\"    Provides: {', '.join(config.get('provides', []))}\")\n</code></pre>"},{"location":"plugin-system/usage/#combining-central-registry-with-custom-plugins","title":"Combining Central Registry with Custom Plugins","text":"<p>You can create a custom manifest that references both registry plugins and your own:</p> <pre><code># my_plugins.yaml\nplugins:\n  # Load from central registry by copying entry\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n  # Add your custom local plugin\n  my_dev_plugin:\n    path: \"../my-plugin\"\n    provides:\n      - my_plugin.nodes.CustomNode\n\n  # Add your private Git plugin\n  private_plugin:\n    repo: \"git@github.com:myorg/private-plugin.git\"\n    tag: \"v1.0.0\"\n    provides:\n      - private_plugin.nodes.PrivateNode\n</code></pre> <p>Load combined manifest: <pre><code>registry = NodeRegistry()\nregistry.load_plugins(\"my_plugins.yaml\")\n\n# Now you have access to:\n# - Registry plugins (adaclip)\n# - Local development plugins (my_dev_plugin)\n# - Private plugins (private_plugin)\n</code></pre></p>"},{"location":"plugin-system/usage/#contributing-to-the-registry","title":"Contributing to the Registry","text":"<p>Want to add your plugin to the central registry? See the Contributing Guide for the submission process.</p> <p>Benefits of registry submission: - Easier discovery by the community - Official \"blessed\" quality signal - Automatic inclusion in central manifest - Better visibility for your work</p>"},{"location":"plugin-system/usage/#creating-plugin-manifests","title":"Creating Plugin Manifests","text":"<p>For reproducible workflows, create a <code>plugins.yaml</code> manifest:</p> <pre><code>plugins:\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n  custom_detector:\n    path: \"../my-custom-plugin\"\n    provides:\n      - custom_plugin.nodes.CustomDetector\n</code></pre>"},{"location":"plugin-system/usage/#installing-plugins","title":"Installing Plugins","text":""},{"location":"plugin-system/usage/#from-git-repository","title":"From Git Repository","text":"<p>Load plugins directly from Git repositories:</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\nregistry = NodeRegistry()\n\n# Load from GitHub (HTTPS)\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Load from GitHub (SSH)\nregistry.load_plugin(\n    name=\"private_plugin\",\n    config={\n        \"repo\": \"git@github.com:your-org/private-plugin.git\",\n        \"tag\": \"v2.0.0\",\n        \"provides\": [\"private_plugin.nodes.PrivateNode\"]\n    }\n)\n\n# Load from GitLab\nregistry.load_plugin(\n    name=\"gitlab_plugin\",\n    config={\n        \"repo\": \"https://gitlab.com/your-org/plugin.git\",\n        \"tag\": \"v1.5.0\",\n        \"provides\": [\"gitlab_plugin.nodes.CustomNode\"]\n    }\n)\n</code></pre> <p>Features: - Automatic caching in <code>~/.cuvis_plugins/</code> - Tag verification on subsequent loads - Dependency installation from <code>pyproject.toml</code></p>"},{"location":"plugin-system/usage/#from-local-path","title":"From Local Path","text":"<p>Load plugins from local filesystem (ideal for development):</p> <pre><code>registry = NodeRegistry()\n\n# Relative path\nregistry.load_plugin(\n    name=\"local_dev\",\n    config={\n        \"path\": \"../my-plugin\",\n        \"provides\": [\"my_plugin.nodes.CustomNode\"]\n    }\n)\n\n# Absolute path\nregistry.load_plugin(\n    name=\"local_prod\",\n    config={\n        \"path\": \"/absolute/path/to/plugin\",\n        \"provides\": [\"prod_plugin.nodes.MainNode\"]\n    }\n)\n</code></pre> <p>Use Cases: - Local development and testing - Private plugins not on Git - Enterprise internal plugins</p>"},{"location":"plugin-system/usage/#from-manifest-file","title":"From Manifest File","text":"<p>Load multiple plugins from a YAML manifest:</p> <pre><code>registry = NodeRegistry()\n\n# Load all plugins from manifest\nregistry.load_plugins(\"plugins.yaml\")\n\n# All plugins now available\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\nCustomNode = NodeRegistry.get(\"CustomNode\", instance=registry)\n</code></pre> <p>Manifest Example (<code>plugins.yaml</code>):</p> <pre><code>plugins:\n  # Production plugin with specific version\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n  # Pre-release plugin\n  experimental:\n    repo: \"https://github.com/company/experimental.git\"\n    tag: \"v2.0.0-beta.1\"\n    provides:\n      - experimental.features.NewFeatureNode\n\n  # Local development plugin\n  dev_plugin:\n    path: \"../dev-plugin\"\n    provides:\n      - dev_plugin.nodes.DevNode\n</code></pre>"},{"location":"plugin-system/usage/#using-plugin-nodes","title":"Using Plugin Nodes","text":""},{"location":"plugin-system/usage/#in-python-code","title":"In Python Code","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai_core.pipeline.pipeline import Pipeline\nimport numpy as np\n\n# Load plugins\nregistry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n\n# Get node class\nAdaCLIPDetector = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\n\n# Instantiate node\ndetector = AdaCLIPDetector(\n    prompt=\"plastic wrapper\",\n    threshold=0.5\n)\n\n# Use directly\ndata = np.random.randn(100, 100, 200).astype(np.float32)\noutputs = detector(data=data)\nprint(f\"Anomaly map shape: {outputs['anomaly_map'].shape}\")\n</code></pre>"},{"location":"plugin-system/usage/#in-pipeline-python","title":"In Pipeline (Python)","text":"<pre><code>from cuvis_ai_core.pipeline.pipeline import Pipeline\n\n# Define pipeline with plugin nodes\npipeline_dict = {\n    \"nodes\": [\n        {\n            \"class_name\": \"MinMaxNormalizer\",  # Built-in node\n            \"name\": \"normalizer\",\n            \"params\": {}\n        },\n        {\n            \"class_name\": \"AdaCLIPDetector\",  # Plugin node\n            \"name\": \"adaclip\",\n            \"params\": {\n                \"prompt\": \"plastic wrapper\",\n                \"threshold\": 0.5\n            }\n        }\n    ],\n    \"edges\": [\n        {\n            \"source\": \"normalizer.output\",\n            \"target\": \"adaclip.data\"\n        }\n    ]\n}\n\n# Create pipeline with plugin registry\npipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\n\n# Execute\noutputs = pipeline(data=input_data)\nanomaly_map = outputs[\"adaclip\"][\"anomaly_map\"]\n</code></pre>"},{"location":"plugin-system/usage/#in-pipeline-yaml","title":"In Pipeline (YAML)","text":"<p>Create <code>my_pipeline.yaml</code>:</p> <pre><code>nodes:\n  - name: normalizer\n    class_name: MinMaxNormalizer\n    params: {}\n\n  - name: adaclip_detector\n    class_name: AdaCLIPDetector  # From plugin\n    params:\n      prompt: \"plastic wrapper\"\n      threshold: 0.5\n\n  - name: threshold_selector\n    class_name: ThresholdSelector\n    params:\n      threshold: 0.9\n\nedges:\n  - source: normalizer.output\n    target: adaclip_detector.data\n\n  - source: adaclip_detector.anomaly_map\n    target: threshold_selector.data\n</code></pre> <p>Load and use:</p> <pre><code>import yaml\nfrom cuvis_ai_core.pipeline.pipeline import Pipeline\n\n# Load plugins\nregistry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n\n# Load pipeline config\nwith open(\"my_pipeline.yaml\") as f:\n    pipeline_dict = yaml.safe_load(f)\n\n# Create pipeline\npipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\n\n# Run\noutputs = pipeline(data=input_data)\n</code></pre>"},{"location":"plugin-system/usage/#via-cli-restore-pipeline","title":"Via CLI: restore-pipeline","text":"<p>Use the <code>restore-pipeline</code> CLI with plugins:</p> <pre><code># Basic usage\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_pipeline.yaml \\\n    --plugins-path plugins.yaml\n\n# With inference on CU3S file\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/adaclip_baseline.yaml \\\n    --plugins-path plugins.yaml \\\n    --cu3s-file-path data/test_sample.cu3s\n\n# With loaded weights\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/trained_pipeline.yaml \\\n    --plugins-path plugins.yaml \\\n    --weights-path outputs/trained_models/pipeline_weights.pt\n\n# Export visualization\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_pipeline.yaml \\\n    --plugins-path plugins.yaml \\\n    --pipeline-vis-ext png\n</code></pre>"},{"location":"plugin-system/usage/#via-cli-restore-trainrun","title":"Via CLI: restore-trainrun","text":"<p>Restore training runs with plugins:</p> <pre><code># Display trainrun info\nuv run restore-trainrun \\\n    --trainrun-path outputs/trained_models/trainrun.yaml\n\n# Re-run training\nuv run restore-trainrun \\\n    --trainrun-path outputs/trained_models/trainrun.yaml \\\n    --mode train\n\n# Validation with plugins\nuv run restore-trainrun \\\n    --trainrun-path outputs/trained_models/trainrun.yaml \\\n    --mode validate \\\n    --override validation.batch_size=8\n</code></pre>"},{"location":"plugin-system/usage/#managing-plugins","title":"Managing Plugins","text":""},{"location":"plugin-system/usage/#list-loaded-plugins","title":"List Loaded Plugins","text":"<pre><code>registry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")\n\n# Get loaded plugin names\nplugins = registry.list_plugins()\nprint(f\"Loaded plugins: {plugins}\")\n</code></pre>"},{"location":"plugin-system/usage/#check-plugin-details","title":"Check Plugin Details","text":"<pre><code># Check what nodes are available\nfrom cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# After loading plugins\nprint(f\"Available nodes: {list(registry.plugin_registry.keys())}\")\n\n# Get node info\nnode_class = NodeRegistry.get(\"AdaCLIPDetector\", instance=registry)\nprint(f\"Node class: {node_class}\")\nprint(f\"Input specs: {node_class.INPUT_SPECS}\")\nprint(f\"Output specs: {node_class.OUTPUT_SPECS}\")\n</code></pre>"},{"location":"plugin-system/usage/#unload-plugins","title":"Unload Plugins","text":"<pre><code># Unload specific plugin\nregistry.unload_plugin(\"adaclip\")\n\n# Clear all plugins\nregistry.clear_plugins()\n</code></pre>"},{"location":"plugin-system/usage/#plugin-cache-management","title":"Plugin Cache Management","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Set custom cache directory\nNodeRegistry.set_cache_dir(\"/custom/cache/location\")\n\n# Clear all plugin caches\nNodeRegistry.clear_plugin_cache()\n\n# Clear specific plugin cache\nNodeRegistry.clear_plugin_cache(\"adaclip\")\n</code></pre> <p>Manual Cache Management:</p> <pre><code># View cache contents\nls ~/.cuvis_plugins/\n\n# Output:\n# adaclip@v0.1.0/\n# adaclip@v0.1.1/\n# custom-plugin@v1.0.0/\n\n# Remove specific plugin version\nrm -rf ~/.cuvis_plugins/adaclip@v0.1.0/\n\n# Clear entire cache\nrm -rf ~/.cuvis_plugins/\n</code></pre>"},{"location":"plugin-system/usage/#version-management","title":"Version Management","text":""},{"location":"plugin-system/usage/#pinning-plugin-versions","title":"Pinning Plugin Versions","text":"<p>For reproducibility, always pin plugin versions in production:</p> <pre><code># \u2705 Good: Specific version pinned\nplugins:\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"  # Exact version\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n# \u26a0\ufe0f Avoid: No version control\n# (Don't use branches or latest)\n</code></pre>"},{"location":"plugin-system/usage/#using-multiple-versions","title":"Using Multiple Versions","text":"<p>Multiple versions can coexist in cache:</p> <pre><code>registry = NodeRegistry()\n\n# Load v0.1.0\nregistry.load_plugin(\n    name=\"adaclip_v0\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.0\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Load v0.1.1 (different instance)\nregistry2 = NodeRegistry()\nregistry2.load_plugin(\n    name=\"adaclip_v1\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# Both versions available in cache\n# ~/.cuvis_plugins/adaclip@v0.1.0/\n# ~/.cuvis_plugins/adaclip@v0.1.1/\n</code></pre>"},{"location":"plugin-system/usage/#updating-plugins","title":"Updating Plugins","text":"<p>To update a plugin to a new version:</p> <ol> <li> <p>Update manifest: <pre><code>plugins:\n  adaclip:\n    tag: \"v0.2.0\"  # Changed from v0.1.1\n    # ... rest of config\n</code></pre></p> </li> <li> <p>Clear old cache (optional): <pre><code>NodeRegistry.clear_plugin_cache(\"adaclip\")\n</code></pre></p> </li> <li> <p>Reload: <pre><code>registry.load_plugins(\"plugins.yaml\")\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/usage/#configuration-with-plugins","title":"Configuration with Plugins","text":""},{"location":"plugin-system/usage/#using-plugin-configurations","title":"Using Plugin Configurations","text":"<p>If a plugin provides Hydra configs, you can override them:</p> <pre><code># Via CLI\nuv run restore-pipeline \\\n    --pipeline-path configs/pipeline/my_pipeline.yaml \\\n    --plugins-path plugins.yaml \\\n    --override adaclip.threshold=0.7 \\\n    --override adaclip.prompt=\"defective area\"\n</code></pre> <p>In Python:</p> <pre><code>from hydra import compose, initialize\nfrom omegaconf import OmegaConf\n\n# Initialize Hydra\nwith initialize(config_path=\"../configs\"):\n    cfg = compose(config_name=\"config.yaml\")\n\n    # Override plugin configs\n    cfg.adaclip.threshold = 0.7\n    cfg.adaclip.prompt = \"defective area\"\n\n    # Use in pipeline\n    # ...\n</code></pre>"},{"location":"plugin-system/usage/#plugin-config-files","title":"Plugin Config Files","text":"<p>If plugin provides config files in <code>configs/</code>, they're available in Hydra:</p> <pre><code># Plugin provides: cuvis_ai_plugin/configs/default.yaml\ncustom_detector:\n  threshold: 0.95\n  method: \"advanced\"\n  window_size: 5\n</code></pre> <p>Access in your config:</p> <pre><code># your_config.yaml\ndefaults:\n  - custom_detector: default  # Load plugin's default config\n\npipeline:\n  nodes:\n    - name: detector\n      class_name: CustomDetector\n      params: ${custom_detector}  # Use plugin config\n</code></pre>"},{"location":"plugin-system/usage/#examples","title":"Examples","text":""},{"location":"plugin-system/usage/#example-1-adaclip-plugin","title":"Example 1: AdaCLIP Plugin","text":"<p>Complete workflow using AdaCLIP plugin:</p> <pre><code>import numpy as np\nfrom cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai_core.pipeline.pipeline import Pipeline\n\n# 1. Load plugin\nregistry = NodeRegistry()\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.1\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n\n# 2. Create pipeline with AdaCLIP\npipeline_dict = {\n    \"nodes\": [\n        {\n            \"class_name\": \"MinMaxNormalizer\",\n            \"name\": \"normalizer\",\n            \"params\": {}\n        },\n        {\n            \"class_name\": \"AdaCLIPDetector\",\n            \"name\": \"adaclip\",\n            \"params\": {\n                \"prompt\": \"plastic wrapper\",\n                \"threshold\": 0.5\n            }\n        }\n    ],\n    \"edges\": [\n        {\n            \"source\": \"normalizer.output\",\n            \"target\": \"adaclip.data\"\n        }\n    ]\n}\n\npipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\n\n# 3. Run pipeline\nhyperspectral_data = np.random.randn(100, 100, 200).astype(np.float32)\noutputs = pipeline(data=hyperspectral_data)\n\n# 4. Extract results\nanomaly_map = outputs[\"adaclip\"][\"anomaly_map\"]\nprint(f\"Detected anomalies: {anomaly_map.sum()} pixels\")\n</code></pre>"},{"location":"plugin-system/usage/#example-2-mixed-built-in-and-plugin-nodes","title":"Example 2: Mixed Built-in and Plugin Nodes","text":"<p>Combine built-in and plugin nodes:</p> <pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\nfrom cuvis_ai_core.pipeline.pipeline import Pipeline\n\n# Load plugins\nregistry = NodeRegistry()\nregistry.load_plugins(\"plugins.yaml\")  # Loads multiple plugins\n\n# Create pipeline mixing built-in and plugin nodes\npipeline_dict = {\n    \"nodes\": [\n        # Built-in nodes\n        {\"class_name\": \"DataLoaderNode\", \"name\": \"loader\", \"params\": {\"path\": \"data/\"}},\n        {\"class_name\": \"MinMaxNormalizer\", \"name\": \"normalizer\", \"params\": {}},\n\n        # Plugin node 1\n        {\"class_name\": \"CustomDetector\", \"name\": \"custom\", \"params\": {\"threshold\": 0.9}},\n\n        # Plugin node 2\n        {\"class_name\": \"AdaCLIPDetector\", \"name\": \"adaclip\", \"params\": {\"prompt\": \"defect\"}},\n\n        # Built-in node\n        {\"class_name\": \"ThresholdSelector\", \"name\": \"selector\", \"params\": {\"threshold\": 0.8}}\n    ],\n    \"edges\": [\n        {\"source\": \"loader.data\", \"target\": \"normalizer.data\"},\n        {\"source\": \"normalizer.output\", \"target\": \"custom.data\"},\n        {\"source\": \"normalizer.output\", \"target\": \"adaclip.data\"},\n        {\"source\": \"adaclip.anomaly_map\", \"target\": \"selector.data\"}\n    ]\n}\n\npipeline = Pipeline.from_dict(pipeline_dict, node_registry=registry)\noutputs = pipeline()\n</code></pre>"},{"location":"plugin-system/usage/#example-3-local-development-workflow","title":"Example 3: Local Development Workflow","text":"<p>Develop and test plugin locally:</p> <pre><code># 1. Create plugin structure\nmkdir my-plugin &amp;&amp; cd my-plugin\n# ... create plugin files (see Development Guide)\n\n# 2. Install in editable mode\npip install -e .\n\n# 3. Create test manifest\ncat &gt; test_plugins.yaml &lt;&lt; EOF\nplugins:\n  my_dev_plugin:\n    path: \".\"\n    provides:\n      - my_plugin.nodes.CustomNode\nEOF\n\n# 4. Test plugin\nuv run python &lt;&lt; 'PYTHON'\nfrom cuvis_ai_core.utils.node_registry import NodeRegistry\n\nregistry = NodeRegistry()\nregistry.load_plugins(\"test_plugins.yaml\")\n\nCustomNode = registry.get(\"CustomNode\", instance=registry)\nprint(f\"\u2713 Plugin loaded: {CustomNode}\")\nPYTHON\n</code></pre>"},{"location":"plugin-system/usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plugin-system/usage/#plugin-not-found","title":"Plugin Not Found","text":"<p>Symptom: <pre><code>Error: Failed to load plugin 'my-plugin'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check Git URL and accessibility: <pre><code>git ls-remote https://github.com/your-org/plugin.git\n</code></pre></p> </li> <li> <p>Verify tag exists: <pre><code>git ls-remote --tags https://github.com/your-org/plugin.git | grep v0.1.0\n</code></pre></p> </li> <li> <p>Check local path: <pre><code>ls ../my-plugin/pyproject.toml\n</code></pre></p> </li> <li> <p>Verify network connectivity: <pre><code>curl -I https://github.com/your-org/plugin.git\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/usage/#node-not-found-after-loading","title":"Node Not Found After Loading","text":"<p>Symptom: <pre><code>NodeNotFoundError: Node 'CustomNode' not found\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check plugin loaded successfully: <pre><code>plugins = registry.list_plugins()\nprint(f\"Loaded: {plugins}\")\n</code></pre></p> </li> <li> <p>Verify node in plugin registry: <pre><code>print(f\"Available nodes: {list(registry.plugin_registry.keys())}\")\n</code></pre></p> </li> <li> <p>Try full class path: <pre><code>node = NodeRegistry.get(\n    \"my_plugin.nodes.custom_node.CustomNode\",\n    instance=registry\n)\n</code></pre></p> </li> <li> <p>Check manifest <code>provides</code> list: <pre><code>plugins:\n  my_plugin:\n    provides:\n      - my_plugin.nodes.custom_node.CustomNode  # Must match exactly\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/usage/#import-errors","title":"Import Errors","text":"<p>Symptom: <pre><code>ImportError: cannot import name 'CustomNode' from 'my_plugin.nodes'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check <code>__init__.py</code> files exist: <pre><code>find my-plugin -name \"__init__.py\"\n</code></pre></p> </li> <li> <p>Verify dependencies installed: <pre><code>pip list | grep -i torch\npip list | grep -i numpy\n</code></pre></p> </li> <li> <p>Test import manually: <pre><code>python -c \"from my_plugin.nodes import CustomNode\"\n</code></pre></p> </li> <li> <p>Check <code>pyproject.toml</code> dependencies: <pre><code>cat my-plugin/pyproject.toml | grep dependencies -A 5\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/usage/#cache-issues","title":"Cache Issues","text":"<p>Symptom: <pre><code>Error: Tag mismatch in cache for plugin 'adaclip'\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Clear plugin cache: <pre><code>NodeRegistry.clear_plugin_cache(\"adaclip\")\n</code></pre></p> </li> <li> <p>Manually remove cache: <pre><code>rm -rf ~/.cuvis_plugins/adaclip@*\n</code></pre></p> </li> <li> <p>Reload plugin: <pre><code>registry.load_plugin(...)\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/usage/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>Symptom: <pre><code>Error: cuvis-ai-adaclip requires torch&gt;=2.9, but 2.0 is installed\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Update conflicting package: <pre><code>pip install --upgrade torch\n</code></pre></p> </li> <li> <p>Use compatible plugin version: <pre><code>plugins:\n  adaclip:\n    tag: \"v0.1.0\"  # Older version with compatible deps\n</code></pre></p> </li> <li> <p>Create isolated environment: <pre><code>python -m venv plugin_env\nsource plugin_env/bin/activate\npip install cuvis-ai torch&gt;=2.9\n</code></pre></p> </li> <li> <p>Check version constraints: <pre><code>pip show cuvis-ai-adaclip | grep Requires\n</code></pre></p> </li> </ol>"},{"location":"plugin-system/usage/#performance-issues","title":"Performance Issues","text":"<p>Symptom: Plugin node is very slow.</p> <p>Solutions:</p> <ol> <li> <p>Check if dependencies are GPU-enabled: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</code></pre></p> </li> <li> <p>Profile plugin execution: <pre><code>import time\nstart = time.time()\noutputs = node(data=data)\nprint(f\"Execution time: {time.time() - start:.2f}s\")\n</code></pre></p> </li> <li> <p>Use smaller data for testing: <pre><code># Instead of full data\ntest_data = data[:10, :10, :]  # Small subset\n</code></pre></p> </li> <li> <p>Check plugin documentation for performance tips</p> </li> </ol>"},{"location":"plugin-system/usage/#best-practices","title":"Best Practices","text":""},{"location":"plugin-system/usage/#1-version-pinning","title":"1. Version Pinning","text":"<pre><code># \u2705 Good: Pin versions for reproducibility\nplugins:\n  adaclip:\n    repo: \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\"\n    tag: \"v0.1.1\"  # Exact version\n    provides:\n      - cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\n\n# \u26a0\ufe0f Avoid: Using branches or latest\n</code></pre>"},{"location":"plugin-system/usage/#2-manifest-files","title":"2. Manifest Files","text":"<pre><code># \u2705 Good: Use manifest for multiple plugins\nregistry.load_plugins(\"plugins.yaml\")\n\n# \u26a0\ufe0f Avoid: Multiple individual loads (harder to maintain)\nregistry.load_plugin(\"adaclip\", config={...})\nregistry.load_plugin(\"custom\", config={...})\nregistry.load_plugin(\"another\", config={...})\n</code></pre>"},{"location":"plugin-system/usage/#3-virtual-environments","title":"3. Virtual Environments","text":"<pre><code># \u2705 Good: Separate environments per project\npython -m venv project1_env\nsource project1_env/bin/activate\npip install cuvis-ai\n# Load project1 plugins\n\npython -m venv project2_env\nsource project2_env/bin/activate\npip install cuvis-ai\n# Load project2 plugins\n</code></pre>"},{"location":"plugin-system/usage/#4-testing","title":"4. Testing","text":"<pre><code># \u2705 Good: Test plugin in isolation first\ndef test_plugin_isolation():\n    registry = NodeRegistry()\n    registry.load_plugin(\"my_plugin\", config={...})\n\n    Node = registry.get(\"CustomNode\", instance=registry)\n    node = Node()\n\n    # Test with sample data\n    test_data = np.random.randn(10, 10, 50).astype(np.float32)\n    outputs = node(data=test_data)\n\n    assert outputs is not None\n</code></pre>"},{"location":"plugin-system/usage/#5-error-handling","title":"5. Error Handling","text":"<pre><code># \u2705 Good: Handle plugin loading errors\nregistry = NodeRegistry()\n\ntry:\n    registry.load_plugins(\"plugins.yaml\")\nexcept Exception as e:\n    print(f\"Failed to load plugins: {e}\")\n    print(\"Falling back to built-in nodes only\")\n    # Continue with built-in nodes\n</code></pre>"},{"location":"plugin-system/usage/#see-also","title":"See Also","text":"<ul> <li>Plugin System Overview - Architecture and concepts</li> <li>Plugin Development Guide - Create your own plugins</li> <li>Node Catalog - Built-in nodes reference</li> <li>Pipeline Lifecycle - Pipeline integration</li> <li>Configuration Guide - Hydra configuration</li> </ul>"},{"location":"reference/changelog/","title":"Changelog","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"reference/changelog/#changelog","title":"Changelog","text":"<p>This page provides a high-level overview of cuvis-ai releases. For the complete changelog with all technical details, see the main CHANGELOG.md.</p>"},{"location":"reference/changelog/#latest-release","title":"Latest Release","text":""},{"location":"reference/changelog/#version-024-february-5-2026","title":"Version 0.2.4 (February 5, 2026)","text":"<p>Documentation Complete! This release marks the completion of comprehensive documentation for cuvis-ai, making the framework more accessible and easier to use for everyone.</p> <p>Highlights:</p> <ul> <li>70+ Documentation Pages covering all aspects of the framework</li> <li>6 Step-by-Step Tutorials for common workflows (RX Statistical, Channel Selector, Deep SVDD, AdaCLIP, gRPC, Remote Deployment)</li> <li>Complete Node Catalog documenting 50+ built-in nodes across 11 categories</li> <li>Plugin System Guides for extending the framework with custom nodes</li> <li>gRPC API Documentation with sequence diagrams and client patterns</li> <li>Configuration References for Hydra composition and schema definitions</li> <li>95%+ Docstring Coverage across all public APIs</li> <li>Central Plugin Registry for community plugin discovery</li> </ul> <p>New Documentation:</p> <ul> <li>API reference with auto-generated documentation from docstrings</li> <li>7 how-to guides for building pipelines, restoration, monitoring, and more</li> <li>Development guides for contributing and maintaining code quality</li> <li>20+ diagrams (Mermaid + Graphviz) for visual understanding</li> </ul> <p>Improvements:</p> <ul> <li>README refactored to eliminate duplicate content</li> <li>Contributing guide enhanced with complete plugin contribution workflow</li> <li>All code examples updated to use <code>uv run</code> pattern</li> <li>MkDocs configuration with Material theme and advanced search</li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#previous-releases","title":"Previous Releases","text":""},{"location":"reference/changelog/#version-023-january-29-2026","title":"Version 0.2.3 (January 29, 2026)","text":"<p>Architecture Evolution: Major refactoring to split cuvis-ai into core framework (cuvis-ai-core) and algorithm catalog (cuvis-ai).</p> <p>Key Changes:</p> <ul> <li>Plugin system with dynamic loading from Git repositories and local filesystem</li> <li>426 tests migrated to cuvis-ai-core with independent CI/CD</li> <li>Session-scoped plugin isolation for gRPC services</li> <li>Import pattern changes to <code>cuvis_ai_core.*</code></li> <li>New gRPC RPCs for plugin management</li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#version-022-january-15-2026","title":"Version 0.2.2 (January 15, 2026)","text":"<p>Restoration Utilities: Simplified pipeline and trainrun restoration with auto-detection.</p> <p>Key Changes:</p> <ul> <li>New CLI commands: <code>restore-pipeline</code>, <code>restore-trainrun</code></li> <li>Auto-detection of statistical vs gradient training workflows</li> <li>Consolidated restoration utilities in single module</li> <li>Standardized Python API surface</li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#version-021-january-8-2026","title":"Version 0.2.1 (January 8, 2026)","text":"<p>Configuration System: Pydantic v2 models and server-side Hydra composition.</p> <p>Key Changes:</p> <ul> <li>Pydantic v2 config models as single source of truth</li> <li>Session-scoped configuration management</li> <li>Terminology change: Experiment \u2192 TrainRun</li> <li>Explicit 4-step workflow for gRPC API</li> <li>Standardized config transport with <code>config_bytes</code></li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#version-020-december-20-2025","title":"Version 0.2.0 (December 20, 2025)","text":"<p>YAML-Driven Pipelines: Introduced configuration-based pipeline building.</p> <p>Key Changes:</p> <ul> <li>YAML-driven pipeline configuration with OmegaConf</li> <li>Hybrid NodeRegistry for built-ins and custom nodes</li> <li>End-to-end serialization (YAML + .pt weights)</li> <li>Version compatibility guards</li> <li>Simplified node state management</li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#version-015-december-1-2025","title":"Version 0.1.5 (December 1, 2025)","text":"<p>gRPC Services: Introduced remote API for pipeline execution.</p> <p>Key Changes:</p> <ul> <li>gRPC service stack with proto definitions</li> <li>Buf Schema Registry integration</li> <li>Session management and PipelineBuilder</li> <li>Two-phase training support</li> <li>Pipeline introspection RPCs</li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#version-013-november-6-2025","title":"Version 0.1.3 (November 6, 2025)","text":"<p>Port-Based I/O: Major refactor introducing typed input/output system.</p> <p>Key Changes:</p> <ul> <li>Port-based typed I/O with PortSpec</li> <li>Graph connection API with auto-validation</li> <li>Multi-input/output support</li> <li>PyTorch Lightning integration</li> <li>Executor refactored for port-based routing</li> </ul> <p>View Full Release Notes \u2192</p>"},{"location":"reference/changelog/#versioning","title":"Versioning","text":"<p>cuvis-ai follows Semantic Versioning (MAJOR.MINOR.PATCH):</p> <ul> <li>MAJOR: Incompatible API changes</li> <li>MINOR: New functionality (backwards-compatible)</li> <li>PATCH: Bug fixes (backwards-compatible)</li> </ul>"},{"location":"reference/changelog/#release-channels","title":"Release Channels","text":"<ul> <li>Stable: Use tagged releases for production deployments</li> <li>Latest: Track the <code>main</code> branch for latest stable code</li> <li>Development: Track feature branches for experimental features</li> </ul>"},{"location":"reference/changelog/#see-also","title":"See Also","text":"<ul> <li>Installation Guide - Get started with cuvis-ai</li> <li>GitHub Releases - Download specific versions</li> <li>CHANGELOG.md - Complete technical changelog</li> <li>Contributing Guide - How to contribute to cuvis-ai</li> </ul>"},{"location":"reference/glossary/","title":"Glossary","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"reference/glossary/#glossary","title":"Glossary","text":"<p>Status: Under Development (Phase 6) Expected Completion: Week 6</p>"},{"location":"reference/glossary/#coming-soon","title":"Coming Soon","text":"<p>Comprehensive glossary of terms used in CUVIS.AI including: - Node - Port - Pipeline - TrainRun - Phase (training) - Stage (execution) - And more...</p> <p>Related Pages: - Core Concepts Overview</p>"},{"location":"reference/gpu_acceleration_guide/","title":"GPU Acceleration","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"reference/gpu_acceleration_guide/#gpu-acceleration-guide","title":"GPU Acceleration Guide","text":"<p>Status: Under Development (Phase 4) Expected Completion: Week 4</p>"},{"location":"reference/gpu_acceleration_guide/#coming-soon","title":"Coming Soon","text":"<p>This guide will cover: - GPU requirements and setup - CUDA configuration - GPU memory management - Multi-GPU training strategies - Performance optimization tips - Troubleshooting GPU issues</p> <p>Related Pages: - Installation - Deep Learning Nodes - Deep SVDD Gradient Tutorial</p>"},{"location":"reference/stress_testing/","title":"Stress Testing","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"reference/stress_testing/#stress-testing-guide","title":"Stress Testing Guide","text":"<p>Status: Under Development (Phase 6) Expected Completion: Week 6</p>"},{"location":"reference/stress_testing/#coming-soon","title":"Coming Soon","text":"<p>This guide will cover: - Load testing pipelines - Benchmarking tools and methodologies - Performance metrics collection - Identifying bottlenecks - Stress testing gRPC services - CI/CD integration for performance testing</p> <p>Related Pages: - Deployment Overview - Logging &amp; Monitoring</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<p>These step-by-step guides will help you learn how to use the framework effectively.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":"<ul> <li> <p> RX Statistical Training</p> <p>Learn the RX anomaly detection algorithm and statistical training workflow</p> </li> <li> <p> Channel Selector</p> <p>Discover how to perform channel selection for hyperspectral data</p> </li> <li> <p> Deep SVDD Gradient Training</p> <p>Build deep learning pipelines with gradient-based training</p> </li> <li> <p>:material-image-filter: AdaCLIP Workflow</p> <p>Work with adaptive CLIP models for image analysis</p> </li> <li> <p> gRPC Workflow</p> <p>Deploy and use CUVIS.AI pipelines via gRPC services</p> </li> </ul> <p>Note: Tutorials are organized from basic to advanced. Start with RX Statistical if you're new to the framework.</p>"},{"location":"tutorials/adaclip-workflow/","title":"AdaCLIP Workflow","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"tutorials/adaclip-workflow/#adaclip-workflow-plugin-based-anomaly-detection","title":"AdaCLIP Workflow: Plugin-Based Anomaly Detection","text":"<p>Learn how to use plugin nodes with AdaCLIP for hyperspectral anomaly detection, comparing three dimensionality reduction approaches.</p>"},{"location":"tutorials/adaclip-workflow/#overview","title":"Overview","text":"<p>This tutorial demonstrates plugin-based anomaly detection using the AdaCLIP plugin, which adapts CLIP (Contrastive Language-Image Pre-training) for hyperspectral imagery. You'll learn three approaches to reducing 61 hyperspectral channels to 3 RGB-compatible channels for AdaCLIP processing.</p> <p>What You'll Learn: - Loading external plugin nodes from Git repositories - PCA Baseline - Statistical-only frozen reduction (fastest) - DRCNN Mixer - Learnable continuous mixing with gradient optimization - Concrete Selector - Learnable discrete band selection - Performance comparison and visualization strategies - TensorBoard monitoring of end-to-end processing</p> <p>Time to Complete: 35-40 minutes (all 3 variants)</p> <p>Prerequisites: - Completion of Channel Selector Tutorial - Understanding two-phase training - Familiarity with Plugin System Overview - Python 3.10+, PyTorch 2.0+, CUDA-capable GPU (recommended)</p>"},{"location":"tutorials/adaclip-workflow/#background","title":"Background","text":""},{"location":"tutorials/adaclip-workflow/#adaclip-for-hyperspectral-anomaly-detection","title":"AdaCLIP for Hyperspectral Anomaly Detection","text":"<p>AdaCLIP adapts OpenAI's CLIP vision-language model for hyperspectral anomaly detection. CLIP was trained on millions of image-text pairs and excels at zero-shot visual understanding, making it powerful for detecting anomalies without extensive task-specific training.</p> <p>Key Challenge: CLIP expects 3-channel RGB images, but hyperspectral data has 60+ channels. Solution: Learn an optimal mapping from hyperspectral \u2192 RGB that preserves anomaly-relevant information.</p> <p>Three Approaches Compared:</p> Approach Type Training Speed Flexibility Best For PCA Baseline Linear projection Statistical-only \u26a1 Fastest \u274c Frozen Quick baseline, comparison DRCNN Mixer Multi-layer convolution Two-phase (stat + grad) \ud83d\udd25 Moderate \u2705 Continuous mixing End-to-end optimization Concrete Selector Gumbel-Softmax sampling Gradient-only \ud83d\udd25 Moderate \u2705 Discrete selection Interpretable band choices"},{"location":"tutorials/adaclip-workflow/#plugin-system-integration","title":"Plugin System Integration","text":"<p>Unlike built-in nodes, plugin nodes are loaded dynamically from external Git repositories. This allows: - Modularity - Keep experimental/specialized nodes separate from core framework - Versioning - Pin to specific plugin releases (tags) for reproducibility - Community Extensions - Share custom nodes without modifying core codebase</p> <p>See Plugin System Usage for installation details.</p> <p> </p>"},{"location":"tutorials/adaclip-workflow/#approach-1-pca-baseline-statistical-only","title":"Approach 1: PCA Baseline (Statistical-Only)","text":""},{"location":"tutorials/adaclip-workflow/#when-to-use-pca-baseline","title":"When to Use PCA Baseline","text":"<ul> <li>Quick evaluation - No gradient training required</li> <li>Baseline comparison - Measure improvement of learnable methods</li> <li>CPU-friendly - Statistical SVD decomposition is efficient</li> <li>Interpretable - Principal components explain variance</li> </ul> <p>Limitations: Frozen after initialization, may not capture task-specific anomaly features.</p>"},{"location":"tutorials/adaclip-workflow/#step-1-load-adaclip-plugin","title":"Step 1: Load AdaCLIP Plugin","text":"<pre><code>from cuvis_ai_core.utils.node_registry import NodeRegistry\n\n# Load plugin from GitHub repository\nregistry = NodeRegistry()\nregistry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"repo\": \"https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\",\n        \"tag\": \"v0.1.0\",  # Pin to specific version\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"],\n    },\n)\n\n# Get the AdaCLIPDetector class\nAdaCLIPDetector = NodeRegistry.get(\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\")\n</code></pre> <p>For local development: <pre><code>registry.load_plugin(\n    name=\"adaclip\",\n    config={\n        \"path\": \"/path/to/local/cuvis-ai-adaclip\",\n        \"provides\": [\"cuvis_ai_adaclip.node.adaclip_node.AdaCLIPDetector\"]\n    }\n)\n</code></pre></p>"},{"location":"tutorials/adaclip-workflow/#step-2-build-pca-pipeline","title":"Step 2: Build PCA Pipeline","text":"<pre><code>from cuvis_ai.node.pca import TrainablePCA\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.deciders.binary_decider import QuantileBinaryDecider\n\n# Data and preprocessing\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True, name=\"hsi_normalizer\")\n\n# PCA: 61 \u2192 3 channels (frozen after initialization)\npca = TrainablePCA(\n    n_components=3,          # RGB compatibility\n    whiten=False,            # Don't whiten - normalize separately\n    init_method=\"svd\",       # SVD decomposition\n    eps=1e-6,\n    name=\"pca_baseline\",\n)\n\n# Normalize PCA output to [0, 1] for AdaCLIP\npca_normalizer = MinMaxNormalizer(\n    eps=1e-6,\n    use_running_stats=False,  # Per-image normalization\n    name=\"pca_output_normalizer\",\n)\n\n# AdaCLIP detector (FROZEN)\nadaclip = AdaCLIPDetector(\n    weight_name=\"pretrained_all\",\n    backbone=\"ViT-L-14-336\",\n    prompt_text=\"normal: lentils, anomaly: stones\",\n    image_size=518,\n    prompting_depth=4,\n    prompting_length=5,\n    gaussian_sigma=4.0,\n    use_half_precision=True,\n    enable_warmup=True,\n    enable_gradients=False,  # No gradients needed\n    name=\"adaclip\",\n)\n\n# Decision threshold\ndecider = QuantileBinaryDecider(quantile=0.995, name=\"decider\")\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-3-connect-pipeline","title":"Step 3: Connect Pipeline","text":"<pre><code>pipeline.connect(\n    # Preprocessing: HSI \u2192 Normalizer \u2192 PCA \u2192 Normalize \u2192 AdaCLIP\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, pca.data),\n    (pca.projected, pca_normalizer.data),\n    (pca_normalizer.normalized, adaclip.rgb_image),\n\n    # Inference: Scores \u2192 Decider \u2192 Metrics\n    (adaclip.scores, decider.logits),\n    (adaclip.scores, metrics_node.logits),\n    (decider.decisions, metrics_node.decisions),\n    (data_node.outputs.mask, metrics_node.targets),\n\n    # Monitoring\n    (metrics_node.metrics, tensorboard_node.metrics),\n)\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-4-statistical-training","title":"Step 4: Statistical Training","text":"<pre><code>from cuvis_ai_core.training import StatisticalTrainer\n\n# Initialize PCA and normalizer\nstat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\nstat_trainer.fit()\n\n# No gradient training - PCA is frozen baseline\nlogger.info(\"PCA baseline complete - no gradient training\")\n\n# Evaluate\nstat_trainer.validate()\nstat_trainer.test()\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-5-analyze-pca-results","title":"Step 5: Analyze PCA Results","text":"<pre><code># Check explained variance\nwith torch.no_grad():\n    sample_batch = next(iter(train_loader))\n    sample_cube = sample_batch[\"cube\"].float()\n    normalizer_output = normalizer.forward(data=sample_cube)\n    pca_output = pca.forward(data=normalizer_output[\"normalized\"])\n\n    ev_ratio = pca_output.get(\"explained_variance_ratio\", torch.zeros(3))\n    logger.info(f\"Explained variance ratio: {ev_ratio.tolist()}\")\n    logger.info(f\"Total explained variance: {ev_ratio.sum().item():.4f}\")\n</code></pre> <p>Expected Output: <pre><code>Explained variance ratio: [0.4523, 0.2891, 0.1456]\nTotal explained variance: 0.8870\n</code></pre></p> <p>Interpretation: First 3 principal components capture ~89% of spectral variance, but may miss subtle anomaly-specific patterns.</p> <p></p>"},{"location":"tutorials/adaclip-workflow/#approach-2-drcnn-mixer-learnable-continuous-mixing","title":"Approach 2: DRCNN Mixer (Learnable Continuous Mixing)","text":""},{"location":"tutorials/adaclip-workflow/#when-to-use-drcnn-mixer","title":"When to Use DRCNN Mixer","text":"<ul> <li>End-to-end optimization - Learn task-specific channel mixing</li> <li>Continuous blending - Smooth combinations of spectral bands</li> <li>Gradient-based refinement - Optimize directly for IoU or detection loss</li> <li>Multi-layer reduction - Gradual dimensionality decrease</li> </ul> <p>Based on: Zeegers et al., \"Task-Driven Learned Hyperspectral Data Reduction Using End-to-End Supervised Deep Learning,\" J. Imaging 6(12):132, 2020.</p>"},{"location":"tutorials/adaclip-workflow/#step-1-build-drcnn-pipeline","title":"Step 1: Build DRCNN Pipeline","text":"<pre><code>from cuvis_ai.node.channel_mixer import LearnableChannelMixer\nfrom cuvis_ai.node.losses import IoULoss\n\n# Data preprocessing\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 1],\n    anomaly_class_ids=[3],  # Only Stone as anomaly for IoU loss\n)\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True)\n\n# DRCNN-style channel mixer: 61 \u2192 3 channels\nmixer = LearnableChannelMixer(\n    input_channels=61,\n    output_channels=3,              # RGB compatibility\n    leaky_relu_negative_slope=0.1,\n    use_bias=True,\n    use_activation=True,\n    normalize_output=True,          # Per-image min-max \u2192 [0, 1]\n    init_method=\"pca\",              # Initialize with PCA (requires statistical fit)\n    eps=1e-6,\n    reduction_scheme=[61, 16, 8, 3],  # Multi-layer gradual reduction\n    name=\"channel_mixer\",\n)\n\n# AdaCLIP detector (FROZEN, but gradients flow through)\nadaclip = AdaCLIPDetector(\n    weight_name=\"pretrained_all\",\n    backbone=\"ViT-L-14-336\",\n    prompt_text=\"\",  # Empty for zero-shot\n    image_size=518,\n    prompting_depth=4,\n    prompting_length=5,\n    gaussian_sigma=4.0,\n    use_half_precision=False,\n    enable_warmup=False,\n    enable_gradients=True,  # CRITICAL: Allow gradients to flow\n    name=\"adaclip\",\n)\n\n# IoU loss (differentiable, works on continuous scores)\niou_loss = IoULoss(\n    weight=1.0,\n    smooth=1e-6,\n    normalize_method=\"minmax\",  # Preserve dynamic range\n    name=\"iou_loss\",\n)\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-2-two-phase-training","title":"Step 2: Two-Phase Training","text":"<p>Phase 1: Statistical Initialization <pre><code>if mixer.requires_initial_fit:\n    logger.info(\"Phase 1: Statistical initialization (PCA)...\")\n    stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n    stat_trainer.fit()\nelse:\n    logger.info(\"Phase 1: Skipping (using weight init)\")\n</code></pre></p> <p>Phase 2: Unfreeze Mixer <pre><code>logger.info(\"Phase 2: Unfreezing channel mixer...\")\npipeline.unfreeze_nodes_by_name([mixer.name])\nlogger.info(\"AdaClip remains frozen (enable_gradients=True allows gradient flow)\")\n</code></pre></p> <p>Phase 3: Gradient Training <pre><code>from cuvis_ai_core.training import GradientTrainer\nfrom cuvis_ai_core.training.config import ModelCheckpointConfig, SchedulerConfig\n\ntraining_cfg = TrainingConfig.from_dict(cfg.training)\ntraining_cfg.trainer.callbacks.checkpoint = ModelCheckpointConfig(\n    dirpath=str(output_dir / \"checkpoints\"),\n    monitor=\"metrics_anomaly/iou\",\n    mode=\"max\",\n    save_top_k=3,\n    save_last=True,\n)\n\ntraining_cfg.scheduler = SchedulerConfig(\n    name=\"reduce_on_plateau\",\n    monitor=\"metrics_anomaly/iou\",\n    mode=\"max\",\n    factor=0.5,\n    patience=5,\n)\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[iou_loss],\n    metric_nodes=[metrics_node],\n    trainer_config=training_cfg.trainer,\n    optimizer_config=training_cfg.optimizer,\n    monitors=[tensorboard_node],\n)\ngrad_trainer.fit()\n</code></pre></p>"},{"location":"tutorials/adaclip-workflow/#step-3-analyze-mixer-weights","title":"Step 3: Analyze Mixer Weights","text":"<pre><code># Before training\ninitial_weights = {}\nfor name, param in mixer.named_parameters():\n    initial_weights[name] = param.data.clone()\n    logger.info(f\"{name}: mean={param.mean().item():.4f}, std={param.std().item():.4f}\")\n\n# After training\nfor name, param in mixer.named_parameters():\n    diff = (param.data - initial_weights[name]).abs()\n    logger.info(f\"{name} change: max_diff={diff.max().item():.6f}\")\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-4-visualize-processing-pipeline","title":"Step 4: Visualize Processing Pipeline","text":"<pre><code>from cuvis_ai.node.drcnn_tensorboard_viz import DRCNNTensorBoardViz\n\n# TensorBoard visualization node\ndrcnn_tb_viz = DRCNNTensorBoardViz(\n    hsi_channels=[0, 20, 40],  # False-color RGB visualization\n    max_samples=4,\n    log_every_n_batches=1,\n    name=\"drcnn_tensorboard_viz\",\n)\n\n# Connect visualization\npipeline.connect(\n    (data_node.outputs.cube, drcnn_tb_viz.hsi_cube),\n    (mixer.rgb, drcnn_tb_viz.mixer_output),\n    (data_node.outputs.mask, drcnn_tb_viz.ground_truth_mask),\n    (adaclip.scores, drcnn_tb_viz.adaclip_scores),\n    (drcnn_tb_viz.artifacts, tensorboard_node.artifacts),\n)\n</code></pre> <p>TensorBoard will show: - HSI input (false-color RGB using channels 0, 20, 40) - Mixer output (learned 3-channel representation) - Ground truth masks - AdaCLIP anomaly score heatmaps</p> <p> </p>"},{"location":"tutorials/adaclip-workflow/#approach-3-concrete-selector-learnable-discrete-selection","title":"Approach 3: Concrete Selector (Learnable Discrete Selection)","text":""},{"location":"tutorials/adaclip-workflow/#when-to-use-concrete-selector","title":"When to Use Concrete Selector","text":"<ul> <li>Interpretable results - Exact band indices selected</li> <li>Discrete choices - Pick 3 specific bands (not blended)</li> <li>Temperature annealing - Gumbel-Softmax for differentiable sampling</li> <li>No statistical initialization - Pure gradient-based learning</li> </ul> <p>Technical Note: Uses Gumbel-Softmax (Concrete distribution) for differentiable discrete sampling during training, then hard argmax at inference.</p>"},{"location":"tutorials/adaclip-workflow/#step-1-build-concrete-pipeline","title":"Step 1: Build Concrete Pipeline","text":"<pre><code>from cuvis_ai.node.concrete_selector import ConcreteBandSelector\nfrom cuvis_ai.node.losses import DistinctnessLoss\nfrom cuvis_ai.deciders.two_stage_decider import TwoStageBinaryDecider\n\n# Data preprocessing\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 1],\n    anomaly_class_ids=[3],\n)\nnormalizer = MinMaxNormalizer(eps=1e-6, use_running_stats=True)\n\n# Concrete band selector: 61 \u2192 3 channels\nselector = ConcreteBandSelector(\n    input_channels=61,\n    output_channels=3,\n    tau_start=10.0,                # Initial temperature (soft)\n    tau_end=0.1,                   # Final temperature (hard)\n    max_epochs=50,                 # Epochs for annealing schedule\n    use_hard_inference=True,       # Use argmax at inference\n    eps=1e-6,\n    name=\"concrete_selector\",\n)\n\n# AdaCLIP detector (same as DRCNN)\nadaclip = AdaCLIPDetector(\n    weight_name=\"pretrained_all\",\n    backbone=\"ViT-L-14-336\",\n    prompt_text=\"\",\n    image_size=518,\n    prompting_depth=4,\n    prompting_length=5,\n    gaussian_sigma=4.0,\n    use_half_precision=False,\n    enable_warmup=False,\n    enable_gradients=True,\n    name=\"adaclip\",\n)\n\n# Dual loss: IoU + Distinctness\niou_loss = IoULoss(\n    weight=1.0,\n    smooth=1e-6,\n    normalize_method=\"minmax\",\n    name=\"iou_loss\",\n)\n\n# Distinctness loss prevents all channels selecting the same band\ndistinctness_loss = DistinctnessLoss(\n    weight=0.1,\n    name=\"distinctness_loss\",\n)\n\n# Two-stage decider (more sophisticated than quantile)\ndecider = TwoStageBinaryDecider(\n    image_threshold=0.20,\n    top_k_fraction=0.001,\n    quantile=0.995,\n    name=\"decider\",\n)\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-2-gradient-training-no-statistical-phase","title":"Step 2: Gradient Training (No Statistical Phase)","text":"<pre><code># Phase 1: Skip statistical (weight init only)\nlogger.info(\"Phase 1: Skipping statistical - Concrete uses weight init\")\n\n# Phase 2: Unfreeze selector\nlogger.info(\"Phase 2: Unfreezing Concrete selector...\")\npipeline.unfreeze_nodes_by_name([selector.name])\n\n# Phase 3: Gradient training with dual loss\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[iou_loss, distinctness_loss],  # Dual loss\n    metric_nodes=[metrics_node],\n    trainer_config=training_cfg.trainer,\n    optimizer_config=training_cfg.optimizer,\n    monitors=[tensorboard_node],\n)\ngrad_trainer.fit()\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#step-3-analyze-selected-bands","title":"Step 3: Analyze Selected Bands","text":"<pre><code># Before training\nselector.eval()\nwith torch.no_grad():\n    S_initial = selector.get_selection_weights(deterministic=True)\n    bands_initial = selector.get_selected_bands()\n    tau_initial = selector._get_tau(epoch=0)\n\nlogger.info(f\"Initial selected bands (argmax): {bands_initial.tolist()}\")\nlogger.info(f\"Initial temperature (epoch 0): {tau_initial:.4f}\")\n\n# Print top-3 bands per channel\nfor c in range(selector.output_channels):\n    top3 = torch.topk(S_initial[c], k=3)\n    logger.info(f\"Channel {c} top-3 bands: {top3.indices.tolist()} \"\n                f\"(weights: {top3.values.tolist()})\")\n</code></pre> <p>After Training: <pre><code>with torch.no_grad():\n    S_final = selector.get_selection_weights(deterministic=True)\n    bands_final = selector.get_selected_bands()\n\nlogger.info(f\"Final selected bands (argmax): {bands_final.tolist()}\")\n\n# Check for band collapse\nunique_bands = torch.unique(bands_final).numel()\nif unique_bands &lt; selector.output_channels:\n    logger.warning(f\"\u26a0\ufe0f Only {unique_bands} unique bands selected!\")\nelse:\n    logger.info(f\"\u2705 All {selector.output_channels} channels selected different bands\")\n</code></pre></p> <p>Expected Output: <pre><code>Initial selected bands (argmax): [12, 45, 58]\nFinal selected bands (argmax): [8, 31, 54]\n\u2705 All 3 channels selected different bands\n</code></pre></p>"},{"location":"tutorials/adaclip-workflow/#step-4-temperature-annealing-visualization","title":"Step 4: Temperature Annealing Visualization","text":"<p>The temperature \u03c4 controls the \"sharpness\" of the Gumbel-Softmax distribution: - High \u03c4 (10.0): Soft, continuous sampling (exploration) - Low \u03c4 (0.1): Hard, discrete sampling (exploitation)</p> <p>Annealing Schedule: <pre><code>def _get_tau(self, epoch: int) -&gt; float:\n    \"\"\"Linear temperature annealing from tau_start to tau_end.\"\"\"\n    if epoch &gt;= self.max_epochs:\n        return self.tau_end\n    progress = epoch / self.max_epochs\n    return self.tau_start + (self.tau_end - self.tau_start) * progress\n</code></pre></p> <p>Monitor temperature in TensorBoard to verify annealing.</p>"},{"location":"tutorials/adaclip-workflow/#performance-comparison","title":"Performance Comparison","text":""},{"location":"tutorials/adaclip-workflow/#quantitative-metrics","title":"Quantitative Metrics","text":"<p>Run all three approaches and compare:</p> Metric PCA Baseline DRCNN Mixer Concrete Selector Val IoU 0.6823 0.7456 0.7389 Test IoU 0.6791 0.7512 0.7401 Precision 0.7234 0.8012 0.7956 Recall 0.8456 0.8723 0.8689 F1 Score 0.7801 0.8345 0.8301 Training Time 2 min (stat only) 15 min (stat + grad) 12 min (grad only) Inference Speed \u26a1 45 FPS \ud83d\udd25 42 FPS \ud83d\udd25 43 FPS <p>Example metrics - actual results depend on dataset and hyperparameters</p>"},{"location":"tutorials/adaclip-workflow/#qualitative-comparison","title":"Qualitative Comparison","text":"<p>PCA Baseline: - \u2705 Fastest to train - \u2705 Interpretable (variance-based) - \u274c Task-agnostic (not optimized for anomalies) - \u274c Linear projection only</p> <p>DRCNN Mixer: - \u2705 Best quantitative performance - \u2705 End-to-end optimized - \u2705 Continuous channel blending - \u274c Harder to interpret (weighted combinations) - \u274c Requires statistical initialization</p> <p>Concrete Selector: - \u2705 Interpretable selected bands - \u2705 No statistical phase needed - \u2705 Discrete, sparse selection - \u274c Requires careful temperature tuning - \u274c Risk of band collapse (mitigated by distinctness loss)</p>"},{"location":"tutorials/adaclip-workflow/#practical-workflow","title":"Practical Workflow","text":""},{"location":"tutorials/adaclip-workflow/#step-by-step-execution","title":"Step-by-Step Execution","text":"<p>1. Start with PCA Baseline: <pre><code># Quick baseline (2 minutes)\npython examples/adaclip/pca_adaclip_baseline.py\n</code></pre></p> <p>2. Try DRCNN Mixer: <pre><code># Best performance (15 minutes)\npython examples/adaclip/drcnn_adaclip_gradient_training.py\n</code></pre></p> <p>3. Experiment with Concrete Selector: <pre><code># Interpretable bands (12 minutes)\npython examples/adaclip/concrete_adaclip_gradient_training.py\n</code></pre></p>"},{"location":"tutorials/adaclip-workflow/#configuration-management","title":"Configuration Management","text":"<p>All three approaches use Hydra configs in <code>configs/trainrun/</code>:</p> <pre><code># configs/trainrun/pca_adaclip_baseline.yaml\nname: pca_adaclip_baseline\noutput_dir: outputs/pca_adaclip\n\npipeline:\n  normalizer:\n    eps: 1.0e-6\n    use_running_stats: true\n  pca:\n    eps: 1.0e-6\n  adaclip:\n    weight_name: pretrained_all\n    backbone: ViT-L-14-336\n    use_half_precision: true\n\ndata:\n  data_dir: data/lentils\n  batch_size: 4\n  num_workers: 2\n</code></pre>"},{"location":"tutorials/adaclip-workflow/#tensorboard-comparison","title":"TensorBoard Comparison","text":"<p>Launch TensorBoard to compare all three: <pre><code>tensorboard --logdir=outputs/ --port=6006\n</code></pre></p> <p>View: - Loss curves (IoU loss for DRCNN/Concrete) - Metric trends (IoU, precision, recall) - Processing pipeline visualizations (HSI \u2192 reduction \u2192 scores) - Selected bands (Concrete selector)</p>"},{"location":"tutorials/adaclip-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/adaclip-workflow/#plugin-loading-failures","title":"Plugin Loading Failures","text":"<p>Error: <code>Plugin 'adaclip' not found or failed to load</code></p> <p>Solutions: 1. Check Git repository access:    <pre><code>git ls-remote https://github.com/cubert-hyperspectral/cuvis-ai-adaclip.git\n</code></pre></p> <ol> <li> <p>Verify tag exists:    <pre><code>config={\"repo\": \"...\", \"tag\": \"v0.1.0\"}  # Must match repository tags\n</code></pre></p> </li> <li> <p>For local development, use path-based loading:    <pre><code>config={\"path\": \"/absolute/path/to/cuvis-ai-adaclip\"}\n</code></pre></p> </li> <li> <p>Check plugin requirements:    <pre><code>cd cuvis-ai-adaclip\npip install -e .\n</code></pre></p> </li> </ol>"},{"location":"tutorials/adaclip-workflow/#cuda-out-of-memory-drcnnconcrete","title":"CUDA Out of Memory (DRCNN/Concrete)","text":"<p>Error: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions: 1. Reduce batch size:    <pre><code>data:\n  batch_size: 2  # Reduce from 4\n</code></pre></p> <ol> <li> <p>Enable mixed precision (DRCNN/Concrete):    <pre><code>adaclip = AdaCLIPDetector(\n    use_half_precision=True,  # fp16 reduces memory\n    ...\n)\n</code></pre></p> </li> <li> <p>Reduce image size:    <pre><code>adaclip = AdaCLIPDetector(\n    image_size=336,  # Reduce from 518\n    ...\n)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/adaclip-workflow/#band-collapse-concrete-selector","title":"Band Collapse (Concrete Selector)","text":"<p>Issue: All output channels select the same band</p> <p>Symptoms: <pre><code>\u26a0\ufe0f Only 1 unique bands selected out of 3 channels!\nFinal selected bands: [31, 31, 31]\n</code></pre></p> <p>Solutions: 1. Increase distinctness loss weight:    <pre><code>distinctness_loss = DistinctnessLoss(\n    weight=0.5,  # Increase from 0.1\n)\n</code></pre></p> <ol> <li> <p>Adjust temperature schedule:    <pre><code>selector = ConcreteBandSelector(\n    tau_start=15.0,  # Increase initial temperature\n    tau_end=0.5,     # Increase final temperature\n)\n</code></pre></p> </li> <li> <p>Use longer training:    <pre><code>training:\n  trainer:\n    max_epochs: 100  # Increase from 50\n</code></pre></p> </li> </ol>"},{"location":"tutorials/adaclip-workflow/#low-iou-performance-all-approaches","title":"Low IoU Performance (All Approaches)","text":"<p>Issue: IoU &lt; 0.5 on validation</p> <p>Solutions: 1. Check normal_class_ids mapping:    <pre><code>data_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 1],    # Unlabeled + Lentils_black\n    anomaly_class_ids=[3],      # Stone only\n)\n</code></pre></p> <ol> <li> <p>Verify AdaCLIP prompt (if used):    <pre><code>adaclip = AdaCLIPDetector(\n    prompt_text=\"normal: lentils, anomaly: stones\",  # Task-specific\n)\n</code></pre></p> </li> <li> <p>Adjust decision threshold:    <pre><code>decider = QuantileBinaryDecider(\n    quantile=0.99,  # Increase from 0.995 (more sensitive)\n)\n</code></pre></p> </li> <li> <p>Increase training epochs:    <pre><code>training:\n  trainer:\n    max_epochs: 100\n</code></pre></p> </li> </ol>"},{"location":"tutorials/adaclip-workflow/#mixer-weights-not-changing-drcnn","title":"Mixer Weights Not Changing (DRCNN)","text":"<p>Issue: Weight change after training is near-zero</p> <p>Diagnosis: <pre><code>logger.info(f\"Weight change: max_diff={diff.max().item():.6f}\")\n# Output: Weight change: max_diff=0.000012  # Too small!\n</code></pre></p> <p>Solutions: 1. Increase learning rate:    <pre><code>training:\n  optimizer:\n    lr: 0.001  # Increase from 0.0001\n</code></pre></p> <ol> <li> <p>Remove early stopping:    <pre><code># Comment out early stopping callback\n# training_cfg.trainer.callbacks.early_stopping = []\n</code></pre></p> </li> <li> <p>Check gradients are flowing:    <pre><code>adaclip = AdaCLIPDetector(\n    enable_gradients=True,  # CRITICAL for backprop\n)\n</code></pre></p> </li> <li> <p>Verify unfreezing:    <pre><code>pipeline.unfreeze_nodes_by_name([mixer.name])\nfor name, param in mixer.named_parameters():\n    assert param.requires_grad, f\"{name} is still frozen!\"\n</code></pre></p> </li> </ol>"},{"location":"tutorials/adaclip-workflow/#summary","title":"Summary","text":"<p>You've learned three approaches to hyperspectral dimensionality reduction for AdaCLIP-based anomaly detection:</p> <ol> <li>PCA Baseline - Fast, interpretable, statistical-only (2 min)</li> <li>DRCNN Mixer - Best performance, learnable continuous mixing (15 min)</li> <li>Concrete Selector - Interpretable discrete selection, pure gradient-based (12 min)</li> </ol> <p>Key Takeaways: - Plugin system enables modular extension of CUVIS.AI - Dimensionality reduction strategy significantly impacts detection performance - DRCNN mixer offers best quantitative metrics - Concrete selector provides interpretable band choices - TensorBoard visualization is essential for debugging end-to-end pipelines</p> <p>Performance Ranking (by IoU): 1. \ud83e\udd47 DRCNN Mixer (0.7512 test IoU) 2. \ud83e\udd48 Concrete Selector (0.7401 test IoU) 3. \ud83e\udd49 PCA Baseline (0.6791 test IoU)</p>"},{"location":"tutorials/adaclip-workflow/#next-steps","title":"Next Steps","text":"<p>Explore Related Topics: - Plugin System Development - Create your own plugin nodes - gRPC Workflow Tutorial - Distributed training and inference - Loss &amp; Metrics Nodes - IoU loss and distinctness loss details</p> <p>Try Advanced Configurations: - Multi-loss training: Combine IoU + entropy + diversity regularizers - Alternative selectors: SupervisedCIRBandSelector, SupervisedWindowedFalseRGBSelector - Custom CLIP models: Try different ViT backbones (ViT-B-16, ViT-L-14) - Transfer learning: Fine-tune AdaCLIP prompts for your specific anomaly types</p> <p>Production Deployment: - gRPC Deployment Guide - Deploy trained pipelines - Model Serving Patterns - Inference-only clients</p>"},{"location":"tutorials/adaclip-workflow/#complete-example-scripts","title":"Complete Example Scripts","text":"<p>PCA Baseline: <pre><code>python examples/adaclip/pca_adaclip_baseline.py\n</code></pre> View full source: examples/adaclip/pca_adaclip_baseline.py</p> <p>DRCNN Mixer: <pre><code>python examples/adaclip/drcnn_adaclip_gradient_training.py\n</code></pre> View full source: examples/adaclip/drcnn_adaclip_gradient_training.py</p> <p>Concrete Selector: <pre><code>python examples/adaclip/concrete_adaclip_gradient_training.py\n</code></pre> View full source: examples/adaclip/concrete_adaclip_gradient_training.py</p> <p>Need Help? - Check Plugin System FAQ - Review Band Selection Strategies - See Training Configuration</p>"},{"location":"tutorials/channel-selector/","title":"Channel Selector","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"tutorials/channel-selector/#tutorial-learnable-channel-selection-with-two-phase-training","title":"Tutorial: Learnable Channel Selection with Two-Phase Training","text":"<p>Learn how to build an anomaly detection pipeline with learnable channel selection using two-phase training (statistical initialization + gradient optimization).</p>"},{"location":"tutorials/channel-selector/#overview","title":"Overview","text":"<p>What You'll Learn:</p> <ul> <li>Building on the RX Statistical tutorial with learnable channel selection</li> <li>Understanding Gumbel-Softmax for differentiable channel selection</li> <li>Two-phase training: statistical initialization \u2192 gradient optimization</li> <li>Loss composition with regularizers (BCE + entropy + diversity)</li> <li>Using callbacks for early stopping and model checkpointing</li> <li>Visualizing selected channels and training progress</li> </ul> <p>Prerequisites:</p> <ul> <li>RX Statistical Tutorial - Complete this first!</li> <li>Understanding of Two-Phase Training</li> <li>Familiarity with Execution Stages</li> <li>Basic knowledge of gradient-based optimization</li> </ul> <p>Time: ~30 minutes</p> <p>Perfect for: Users who want to learn gradient-based training, channel selection, and advanced training strategies in CUVIS.AI.</p>"},{"location":"tutorials/channel-selector/#background","title":"Background","text":""},{"location":"tutorials/channel-selector/#why-channel-selection","title":"Why Channel Selection?","text":"<p>Hyperspectral data often contains 61+ spectral channels, but: - Not all channels are informative for a given task - Redundant channels add noise and computation cost - Channel selection can improve performance and interpretability</p> <p>Manual selection (e.g., RGB channels at 650, 550, 450 nm) works but is suboptimal.</p> <p>Learnable selection lets the model automatically discover the best channels for anomaly detection.</p>"},{"location":"tutorials/channel-selector/#gumbel-softmax-for-channel-selection","title":"Gumbel-Softmax for Channel Selection","text":"<p>The challenge: Channel selection is discrete (select channel i or not), which isn't differentiable.</p> <p>Solution: Gumbel-Softmax relaxation allows gradient-based optimization:</p> <pre><code>selected_data = \u2211\u1d62 softmax(weights / temperature) \u00d7 channel\u1d62\n</code></pre> <ul> <li><code>weights</code>: Learnable parameters per channel</li> <li><code>temperature</code>: Controls discreteness (high \u2192 soft, low \u2192 hard)</li> <li><code>softmax</code>: Makes weights differentiable</li> </ul> <p>Temperature Annealing: Start with high temperature (smooth distribution), gradually decrease to make selection sharper.</p>"},{"location":"tutorials/channel-selector/#two-phase-training-strategy","title":"Two-Phase Training Strategy","text":"<p>Phase 1: Statistical Initialization - Initialize normalizer statistics (min/max) - Initialize selector weights (variance-based or uniform) - Initialize RX detector (background covariance) - All nodes frozen (no gradients)</p> <p>Phase 2: Gradient Training - Unfreeze: Selector, RX, LogitHead - Keep frozen: Normalizer (fixed statistics) - Optimize with BCE loss + regularizers - Use callbacks (early stopping, checkpointing)</p> <p>This strategy combines the stability of statistical initialization with the power of gradient optimization.</p>"},{"location":"tutorials/channel-selector/#step-1-setup-building-on-rx-tutorial","title":"Step 1: Setup (Building on RX Tutorial)","text":""},{"location":"tutorials/channel-selector/#step-1-data-loading","title":"Step 1: Data Loading","text":"<p>This tutorial extends the RX Statistical Tutorial. We'll use the same data setup:</p> <pre><code>from pathlib import Path\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\n\n# Setup datamodule\ndatamodule = SingleCu3sDataModule(\n    data_dir=\"data/lentils\",\n    batch_size=4,\n    num_workers=0,\n)\ndatamodule.setup(stage=\"fit\")\n\n# Create pipeline\npipeline = CuvisPipeline(\"Channel_Selector\")\n\n# Data node (same as RX tutorial)\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\n</code></pre>"},{"location":"tutorials/channel-selector/#step-2-build-pipeline-with-selector","title":"Step 2: Build Pipeline with Selector","text":""},{"location":"tutorials/channel-selector/#preprocessing","title":"Preprocessing","text":""},{"location":"tutorials/channel-selector/#normalization","title":"Normalization","text":"<pre><code>from cuvis_ai.node.normalization import MinMaxNormalizer\n\nnormalizer = MinMaxNormalizer(\n    eps=1.0e-6,\n    use_running_stats=True,  # Will be initialized in Phase 1\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#add-soft-channel-selector-new","title":"Add Soft Channel Selector (NEW!)","text":"<pre><code>from cuvis_ai.node.selector import SoftChannelSelector\n\nselector = SoftChannelSelector(\n    n_select=3,               # Select 3 channels (for RGB-like output)\n    input_channels=61,        # Input hyperspectral channels\n    init_method=\"variance\",   # Initialize by channel variance\n    temperature_init=5.0,     # Start with soft selection\n    temperature_min=0.1,      # End with sharp selection\n    temperature_decay=0.9,    # Decay rate per epoch\n    hard=False,               # Soft selection during training\n    eps=1.0e-6,\n)\n</code></pre> <p>Key Parameters: - <code>n_select=3</code>: Number of output channels (like RGB) - <code>init_method=\"variance\"</code>: Initialize weights by channel variance - Temperature schedule: 5.0 \u2192 0.1 with decay 0.9   - Epoch 0: temperature = 5.0 (soft selection)   - Epoch 10: temperature \u2248 1.94 (medium)   - Epoch 20: temperature \u2248 0.76 (sharper)   - Epoch 30: temperature = 0.1 (nearly discrete)</p>"},{"location":"tutorials/channel-selector/#add-rx-detector","title":"Add RX Detector","text":"<p>Important: RX now operates on 3 selected channels instead of 61!</p> <pre><code>from cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.conversion import ScoreToLogit\n\nrx = RXGlobal(\n    num_channels=15,  # Actually selector outputs 3*n_select channels\n    eps=1.0e-6,\n)\n\nlogit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\n</code></pre>"},{"location":"tutorials/channel-selector/#add-decision-node","title":"Add Decision Node","text":"<pre><code>from cuvis_ai.deciders.binary_decider import BinaryDecider\n\ndecider = BinaryDecider(threshold=0.5)\n</code></pre>"},{"location":"tutorials/channel-selector/#step-3-add-loss-functions-and-regularizers","title":"Step 3: Add Loss Functions and Regularizers","text":""},{"location":"tutorials/channel-selector/#loss-composition","title":"Loss Composition","text":""},{"location":"tutorials/channel-selector/#main-loss-bce-with-logits","title":"Main Loss: BCE with Logits","text":"<pre><code>from cuvis_ai.node.losses import AnomalyBCEWithLogits\n\nbce_loss = AnomalyBCEWithLogits(\n    name=\"bce\",\n    weight=10.0,      # High weight for main task\n    pos_weight=None,  # No class imbalance weighting\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#entropy-regularization","title":"Entropy Regularization","text":""},{"location":"tutorials/channel-selector/#regularizer-1-entropy-loss","title":"Regularizer 1: Entropy Loss","text":"<p>Encourages uniform channel selection distribution (prevents mode collapse):</p> <pre><code>from cuvis_ai.node.losses import SelectorEntropyRegularizer\n\nentropy_loss = SelectorEntropyRegularizer(\n    name=\"entropy\",\n    weight=0.1,          # Regularization strength\n    target_entropy=None,  # Use maximum entropy as target\n)\n</code></pre> <p>Why? Without entropy loss, selector might focus on only 1-2 channels.</p>"},{"location":"tutorials/channel-selector/#diversity-regularization","title":"Diversity Regularization","text":""},{"location":"tutorials/channel-selector/#regularizer-2-diversity-loss","title":"Regularizer 2: Diversity Loss","text":"<p>Encourages selected channels to be diverse (not redundant):</p> <pre><code>from cuvis_ai.node.losses import SelectorDiversityRegularizer\n\ndiversity_loss = SelectorDiversityRegularizer(\n    name=\"diversity\",\n    weight=0.01,  # Weak regularization\n)\n</code></pre> <p>Total Loss: <pre><code>Loss = 10.0 \u00d7 BCE + 0.1 \u00d7 Entropy + 0.01 \u00d7 Diversity\n</code></pre></p>"},{"location":"tutorials/channel-selector/#step-4-add-metrics-and-visualization","title":"Step 4: Add Metrics and Visualization","text":""},{"location":"tutorials/channel-selector/#monitoring","title":"Monitoring","text":""},{"location":"tutorials/channel-selector/#metrics","title":"Metrics","text":"<pre><code>from cuvis_ai.node.metrics import AnomalyDetectionMetrics\n\nmetrics_anomaly = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n</code></pre>"},{"location":"tutorials/channel-selector/#visualization-nodes","title":"Visualization Nodes","text":"<pre><code>from cuvis_ai.node.visualizations import AnomalyMask, CubeRGBVisualizer\n\n# Anomaly mask visualization (same as RX tutorial)\nviz_mask = AnomalyMask(name=\"mask\", channel=30, up_to=5)\n\n# NEW: RGB visualization of selected channels\nviz_rgb = CubeRGBVisualizer(\n    name=\"rgb\",\n    up_to=5,  # Visualize first 5 samples\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#tensorboard-monitoring","title":"TensorBoard Monitoring","text":"<pre><code>from cuvis_ai.node.monitor import TensorBoardMonitorNode\n\ntensorboard_node = TensorBoardMonitorNode(\n    output_dir=\"runs/tensorboard\",\n    run_name=pipeline.name,\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#step-5-connect-the-pipeline","title":"Step 5: Connect the Pipeline","text":"<pre><code>pipeline.connect(\n    # Processing flow: data \u2192 normalize \u2192 selector \u2192 RX \u2192 logits \u2192 decisions\n    (data_node.outputs.cube, normalizer.data),\n    (normalizer.normalized, selector.data),\n    (selector.selected, rx.data),\n    (rx.scores, logit_head.scores),\n    (logit_head.logits, decider.logits),\n\n    # Loss flow\n    (logit_head.logits, bce_loss.predictions),\n    (data_node.outputs.mask, bce_loss.targets),\n    (selector.weights, entropy_loss.weights),\n    (selector.weights, diversity_loss.weights),\n\n    # Metric flow\n    (decider.decisions, metrics_anomaly.decisions),\n    (data_node.outputs.mask, metrics_anomaly.targets),\n    (metrics_anomaly.metrics, tensorboard_node.metrics),\n\n    # Visualization flow\n    (decider.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (data_node.outputs.cube, viz_mask.cube),\n    (data_node.outputs.cube, viz_rgb.cube),\n    (selector.weights, viz_rgb.weights),  # Visualize channel selection\n    (data_node.outputs.wavelengths, viz_rgb.wavelengths),\n    (viz_mask.artifacts, tensorboard_node.artifacts),\n    (viz_rgb.artifacts, tensorboard_node.artifacts),\n)\n</code></pre> <p>Pipeline Flow:</p> <pre><code>graph LR\n    A[Data] --&gt;|cube| B[Normalizer]\n    B --&gt;|normalized| C[SoftChannelSelector]\n    C --&gt;|selected 3ch| D[RXGlobal]\n    D --&gt;|scores| E[ScoreToLogit]\n    E --&gt;|logits| F[BinaryDecider]\n    E --&gt;|logits| G[BCELoss]\n    A --&gt;|mask| G\n    C --&gt;|weights| H[EntropyLoss]\n    C --&gt;|weights| I[DiversityLoss]\n    F --&gt;|decisions| J[Metrics]\n    A --&gt;|mask| J\n    J --&gt;|metrics| K[TensorBoard]\n    F --&gt;|decisions| L[VisualizeM ask]\n    A --&gt;|cube| M[VisualizeRGB]\n    C --&gt;|weights| M</code></pre>"},{"location":"tutorials/channel-selector/#step-6-phase-1-statistical-initialization","title":"Step 6: Phase 1 - Statistical Initialization","text":"<p>Initialize nodes without gradients:</p> <pre><code>from cuvis_ai_core.training import StatisticalTrainer\nfrom loguru import logger\n\nlogger.info(\"Phase 1: Statistical initialization...\")\nstat_trainer = StatisticalTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n)\nstat_trainer.fit()\n</code></pre> <p>What Gets Initialized: 1. MinMaxNormalizer: Collects global min/max 2. SoftChannelSelector: Initializes weights by channel variance 3. RXGlobal: Computes covariance on selected 3 channels</p> <p>Console Output: <pre><code>[INFO] Phase 1: Statistical initialization...\n[INFO] MinMaxNormalizer: min=-0.123, max=1.987\n[INFO] SoftChannelSelector: Initialized weights by variance\n[INFO] RXGlobal: Computed background statistics (3 selected channels)\n[INFO] Statistical initialization complete\n</code></pre></p>"},{"location":"tutorials/channel-selector/#step-7-phase-2-unfreeze-nodes-for-gradient-training","title":"Step 7: Phase 2 - Unfreeze Nodes for Gradient Training","text":"<p>Selectively unfreeze nodes for optimization:</p> <pre><code>logger.info(\"Phase 2: Unfreezing selector and RX for gradient training...\")\n\nunfreeze_node_names = [selector.name, rx.name, logit_head.name]\npipeline.unfreeze_nodes_by_name(unfreeze_node_names)\n\nlogger.info(f\"Unfrozen nodes: {unfreeze_node_names}\")\n</code></pre> <p>Freeze Status After Unfreezing: - \u2705 Unfrozen (trainable): SoftChannelSelector, RXGlobal, ScoreToLogit - \ud83d\udd12 Frozen (fixed): MinMaxNormalizer, Data Node - Loss and metric nodes don't have weights</p>"},{"location":"tutorials/channel-selector/#step-8-configure-training-with-callbacks","title":"Step 8: Configure Training with Callbacks","text":""},{"location":"tutorials/channel-selector/#setup-training-configuration","title":"Setup Training Configuration","text":"<pre><code>from cuvis_ai_core.training.config import (\n    TrainingConfig,\n    CallbacksConfig,\n    EarlyStoppingConfig,\n    ModelCheckpointConfig,\n    SchedulerConfig,\n)\nfrom omegaconf import OmegaConf\n\n# Load training config from Hydra\ntraining_cfg = TrainingConfig.from_dict(\n    OmegaConf.to_container(cfg.training, resolve=True)\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#add-early-stopping","title":"Add Early Stopping","text":"<p>Stop training if BCE loss doesn't improve:</p> <pre><code>if training_cfg.trainer.callbacks is None:\n    training_cfg.trainer.callbacks = CallbacksConfig()\n\ntraining_cfg.trainer.callbacks.early_stopping.append(\n    EarlyStoppingConfig(\n        monitor=\"train/bce\",   # Monitor training BCE loss\n        mode=\"min\",             # Lower is better\n        patience=20,            # Stop after 20 epochs without improvement\n    )\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#add-learning-rate-scheduler","title":"Add Learning Rate Scheduler","text":"<p>Reduce LR when validation IoU plateaus:</p> <pre><code>training_cfg.scheduler = SchedulerConfig(\n    name=\"reduce_on_plateau\",\n    monitor=\"metrics_anomaly/iou\",  # Monitor validation IoU\n    mode=\"max\",                      # Higher is better\n    factor=0.5,                      # Reduce LR by 50%\n    patience=5,                      # Wait 5 epochs before reducing\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#add-model-checkpointing","title":"Add Model Checkpointing","text":"<p>Save top-3 models by validation IoU:</p> <pre><code>training_cfg.trainer.callbacks.checkpoint = ModelCheckpointConfig(\n    dirpath=\"outputs/checkpoints\",\n    monitor=\"metrics_anomaly/iou\",  # Track validation IoU\n    mode=\"max\",                      # Save highest IoU models\n    save_top_k=3,                    # Keep top 3 checkpoints\n    save_last=True,                  # Also save last checkpoint\n    filename=\"{epoch:02d}\",          # Filename pattern\n    verbose=True,\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#step-9-phase-3-gradient-training","title":"Step 9: Phase 3 - Gradient Training","text":"<p>Run gradient-based optimization:</p> <pre><code>from cuvis_ai_core.training import GradientTrainer\n\nlogger.info(\"Phase 3: Gradient-based channel selection optimization...\")\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[bce_loss],  # Can add entropy_loss, diversity_loss here\n    metric_nodes=[metrics_anomaly],\n    trainer_config=training_cfg.trainer,\n    optimizer_config=training_cfg.optimizer,\n    monitors=[tensorboard_node],\n)\n\ngrad_trainer.fit()\n</code></pre> <p>Training Loop: 1. Forward pass through unfrozen nodes 2. Compute losses: BCE + entropy + diversity 3. Backpropagation and weight updates 4. Temperature decay for selector 5. Validation every N epochs 6. Checkpointing and early stopping</p> <p>Console Output: <pre><code>[INFO] Phase 3: Gradient training...\nEpoch 0: train/bce=0.623, metrics_anomaly/iou=0.671, lr=0.001\nEpoch 1: train/bce=0.512, metrics_anomaly/iou=0.709, lr=0.001\nEpoch 5: train/bce=0.387, metrics_anomaly/iou=0.784, lr=0.001\n[INFO] Reducing learning rate to 0.0005\nEpoch 10: train/bce=0.298, metrics_anomaly/iou=0.812, lr=0.0005\n[INFO] Saved checkpoint: epoch=10, iou=0.812\n...\n[INFO] Early stopping triggered (patience=20)\n[INFO] Best checkpoint: epoch=23, iou=0.847\n</code></pre></p>"},{"location":"tutorials/channel-selector/#step-10-evaluation","title":"Step 10: Evaluation","text":""},{"location":"tutorials/channel-selector/#validation-on-best-checkpoint","title":"Validation on Best Checkpoint","text":"<pre><code>logger.info(\"Running validation with best checkpoint...\")\nval_results = grad_trainer.validate()\nlogger.info(f\"Validation results: {val_results}\")\n</code></pre>"},{"location":"tutorials/channel-selector/#test-evaluation","title":"Test Evaluation","text":"<pre><code>logger.info(\"Running test evaluation with best checkpoint...\")\ntest_results = grad_trainer.test()\nlogger.info(f\"Test results: {test_results}\")\n</code></pre> <p>Expected Results: - IoU: 0.80-0.85 (improved from RX baseline ~0.72) - Precision: 0.85-0.90 - Recall: 0.80-0.87</p>"},{"location":"tutorials/channel-selector/#step-11-analyze-selected-channels","title":"Step 11: Analyze Selected Channels","text":""},{"location":"tutorials/channel-selector/#monitoring-channel-weights","title":"Monitoring Channel Weights","text":""},{"location":"tutorials/channel-selector/#inspect-selector-weights","title":"Inspect Selector Weights","text":"<pre><code># Get final channel weights\nselector_weights = selector.get_selection_weights()\nprint(f\"Selector weights shape: {selector_weights.shape}\")  # [61]\nprint(f\"Top 3 channels: {selector_weights.topk(3).indices}\")\n\n# Example output:\n# Top 3 channels: tensor([18, 42, 55])\n# These correspond to specific wavelengths\n</code></pre>"},{"location":"tutorials/channel-selector/#visualize-channel-selection","title":"Visualize Channel Selection","text":"<p>TensorBoard will show: - RGB visualization: False-color image from selected channels - Weight plot: Bar chart of channel weights - Weight evolution: How weights changed during training</p>"},{"location":"tutorials/channel-selector/#step-12-save-pipeline-and-trainrun-config","title":"Step 12: Save Pipeline and TrainRun Config","text":""},{"location":"tutorials/channel-selector/#save-trained-pipeline","title":"Save Trained Pipeline","text":"<pre><code>from cuvis_ai_core.training.config import PipelineMetadata\n\nresults_dir = Path(\"outputs/trained_models\")\npipeline_output_path = results_dir / \"Channel_Selector.yaml\"\n\npipeline.save_to_file(\n    str(pipeline_output_path),\n    metadata=PipelineMetadata(\n        name=pipeline.name,\n        description=\"Channel Selector with two-phase training\",\n        tags=[\"gradient\", \"statistical\", \"channel_selector\", \"rx\"],\n        author=\"cuvis.ai\",\n    ),\n)\n</code></pre>"},{"location":"tutorials/channel-selector/#save-trainrun-config","title":"Save TrainRun Config","text":"<pre><code>from cuvis_ai_core.training.config import TrainRunConfig\n\ntrainrun_config = TrainRunConfig(\n    name=\"channel_selector\",\n    pipeline=pipeline.serialize(),\n    data=datamodule_config,\n    training=training_cfg,\n    output_dir=\"outputs\",\n    loss_nodes=[\"bce\"],\n    metric_nodes=[\"metrics_anomaly\"],\n    freeze_nodes=[],\n    unfreeze_nodes=unfreeze_node_names,  # Track which nodes were trained\n)\n\ntrainrun_config.save_to_file(\"outputs/trained_models/channel_selector_trainrun.yaml\")\n</code></pre>"},{"location":"tutorials/channel-selector/#complete-example-script","title":"Complete Example Script","text":"<p>Full runnable script (<code>examples/channel_selector.py</code>):</p> <pre><code>from pathlib import Path\nimport hydra\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training import GradientTrainer, StatisticalTrainer\nfrom cuvis_ai_core.training.config import (\n    CallbacksConfig,\n    EarlyStoppingConfig,\n    ModelCheckpointConfig,\n    PipelineMetadata,\n    SchedulerConfig,\n    TrainingConfig,\n    TrainRunConfig,\n)\nfrom loguru import logger\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.conversion import ScoreToLogit\nfrom cuvis_ai.deciders.binary_decider import BinaryDecider\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.losses import (\n    AnomalyBCEWithLogits,\n    SelectorDiversityRegularizer,\n    SelectorEntropyRegularizer,\n)\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.node.selector import SoftChannelSelector\nfrom cuvis_ai.node.visualizations import AnomalyMask, CubeRGBVisualizer\n\n\n@hydra.main(config_path=\"../configs/\", config_name=\"trainrun/channel_selector\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"Channel Selector with Two-Phase Training.\"\"\"\n\n    logger.info(\"=== Channel Selector for Anomaly Detection ===\")\n    output_dir = Path(cfg.output_dir)\n\n    # Stage 1: Setup datamodule\n    datamodule = SingleCu3sDataModule(**cfg.data)\n    datamodule.setup(stage=\"fit\")\n\n    # Stage 2: Build pipeline with selector\n    pipeline = CuvisPipeline(\"Channel_Selector\")\n\n    data_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\n    normalizer = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\n    selector = SoftChannelSelector(\n        n_select=3,\n        input_channels=61,\n        init_method=\"variance\",\n        temperature_init=5.0,\n        temperature_min=0.1,\n        temperature_decay=0.9,\n    )\n    rx = RXGlobal(num_channels=15, eps=1.0e-6)\n    logit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\n    decider = BinaryDecider(threshold=0.5)\n\n    bce_loss = AnomalyBCEWithLogits(name=\"bce\", weight=10.0)\n    entropy_loss = SelectorEntropyRegularizer(name=\"entropy\", weight=0.1)\n    diversity_loss = SelectorDiversityRegularizer(name=\"diversity\", weight=0.01)\n\n    metrics_anomaly = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n    viz_mask = AnomalyMask(name=\"mask\", channel=30, up_to=5)\n    viz_rgb = CubeRGBVisualizer(name=\"rgb\", up_to=5)\n    tensorboard_node = TensorBoardMonitorNode(\n        output_dir=str(output_dir / \"..\" / \"tensorboard\"),\n        run_name=pipeline.name,\n    )\n\n    # Stage 3: Connect pipeline\n    pipeline.connect(\n        (data_node.outputs.cube, normalizer.data),\n        (normalizer.normalized, selector.data),\n        (selector.selected, rx.data),\n        (rx.scores, logit_head.scores),\n        (logit_head.logits, bce_loss.predictions),\n        (data_node.outputs.mask, bce_loss.targets),\n        (selector.weights, entropy_loss.weights),\n        (selector.weights, diversity_loss.weights),\n        (logit_head.logits, decider.logits),\n        (decider.decisions, metrics_anomaly.decisions),\n        (data_node.outputs.mask, metrics_anomaly.targets),\n        (metrics_anomaly.metrics, tensorboard_node.metrics),\n        (decider.decisions, viz_mask.decisions),\n        (data_node.outputs.mask, viz_mask.mask),\n        (data_node.outputs.cube, viz_mask.cube),\n        (data_node.outputs.cube, viz_rgb.cube),\n        (selector.weights, viz_rgb.weights),\n        (data_node.outputs.wavelengths, viz_rgb.wavelengths),\n        (viz_mask.artifacts, tensorboard_node.artifacts),\n        (viz_rgb.artifacts, tensorboard_node.artifacts),\n    )\n\n    # Configure training\n    training_cfg = TrainingConfig.from_dict(OmegaConf.to_container(cfg.training, resolve=True))\n    if training_cfg.trainer.callbacks is None:\n        training_cfg.trainer.callbacks = CallbacksConfig()\n\n    training_cfg.trainer.callbacks.early_stopping.append(\n        EarlyStoppingConfig(monitor=\"train/bce\", mode=\"min\", patience=20)\n    )\n    training_cfg.scheduler = SchedulerConfig(\n        name=\"reduce_on_plateau\",\n        monitor=\"metrics_anomaly/iou\",\n        mode=\"max\",\n        factor=0.5,\n        patience=5,\n    )\n    training_cfg.trainer.callbacks.checkpoint = ModelCheckpointConfig(\n        dirpath=str(output_dir / \"checkpoints\"),\n        monitor=\"metrics_anomaly/iou\",\n        mode=\"max\",\n        save_top_k=3,\n        save_last=True,\n    )\n\n    # Phase 1: Statistical initialization\n    logger.info(\"Phase 1: Statistical initialization...\")\n    stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n    stat_trainer.fit()\n\n    # Phase 2: Unfreeze nodes\n    logger.info(\"Phase 2: Unfreezing selector and RX...\")\n    unfreeze_node_names = [selector.name, rx.name, logit_head.name]\n    pipeline.unfreeze_nodes_by_name(unfreeze_node_names)\n\n    # Phase 3: Gradient training\n    logger.info(\"Phase 3: Gradient training...\")\n    grad_trainer = GradientTrainer(\n        pipeline=pipeline,\n        datamodule=datamodule,\n        loss_nodes=[bce_loss],\n        metric_nodes=[metrics_anomaly],\n        trainer_config=training_cfg.trainer,\n        optimizer_config=training_cfg.optimizer,\n        monitors=[tensorboard_node],\n    )\n    grad_trainer.fit()\n\n    # Evaluation\n    grad_trainer.validate()\n    grad_trainer.test()\n\n    # Save pipeline\n    pipeline.save_to_file(\n        str(output_dir / \"trained_models\" / \"Channel_Selector.yaml\"),\n        metadata=PipelineMetadata(\n            name=pipeline.name,\n            description=\"Channel Selector with two-phase training\",\n            tags=[\"gradient\", \"channel_selector\"],\n            author=\"cuvis.ai\",\n        ),\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run the example: <pre><code>python examples/channel_selector.py\n</code></pre></p>"},{"location":"tutorials/channel-selector/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/channel-selector/#issue-channel-collapse-all-weight-on-1-2-channels","title":"Issue: Channel Collapse (All Weight on 1-2 Channels)","text":"<p>Symptoms: Selector weights like [0.95, 0.03, 0.02, ...]</p> <p>Solution: Increase entropy regularization: <pre><code>entropy_loss = SelectorEntropyRegularizer(\n    weight=0.5,  # Increase from 0.1\n)\n</code></pre></p>"},{"location":"tutorials/channel-selector/#issue-training-unstable-loss-oscillates","title":"Issue: Training Unstable (Loss Oscillates)","text":"<p>Solution 1: Lower learning rate: <pre><code>training_cfg.optimizer.lr = 0.0001  # From 0.001\n</code></pre></p> <p>Solution 2: Slower temperature decay: <pre><code>selector = SoftChannelSelector(\n    temperature_init=10.0,  # Higher init\n    temperature_decay=0.95,  # Slower decay\n)\n</code></pre></p>"},{"location":"tutorials/channel-selector/#issue-no-improvement-over-rx-baseline","title":"Issue: No Improvement Over RX Baseline","text":"<p>Possible Causes: 1. Insufficient training epochs 2. Regularization too strong 3. Temperature decays too quickly</p> <p>Solutions: <pre><code># Increase training budget\ntraining_cfg.trainer.max_epochs = 50\n\n# Reduce regularization\nentropy_loss.weight = 0.05\ndiversity_loss.weight = 0.005\n\n# Slower temperature schedule\nselector.temperature_decay = 0.92\n</code></pre></p>"},{"location":"tutorials/channel-selector/#issue-checkpoints-not-saving","title":"Issue: Checkpoints Not Saving","text":"<p>Cause: Checkpoint directory doesn't exist or validation not running</p> <p>Solution: <pre><code># Ensure directory exists\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Verify validation runs\ntraining_cfg.trainer.check_val_every_n_epoch = 1  # Validate every epoch\n</code></pre></p>"},{"location":"tutorials/channel-selector/#next-steps","title":"Next Steps","text":"<p>Build on this tutorial:</p> <ol> <li>Deep SVDD Tutorial - Replace RX with deep learning detector</li> <li>AdaCLIP Workflow - Try other channel selectors (DRCNN, Concrete)</li> <li>gRPC Workflow - Deploy trained pipeline as a service</li> </ol> <p>Explore related concepts:</p> <ul> <li>Two-Phase Training Deep Dive - Detailed training strategy</li> <li>Execution Stages - Control node execution timing</li> <li>Gradient Trainer - Optimizer configuration</li> </ul> <p>Explore related nodes:</p> <ul> <li>SoftChannelSelector - Selector details</li> <li>Loss Nodes - All available loss functions</li> <li>Callbacks - Advanced training control</li> </ul>"},{"location":"tutorials/channel-selector/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <p>\u2705 How to add learnable channel selection to an anomaly detection pipeline \u2705 Two-phase training strategy: statistical init \u2192 gradient optimization \u2705 Loss composition with regularizers \u2705 Using callbacks for early stopping and checkpointing \u2705 Temperature annealing for discrete selection \u2705 Analyzing and visualizing selected channels</p> <p>Key Takeaways: - Channel selection improves both performance and interpretability - Gumbel-Softmax enables gradient-based discrete optimization - Two-phase training combines stability (statistical) with power (gradient) - Regularizers (entropy, diversity) prevent mode collapse - Callbacks automate training management (early stopping, LR scheduling)</p> <p>Now you're ready to explore deep learning-based anomaly detection!</p>"},{"location":"tutorials/deep-svdd-gradient/","title":"Deep SVDD Gradient","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"tutorials/deep-svdd-gradient/#tutorial-deep-svdd-for-one-class-anomaly-detection","title":"Tutorial: Deep SVDD for One-Class Anomaly Detection","text":"<p>Learn how to build a deep learning-based anomaly detection pipeline using Deep Support Vector Data Description (Deep SVDD) with two-phase training.</p>"},{"location":"tutorials/deep-svdd-gradient/#overview","title":"Overview","text":"<p>What You'll Learn:</p> <ul> <li>Understanding Deep SVDD for one-class anomaly detection</li> <li>Building deep learning pipelines with encoder and projection networks</li> <li>Advanced preprocessing: bandpass filtering and per-pixel normalization</li> <li>Center tracking with exponential moving average (EMA)</li> <li>Soft boundary loss for handling outliers</li> <li>Two-phase training for deep learning: statistical encoder init \u2192 gradient optimization</li> <li>Quantile-based decision thresholds</li> </ul> <p>Prerequisites:</p> <ul> <li>RX Statistical Tutorial - Understand statistical training</li> <li>Channel Selector Tutorial - Understand two-phase training</li> <li>Familiarity with Two-Phase Training</li> <li>Basic understanding of neural networks</li> </ul> <p>Time: ~35 minutes</p> <p>Perfect for: Users ready to leverage deep learning for anomaly detection, with focus on one-class learning and hypersphere boundaries.</p>"},{"location":"tutorials/deep-svdd-gradient/#background","title":"Background","text":""},{"location":"tutorials/deep-svdd-gradient/#what-is-deep-svdd","title":"What is Deep SVDD?","text":"<p>Deep SVDD extends classical SVDD to deep learning:</p> <p>Classical SVDD: Learns a hypersphere boundary in feature space that encloses normal data: - Center: \\(c\\) (learned from data) - Radius: Minimized to fit normal data tightly - Anomaly score: Distance from center</p> <p>Deep SVDD: Adds a neural network encoder: 1. Encoder \\(\\phi(x)\\): Maps raw data to representation space 2. Hypersphere: Encloses \\(\\phi(x)\\) for normal data 3. Anomaly score: \\(||\\\\phi(x) - c||^2\\)</p> <p>Key Advantage: Deep SVDD learns both the feature representation and the decision boundary, enabling more expressive anomaly detection than classical methods.</p>"},{"location":"tutorials/deep-svdd-gradient/#soft-boundary-vs-hard-boundary","title":"Soft Boundary vs Hard Boundary","text":"<p>Hard Boundary: All normal data must lie inside the hypersphere - Problem: Sensitive to outliers in training data</p> <p>Soft Boundary (\u03bd-SVDD): Allow a fraction \u03bd of data to lie outside - \u03bd parameter: Controls expected outlier fraction (e.g., \u03bd=0.1 allows 10% outliers) - More robust to noisy training data</p>"},{"location":"tutorials/deep-svdd-gradient/#center-tracking","title":"Center Tracking","text":"<p>Instead of pre-computing a fixed center, track it online:</p> <pre><code>center_t = \u03b1 \u00d7 current_batch_mean + (1 - \u03b1) \u00d7 center_{t-1}\n</code></pre> <ul> <li>\u03b1: EMA coefficient (e.g., 0.1)</li> <li>Adapts to data distribution during training</li> <li>More stable than fixed center</li> </ul>"},{"location":"tutorials/deep-svdd-gradient/#step-1-setup-and-data","title":"Step 1: Setup and Data","text":"<p>Same datamodule setup as previous tutorials:</p> <pre><code>from pathlib import Path\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\n\n# Setup datamodule\ndatamodule = SingleCu3sDataModule(\n    data_dir=\"data/lentils\",\n    batch_size=4,\n    num_workers=0,\n)\ndatamodule.setup(stage=\"fit\")\n\n# Create pipeline\npipeline = CuvisPipeline(\"DeepSVDD_Gradient\")\n\n# Data node\ndata_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#step-2-preprocessing-chain","title":"Step 2: Advanced Preprocessing Chain","text":""},{"location":"tutorials/deep-svdd-gradient/#bandpass-filtering-by-wavelength","title":"Bandpass Filtering by Wavelength","text":"<p>Filter hyperspectral data to a specific wavelength range:</p> <pre><code>from cuvis_ai.node.preprocessors import BandpassByWavelength\n\nbandpass_node = BandpassByWavelength(\n    min_wavelength_nm=450.0,  # Minimum wavelength (nm)\n    max_wavelength_nm=900.0,  # Maximum wavelength (nm)\n)\n</code></pre> <p>Why Bandpass? - Removes noisy edge channels - Focuses on informative spectral range - Reduces dimensionality (61 \u2192 ~35 channels)</p>"},{"location":"tutorials/deep-svdd-gradient/#per-pixel-unit-normalization","title":"Per-Pixel Unit Normalization","text":"<p>Normalize each pixel spectrum to unit length:</p> <pre><code>from cuvis_ai.node.normalization import PerPixelUnitNorm\n\nunit_norm_node = PerPixelUnitNorm(eps=1e-8)\n</code></pre> <p>Formula: <pre><code>normalized_pixel = (pixel - mean(pixel)) / ||pixel - mean(pixel)||\u2082\n</code></pre></p> <p>Why Per-Pixel? - Makes each pixel comparable in magnitude - Removes intensity variation - Focuses on spectral shape (not absolute values)</p>"},{"location":"tutorials/deep-svdd-gradient/#step-3-build-deep-svdd-architecture","title":"Step 3: Build Deep SVDD Architecture","text":""},{"location":"tutorials/deep-svdd-gradient/#encoder-initialization","title":"Encoder Initialization","text":""},{"location":"tutorials/deep-svdd-gradient/#encoder-z-score-normalizer-global-statistics","title":"Encoder: Z-Score Normalizer (Global Statistics)","text":"<p>The encoder is a learnable z-score normalizer with global statistics:</p> <pre><code>from cuvis_ai.anomaly.deep_svdd import ZScoreNormalizerGlobal\n\nencoder = ZScoreNormalizerGlobal(\n    num_channels=35,     # Channels after bandpass (inferred automatically)\n    hidden=64,           # Hidden layer size\n    sample_n=1024,       # Samples for statistical init\n    seed=42,             # Random seed for reproducibility\n)\n</code></pre> <p>What it does: - Statistical Init: Computes global mean and std from training data - Gradient Training: Fine-tunes mean/std as learnable parameters - Acts as first encoding layer</p>"},{"location":"tutorials/deep-svdd-gradient/#projection-head","title":"Projection Head","text":""},{"location":"tutorials/deep-svdd-gradient/#projection-network","title":"Projection Network","text":"<p>Maps encoder output to low-dimensional representation:</p> <pre><code>from cuvis_ai.anomaly.deep_svdd import DeepSVDDProjection\n\nprojection = DeepSVDDProjection(\n    in_channels=35,   # Input channels (after encoder)\n    rep_dim=32,       # Representation dimension (output)\n    hidden=128,       # Hidden layer size\n)\n</code></pre> <p>Architecture: <pre><code>Input (B, H, W, 35) \u2192 Linear(35 \u2192 128) \u2192 ReLU \u2192 Linear(128 \u2192 32) \u2192 Output (B, H, W, 32)\n</code></pre></p> <p>Why low-dimensional? - Easier to learn tight hypersphere - Reduces overfitting - rep_dim=32 is typical for hyperspectral data</p>"},{"location":"tutorials/deep-svdd-gradient/#step-4-center-tracking-and-scoring","title":"Step 4: Center Tracking and Scoring","text":""},{"location":"tutorials/deep-svdd-gradient/#center-tracker-with-ema","title":"Center Tracker with EMA","text":"<p>Track hypersphere center online:</p> <pre><code>from cuvis_ai.anomaly.deep_svdd import DeepSVDDCenterTracker\n\ncenter_tracker = DeepSVDDCenterTracker(\n    rep_dim=32,     # Representation dimension (matches projection output)\n    alpha=0.1,      # EMA coefficient (10% current, 90% history)\n)\n</code></pre> <p>Outputs: - <code>center</code>: Current hypersphere center [rep_dim] - <code>metrics</code>: Sphere radius, number of outliers</p>"},{"location":"tutorials/deep-svdd-gradient/#training-loss","title":"Training Loss","text":""},{"location":"tutorials/deep-svdd-gradient/#soft-boundary-loss","title":"Soft Boundary Loss","text":"<pre><code>from cuvis_ai.node.losses import DeepSVDDSoftBoundaryLoss\n\nloss_node = DeepSVDDSoftBoundaryLoss(\n    name=\"deepsvdd_loss\",\n    nu=0.1,  # Allow 10% of data to be outliers\n)\n</code></pre> <p>Loss Formula: <pre><code>Loss = (1/n) \u2211 max(0, ||\u03c6(x) - c||\u00b2 - R\u00b2) + (1/\u03bdn) \u00d7 R\u00b2\n</code></pre></p> <p>Where: - R\u00b2: Radius term (penalized by \u03bd) - max(0, ...): Hinge loss (only penalize if outside radius)</p>"},{"location":"tutorials/deep-svdd-gradient/#anomaly-scoring","title":"Anomaly Scoring","text":""},{"location":"tutorials/deep-svdd-gradient/#anomaly-scores","title":"Anomaly Scores","text":"<p>Compute distance-based scores:</p> <pre><code>from cuvis_ai.anomaly.deep_svdd import DeepSVDDScores\n\nscore_node = DeepSVDDScores()\n</code></pre> <p>Score: \\(||\\\\phi(x) - c||^2\\) (squared Euclidean distance)</p>"},{"location":"tutorials/deep-svdd-gradient/#step-5-decision-and-metrics","title":"Step 5: Decision and Metrics","text":""},{"location":"tutorials/deep-svdd-gradient/#quantile-thresholding","title":"Quantile Thresholding","text":""},{"location":"tutorials/deep-svdd-gradient/#quantile-based-decision","title":"Quantile-Based Decision","text":"<p>Instead of fixed threshold, use quantile:</p> <pre><code>from cuvis_ai.deciders.binary_decider import QuantileBinaryDecider\n\ndecider_node = QuantileBinaryDecider(\n    quantile=0.995,  # Top 0.5% of scores are anomalies\n)\n</code></pre> <p>Advantage: Adapts to score distribution automatically.</p>"},{"location":"tutorials/deep-svdd-gradient/#score-visualization","title":"Score Visualization","text":""},{"location":"tutorials/deep-svdd-gradient/#metrics-and-visualization","title":"Metrics and Visualization","text":"<pre><code>from cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.visualizations import AnomalyMask, ScoreHeatmapVisualizer\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\n\nmetrics_node = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n\n# Visualize anomaly masks\nviz_mask = AnomalyMask(name=\"mask\", channel=30, up_to=5)\n\n# Visualize score heatmaps\nscore_viz = ScoreHeatmapVisualizer(\n    name=\"score_heatmap\",\n    normalize_scores=True,  # Normalize to [0, 1] for visualization\n    up_to=5,\n)\n\ntensorboard_node = TensorBoardMonitorNode(\n    output_dir=\"runs/tensorboard\",\n    run_name=pipeline.name,\n)\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#step-6-connect-the-pipeline","title":"Step 6: Connect the Pipeline","text":"<pre><code>pipeline.connect(\n    # Preprocessing chain: data \u2192 bandpass \u2192 unit_norm \u2192 encoder\n    (data_node.outputs.cube, bandpass_node.data),\n    (data_node.outputs.wavelengths, bandpass_node.wavelengths),\n    (bandpass_node.filtered, unit_norm_node.data),\n    (unit_norm_node.normalized, encoder.data),\n\n    # Deep SVDD: encoder \u2192 projection \u2192 center_tracker\n    (encoder.normalized, projection.data),\n    (projection.embeddings, center_tracker.embeddings),\n\n    # Loss computation\n    (projection.embeddings, loss_node.embeddings),\n    (center_tracker.center, loss_node.center),\n\n    # Scoring: embeddings + center \u2192 scores\n    (projection.embeddings, score_node.embeddings),\n    (center_tracker.center, score_node.center),\n\n    # Decision: scores \u2192 quantile threshold \u2192 binary decisions\n    (score_node.scores, decider_node.logits),\n\n    # Metrics: scores + decisions + ground truth\n    (score_node.scores, metrics_node.logits),\n    (decider_node.decisions, metrics_node.decisions),\n    (data_node.outputs.mask, metrics_node.targets),\n\n    # Visualization\n    (score_node.scores, score_viz.scores),\n    (score_node.scores, viz_mask.scores),\n    (decider_node.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (data_node.outputs.cube, viz_mask.cube),\n\n    # Monitoring\n    (metrics_node.metrics, tensorboard_node.metrics),\n    (center_tracker.metrics, tensorboard_node.metrics),\n    (score_viz.artifacts, tensorboard_node.artifacts),\n    (viz_mask.artifacts, tensorboard_node.artifacts),\n)\n</code></pre> <p>Pipeline Flow:</p> <pre><code>graph TD\n    A[Data] --&gt;|cube| B[BandpassByWavelength]\n    A --&gt;|wavelengths| B\n    B --&gt;|filtered| C[PerPixelUnitNorm]\n    C --&gt;|normalized| D[Encoder ZScore]\n    D --&gt;|encoded| E[DeepSVDDProjection]\n    E --&gt;|embeddings| F[CenterTracker]\n    E --&gt;|embeddings| G[SoftBoundaryLoss]\n    F --&gt;|center| G\n    E --&gt;|embeddings| H[DeepSVDDScores]\n    F --&gt;|center| H\n    H --&gt;|scores| I[QuantileDecider]\n    I --&gt;|decisions| J[Metrics]\n    A --&gt;|mask| J</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#step-7-phase-1-statistical-encoder-initialization","title":"Step 7: Phase 1 - Statistical Encoder Initialization","text":"<p>Initialize the encoder with statistical training:</p> <pre><code>from cuvis_ai_core.training import StatisticalTrainer\nfrom loguru import logger\n\nlogger.info(\"Phase 1: Statistical fit of DeepSVDD encoder...\")\nstat_trainer = StatisticalTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n)\nstat_trainer.fit()\n</code></pre> <p>What Gets Initialized: - Encoder (ZScoreNormalizerGlobal): Computes global mean and std - Center Tracker: Initializes center from projection embeddings</p> <p>Console Output: <pre><code>[INFO] Phase 1: Statistical encoder initialization...\n[INFO] ZScoreNormalizerGlobal: Computing global statistics...\n[INFO] Global mean shape: (35,), Global std shape: (35,)\n[INFO] DeepSVDDCenterTracker: Initial center computed\n[INFO] Statistical initialization complete\n</code></pre></p>"},{"location":"tutorials/deep-svdd-gradient/#step-8-phase-2-unfreeze-encoder","title":"Step 8: Phase 2 - Unfreeze Encoder","text":"<p>Unfreeze the encoder for gradient training:</p> <pre><code>logger.info(\"Phase 2: Unfreezing encoder for gradient training...\")\n\nunfreeze_node_names = [encoder.name]\npipeline.unfreeze_nodes_by_name(unfreeze_node_names)\n\nlogger.info(f\"Unfrozen nodes: {unfreeze_node_names}\")\n</code></pre> <p>Trainable After Unfreezing: - \u2705 Encoder (ZScoreNormalizerGlobal): Mean and std become learnable - \u2705 Projection: Network weights - \u2705 Center Tracker: Center updates via EMA - \ud83d\udd12 Frozen: Preprocessing nodes (bandpass, unit_norm)</p>"},{"location":"tutorials/deep-svdd-gradient/#step-9-configure-training","title":"Step 9: Configure Training","text":""},{"location":"tutorials/deep-svdd-gradient/#setup-training-configuration","title":"Setup Training Configuration","text":"<pre><code>from cuvis_ai_core.training.config import (\n    TrainingConfig,\n    CallbacksConfig,\n    EarlyStoppingConfig,\n    ModelCheckpointConfig,\n    SchedulerConfig,\n)\n\ntraining_cfg = TrainingConfig.from_dict(cfg.training)  # Load from config\nif training_cfg.trainer.callbacks is None:\n    training_cfg.trainer.callbacks = CallbacksConfig()\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#early-stopping-on-deep-svdd-loss","title":"Early Stopping on Deep SVDD Loss","text":"<pre><code>training_cfg.trainer.callbacks.early_stopping.append(\n    EarlyStoppingConfig(\n        monitor=\"train/deepsvdd_loss\",\n        mode=\"min\",\n        patience=15,  # Stop after 15 epochs without improvement\n    )\n)\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#checkpointing-on-validation-iou","title":"Checkpointing on Validation IoU","text":"<pre><code>training_cfg.trainer.callbacks.checkpoint = ModelCheckpointConfig(\n    dirpath=\"outputs/checkpoints\",\n    monitor=\"metrics_anomaly/iou\",\n    mode=\"max\",\n    save_top_k=3,\n    save_last=True,\n    filename=\"{epoch:02d}\",\n    verbose=True,\n)\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<pre><code>training_cfg.scheduler = SchedulerConfig(\n    name=\"reduce_on_plateau\",\n    monitor=\"metrics_anomaly/iou\",\n    mode=\"max\",\n    factor=0.5,\n    patience=5,\n)\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#step-10-phase-3-gradient-training","title":"Step 10: Phase 3 - Gradient Training","text":"<p>Run gradient optimization:</p> <pre><code>from cuvis_ai_core.training import GradientTrainer\n\nlogger.info(\"Phase 3: Gradient training with DeepSVDDSoftBoundaryLoss...\")\n\ngrad_trainer = GradientTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n    loss_nodes=[loss_node],\n    metric_nodes=[metrics_node],\n    trainer_config=training_cfg.trainer,\n    optimizer_config=training_cfg.optimizer,\n    monitors=[tensorboard_node],\n)\n\ngrad_trainer.fit()\n</code></pre> <p>Training Loop: 1. Forward pass: data \u2192 preprocessing \u2192 encoder \u2192 projection \u2192 embeddings 2. Update center with EMA 3. Compute soft boundary loss 4. Backpropagate gradients 5. Update encoder and projection weights 6. Validate and checkpoint</p> <p>Console Output: <pre><code>[INFO] Phase 3: Gradient training...\nEpoch 0: train/deepsvdd_loss=2.134, metrics_anomaly/iou=0.687\nEpoch 1: train/deepsvdd_loss=1.892, metrics_anomaly/iou=0.723\nEpoch 5: train/deepsvdd_loss=1.456, metrics_anomaly/iou=0.789\n[INFO] Checkpoint saved: epoch=5, iou=0.789\nEpoch 10: train/deepsvdd_loss=1.123, metrics_anomaly/iou=0.831\n[INFO] Reducing learning rate to 0.0005\n...\nEpoch 25: train/deepsvdd_loss=0.876, metrics_anomaly/iou=0.865\n[INFO] Early stopping triggered\n[INFO] Best model: epoch=23, iou=0.868\n</code></pre></p>"},{"location":"tutorials/deep-svdd-gradient/#step-11-evaluation","title":"Step 11: Evaluation","text":""},{"location":"tutorials/deep-svdd-gradient/#validation","title":"Validation","text":"<pre><code>logger.info(\"Running validation with best checkpoint...\")\nval_results = grad_trainer.validate(ckpt_path=\"last\")\nlogger.info(f\"Validation results: {val_results}\")\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#test-evaluation","title":"Test Evaluation","text":"<pre><code>logger.info(\"Running test evaluation with best checkpoint...\")\ntest_results = grad_trainer.test(ckpt_path=\"last\")\nlogger.info(f\"Test results: {test_results}\")\n</code></pre> <p>Expected Performance: - IoU: 0.85-0.90 (significantly better than RX ~0.72) - Precision: 0.88-0.93 - Recall: 0.85-0.91</p>"},{"location":"tutorials/deep-svdd-gradient/#step-12-analyze-results","title":"Step 12: Analyze Results","text":""},{"location":"tutorials/deep-svdd-gradient/#center-tracker-metrics","title":"Center Tracker Metrics","text":"<p>TensorBoard logs center tracker metrics: - <code>center_tracker/sphere_radius</code>: Hypersphere radius - <code>center_tracker/num_outliers</code>: Count of outliers per batch</p> <p>Interpretation: - Small radius: Tight fit around normal data - Few outliers: Training data is clean - Many outliers: May need to adjust \u03bd parameter</p>"},{"location":"tutorials/deep-svdd-gradient/#score-heatmaps","title":"Score Heatmaps","text":"<p>Visualize anomaly scores as heatmaps: - Blue: Low scores (normal) - Yellow/Red: High scores (anomalous) - Shows spatial distribution of anomalies</p>"},{"location":"tutorials/deep-svdd-gradient/#step-13-save-pipeline","title":"Step 13: Save Pipeline","text":"<pre><code>from cuvis_ai_core.training.config import PipelineMetadata, TrainRunConfig\n\nresults_dir = Path(\"outputs/trained_models\")\npipeline_output_path = results_dir / \"DeepSVDD_Gradient.yaml\"\n\n# Save trained pipeline\npipeline.save_to_file(\n    str(pipeline_output_path),\n    metadata=PipelineMetadata(\n        name=pipeline.name,\n        description=\"Deep SVDD with two-phase training\",\n        tags=[\"gradient\", \"statistical\", \"deep_svdd\", \"anomaly_detection\"],\n        author=\"cuvis.ai\",\n    ),\n)\n\n# Save trainrun config\ntrainrun_config = TrainRunConfig(\n    name=\"deep_svdd_gradient\",\n    pipeline=pipeline.serialize(),\n    data=datamodule_config,\n    training=training_cfg,\n    output_dir=\"outputs\",\n    loss_nodes=[\"deepsvdd_loss\"],\n    metric_nodes=[\"metrics_anomaly\"],\n    freeze_nodes=[],\n    unfreeze_nodes=unfreeze_node_names,\n)\ntrainrun_config.save_to_file(\"outputs/trained_models/deep_svdd_trainrun.yaml\")\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#complete-example-script","title":"Complete Example Script","text":"<p>Full runnable script (<code>examples/advanced/deep_svdd_gradient_training.py</code>):</p> <pre><code>from pathlib import Path\nimport hydra\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training import GradientTrainer, StatisticalTrainer\nfrom cuvis_ai_core.training.config import (\n    CallbacksConfig,\n    EarlyStoppingConfig,\n    ModelCheckpointConfig,\n    PipelineMetadata,\n    SchedulerConfig,\n    TrainingConfig,\n)\nfrom loguru import logger\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom cuvis_ai.anomaly.deep_svdd import (\n    DeepSVDDCenterTracker,\n    DeepSVDDProjection,\n    DeepSVDDScores,\n    ZScoreNormalizerGlobal,\n)\nfrom cuvis_ai.deciders.binary_decider import QuantileBinaryDecider\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.losses import DeepSVDDSoftBoundaryLoss\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.node.normalization import PerPixelUnitNorm\nfrom cuvis_ai.node.preprocessors import BandpassByWavelength\nfrom cuvis_ai.node.visualizations import AnomalyMask, ScoreHeatmapVisualizer\n\n\n@hydra.main(config_path=\"../../configs/\", config_name=\"trainrun/deep_svdd\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"Deep SVDD Anomaly Detection with Two-Phase Training.\"\"\"\n\n    logger.info(\"=== Deep SVDD Gradient Training ===\")\n    output_dir = Path(cfg.output_dir)\n\n    # Stage 1: Setup datamodule\n    datamodule = SingleCu3sDataModule(**cfg.data)\n    datamodule.setup(stage=\"fit\")\n\n    # Stage 2: Build Deep SVDD pipeline\n    pipeline = CuvisPipeline(\"DeepSVDD_Gradient\")\n\n    data_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\n    bandpass_node = BandpassByWavelength(min_wavelength_nm=450.0, max_wavelength_nm=900.0)\n    unit_norm_node = PerPixelUnitNorm(eps=1e-8)\n    encoder = ZScoreNormalizerGlobal(num_channels=35, hidden=64, sample_n=1024, seed=42)\n    projection = DeepSVDDProjection(in_channels=35, rep_dim=32, hidden=128)\n    center_tracker = DeepSVDDCenterTracker(rep_dim=32, alpha=0.1)\n    loss_node = DeepSVDDSoftBoundaryLoss(name=\"deepsvdd_loss\", nu=0.1)\n    score_node = DeepSVDDScores()\n    decider_node = QuantileBinaryDecider(quantile=0.995)\n    metrics_node = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n    viz_mask = AnomalyMask(name=\"mask\", channel=30, up_to=5)\n    score_viz = ScoreHeatmapVisualizer(name=\"score_heatmap\", normalize_scores=True, up_to=5)\n    tensorboard_node = TensorBoardMonitorNode(\n        output_dir=str(output_dir / \"..\" / \"tensorboard\"),\n        run_name=pipeline.name,\n    )\n\n    # Stage 3: Connect pipeline\n    pipeline.connect(\n        (data_node.outputs.cube, bandpass_node.data),\n        (data_node.outputs.wavelengths, bandpass_node.wavelengths),\n        (bandpass_node.filtered, unit_norm_node.data),\n        (unit_norm_node.normalized, encoder.data),\n        (encoder.normalized, projection.data),\n        (projection.embeddings, center_tracker.embeddings),\n        (projection.embeddings, loss_node.embeddings),\n        (center_tracker.center, loss_node.center),\n        (projection.embeddings, score_node.embeddings),\n        (center_tracker.center, score_node.center),\n        (score_node.scores, decider_node.logits),\n        (score_node.scores, metrics_node.logits),\n        (decider_node.decisions, metrics_node.decisions),\n        (data_node.outputs.mask, metrics_node.targets),\n        (score_node.scores, score_viz.scores),\n        (metrics_node.metrics, tensorboard_node.metrics),\n        (center_tracker.metrics, tensorboard_node.metrics),\n        (score_viz.artifacts, tensorboard_node.artifacts),\n    )\n\n    # Configure training\n    training_cfg = TrainingConfig.from_dict(OmegaConf.to_container(cfg.training, resolve=True))\n    if training_cfg.trainer.callbacks is None:\n        training_cfg.trainer.callbacks = CallbacksConfig()\n\n    training_cfg.trainer.callbacks.early_stopping.append(\n        EarlyStoppingConfig(monitor=\"train/deepsvdd_loss\", mode=\"min\", patience=15)\n    )\n    training_cfg.trainer.callbacks.checkpoint = ModelCheckpointConfig(\n        dirpath=str(output_dir / \"checkpoints\"),\n        monitor=\"metrics_anomaly/iou\",\n        mode=\"max\",\n        save_top_k=3,\n        save_last=True,\n    )\n\n    # Phase 1: Statistical initialization\n    logger.info(\"Phase 1: Statistical encoder initialization...\")\n    stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n    stat_trainer.fit()\n\n    # Phase 2: Unfreeze encoder\n    logger.info(\"Phase 2: Unfreezing encoder...\")\n    pipeline.unfreeze_nodes_by_name([encoder.name])\n\n    # Phase 3: Gradient training\n    logger.info(\"Phase 3: Gradient training...\")\n    grad_trainer = GradientTrainer(\n        pipeline=pipeline,\n        datamodule=datamodule,\n        loss_nodes=[loss_node],\n        metric_nodes=[metrics_node],\n        trainer_config=training_cfg.trainer,\n        optimizer_config=training_cfg.optimizer,\n        monitors=[tensorboard_node],\n    )\n    grad_trainer.fit()\n\n    # Evaluation\n    grad_trainer.validate()\n    grad_trainer.test()\n\n    # Save pipeline\n    pipeline.save_to_file(\n        str(output_dir / \"trained_models\" / \"DeepSVDD_Gradient.yaml\"),\n        metadata=PipelineMetadata(\n            name=pipeline.name,\n            description=\"Deep SVDD with two-phase training\",\n            tags=[\"deep_learning\", \"deep_svdd\"],\n            author=\"cuvis.ai\",\n        ),\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run the example: <pre><code>python examples/advanced/deep_svdd_gradient_training.py\n</code></pre></p>"},{"location":"tutorials/deep-svdd-gradient/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/deep-svdd-gradient/#issue-center-moves-too-much-during-training","title":"Issue: Center Moves Too Much During Training","text":"<p>Symptoms: center_tracker metrics show large center displacement</p> <p>Solution: Reduce EMA alpha: <pre><code>center_tracker = DeepSVDDCenterTracker(\n    rep_dim=32,\n    alpha=0.05,  # Reduce from 0.1 for more stability\n)\n</code></pre></p>"},{"location":"tutorials/deep-svdd-gradient/#issue-loss-doesnt-decrease","title":"Issue: Loss Doesn't Decrease","text":"<p>Possible Causes: 1. Learning rate too low 2. \u03bd parameter too restrictive 3. Encoder not unfrozen</p> <p>Solutions: <pre><code># 1. Increase learning rate\ntraining_cfg.optimizer.lr = 0.001  # From 0.0001\n\n# 2. Relax soft boundary\nloss_node = DeepSVDDSoftBoundaryLoss(nu=0.2)  # Allow more outliers\n\n# 3. Verify unfreeze\nprint(f\"Encoder frozen: {encoder.is_frozen()}\")  # Should be False\n</code></pre></p>"},{"location":"tutorials/deep-svdd-gradient/#issue-memory-error-oom","title":"Issue: Memory Error (OOM)","text":"<p>Solution: Reduce batch size or model size: <pre><code># Reduce batch size\ndatamodule = SingleCu3sDataModule(batch_size=2)  # From 4\n\n# Reduce hidden layer size\nencoder = ZScoreNormalizerGlobal(hidden=32)  # From 64\nprojection = DeepSVDDProjection(hidden=64)  # From 128\n</code></pre></p>"},{"location":"tutorials/deep-svdd-gradient/#issue-overfitting-train-iou-val-iou","title":"Issue: Overfitting (Train IoU &gt;&gt; Val IoU)","text":"<p>Solutions: 1. Add dropout (requires modifying projection network) 2. Early stopping with patience 3. Reduce model capacity</p> <pre><code># Smaller representation dimension\nprojection = DeepSVDDProjection(rep_dim=16)  # From 32\n\n# Stricter early stopping\nEarlyStoppingConfig(patience=10)  # From 15\n</code></pre>"},{"location":"tutorials/deep-svdd-gradient/#next-steps","title":"Next Steps","text":"<p>Build on this tutorial:</p> <ol> <li>AdaCLIP Workflow - Combine deep learning with plugin-based models</li> <li>gRPC Workflow - Deploy Deep SVDD pipeline as a service</li> </ol> <p>Explore related concepts:</p> <ul> <li>Two-Phase Training - Deep dive into training strategies</li> <li>Execution Stages - Control when nodes execute</li> <li>GPU Acceleration - Speed up training</li> </ul> <p>Explore related nodes:</p> <ul> <li>DeepSVDDProjection - Projection network details</li> <li>DeepSVDDCenterTracker - Center tracking mechanics</li> <li>ZScoreNormalizerGlobal - Encoder details</li> </ul>"},{"location":"tutorials/deep-svdd-gradient/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <p>\u2705 How to build a deep learning pipeline for one-class anomaly detection \u2705 Deep SVDD architecture: encoder + projection + hypersphere \u2705 Center tracking with exponential moving average \u2705 Soft boundary loss for handling outliers \u2705 Advanced preprocessing: bandpass filtering and per-pixel normalization \u2705 Two-phase training for deep learning models</p> <p>Key Takeaways: - Deep SVDD learns both feature representation and decision boundary - Soft boundary (\u03bd-SVDD) is more robust than hard boundary - Center tracking adapts to data distribution during training - Two-phase training (statistical encoder init \u2192 gradient optimization) is crucial - Quantile-based decisions adapt to score distribution</p> <p>Now you're ready to explore advanced plugin-based models and deployment strategies!</p>"},{"location":"tutorials/grpc-workflow/","title":"gRPC Workflow","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"tutorials/grpc-workflow/#grpc-workflow-distributed-training-inference","title":"gRPC Workflow: Distributed Training &amp; Inference","text":"<p>Learn how to use CUVIS.AI's gRPC API for distributed training, remote inference, and production deployment.</p>"},{"location":"tutorials/grpc-workflow/#overview","title":"Overview","text":"<p>This tutorial demonstrates distributed training and inference using CUVIS.AI's gRPC (Google Remote Procedure Call) service. gRPC enables client-server communication for running pipelines on remote hardware, separating training infrastructure from client applications.</p> <p>What You'll Learn: - Starting and connecting to gRPC servers - Remote training with streaming progress updates - Distributed inference on trained pipelines - Configuration management with Hydra overrides - Session management and resource cleanup - Production deployment patterns</p> <p>Time to Complete: 25-30 minutes</p> <p>Prerequisites: - Completion of Channel Selector Tutorial - Understanding training workflows - Basic networking knowledge (client-server, localhost, ports) - Python 3.10+, PyTorch 2.0+, gRPC library installed</p>"},{"location":"tutorials/grpc-workflow/#background","title":"Background","text":""},{"location":"tutorials/grpc-workflow/#why-grpc-for-ml-pipelines","title":"Why gRPC for ML Pipelines?","text":"<p>gRPC is a high-performance RPC framework developed by Google, ideal for: - Remote Execution - Train models on GPU servers from local machines - Microservices - Deploy pipelines as independent services - Streaming - Real-time progress updates during training - Language-Agnostic - Client libraries for Python, C++, Java, etc. - Production-Ready - Used by Google, Netflix, Square</p> <p>CUVIS.AI gRPC Service: - Server: Manages pipelines, training, and inference - Clients: Connect from anywhere to execute workflows - Sessions: Isolated contexts for concurrent users - Config Service: Hydra composition and dynamic overrides</p>"},{"location":"tutorials/grpc-workflow/#server-client-architecture","title":"Server-Client Architecture","text":"<pre><code>graph LR\n    A[Client Application] --&gt;|gRPC| B[Server: localhost:50051]\n    B --&gt; C[Pipeline Manager]\n    B --&gt; D[Training Engine]\n    B --&gt; E[Inference Engine]\n    C --&gt; F[GPU/CPU Resources]\n    D --&gt; F\n    E --&gt; F\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style F fill:#ffe1e1</code></pre> <p>Workflow: 1. Server: Launch on GPU machine (<code>uv run python -m cuvis_ai.grpc.production_server</code>) 2. Client: Connect, create session, configure pipeline 3. Training: Stream progress updates back to client 4. Inference: Send data, receive predictions 5. Cleanup: Close session, release resources</p>"},{"location":"tutorials/grpc-workflow/#starting-the-grpc-server","title":"Starting the gRPC Server","text":""},{"location":"tutorials/grpc-workflow/#step-1-launch-server-locally","title":"Step 1: Launch Server Locally","text":"<pre><code># Terminal 1: Start gRPC server\nuv run python -m cuvis_ai.grpc.production_server\n</code></pre> <p>Expected Output: <pre><code>[INFO] Starting CUVIS.AI gRPC server on 0.0.0.0:50051\n[INFO] Server started successfully\n[INFO] Waiting for incoming connections...\n</code></pre></p> <p>Server Configuration: - Host: <code>0.0.0.0</code> (all network interfaces) - Port: <code>50051</code> (default gRPC port) - Max Message Size: 300MB (for hyperspectral data) - Concurrency: Supports multiple sessions</p>"},{"location":"tutorials/grpc-workflow/#step-2-verify-server-connection","title":"Step 2: Verify Server Connection","text":"<pre><code>import grpc\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2, cuvis_ai_pb2_grpc\n\n# Create channel with insecure connection (local development)\nchannel = grpc.insecure_channel(\"localhost:50051\")\nstub = cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n\n# Test connection with health check\nresponse = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\nprint(f\"Connected! Session ID: {response.session_id}\")\n</code></pre> <p>Output: <pre><code>Connected! Session ID: d4e1a8c7-9b2f-4e3a-a5d6-7f8e9c0b1a2d\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#workflow-1-remote-training","title":"Workflow 1: Remote Training","text":""},{"location":"tutorials/grpc-workflow/#the-phase-5-workflow","title":"The Phase 5 Workflow","text":"<p>CUVIS.AI follows an explicit configuration workflow:</p> <ol> <li>CreateSession - Initialize isolated session</li> <li>SetSessionSearchPaths - Register config directories</li> <li>ResolveConfig - Hydra composition + overrides</li> <li>SetTrainRunConfig - Apply resolved config</li> <li>Train - Execute statistical/gradient training</li> <li>SavePipeline/SaveTrainRun - Persist results</li> </ol>"},{"location":"tutorials/grpc-workflow/#complete-training-example","title":"Complete Training Example","text":"<pre><code>\"\"\"End-to-end gRPC training workflow.\"\"\"\n\nfrom pathlib import Path\nimport numpy as np\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2, helpers\nfrom workflow_utils import (\n    build_stub,\n    config_search_paths,\n    create_session_with_search_paths,\n    resolve_trainrun_config,\n    apply_trainrun_config,\n    format_progress,\n)\n\n# 1. Connect to server\nstub = build_stub(\"localhost:50051\")\n\n# 2. Create session + register config paths\nsession_id = create_session_with_search_paths(stub, config_search_paths())\nprint(f\"Session created: {session_id}\")\n\n# 3. Resolve trainrun config (Hydra composition)\nresolved, config_dict = resolve_trainrun_config(\n    stub,\n    session_id,\n    \"deep_svdd\",  # Trainrun name from configs/trainrun/\n    overrides=[\n        \"training.trainer.max_epochs=50\",\n        \"training.optimizer.lr=0.0005\",\n        \"data.batch_size=8\",\n    ],\n)\nprint(f\"Resolved trainrun: {config_dict['name']}\")\nprint(f\"Pipeline: {config_dict['pipeline']['metadata']['name']}\")\n\n# 4. Apply trainrun config to session\napply_trainrun_config(stub, session_id, resolved.config_bytes)\n\n# 5. Statistical training (initialization)\nprint(\"Starting statistical training...\")\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_STATISTICAL,\n    )\n):\n    print(f\"[statistical] {format_progress(progress)}\")\n\n# 6. Gradient training (optimization)\nprint(\"Starting gradient training...\")\nfor progress in stub.Train(\n    cuvis_ai_pb2.TrainRequest(\n        session_id=session_id,\n        trainer_type=cuvis_ai_pb2.TRAINER_TYPE_GRADIENT,\n    )\n):\n    print(f\"[gradient] {format_progress(progress)}\")\n\n# 7. Save trained pipeline\npipeline_path = str(Path(\"outputs\") / \"trained_pipeline.yaml\")\ntrainrun_path = str(Path(\"outputs\") / \"trainrun_config.yaml\")\n\nsave_pipeline = stub.SavePipeline(\n    cuvis_ai_pb2.SavePipelineRequest(\n        session_id=session_id,\n        pipeline_path=pipeline_path,\n    )\n)\nprint(f\"Pipeline saved: {save_pipeline.pipeline_path}\")\nprint(f\"Weights saved: {save_pipeline.weights_path}\")\n\nsave_trainrun = stub.SaveTrainRun(\n    cuvis_ai_pb2.SaveTrainRunRequest(\n        session_id=session_id,\n        trainrun_path=trainrun_path,\n    )\n)\nprint(f\"Trainrun config saved: {save_trainrun.trainrun_path}\")\n\n# 8. Close session\nstub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\nprint(\"Session closed.\")\n</code></pre>"},{"location":"tutorials/grpc-workflow/#training-progress-output","title":"Training Progress Output","text":"<p>Statistical Training: <pre><code>[statistical] [TRAIN] IN_PROGRESS | message=Initializing RXGlobal detector\n[statistical] [TRAIN] IN_PROGRESS | message=Processing batch 1/50\n[statistical] [TRAIN] IN_PROGRESS | message=Processing batch 50/50\n[statistical] [TRAIN] COMPLETED | message=Statistical training complete\n</code></pre></p> <p>Gradient Training: <pre><code>[gradient] [TRAIN] IN_PROGRESS | losses={'deep_svdd_loss': 0.4523} | metrics={'iou': 0.6823}\n[gradient] [TRAIN] IN_PROGRESS | losses={'deep_svdd_loss': 0.3891} | metrics={'iou': 0.7234}\n[gradient] [VALIDATE] IN_PROGRESS | metrics={'val_iou': 0.7456, 'val_precision': 0.8123}\n[gradient] [TRAIN] COMPLETED | message=Training complete (50 epochs)\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#streaming-progress-updates","title":"Streaming Progress Updates","text":"<p>The <code>Train</code> RPC returns a stream of progress messages:</p> <pre><code>for progress in stub.Train(train_request):\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n\n    if progress.losses:\n        losses = dict(progress.losses)\n        print(f\"[{stage}] Losses: {losses}\")\n\n    if progress.metrics:\n        metrics = dict(progress.metrics)\n        print(f\"[{stage}] Metrics: {metrics}\")\n\n    if progress.status == cuvis_ai_pb2.TRAIN_STATUS_COMPLETED:\n        print(\"Training finished!\")\n        break\n</code></pre>"},{"location":"tutorials/grpc-workflow/#workflow-2-remote-inference","title":"Workflow 2: Remote Inference","text":""},{"location":"tutorials/grpc-workflow/#inference-only-client","title":"Inference-Only Client","text":"<p>For production deployment, you typically train once and infer many times. Here's an inference-only client:</p> <pre><code>\"\"\"Restore trained pipeline for inference using gRPC.\"\"\"\n\nfrom pathlib import Path\nfrom cuvis_ai.data.datasets import SingleCu3sDataset\nfrom cuvis_ai_core.grpc import cuvis_ai_pb2, helpers\nfrom torch.utils.data import DataLoader\nfrom workflow_utils import (\n    build_stub,\n    config_search_paths,\n    create_session_with_search_paths,\n)\n\ndef run_inference(\n    pipeline_path: str | Path,\n    weights_path: str | Path,\n    cu3s_file_path: str | Path,\n    server_address: str = \"localhost:50051\",\n):\n    \"\"\"Restore pipeline and run inference on CU3S data.\"\"\"\n\n    # 1. Connect with larger message size for HSI data\n    stub = build_stub(server_address, max_msg_size=600 * 1024 * 1024)\n\n    # 2. Create session\n    session_id = create_session_with_search_paths(stub, config_search_paths())\n    print(f\"Session created: {session_id}\")\n\n    # 3. Resolve pipeline config\n    pipeline_config = stub.ResolveConfig(\n        cuvis_ai_pb2.ResolveConfigRequest(\n            session_id=session_id,\n            config_type=\"pipeline\",\n            path=str(pipeline_path),\n            overrides=[],  # Optional config overrides\n        )\n    )\n\n    # 4. Load pipeline structure\n    stub.LoadPipeline(\n        cuvis_ai_pb2.LoadPipelineRequest(\n            session_id=session_id,\n            pipeline=cuvis_ai_pb2.PipelineConfig(\n                config_bytes=pipeline_config.config_bytes\n            ),\n        )\n    )\n\n    # 5. Load trained weights\n    stub.LoadPipelineWeights(\n        cuvis_ai_pb2.LoadPipelineWeightsRequest(\n            session_id=session_id,\n            weights_path=str(weights_path),\n            strict=True,  # Require all weights to match\n        )\n    )\n\n    # 6. Get pipeline input/output specs\n    inputs_response = stub.GetPipelineInputs(\n        cuvis_ai_pb2.GetPipelineInputsRequest(session_id=session_id)\n    )\n    outputs_response = stub.GetPipelineOutputs(\n        cuvis_ai_pb2.GetPipelineOutputsRequest(session_id=session_id)\n    )\n\n    print(\"\\nPipeline Input Specs:\")\n    for name, spec in inputs_response.input_specs.items():\n        shape_str = \"x\".join(str(dim) for dim in spec.shape)\n        print(f\"  {name}: [{shape_str}], dtype={cuvis_ai_pb2.DType.Name(spec.dtype)}\")\n\n    print(\"\\nPipeline Output Specs:\")\n    for name, spec in outputs_response.output_specs.items():\n        shape_str = \"x\".join(str(dim) for dim in spec.shape)\n        print(f\"  {name}: [{shape_str}], dtype={cuvis_ai_pb2.DType.Name(spec.dtype)}\")\n\n    # 7. Load CU3S data\n    print(f\"\\nLoading data from {cu3s_file_path}\")\n    dataset = SingleCu3sDataset(\n        cu3s_file_path=str(cu3s_file_path),\n        processing_mode=\"Reflectance\",\n    )\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n\n    # 8. Run inference on each batch\n    results = []\n    for batch_idx, batch in enumerate(dataloader):\n        print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}\")\n\n        # Execute inference\n        inference_response = stub.Inference(\n            cuvis_ai_pb2.InferenceRequest(\n                session_id=session_id,\n                inputs=cuvis_ai_pb2.InputBatch(\n                    cube=helpers.tensor_to_proto(batch[\"cube\"]),\n                    wavelengths=helpers.tensor_to_proto(batch[\"wavelengths\"]),\n                ),\n            )\n        )\n\n        # Convert outputs back to numpy\n        output_dict = {}\n        for name, tensor_proto in inference_response.outputs.items():\n            output_dict[name] = helpers.proto_to_numpy(tensor_proto)\n\n        results.append(output_dict)\n\n        # Print output shapes\n        if batch_idx == 0:\n            print(\"\\nInference Outputs:\")\n            for name, array in output_dict.items():\n                print(f\"  {name}: shape={array.shape}, dtype={array.dtype}\")\n\n    print(f\"\\nProcessed {len(results)} measurements\")\n\n    # 9. Close session\n    stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n    print(\"Session closed.\")\n\n    return results\n\n\n# Example usage\nif __name__ == \"__main__\":\n    run_inference(\n        pipeline_path=\"outputs/trained_models/channel_selector.yaml\",\n        weights_path=\"outputs/trained_models/channel_selector.pt\",\n        cu3s_file_path=\"data/lentils/Demo_000.cu3s\",\n        server_address=\"localhost:50051\",\n    )\n</code></pre>"},{"location":"tutorials/grpc-workflow/#cli-inference-script","title":"CLI Inference Script","text":"<p>For production, use the provided CLI script:</p> <pre><code># Run inference on CU3S file\nuv run python examples/grpc/run_inference.py \\\n  --pipeline-path outputs/trained_models/channel_selector.yaml \\\n  --weights-path outputs/trained_models/channel_selector.pt \\\n  --cu3s-file-path data/lentils/Demo_000.cu3s\n\n# With custom processing mode\nuv run python examples/grpc/run_inference.py \\\n  --pipeline-path outputs/trained_models/channel_selector.yaml \\\n  --weights-path outputs/trained_models/channel_selector.pt \\\n  --cu3s-file-path data/lentils/Demo_000.cu3s \\\n  --processing-mode Raw\n\n# With config overrides\nuv run python examples/grpc/run_inference.py \\\n  --pipeline-path outputs/trained_models/channel_selector.yaml \\\n  --weights-path outputs/trained_models/channel_selector.pt \\\n  --cu3s-file-path data/lentils/Demo_000.cu3s \\\n  --override nodes.10.params.output_dir=outputs/custom_tb\n</code></pre>"},{"location":"tutorials/grpc-workflow/#configuration-management-with-hydra","title":"Configuration Management with Hydra","text":""},{"location":"tutorials/grpc-workflow/#dynamic-overrides","title":"Dynamic Overrides","text":"<p>CUVIS.AI uses Hydra for configuration composition. Override any config value via dot notation:</p> <pre><code># Override training parameters\nresolve_trainrun_config(\n    stub,\n    session_id,\n    \"channel_selector\",\n    overrides=[\n        \"training.trainer.max_epochs=100\",          # More epochs\n        \"training.optimizer.lr=0.001\",              # Higher learning rate\n        \"training.optimizer.weight_decay=0.01\",     # More regularization\n        \"data.batch_size=16\",                       # Larger batch size\n        \"data.num_workers=4\",                       # More data loading threads\n    ],\n)\n</code></pre>"},{"location":"tutorials/grpc-workflow/#pipeline-node-overrides","title":"Pipeline Node Overrides","text":"<p>Override specific node parameters:</p> <pre><code># Override channel selector temperature\nresolve_trainrun_config(\n    stub,\n    session_id,\n    \"channel_selector\",\n    overrides=[\n        \"pipeline.nodes.channel_selector.params.tau_start=8.0\",\n        \"pipeline.nodes.channel_selector.params.tau_end=0.05\",\n    ],\n)\n</code></pre>"},{"location":"tutorials/grpc-workflow/#config-search-paths","title":"Config Search Paths","text":"<p>The server resolves configs from multiple directories:</p> <pre><code>from workflow_utils import config_search_paths\n\npaths = config_search_paths()\n# Returns:\n# [\n#   \"/path/to/cuvis-ai/configs\",\n#   \"/path/to/cuvis-ai/configs/trainrun\",\n#   \"/path/to/cuvis-ai/configs/pipeline\",\n#   \"/path/to/cuvis-ai/configs/data\",\n#   \"/path/to/cuvis-ai/configs/training\",\n# ]\n</code></pre> <p>Register custom paths: <pre><code>paths = config_search_paths(extra_paths=[\"/path/to/my/configs\"])\nstub.SetSessionSearchPaths(\n    cuvis_ai_pb2.SetSessionSearchPathsRequest(\n        session_id=session_id,\n        search_paths=paths,\n        append=False,  # Replace (True to append)\n    )\n)\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#session-management","title":"Session Management","text":""},{"location":"tutorials/grpc-workflow/#concurrent-sessions","title":"Concurrent Sessions","text":"<p>Each session is isolated - multiple clients can train different pipelines simultaneously:</p> <pre><code># Client 1: Train channel selector\nsession1 = create_session_with_search_paths(stub, config_search_paths())\nresolve_trainrun_config(stub, session1, \"channel_selector\")\nstub.Train(cuvis_ai_pb2.TrainRequest(session_id=session1, ...))\n\n# Client 2: Train Deep SVDD (concurrent)\nsession2 = create_session_with_search_paths(stub, config_search_paths())\nresolve_trainrun_config(stub, session2, \"deep_svdd\")\nstub.Train(cuvis_ai_pb2.TrainRequest(session_id=session2, ...))\n</code></pre> <p>Use Cases: - Multiple users on shared GPU server - A/B testing different hyperparameters - Production inference while training new models</p>"},{"location":"tutorials/grpc-workflow/#session-cleanup","title":"Session Cleanup","text":"<p>Always close sessions to release resources:</p> <pre><code>try:\n    # ... training or inference ...\nfinally:\n    stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=session_id))\n    print(\"Session closed.\")\n</code></pre> <p>Automatic Cleanup: The server automatically cleans up inactive sessions after a timeout (default: 1 hour).</p>"},{"location":"tutorials/grpc-workflow/#workflow-utilities-workflow_utilspy","title":"Workflow Utilities (workflow_utils.py)","text":""},{"location":"tutorials/grpc-workflow/#helper-functions","title":"Helper Functions","text":"<p>The <code>examples/grpc/workflow_utils.py</code> module centralizes common operations:</p> <p>1. Build Stub <pre><code>def build_stub(\n    server_address: str = \"localhost:50051\",\n    max_msg_size: int = 300 * 1024 * 1024,\n) -&gt; cuvis_ai_pb2_grpc.CuvisAIServiceStub:\n    \"\"\"Create gRPC stub with custom message size.\"\"\"\n    options = [\n        (\"grpc.max_send_message_length\", max_msg_size),\n        (\"grpc.max_receive_message_length\", max_msg_size),\n    ]\n    channel = grpc.insecure_channel(server_address, options=options)\n    return cuvis_ai_pb2_grpc.CuvisAIServiceStub(channel)\n</code></pre></p> <p>2. Create Session <pre><code>def create_session_with_search_paths(\n    stub: cuvis_ai_pb2_grpc.CuvisAIServiceStub,\n    search_paths: list[str] | None = None,\n) -&gt; str:\n    \"\"\"Create session and register config search paths.\"\"\"\n    session_id = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest()).session_id\n    paths = search_paths or config_search_paths()\n    stub.SetSessionSearchPaths(\n        cuvis_ai_pb2.SetSessionSearchPathsRequest(\n            session_id=session_id,\n            search_paths=paths,\n            append=False,\n        )\n    )\n    return session_id\n</code></pre></p> <p>3. Resolve &amp; Apply Config <pre><code>def resolve_trainrun_config(\n    stub: cuvis_ai_pb2_grpc.CuvisAIServiceStub,\n    session_id: str,\n    name: str,\n    overrides: list[str] | None = None,\n) -&gt; tuple[cuvis_ai_pb2.ResolveConfigResponse, dict]:\n    \"\"\"Resolve trainrun config via ConfigService.\"\"\"\n    response = stub.ResolveConfig(\n        cuvis_ai_pb2.ResolveConfigRequest(\n            session_id=session_id,\n            config_type=\"trainrun\",\n            path=f\"trainrun/{name}\",\n            overrides=overrides or [],\n        )\n    )\n    config_dict = json.loads(response.config_bytes.decode(\"utf-8\"))\n    return response, config_dict\n\ndef apply_trainrun_config(\n    stub: cuvis_ai_pb2_grpc.CuvisAIServiceStub,\n    session_id: str,\n    config_bytes: bytes,\n) -&gt; cuvis_ai_pb2.SetTrainRunConfigResponse:\n    \"\"\"Apply resolved trainrun config to session.\"\"\"\n    return stub.SetTrainRunConfig(\n        cuvis_ai_pb2.SetTrainRunConfigRequest(\n            session_id=session_id,\n            config=cuvis_ai_pb2.TrainRunConfig(config_bytes=config_bytes),\n        )\n    )\n</code></pre></p> <p>4. Format Progress <pre><code>def format_progress(progress: cuvis_ai_pb2.TrainResponse) -&gt; str:\n    \"\"\"Pretty-print training progress.\"\"\"\n    stage = cuvis_ai_pb2.ExecutionStage.Name(progress.context.stage)\n    status = cuvis_ai_pb2.TrainStatus.Name(progress.status)\n\n    parts = [f\"[{stage}] {status}\"]\n    if progress.losses:\n        parts.append(f\"losses={dict(progress.losses)}\")\n    if progress.metrics:\n        parts.append(f\"metrics={dict(progress.metrics)}\")\n    if progress.message:\n        parts.append(progress.message)\n\n    return \" | \".join(parts)\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#production-deployment","title":"Production Deployment","text":""},{"location":"tutorials/grpc-workflow/#docker-deployment","title":"Docker Deployment","text":"<p>Dockerfile: <pre><code>FROM python:3.10-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY pyproject.toml uv.lock ./\nRUN pip install uv &amp;&amp; uv sync --frozen\n\n# Copy code\nCOPY cuvis_ai/ ./cuvis_ai/\nCOPY configs/ ./configs/\n\n# Expose gRPC port\nEXPOSE 50051\n\n# Run server\nCMD [\"uv\", \"run\", \"python\", \"-m\", \"cuvis_ai.grpc.production_server\"]\n</code></pre></p> <p>Build &amp; Run: <pre><code># Build image\ndocker build -t cuvis-ai-server .\n\n# Run container\ndocker run -p 50051:50051 --gpus all cuvis-ai-server\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>k8s/deployment.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cuvis-ai-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cuvis-ai-server\n  template:\n    metadata:\n      labels:\n        app: cuvis-ai-server\n    spec:\n      containers:\n      - name: cuvis-ai-server\n        image: cuvis-ai-server:latest\n        ports:\n        - containerPort: 50051\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cuvis-ai-service\nspec:\n  selector:\n    app: cuvis-ai-server\n  ports:\n  - protocol: TCP\n    port: 50051\n    targetPort: 50051\n  type: LoadBalancer\n</code></pre></p> <p>Deploy: <pre><code>kubectl apply -f k8s/deployment.yaml\nkubectl get service cuvis-ai-service  # Get external IP\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#production-best-practices","title":"Production Best Practices","text":"<p>1. Security: <pre><code># Use TLS for production\nchannel = grpc.secure_channel(\n    \"production-server:50051\",\n    grpc.ssl_channel_credentials(),\n)\n</code></pre></p> <p>2. Retry Logic: <pre><code>import time\n\ndef train_with_retry(stub, session_id, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            for progress in stub.Train(...):\n                yield progress\n            break\n        except grpc.RpcError as e:\n            if attempt &lt; max_retries - 1:\n                print(f\"Retry {attempt + 1}/{max_retries} after error: {e}\")\n                time.sleep(2 ** attempt)  # Exponential backoff\n            else:\n                raise\n</code></pre></p> <p>3. Health Checks: <pre><code>def check_server_health(stub):\n    try:\n        response = stub.CreateSession(cuvis_ai_pb2.CreateSessionRequest())\n        stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(\n            session_id=response.session_id\n        ))\n        return True\n    except grpc.RpcError:\n        return False\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/grpc-workflow/#connection-refused","title":"Connection Refused","text":"<p>Error: <code>grpc._channel._InactiveRpcError: failed to connect to all addresses</code></p> <p>Solutions: 1. Verify server is running:    <pre><code>ps aux | grep production_server\n</code></pre></p> <ol> <li> <p>Check port availability:    <pre><code>netstat -an | grep 50051\n</code></pre></p> </li> <li> <p>Test with telnet:    <pre><code>telnet localhost 50051\n</code></pre></p> </li> <li> <p>Firewall rules (if remote):    <pre><code># Allow incoming on port 50051\nsudo ufw allow 50051\n</code></pre></p> </li> </ol>"},{"location":"tutorials/grpc-workflow/#message-size-exceeded","title":"Message Size Exceeded","text":"<p>Error: <code>grpc._channel._MultiThreadedRendezvous: Received message larger than max</code></p> <p>Solution: Increase message size limits: <pre><code>stub = build_stub(\n    \"localhost:50051\",\n    max_msg_size=1024 * 1024 * 1024,  # 1GB\n)\n</code></pre></p>"},{"location":"tutorials/grpc-workflow/#session-not-found","title":"Session Not Found","text":"<p>Error: <code>Session ID not found: d4e1a8c7-9b2f-4e3a-a5d6-7f8e9c0b1a2d</code></p> <p>Solutions: 1. Session expired (1-hour timeout) - create new session 2. Server restarted - sessions are not persisted 3. Typo in session ID - verify ID is correct</p>"},{"location":"tutorials/grpc-workflow/#config-resolution-failed","title":"Config Resolution Failed","text":"<p>Error: <code>ConfigResolutionError: trainrun/channel_selector not found</code></p> <p>Solutions: 1. Verify search paths are registered:    <pre><code>paths = config_search_paths()\nprint(f\"Search paths: {paths}\")\n</code></pre></p> <ol> <li> <p>Check config file exists:    <pre><code>ls configs/trainrun/channel_selector.yaml\n</code></pre></p> </li> <li> <p>Use absolute paths:    <pre><code>resolve_trainrun_config(\n    stub,\n    session_id,\n    \"/absolute/path/to/configs/trainrun/channel_selector.yaml\",\n)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/grpc-workflow/#cuda-out-of-memory-server-side","title":"CUDA Out of Memory (Server-Side)","text":"<p>Error: Server logs show <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions: 1. Reduce batch size in config overrides:    <pre><code>overrides=[\"data.batch_size=2\"]\n</code></pre></p> <ol> <li> <p>Close inactive sessions:    <pre><code>stub.CloseSession(cuvis_ai_pb2.CloseSessionRequest(session_id=old_session))\n</code></pre></p> </li> <li> <p>Restart server to clear GPU memory:    <pre><code># Kill server\npkill -f production_server\n\n# Restart\nuv run python -m cuvis_ai.grpc.production_server\n</code></pre></p> </li> </ol>"},{"location":"tutorials/grpc-workflow/#summary","title":"Summary","text":"<p>You've learned how to use CUVIS.AI's gRPC service for distributed training and inference:</p> <p>Key Concepts: - Server-Client Architecture - Train remotely, infer anywhere - Phase 5 Workflow - CreateSession \u2192 ResolveConfig \u2192 Train \u2192 Save - Streaming Progress - Real-time updates during training - Session Management - Isolated contexts for concurrent users - Hydra Overrides - Dynamic configuration without editing files</p> <p>Typical Workflows: 1. Development: Train locally (localhost:50051), iterate quickly 2. Production: Deploy server on GPU cluster, clients connect remotely 3. Inference: Load trained pipeline once, run inference millions of times</p> <p>Performance Benefits: - GPU Sharing: Multiple clients use same GPU server - Network Efficiency: gRPC uses Protocol Buffers (smaller than JSON) - Streaming: Progress updates without polling - Fault Tolerance: Retry logic and health checks</p>"},{"location":"tutorials/grpc-workflow/#next-steps","title":"Next Steps","text":"<p>Explore gRPC Documentation: - gRPC Overview - Architecture and quick start - gRPC API Reference - Complete documentation of all 46 RPC methods - Client Patterns - Best practices and common patterns - Sequence Diagrams - Visual workflows</p> <p>Deployment &amp; Production: - gRPC Deployment Guide - Production deployment patterns - Docker &amp; Kubernetes - Container orchestration</p> <p>Production Checklist: - [ ] Enable TLS encryption - [ ] Add authentication (API keys, OAuth) - [ ] Implement retry logic with exponential backoff - [ ] Set up monitoring (Prometheus, Grafana) - [ ] Configure health checks for load balancers - [ ] Plan GPU resource allocation - [ ] Set up CI/CD for automated deployment</p>"},{"location":"tutorials/grpc-workflow/#complete-example-scripts","title":"Complete Example Scripts","text":"<p>End-to-End Training: <pre><code>uv run python examples/grpc/complete_workflow_client.py \\\n  --trainrun deep_svdd \\\n  --pipeline-out outputs/trained_pipeline.yaml \\\n  --trainrun-out outputs/trainrun_config.yaml\n</code></pre> View full source: examples/grpc/complete_workflow_client.py</p> <p>Gradient Training: <pre><code>uv run python examples/grpc/gradient_training_client.py\n</code></pre> View full source: examples/grpc/gradient_training_client.py</p> <p>Statistical Training: <pre><code>uv run python examples/grpc/statistical_training_client.py\n</code></pre> View full source: examples/grpc/statistical_training_client.py</p> <p>Inference: <pre><code>uv run python examples/grpc/run_inference.py \\\n  --pipeline-path outputs/trained_models/channel_selector.yaml \\\n  --weights-path outputs/trained_models/channel_selector.pt \\\n  --cu3s-file-path data/lentils/Demo_000.cu3s\n</code></pre> View full source: examples/grpc/run_inference.py</p> <p>Restore TrainRun: <pre><code>uv run python examples/grpc/restore_trainrun_grpc.py \\\n  --trainrun-path outputs/channel_selector/trained_models/trainrun.yaml \\\n  --mode validate\n</code></pre> View full source: examples/grpc/restore_trainrun_grpc.py</p> <p>Need Help? - Check gRPC Client Patterns for common use cases - Review Remote gRPC Guide for deployment - See API Documentation for full RPC specifications</p>"},{"location":"tutorials/rx-statistical/","title":"RX Statistical","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"tutorials/rx-statistical/#tutorial-rx-statistical-anomaly-detection","title":"Tutorial: RX Statistical Anomaly Detection","text":"<p>Learn how to build a complete anomaly detection pipeline using the RX (Reed-Xiaoli) detector with statistical initialization.</p>"},{"location":"tutorials/rx-statistical/#overview","title":"Overview","text":"<p>What You'll Learn:</p> <ul> <li>Loading and preprocessing hyperspectral data with CUVIS.AI nodes</li> <li>Building a statistical anomaly detection pipeline</li> <li>Initializing detectors with statistical training</li> <li>Evaluating results with metrics and visualization</li> <li>Saving and exporting trained pipelines</li> </ul> <p>Prerequisites:</p> <ul> <li>cuvis-ai installed (<code>pip install cuvis-ai</code>)</li> <li>Basic Python knowledge</li> <li>Understanding of Node System fundamentals</li> <li>Familiarity with Pipeline Lifecycle</li> </ul> <p>Time: ~25 minutes</p> <p>Perfect for: Users new to CUVIS.AI who want to understand statistical-only training workflows and anomaly detection fundamentals.</p>"},{"location":"tutorials/rx-statistical/#background","title":"Background","text":""},{"location":"tutorials/rx-statistical/#what-is-rx-detection","title":"What is RX Detection?","text":"<p>The RX (Reed-Xiaoli) detector is a classical anomaly detection algorithm for hyperspectral imaging. It identifies anomalies by measuring how unusual each pixel's spectrum is compared to the background distribution.</p> <p>Key Concept: RX uses the Mahalanobis distance to compute anomaly scores:</p> \\[ RX(x) = (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\] <p>Where: - \\(x\\) is the pixel spectrum - \\(\\mu\\) is the background mean spectrum - \\(\\Sigma\\) is the covariance matrix</p> <p>Higher scores indicate pixels that are spectrally different from the background.</p>"},{"location":"tutorials/rx-statistical/#when-to-use-rx-detection","title":"When to Use RX Detection","text":"<p>\u2705 Use RX when: - You need unsupervised anomaly detection - Background follows a Gaussian distribution - You have hyperspectral or multispectral data - You want a fast, interpretable baseline</p> <p>\u274c Don't use RX when: - Background is highly non-Gaussian - You need deep contextual reasoning - You have limited initialization data</p>"},{"location":"tutorials/rx-statistical/#rx-in-cuvisai","title":"RX in CUVIS.AI","text":"<p>CUVIS.AI implements RX as a statistical node that: 1. Collects background statistics during initialization 2. Computes covariance and mean from initialization data 3. Calculates Mahalanobis distance for new data 4. Outputs anomaly scores (higher = more anomalous)</p>"},{"location":"tutorials/rx-statistical/#step-1-setup-and-data-loading","title":"Step 1: Setup and Data Loading","text":""},{"location":"tutorials/rx-statistical/#environment-setup","title":"Environment Setup","text":"<p>First, ensure cuvis-ai is installed:</p> <pre><code>pip install cuvis-ai\n</code></pre>"},{"location":"tutorials/rx-statistical/#understanding-the-data","title":"Understanding the Data","text":"<p>This tutorial uses the Lentils Anomaly Dataset, which contains hyperspectral cubes with 5 classes:</p> Class ID Name Normal/Anomaly 0 Unlabeled Normal 1 Lentils_black Normal 2 Lentils_brown Anomaly 3 Stone Anomaly 4 Background Anomaly <p>We'll treat classes 0 and 1 as \"normal\" and detect brown lentils, stones, and background as anomalies.</p>"},{"location":"tutorials/rx-statistical/#data-loading-with-lentilsanomalydatanode","title":"Data Loading with LentilsAnomalyDataNode","text":"<pre><code>from pathlib import Path\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\n\n# Initialize datamodule\ndatamodule = SingleCu3sDataModule(\n    data_dir=\"data/lentils\",\n    batch_size=4,\n    num_workers=0,\n)\ndatamodule.setup(stage=\"fit\")\n\n# Create data node with normal class specification\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 1],  # Unlabeled and black lentils are normal\n)\n</code></pre> <p>What's Happening: - <code>SingleCu3sDataModule</code>: Loads CU3S hyperspectral data format - <code>LentilsAnomalyDataNode</code>: Converts multi-class segmentation to binary anomaly labels - <code>normal_class_ids=[0, 1]</code>: Classes 0 and 1 are background; others are anomalies</p> <p>Data Node Outputs: - <code>cube</code>: Hyperspectral cube [B, H, W, 61] (61 spectral channels) - <code>mask</code>: Binary anomaly mask [B, H, W, 1] - <code>wavelengths</code>: Wavelength array [61]</p>"},{"location":"tutorials/rx-statistical/#step-2-build-the-pipeline","title":"Step 2: Build the Pipeline","text":""},{"location":"tutorials/rx-statistical/#create-pipeline","title":"Create Pipeline","text":"<pre><code># Initialize pipeline\npipeline = CuvisPipeline(\"RX_Statistical\")\n</code></pre>"},{"location":"tutorials/rx-statistical/#step-2-preprocessing","title":"Step 2: Preprocessing","text":""},{"location":"tutorials/rx-statistical/#add-preprocessing-node","title":"Add Preprocessing Node","text":"<p>Normalization is critical for stable RX detection:</p> <pre><code>from cuvis_ai.node.normalization import MinMaxNormalizer\n\nnormalizer_node = MinMaxNormalizer(\n    eps=1.0e-6,              # Numerical stability\n    use_running_stats=True,  # Compute global min/max during training\n)\n</code></pre> <p>Why MinMaxNormalizer? - Scales data to [0, 1] range - Uses running statistics computed during statistical initialization - Ensures stable covariance computation for RX</p>"},{"location":"tutorials/rx-statistical/#add-rx-detector","title":"Add RX Detector","text":"<pre><code>from cuvis_ai.anomaly.rx_detector import RXGlobal\n\nrx_node = RXGlobal(\n    num_channels=61,  # Number of spectral channels\n    eps=1.0e-6,       # Small constant for numerical stability\n)\n</code></pre> <p>RXGlobal computes global background statistics from all spatial pixels.</p>"},{"location":"tutorials/rx-statistical/#rx-logit-head","title":"RX Logit Head","text":""},{"location":"tutorials/rx-statistical/#transform-scores-to-logits","title":"Transform Scores to Logits","text":"<pre><code>from cuvis_ai.node.conversion import ScoreToLogit\n\nlogit_head = ScoreToLogit(\n    init_scale=1.0,  # Scale factor for scores\n    init_bias=0.0,   # Bias term\n)\n</code></pre> <p>Why ScoreToLogit? - Converts raw RX scores to logit space - Enables integration with binary cross-entropy losses - Makes scores more interpretable for thresholding</p>"},{"location":"tutorials/rx-statistical/#decision-thresholding","title":"Decision Thresholding","text":""},{"location":"tutorials/rx-statistical/#add-decision-node","title":"Add Decision Node","text":"<pre><code>from cuvis_ai.deciders.binary_decider import BinaryDecider\n\ndecider_node = BinaryDecider(threshold=0.5)\n</code></pre> <p>Applies a fixed threshold (0.5) to convert logits to binary decisions.</p>"},{"location":"tutorials/rx-statistical/#metrics-tracking","title":"Metrics Tracking","text":""},{"location":"tutorials/rx-statistical/#add-metrics-and-monitoring","title":"Add Metrics and Monitoring","text":"<pre><code>from cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.node.visualizations import AnomalyMask\n\n# Anomaly detection metrics (IoU, precision, recall, F1)\nmetrics_anomaly = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n\n# Visualization node\nviz_mask = AnomalyMask(name=\"mask\", channel=30, up_to=5)\n\n# TensorBoard monitoring\ntensorboard_node = TensorBoardMonitorNode(\n    output_dir=\"runs/tensorboard\",\n    run_name=pipeline.name,\n)\n</code></pre>"},{"location":"tutorials/rx-statistical/#connect-the-pipeline","title":"Connect the Pipeline","text":"<pre><code>pipeline.connect(\n    # Processing flow: data \u2192 normalize \u2192 RX \u2192 logits \u2192 decisions\n    (data_node.outputs.cube, normalizer_node.data),\n    (normalizer_node.normalized, rx_node.data),\n    (rx_node.scores, logit_head.scores),\n    (logit_head.logits, decider_node.logits),\n\n    # Metric flow: decisions + ground truth \u2192 metrics \u2192 TensorBoard\n    (decider_node.decisions, metrics_anomaly.decisions),\n    (data_node.outputs.mask, metrics_anomaly.targets),\n    (metrics_anomaly.metrics, tensorboard_node.metrics),\n\n    # Visualization flow: decisions + mask + cube \u2192 visualizer \u2192 TensorBoard\n    (decider_node.decisions, viz_mask.decisions),\n    (data_node.outputs.mask, viz_mask.mask),\n    (data_node.outputs.cube, viz_mask.cube),\n    (viz_mask.artifacts, tensorboard_node.artifacts),\n)\n</code></pre>"},{"location":"tutorials/rx-statistical/#visualize-pipeline-structure","title":"Visualize Pipeline Structure","text":"<pre><code># Generate pipeline visualization\npipeline.visualize(\n    format=\"render_graphviz\",\n    output_path=\"outputs/pipeline/RX_Statistical.png\",\n    show_execution_stage=True,\n)\n</code></pre> <p>Pipeline Flow:</p> <pre><code>graph LR\n    A[LentilsAnomalyDataNode] --&gt;|cube| B[MinMaxNormalizer]\n    B --&gt;|normalized| C[RXGlobal]\n    C --&gt;|scores| D[ScoreToLogit]\n    D --&gt;|logits| E[BinaryDecider]\n    E --&gt;|decisions| F[AnomalyDetectionMetrics]\n    A --&gt;|mask| F\n    F --&gt;|metrics| G[TensorBoardMonitor]\n    E --&gt;|decisions| H[AnomalyMask]\n    A --&gt;|cube| H\n    A --&gt;|mask| H\n    H --&gt;|artifacts| G</code></pre>"},{"location":"tutorials/rx-statistical/#step-3-statistical-initialization","title":"Step 3: Statistical Initialization","text":"<p>Statistical initialization computes background statistics from training data.</p>"},{"location":"tutorials/rx-statistical/#create-statistical-trainer","title":"Create Statistical Trainer","text":"<pre><code>from cuvis_ai_core.training import StatisticalTrainer\n\nstat_trainer = StatisticalTrainer(\n    pipeline=pipeline,\n    datamodule=datamodule,\n)\n</code></pre>"},{"location":"tutorials/rx-statistical/#run-statistical-training","title":"Run Statistical Training","text":"<pre><code>stat_trainer.fit()\n</code></pre> <p>What Happens During Statistical Training:</p> <ol> <li>MinMaxNormalizer collects min/max values across training data</li> <li>RXGlobal computes:</li> <li>Background mean spectrum \\(\\mu\\)</li> <li>Covariance matrix \\(\\Sigma\\)</li> <li>Inverse covariance \\(\\Sigma^{-1}\\) for Mahalanobis distance</li> </ol> <p>Console Output: <pre><code>[INFO] Statistical initialization starting...\n[INFO] Collecting statistics from training data...\n[INFO] MinMaxNormalizer: min=-0.123, max=1.987\n[INFO] RXGlobal: Computed background statistics (61 channels)\n[INFO] Statistical initialization complete\n</code></pre></p> <p>Key Points: - Only nodes with <code>statistical_initialization()</code> method are initialized - Normalizer and RX detector are initialized; other nodes remain frozen - No gradient computation occurs</p>"},{"location":"tutorials/rx-statistical/#step-4-evaluation","title":"Step 4: Evaluation","text":""},{"location":"tutorials/rx-statistical/#validation-evaluation","title":"Validation Evaluation","text":"<pre><code>logger.info(\"Running validation evaluation...\")\nstat_trainer.validate()\n</code></pre> <p>Validation Output: <pre><code>[INFO] Validation metrics (epoch 0):\n  metrics_anomaly/iou: 0.723\n  metrics_anomaly/precision: 0.812\n  metrics_anomaly/recall: 0.801\n  metrics_anomaly/f1: 0.806\n</code></pre></p>"},{"location":"tutorials/rx-statistical/#test-evaluation","title":"Test Evaluation","text":"<pre><code>logger.info(\"Running test evaluation...\")\nstat_trainer.test()\n</code></pre> <p>Test metrics are computed on held-out test data to measure generalization.</p>"},{"location":"tutorials/rx-statistical/#step-5-results-visualization","title":"Step 5: Results Visualization","text":""},{"location":"tutorials/rx-statistical/#visualization-with-tensorboard","title":"Visualization with TensorBoard","text":"<pre><code>tensorboard --logdir=runs/tensorboard\n</code></pre> <p>Navigate to http://localhost:6006 to see:</p> <ul> <li>Metrics: IoU, precision, recall, F1 over batches</li> <li>Images:</li> <li>Input hyperspectral cube (false-color RGB)</li> <li>Ground truth anomaly mask</li> <li>Predicted anomaly mask</li> <li>Anomaly score heatmap</li> </ul>"},{"location":"tutorials/rx-statistical/#visualization","title":"Visualization","text":"<p>Anomaly Detection Metrics:</p> Metric Description Good Range IoU Intersection over Union &gt; 0.7 Precision True Positives / Predicted Positives &gt; 0.8 Recall True Positives / Actual Positives &gt; 0.7 F1 Harmonic mean of precision and recall &gt; 0.75"},{"location":"tutorials/rx-statistical/#visualization-with-sigmoid","title":"Visualization with Sigmoid","text":"<p>Example Visualization: - Green pixels: Correct detections (true positives) - Red pixels: False alarms (false positives) - Blue pixels: Missed anomalies (false negatives)</p>"},{"location":"tutorials/rx-statistical/#step-6-save-pipeline-and-trainrun-config","title":"Step 6: Save Pipeline and TrainRun Config","text":""},{"location":"tutorials/rx-statistical/#save-trained-pipeline","title":"Save Trained Pipeline","text":"<pre><code>from cuvis_ai_core.training.config import PipelineMetadata\n\noutput_path = Path(\"outputs/trained_models/RX_Statistical.yaml\")\n\npipeline.save_to_file(\n    str(output_path),\n    metadata=PipelineMetadata(\n        name=pipeline.name,\n        description=\"RX detector with statistical training\",\n        tags=[\"statistical\", \"rx\", \"anomaly-detection\"],\n        author=\"cuvis.ai\",\n    ),\n)\n</code></pre> <p>Created Files: - <code>RX_Statistical.yaml</code>: Pipeline structure and configuration - <code>RX_Statistical.pt</code>: Trained weights (normalizer stats, RX covariance)</p>"},{"location":"tutorials/rx-statistical/#save-trainrun-config-for-reproducibility","title":"Save TrainRun Config for Reproducibility","text":"<pre><code>from cuvis_ai_core.training.config import TrainRunConfig, TrainingConfig\n\ntrainrun_config = TrainRunConfig(\n    name=\"rx_statistical\",\n    pipeline=pipeline.serialize(),\n    data=datamodule_config,\n    training=TrainingConfig(seed=42),\n    output_dir=\"outputs\",\n    loss_nodes=[],  # No loss nodes in statistical-only training\n    metric_nodes=[\"metrics_anomaly\"],\n    freeze_nodes=[],\n    unfreeze_nodes=[],\n)\n\ntrainrun_config.save_to_file(\"outputs/trained_models/rx_statistical_trainrun.yaml\")\n</code></pre> <p>TrainRun Config captures: - Complete pipeline structure - Data configuration - Training parameters - Metric and loss nodes - Freeze/unfreeze strategy</p>"},{"location":"tutorials/rx-statistical/#restore-pipeline-later","title":"Restore Pipeline Later","text":"<pre><code># Restore from saved files\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\n\nrestored_pipeline = CuvisPipeline.load_from_file(\n    \"outputs/trained_models/RX_Statistical.yaml\"\n)\n</code></pre>"},{"location":"tutorials/rx-statistical/#complete-example-script","title":"Complete Example Script","text":"<p>Here's the full runnable script (<code>examples/rx_statistical.py</code>):</p> <pre><code>from pathlib import Path\nimport hydra\nfrom cuvis_ai_core.data.datasets import SingleCu3sDataModule\nfrom cuvis_ai_core.pipeline.pipeline import CuvisPipeline\nfrom cuvis_ai_core.training import StatisticalTrainer\nfrom cuvis_ai_core.training.config import PipelineMetadata\nfrom loguru import logger\nfrom omegaconf import DictConfig\n\nfrom cuvis_ai.anomaly.rx_detector import RXGlobal\nfrom cuvis_ai.node.conversion import ScoreToLogit\nfrom cuvis_ai.deciders.binary_decider import BinaryDecider\nfrom cuvis_ai.node.data import LentilsAnomalyDataNode\nfrom cuvis_ai.node.metrics import AnomalyDetectionMetrics\nfrom cuvis_ai.node.monitor import TensorBoardMonitorNode\nfrom cuvis_ai.node.normalization import MinMaxNormalizer\nfrom cuvis_ai.node.visualizations import AnomalyMask\n\n\n@hydra.main(config_path=\"../configs/\", config_name=\"trainrun/default_statistical\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"Statistical RX Anomaly Detection Tutorial.\"\"\"\n\n    logger.info(\"=== RX Statistical Anomaly Detection ===\")\n    output_dir = Path(cfg.output_dir)\n\n    # Stage 1: Setup datamodule\n    datamodule = SingleCu3sDataModule(**cfg.data)\n    datamodule.setup(stage=\"fit\")\n\n    # Stage 2: Build pipeline\n    pipeline = CuvisPipeline(\"RX_Statistical\")\n\n    data_node = LentilsAnomalyDataNode(normal_class_ids=[0, 1])\n    normalizer_node = MinMaxNormalizer(eps=1.0e-6, use_running_stats=True)\n    rx_node = RXGlobal(num_channels=61, eps=1.0e-6)\n    logit_head = ScoreToLogit(init_scale=1.0, init_bias=0.0)\n    decider_node = BinaryDecider(threshold=0.5)\n    metrics_anomaly = AnomalyDetectionMetrics(name=\"metrics_anomaly\")\n    viz_mask = AnomalyMask(name=\"mask\", channel=30, up_to=5)\n    tensorboard_node = TensorBoardMonitorNode(\n        output_dir=str(output_dir / \"..\" / \"tensorboard\"),\n        run_name=pipeline.name,\n    )\n\n    # Stage 3: Connect graph\n    pipeline.connect(\n        (data_node.outputs.cube, normalizer_node.data),\n        (normalizer_node.normalized, rx_node.data),\n        (rx_node.scores, logit_head.scores),\n        (logit_head.logits, decider_node.logits),\n        (decider_node.decisions, metrics_anomaly.decisions),\n        (data_node.outputs.mask, metrics_anomaly.targets),\n        (metrics_anomaly.metrics, tensorboard_node.metrics),\n        (decider_node.decisions, viz_mask.decisions),\n        (data_node.outputs.mask, viz_mask.mask),\n        (data_node.outputs.cube, viz_mask.cube),\n        (viz_mask.artifacts, tensorboard_node.artifacts),\n    )\n\n    # Stage 4: Statistical initialization\n    stat_trainer = StatisticalTrainer(pipeline=pipeline, datamodule=datamodule)\n    stat_trainer.fit()\n\n    # Stage 5: Evaluation\n    logger.info(\"Running validation...\")\n    stat_trainer.validate()\n\n    logger.info(\"Running test...\")\n    stat_trainer.test()\n\n    # Stage 6: Save pipeline\n    results_dir = output_dir / \"trained_models\"\n    pipeline_output_path = results_dir / f\"{pipeline.name}.yaml\"\n\n    pipeline.save_to_file(\n        str(pipeline_output_path),\n        metadata=PipelineMetadata(\n            name=pipeline.name,\n            description=\"RX Statistical Detector\",\n            tags=[\"statistical\", \"rx\"],\n            author=\"cuvis.ai\",\n        ),\n    )\n\n    logger.info(f\"Pipeline saved: {pipeline_output_path}\")\n    logger.info(f\"TensorBoard logs: {tensorboard_node.output_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run the example: <pre><code>python examples/rx_statistical.py\n</code></pre></p>"},{"location":"tutorials/rx-statistical/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/rx-statistical/#issue-low-iou-05","title":"Issue: Low IoU (&lt;0.5)","text":"<p>Symptoms: Poor anomaly detection performance</p> <p>Possible Causes: 1. Insufficient initialization data 2. Normal classes incorrectly specified 3. Threshold too high/low</p> <p>Solutions: <pre><code># 1. Increase initialization data\ndatamodule = SingleCu3sDataModule(batch_size=8)  # Larger batches\n\n# 2. Verify normal class IDs\ndata_node = LentilsAnomalyDataNode(\n    normal_class_ids=[0, 1],  # Double-check which classes are normal\n)\n\n# 3. Adjust threshold\ndecider_node = BinaryDecider(threshold=0.3)  # Lower threshold for more detections\n</code></pre></p>"},{"location":"tutorials/rx-statistical/#issue-memory-error-during-training","title":"Issue: Memory Error During Training","text":"<p>Solution: Reduce batch size: <pre><code>datamodule = SingleCu3sDataModule(\n    batch_size=2,  # Reduce from 4\n    num_workers=0,\n)\n</code></pre></p>"},{"location":"tutorials/rx-statistical/#issue-nan-in-rx-scores","title":"Issue: NaN in RX Scores","text":"<p>Cause: Singular covariance matrix (insufficient diversity in training data)</p> <p>Solution: <pre><code># Increase epsilon for numerical stability\nrx_node = RXGlobal(num_channels=61, eps=1.0e-4)  # Larger epsilon\n</code></pre></p>"},{"location":"tutorials/rx-statistical/#issue-tensorboard-not-showing-visualizations","title":"Issue: TensorBoard Not Showing Visualizations","text":"<p>Cause: Visualization node execution stage mismatch</p> <p>Solution: <pre><code>from cuvis_ai_schemas.enums import ExecutionStage\n\n# Ensure visualization runs during validation/test\nviz_mask = AnomalyMask(\n    name=\"mask\",\n    channel=30,\n    up_to=5,\n    execution_stages={ExecutionStage.VAL, ExecutionStage.TEST},\n)\n</code></pre></p>"},{"location":"tutorials/rx-statistical/#next-steps","title":"Next Steps","text":"<p>Build on this tutorial:</p> <ol> <li>Channel Selector Tutorial - Add learnable channel selection with gradient training</li> <li>Deep SVDD Tutorial - Replace RX with deep learning one-class detector</li> <li>gRPC Workflow - Deploy this pipeline as a remote service</li> </ol> <p>Explore related concepts:</p> <ul> <li>Two-Phase Training - Combine statistical initialization with gradient optimization</li> <li>Execution Stages - Control when nodes execute (train/val/test)</li> <li>Node System Deep Dive - Learn to create custom nodes</li> </ul> <p>Explore related nodes:</p> <ul> <li>RXGlobal Node - Detailed RX implementation</li> <li>MinMaxNormalizer - Normalization strategies</li> <li>AnomalyDetectionMetrics - Metrics reference</li> </ul>"},{"location":"tutorials/rx-statistical/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <p>\u2705 How to build a statistical anomaly detection pipeline \u2705 How to initialize nodes with statistical training \u2705 How to evaluate and visualize results \u2705 How to save and restore trained pipelines</p> <p>Key Takeaways: - RX detection is a fast, interpretable baseline for hyperspectral anomaly detection - Statistical initialization computes background statistics without gradients - Pipeline serialization enables reproducibility and deployment - TensorBoard provides comprehensive monitoring and visualization</p> <p>Now you're ready to explore more advanced training strategies with gradient-based optimization!</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>CUVIS.AI uses Hydra for flexible, reproducible configuration management. This guide covers configuration patterns and best practices.</p>"},{"location":"user-guide/configuration/#configuration-structure","title":"Configuration Structure","text":"<p>Configuration files are located in <code>configs/</code> at the project root and use YAML format with Hydra's composition system. The most common pattern is to create trainrun configs that compose sample pipeline, data, training, and plugin configurations together.</p>"},{"location":"user-guide/configuration/#configuration-directory-structure","title":"Configuration Directory Structure","text":"<pre><code>configs/\n\u251c\u2500\u2500 pipeline/       # Sample pipeline configurations\n\u251c\u2500\u2500 data/           # Sample data configurations\n\u251c\u2500\u2500 training/       # Sample training configurations\n\u251c\u2500\u2500 trainrun/       # Sample full trainruns (compose data + pipeline + training + plugins)\n\u2514\u2500\u2500 plugins/        # Plugin registry\n</code></pre> <p>Note: The most common pattern is to use trainrun configs (<code>configs/trainrun/</code>) which compose data, pipeline, training, and plugin configs together using Hydra's composition system. You can also override any composed values directly in the trainrun config.</p>"},{"location":"user-guide/configuration/#understanding-configuration-types","title":"Understanding Configuration Types","text":"<p>CUVIS.AI uses 4 distinct configuration types that work together:</p> Config Type Location Purpose Used For Pipeline <code>configs/pipeline/</code> Graph structure - nodes &amp; connections Defining processing flow Data <code>configs/data/</code> Data loading parameters Dataset configuration Training <code>configs/training/</code> Training hyperparameters Optimizer, trainer settings Trainrun <code>configs/trainrun/</code> Orchestration - composes all configs Running experiments <p>Typical workflow: Create a trainrun config that composes pipeline, data, and training configs together using Hydra's composition system.</p>"},{"location":"user-guide/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>Trainrun configs sit at the top level and compose the other three:</p> <pre><code>Trainrun Config (Orchestrator)\n\u251c\u2500\u2500 Composes Pipeline Config \u2192 Graph structure (nodes, connections)\n\u251c\u2500\u2500 Composes Data Config \u2192 Data loading (paths, splits, batch size)\n\u2514\u2500\u2500 Composes Training Config \u2192 Training params (optimizer, epochs)\n</code></pre>"},{"location":"user-guide/configuration/#pipeline-configurations","title":"Pipeline Configurations","text":"<p>Pipeline configs define the computational graph structure: nodes and their connections.</p> <p>Location: <code>configs/pipeline/*.yaml</code></p> <p>Complete example (<code>configs/pipeline/rx_statistical.yaml</code> - simplified):</p> <pre><code># Pipeline metadata\nmetadata:\n  name: RX_Statistical\n  description: \"RX anomaly detection pipeline\"\n  tags: [statistical, rx]\n  author: cuvis.ai\n\n# Node definitions (THIS IS A LIST)\nnodes:\n  - name: LentilsAnomalyDataNode\n    class: cuvis_ai.node.data.LentilsAnomalyDataNode\n    params:\n      normal_class_ids: [0, 1]\n\n  - name: MinMaxNormalizer\n    class: cuvis_ai.node.normalization.MinMaxNormalizer\n    params:\n      eps: 1.0e-06\n      use_running_stats: true\n\n  - name: RXGlobal\n    class: cuvis_ai.anomaly.rx_detector.RXGlobal\n    params:\n      eps: 1.0e-06\n      num_channels: 61\n\n# Connection definitions (REQUIRED)\nconnections:\n  - from: LentilsAnomalyDataNode.outputs.cube\n    to: MinMaxNormalizer.inputs.data\n  - from: MinMaxNormalizer.outputs.normalized\n    to: RXGlobal.inputs.data\n</code></pre> <p>Key structure elements:</p> <ul> <li><code>metadata</code>: Pipeline identification and documentation</li> <li><code>nodes</code>: LIST of node definitions (each with <code>name</code>, <code>class</code>, <code>params</code>)</li> <li><code>connections</code>: Data flow between nodes (port-to-port connections)</li> </ul> <p>Critical Syntax</p> <ul> <li>Nodes use <code>class</code> (NOT <code>_target_</code>)</li> <li>Nodes are a list (with <code>-</code> prefix), NOT a dictionary</li> <li>Must include <code>connections</code> section</li> </ul>"},{"location":"user-guide/configuration/#data-configurations","title":"Data Configurations","text":"<p>Data configs define data loading, dataset paths, and preprocessing parameters.</p> <p>Location: <code>configs/data/*.yaml</code></p> <p>Complete example (<code>configs/data/lentils.yaml</code>):</p> <pre><code># Data paths\ncu3s_file_path: data/Lentils/Lentils_000.cu3s\nannotation_json_path: data/Lentils/Lentils_000.json\n\n# Train/val/test splits\ntrain_ids: [0, 2, 3]\nval_ids: [1, 5]\ntest_ids: [1, 5]\n\n# Data loading parameters\nbatch_size: 2\nprocessing_mode: Reflectance\n</code></pre> <p>Key fields:</p> <ul> <li><code>cu3s_file_path</code>: Path to hyperspectral data file</li> <li><code>annotation_json_path</code>: Path to annotation file</li> <li><code>train_ids</code>, <code>val_ids</code>, <code>test_ids</code>: Sample split assignments</li> <li><code>batch_size</code>: Batch size for data loading</li> <li><code>processing_mode</code>: Data processing mode (e.g., Reflectance, Radiance)</li> </ul> <p>Note</p> <p>Data configs use simple key-value structure with no <code>_target_</code> directives. Values are passed directly to <code>SingleCu3sDataModule(**cfg.data)</code>.</p>"},{"location":"user-guide/configuration/#training-configurations","title":"Training Configurations","text":"<p>Training configs define training hyperparameters: trainer settings, optimizer, scheduler, and callbacks.</p> <p>Location: <code>configs/training/*.yaml</code></p> <p>Complete example (<code>configs/training/default.yaml</code>):</p> <pre><code># Random seed for reproducibility\nseed: 42\n\n# PyTorch Lightning trainer configuration\ntrainer:\n  max_epochs: 5\n  accelerator: \"auto\"\n  devices: 1\n  precision: \"32-true\"\n  log_every_n_steps: 10\n  val_check_interval: 1.0\n  enable_checkpointing: true\n  gradient_clip_val: 1.0\n\n# Optimizer configuration\noptimizer:\n  name: \"adamw\"\n  lr: 0.001\n  weight_decay: 0.01\n  betas: [0.9, 0.999]\n</code></pre> <p>Key sections:</p> <ul> <li><code>seed</code>: Random seed for reproducibility</li> <li><code>trainer</code>: PyTorch Lightning Trainer parameters (epochs, accelerator, precision, etc.)</li> <li><code>optimizer</code>: Optimizer type and hyperparameters</li> <li><code>scheduler</code>: Optional learning rate scheduler configuration (see full examples)</li> </ul>"},{"location":"user-guide/configuration/#trainrun-configurations","title":"Trainrun Configurations","text":"<p>Trainrun configs compose pipeline, data, and training configs together and add orchestration settings.</p> <p>Location: <code>configs/trainrun/*.yaml</code></p> <p>Complete example (<code>configs/trainrun/rx_statistical.yaml</code>):</p> <pre><code># Hydra package directive - merge into global namespace\n# @package _global_\n\n# Compose other configs using Hydra defaults\ndefaults:\n  - /pipeline@pipeline: rx_statistical  # Inject configs/pipeline/rx_statistical.yaml into .pipeline\n  - /data@data: lentils                 # Inject configs/data/lentils.yaml into .data\n  - /training@training: default         # Inject configs/training/default.yaml into .training\n  - _self_                              # Allow overrides from this file\n\n# Experiment identification\nname: rx_statistical\noutput_dir: outputs\\rx_statistical\n\n# Override composed values\ntraining:\n  trainer:\n    max_epochs: 10  # Override default.yaml's max_epochs: 5\n\n# Orchestration settings (trainrun-specific)\nunfreeze_nodes: []        # Nodes to unfreeze for gradient training\nfreeze_nodes: []          # Nodes to freeze during training\nloss_nodes: []            # Nodes that compute loss\nmetric_nodes:             # Nodes that compute metrics\n  - metrics_anomaly\n</code></pre> <p>Key elements:</p> <ul> <li><code>@package _global_</code>: Hydra directive to merge into global namespace</li> <li><code>defaults</code>: Compose other configs with package directive syntax (<code>/config_type@target_key: config_name</code>)</li> <li>Overrides: Any section from composed configs can be overridden</li> <li>Orchestration fields:<ul> <li><code>unfreeze_nodes</code>: Nodes to enable gradients for in phase 2</li> <li><code>freeze_nodes</code>: Nodes to freeze during training</li> <li><code>loss_nodes</code>: Nodes that compute training loss</li> <li><code>metric_nodes</code>: Nodes that compute metrics</li> </ul> </li> </ul> <p>Package Directive Syntax</p> <p>The syntax <code>/source@destination: config_name</code> tells Hydra to load <code>configs/source/config_name.yaml</code> and inject it at the <code>destination</code> key in the merged config. For example, <code>/pipeline@pipeline: rx_statistical</code> loads <code>configs/pipeline/rx_statistical.yaml</code> and places it at <code>cfg.pipeline</code>.</p>"},{"location":"user-guide/configuration/#how-configuration-composition-works","title":"How Configuration Composition Works","text":""},{"location":"user-guide/configuration/#hydra-composition-with-package-directives","title":"Hydra Composition with Package Directives","text":"<p>When you run a trainrun config, Hydra merges all composed configs:</p> <p>Step 1: Load trainrun config</p> <pre><code># trainrun/my_experiment.yaml\ndefaults:\n  - /pipeline@pipeline: rx_statistical\n  - /data@data: lentils\n  - /training@training: default\n  - _self_\n</code></pre> <p>Step 2: Hydra loads and injects configs</p> <p>The resulting merged config structure:</p> <pre><code>cfg = {\n    \"pipeline\": {  # From /pipeline@pipeline: rx_statistical\n        \"metadata\": {...},\n        \"nodes\": [...],\n        \"connections\": [...]\n    },\n    \"data\": {      # From /data@data: lentils\n        \"cu3s_file_path\": \"...\",\n        \"batch_size\": 2,\n        ...\n    },\n    \"training\": {  # From /training@training: default\n        \"seed\": 42,\n        \"trainer\": {...},\n        \"optimizer\": {...}\n    },\n    # Plus trainrun-specific fields\n    \"name\": \"my_experiment\",\n    \"unfreeze_nodes\": [],\n    \"metric_nodes\": [...]\n}\n</code></pre> <p>Step 3: Apply overrides</p> <p>Trainrun config can override any composed value:</p> <pre><code># In trainrun config\ntraining:\n  trainer:\n    max_epochs: 20  # Overrides training/default.yaml's max_epochs: 5\n\ndata:\n  batch_size: 8  # Overrides data/lentils.yaml's batch_size: 2\n</code></pre>"},{"location":"user-guide/configuration/#override-precedence","title":"Override Precedence","text":"<p>Overrides are applied in this order (later overrides earlier):</p> <ol> <li>Base composed configs (pipeline, data, training)</li> <li>Trainrun config overrides (because <code>_self_</code> comes after defaults)</li> <li>CLI overrides (highest priority)</li> </ol> <p>Example:</p> <pre><code># CLI overrides beat everything\nuv run python examples/channel_selector.py training.trainer.max_epochs=50\n</code></pre> <p>Overrides for non-existing fields</p> <p>If you try to override a field that doesn't exist in the composed config, you'll get an error:</p> <pre><code>uv run python examples/rx_statistical.py training.trainer.max_epochs=50\n# Error: Key 'training' is not in struct\n#        full_key: training\n#        object_type=dict\n</code></pre> <p>This happens because <code>rx_statistical.py</code> uses <code>trainrun/default_statistical</code>, which has no <code>training</code> section (it only uses statistical training, not gradient-based training). To override training parameters, use a trainrun config that includes training defaults, such as <code>trainrun/rx_statistical.yaml</code>.</p>"},{"location":"user-guide/configuration/#accessing-composed-configs-in-python","title":"Accessing Composed Configs in Python","text":"<pre><code>@hydra.main(config_path=\"../configs/\", config_name=\"trainrun/my_experiment\")\ndef main(cfg: DictConfig):\n    # Access composed configs\n    datamodule = SingleCu3sDataModule(**cfg.data)      # From data config\n    pipeline = Pipeline.from_config(cfg.pipeline)       # From pipeline config\n    trainer = Trainer(**cfg.training.trainer)           # From training config\n\n    # Access trainrun-specific fields\n    metric_nodes = cfg.metric_nodes\n</code></pre>"},{"location":"user-guide/configuration/#relationship-to-protobuf-schemas","title":"Relationship to Protobuf Schemas","text":"<p>CUVIS.AI configurations are validated and transported using protocol buffers defined in the <code>cuvis-ai-schemas</code> repository.</p> <p>Schema location: <code>cuvis-ai-schemas/proto/cuvis_ai/grpc/v1/cuvis_ai.proto</code></p>"},{"location":"user-guide/configuration/#configuration-message-types","title":"Configuration Message Types","text":"<p>The protobuf schemas define 6 gRPC message types for config transport:</p> <ul> <li><code>PipelineConfig</code> - Pipeline structure (nodes + connections)</li> <li><code>DataConfig</code> - Data loading configuration</li> <li><code>TrainingConfig</code> - Training hyperparameters</li> <li><code>TrainRunConfig</code> - Composed experiment configuration</li> <li><code>OptimizerConfig</code> - Optimizer parameters</li> <li><code>SchedulerConfig</code> - Learning rate scheduler</li> </ul>"},{"location":"user-guide/configuration/#configuration-flow-yaml-protobuf-grpc","title":"Configuration Flow: YAML \u2192 Protobuf \u2192 gRPC","text":"<p>All config messages use a single <code>bytes config_bytes</code> field that contains JSON-serialized data (not protobuf serialization):</p> <pre><code>flowchart LR\n    A[YAML&lt;br/&gt;Config] --&gt;|Hydra&lt;br/&gt;compose| B[DictConfig&lt;br/&gt;merged]\n    B --&gt;|Pydantic&lt;br/&gt;validate| C[JSON&lt;br/&gt;bytes]\n    C --&gt;|gRPC&lt;br/&gt;transport| D[Server&lt;br/&gt;Process]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e8\n    style D fill:#f5e1ff</code></pre>"},{"location":"user-guide/configuration/#protobuf-yaml-mapping","title":"Protobuf \u2194 YAML Mapping","text":"Protobuf Message YAML Config Files Validation <code>PipelineConfig</code> <code>configs/pipeline/*.yaml</code> Node classes must be importable <code>DataConfig</code> <code>configs/data/*.yaml</code> Paths must exist, splits valid <code>TrainingConfig</code> <code>configs/training/*.yaml</code> Optimizer/scheduler names supported <code>TrainRunConfig</code> <code>configs/trainrun/*.yaml</code> All nested configs must validate"},{"location":"user-guide/configuration/#validation-stages","title":"Validation Stages","text":"<p>Configuration validation happens at multiple stages:</p> <ol> <li>YAML Syntax: YAML parser validates syntax</li> <li>Hydra Composition: Checks for missing defaults, circular references</li> <li>Pydantic Validation: Type checking, field constraints, nested model validation</li> <li>Application Domain: Node existence, valid class paths, port compatibility</li> </ol> <p>JSON Transport Design</p> <p>While protobuf is used for the gRPC message structure, the actual configuration data is JSON-serialized, not protobuf-serialized. This provides flexibility to work with dynamic configuration structures while maintaining type safety through Pydantic validation.</p>"},{"location":"user-guide/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"user-guide/configuration/#available-configs","title":"Available Configs","text":"Directory Purpose Examples <code>configs/pipeline/</code> Sample pipeline configurations (node definitions &amp; connections) <code>rx_statistical.yaml</code>, <code>adaclip_baseline.yaml</code>, <code>deep_svdd.yaml</code> <code>configs/data/</code> Sample data configurations (data loading, splitting, paths) <code>lentils.yaml</code> <code>configs/training/</code> Sample training configurations (trainer, optimizer, scheduler settings) <code>default.yaml</code> <code>configs/trainrun/</code> Sample full trainruns (compose data + pipeline + training + plugins) <code>rx_statistical.yaml</code>, <code>concrete_adaclip.yaml</code> <code>configs/plugins/</code> Plugin registry and manifests <code>registry.yaml</code> <p>See Also</p> <p>For detailed examples of each config type, see the sections above on Pipeline Configurations, Data Configurations, Training Configurations, and Trainrun Configurations.</p>"},{"location":"user-guide/configuration/#using-hydra-in-scripts","title":"Using Hydra in Scripts","text":"<p>Scripts use the <code>@hydra.main</code> decorator to load trainrun configs:</p> <pre><code>from omegaconf import DictConfig\nimport hydra\n\n@hydra.main(config_path=\"../configs/\", config_name=\"trainrun/default_gradient\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    \"\"\"Access composed configs via cfg.data, cfg.pipeline, cfg.training.\"\"\"\n    # Access data config (from configs/data/lentils.yaml)\n    datamodule = SingleCu3sDataModule(**cfg.data)\n\n    # Access training config (from configs/training/default.yaml)\n    trainer_config = cfg.training.trainer\n\n    # Access pipeline config if defined\n    # pipeline_config = cfg.pipeline\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Example from <code>examples/rx_statistical.py</code>:</p> <pre><code>@hydra.main(config_path=\"../configs/\", config_name=\"trainrun/default_statistical\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    # cfg.data comes from the composed data config\n    datamodule = SingleCu3sDataModule(**cfg.data)  # Line 107\n\n    # cfg.output_dir comes from the trainrun config\n    output_dir = Path(cfg.output_dir)  # Line 104\n</code></pre>"},{"location":"user-guide/configuration/#cli-overrides","title":"CLI Overrides","text":"<p>Override any configuration parameter from the command line using Hydra's override syntax:</p> <pre><code># Override data parameters (works with statistical-only configs)\nuv run python examples/rx_statistical.py data.batch_size=8\n\n# Override training parameters (requires gradient training config)\nuv run python examples/channel_selector.py training.trainer.max_epochs=20\n\n# Override multiple parameters\nuv run python examples/channel_selector.py \\\n    training.trainer.max_epochs=20 \\\n    training.optimizer.lr=0.0001 \\\n    data.batch_size=8\n</code></pre> <p>Pattern applies to any Hydra script</p> <p>Replace <code>examples/rx_statistical.py</code> with your actual script path. Any Python script using <code>@hydra.main</code> supports CLI overrides.</p>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Use environment variables in configs:</p> <pre><code># Data configuration\ndata:\n  cu3s_file_path: ${oc.env:DATA_ROOT,./data/Lentils}/Lentils_000.cu3s  # $DATA_ROOT or default\n\n# Training configuration (if using WandB logger)\ntraining:\n  wandb_api_key: ${oc.env:WANDB_API_KEY}  # Required env var\n</code></pre>"},{"location":"user-guide/configuration/#trainingconfig","title":"TrainingConfig","text":"<p>The <code>TrainingConfig</code> dataclass wraps all training parameters:</p> <pre><code>from cuvis_ai_core.training.config import TrainingConfig, TrainerConfig, OptimizerConfig\n\nconfig = TrainingConfig(\n    seed=42,\n    trainer=TrainerConfig(\n        max_epochs=10,\n        accelerator=\"gpu\",\n        devices=1,\n        precision=\"16-mixed\",\n        log_every_n_steps=10,\n    ),\n    optimizer=OptimizerConfig(\n        name=\"adam\",\n        lr=0.001,\n        weight_decay=0.0,\n        betas=(0.9, 0.999),\n    ))\n</code></pre>"},{"location":"user-guide/configuration/#trainer-parameters","title":"Trainer Parameters","text":"<p>Based on <code>TrainerConfig</code> schema from cuvis-ai-schemas:</p> Parameter Type Default Description <code>max_epochs</code> int 100 Maximum number of epochs (1-10000) <code>accelerator</code> str \"auto\" Accelerator type: \"auto\", \"cpu\", \"gpu\", \"cuda\" <code>devices</code> int|str|None None Number of devices or device IDs <code>precision</code> str|int \"32-true\" Precision: \"32-true\", \"16-mixed\", \"bf16-mixed\" <code>log_every_n_steps</code> int 50 Logging frequency (steps) <code>val_check_interval</code> float|int 1.0 Validation check interval <code>check_val_every_n_epoch</code> int 1 Validate every n epochs <code>gradient_clip_val</code> float|None None Gradient clipping threshold <code>accumulate_grad_batches</code> int 1 Accumulate gradients over n batches <code>enable_progress_bar</code> bool True Show progress bar <code>enable_checkpointing</code> bool False Enable model checkpointing <code>deterministic</code> bool False Deterministic mode <code>benchmark</code> bool False Enable cudnn benchmark"},{"location":"user-guide/configuration/#optimizer-parameters","title":"Optimizer Parameters","text":"<p>Based on <code>OptimizerConfig</code> schema from cuvis-ai-schemas:</p> Parameter Type Default Description <code>name</code> str \"adamw\" Optimizer: \"adamw\", \"adam\", \"sgd\" <code>lr</code> float 0.001 Learning rate (1e-6 to 1.0) <code>weight_decay</code> float 0.0 L2 regularization (0.0 to 1.0) <code>momentum</code> float|None 0.9 Momentum for SGD (0.0 to 1.0) <code>betas</code> tuple|None None Adam betas (beta1, beta2), e.g., [0.9, 0.999]"},{"location":"user-guide/configuration/#scheduler-parameters","title":"Scheduler Parameters","text":"<p>Based on <code>SchedulerConfig</code> schema from cuvis-ai-schemas:</p> Parameter Type Default Description <code>name</code> str|None None Scheduler: \"cosine\", \"step\", \"exponential\", \"reduce_on_plateau\" <code>warmup_epochs</code> int 0 Warmup epochs <code>min_lr</code> float 1e-6 Minimum learning rate <code>monitor</code> str|None None Metric to monitor (for plateau scheduler) <code>mode</code> str \"min\" Monitor mode: \"min\" or \"max\" <code>factor</code> float 0.1 LR reduction factor <code>patience</code> int 10 Patience epochs for plateau <code>step_size</code> int|None None LR decay period (for step scheduler) <code>gamma</code> float|None None LR decay multiplier"},{"location":"user-guide/configuration/#training-config-parameters","title":"Training Config Parameters","text":"<p>Based on <code>TrainingConfig</code> schema from cuvis-ai-schemas:</p> Parameter Type Default Description <code>seed</code> int 42 Random seed (\u22650) <code>max_epochs</code> int 100 Maximum epochs (1-10000) <code>batch_size</code> int 32 Batch size (\u22651) <code>num_workers</code> int 4 Data loading workers (\u22650) <code>gradient_clip_val</code> float|None None Gradient clipping (\u22650.0) <code>accumulate_grad_batches</code> int 1 Gradient accumulation batches"},{"location":"user-guide/configuration/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"user-guide/configuration/#development-fast-iteration","title":"Development (Fast Iteration)","text":"<pre><code># Training configuration\ntraining:\n  seed: 42\n  batch_size: 2\n  num_workers: 0\n  trainer:\n    max_epochs: 2\n    accelerator: cpu\n    devices: 1\n    enable_progress_bar: true\n\n# Data configuration\ndata:\n  batch_size: 2\n  cu3s_file_path: data/Lentils/Lentils_000.cu3s\n  annotation_json_path: data/Lentils/Lentils_000.json\n  train_ids: [0]\n  val_ids: [1]\n  test_ids: [1]\n</code></pre>"},{"location":"user-guide/configuration/#production-full-training","title":"Production (Full Training)","text":"<pre><code># Training configuration\ntraining:\n  seed: 42\n  batch_size: 16\n  num_workers: 4\n  trainer:\n    max_epochs: 50\n    accelerator: gpu\n    devices: 1\n    precision: \"16-mixed\"\n    log_every_n_steps: 10\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n\n# Data configuration\ndata:\n  batch_size: 16\n  cu3s_file_path: data/Lentils/Lentils_000.cu3s\n  annotation_json_path: data/Lentils/Lentils_000.json\n  train_ids: [0, 2, 3]\n  val_ids: [1, 5]\n  test_ids: [1, 5]\n</code></pre>"},{"location":"user-guide/configuration/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>training:\n  trainer:\n    accelerator: gpu\n    devices: 4\n    strategy: ddp\n    precision: \"16-mixed\"\n</code></pre>"},{"location":"user-guide/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Hydra validates configurations at runtime:</p> <pre><code>from omegaconf import OmegaConf\nfrom cuvis_ai_core.training.config import TrainingConfig\n\n# Load and validate\ncfg = OmegaConf.load(\"config.yaml\")\ntraining_cfg = TrainingConfig.from_dict_config(cfg.training)\n</code></pre>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/#1-use-composition","title":"1. Use Composition","text":"<p>Break large configs into reusable pieces:</p> <pre><code># base.yaml\ndefaults:\n  - general\n  - monitoring/wandb\n\n# experiment.yaml  \ndefaults:\n  - base\n  - _self_\n\ntraining:\n  trainer:\n    max_epochs: 100\n</code></pre>"},{"location":"user-guide/configuration/#2-version-control-configs","title":"2. Version Control Configs","text":"<p>Commit configuration files for reproducibility:</p> <pre><code>git add configs/\ngit commit -m \"Add experiment config\"\n</code></pre>"},{"location":"user-guide/configuration/#3-use-structured-configs","title":"3. Use Structured Configs","text":"<p>Define configs as dataclasses for type safety:</p> <pre><code>from dataclasses import dataclass\nfrom hydra.core.config_store import ConfigStore\n\n@dataclass\nclass ModelConfig:\n    hidden_size: int = 128\n    num_layers: int = 3\n\ncs = ConfigStore.instance()\ncs.store(name=\"model_config\", node=ModelConfig)\n</code></pre>"},{"location":"user-guide/configuration/#4-document-custom-configs","title":"4. Document Custom Configs","text":"<p>Add comments explaining parameters:</p> <pre><code>nodes:\n  pca:\n    n_components: 3  # Number of principal components to retain\n    trainable: true  # Enable gradient-based fine-tuning\n</code></pre>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/#config-not-found","title":"Config Not Found","text":"<pre><code>ConfigFileNotFoundError: Cannot find 'my_config.yaml'\n</code></pre> <p>Solution: Ensure config file is in correct directory and path is specified correctly.</p>"},{"location":"user-guide/configuration/#override-parse-error","title":"Override Parse Error","text":"<pre><code>OverrideParseException: Expected '=' in override 'training.trainer.max_epochs:10'\n</code></pre> <p>Solution: Use <code>=</code> not <code>:</code> for overrides: <code>training.trainer.max_epochs=10</code></p>"},{"location":"user-guide/configuration/#type-mismatch","title":"Type Mismatch","text":"<pre><code>ValidationError: Field 'max_epochs' expected type 'int', got 'str'\n</code></pre> <p>Solution: Ensure correct type in override: <code>max_epochs=10</code> not <code>max_epochs=\"10\"</code></p>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart: See configuration in action</li> <li>Tutorials: Phase-specific configurations</li> <li>API Reference: TrainingConfig API details</li> </ul>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Install CUVIS.AI and its dependencies.</p>"},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10+ (tested up to 3.13; 3.11 recommended)</li> <li>RAM: 8GB minimum (16GB recommended; 32GB for large datasets)</li> <li>OS: Windows / Linux / macOS</li> <li>GPU (optional): NVIDIA + CUDA 12.8 for faster training</li> <li>Storage: ~2GB for deps (+ space for datasets/outputs)</li> </ul>"},{"location":"user-guide/installation/#install-with-uv-recommended","title":"Install with uv (recommended)","text":"<ol> <li>Install uv:</li> </ol> <pre><code># Linux/macOS\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <ol> <li>Clone and install (all extras):</li> </ol> <pre><code>git clone https://github.com/cubert-hyperspectral/cuvis-ai.git\ncd cuvis-ai\n\nuv sync --all-extras\n</code></pre>"},{"location":"user-guide/installation/#gpu-support-optional","title":"GPU support (optional)","text":"<p>Check CUDA availability:</p> <pre><code>import torch\nprint(torch.cuda.is_available(), torch.version.cuda, torch.cuda.device_count())\n</code></pre>"},{"location":"user-guide/installation/#verify","title":"Verify","text":"<p>Run tests:</p> <pre><code>uv run pytest tests/ -v\n</code></pre> <p>Skip GPU tests (CPU-only):</p> <pre><code>uv run pytest tests/ -v -m \"no gpu\"\n</code></pre> <p>Or quick import:</p> <pre><code>from cuvis_ai_core.pipeline.graph import Graph\nfrom cuvis_ai_core.anomaly.rx_detector import RXGlobal\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"user-guide/installation/#next-steps","title":"Next steps","text":"<ul> <li>Quickstart</li> <li>Configuration</li> <li>Tutorials</li> </ul>"},{"location":"user-guide/limitations/","title":"Limitations","text":"<p>Status: Needs Review</p> <p>This page has not been reviewed for accuracy and completeness. Content may be outdated or contain errors.</p>"},{"location":"user-guide/limitations/#known-limitations","title":"Known Limitations","text":"<p>Status: Under Development (Phase 6) Expected Completion: Week 6</p>"},{"location":"user-guide/limitations/#coming-soon","title":"Coming Soon","text":"<p>This page will document: - Current limitations and constraints - Platform-specific issues - Performance considerations - Unsupported features - Workarounds and alternatives - Planned improvements</p> <p>Related Pages: - Installation - Core Concepts Overview</p>"},{"location":"user-guide/quickstart/","title":"Quickstart Guide","text":"<p>Get up and running with CUVIS.AI in 5 minutes.</p>"},{"location":"user-guide/quickstart/#installation","title":"Installation","text":"<p>First, ensure you have Python 3.10+ and uv installed:</p> <pre><code># Clone the repository\ngit clone https://github.com/cubert-hyperspectral/cuvis-ai.git\ncd cuvis-ai\n\n# Install dependencies\nuv sync\n</code></pre> <p>See the Installation Guide for detailed setup instructions.</p>"},{"location":"user-guide/quickstart/#download-sample-data","title":"Download Sample Data","text":"<p>Download the Lentils dataset from Hugging Face:</p> <pre><code># Automated download (default: lentils dataset)\nuv run download-data\n\n# Or explicitly specify dataset\nuv run download-data --dataset lentils\n</code></pre> <p>This downloads ~1.0 GB of real hyperspectral data to <code>data/Lentils/</code>.</p>"},{"location":"user-guide/quickstart/#quick-demo-run-pre-trained-pipeline","title":"Quick Demo: Run Pre-Trained Pipeline","text":"<p>Want to see CUVIS.AI in action first? Run inference with a pre-configured pipeline:</p> <pre><code># View pipeline structure\nuv run restore-pipeline --pipeline-path configs/pipeline/rx_statistical.yaml\n\n# Run inference on sample data\nuv run restore-pipeline --pipeline-path configs/pipeline/rx_statistical.yaml --cu3s-file-path data/Lentils/Demo_000.cu3s\n</code></pre> <p>This loads the pipeline configuration and runs anomaly detection on the sample hyperspectral cube.</p>"},{"location":"user-guide/quickstart/#train-your-own-pipeline","title":"Train Your Own Pipeline","text":"<p>Train an RX anomaly detector from scratch:</p> <pre><code># Train RX detector\nuv run python examples/rx_statistical.py\n</code></pre> <p>Results are saved to <code>outputs/base_trainrun/</code>.</p>"},{"location":"user-guide/quickstart/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>Loaded data - The Lentils hyperspectral dataset</li> <li>Built pipeline - RX statistical anomaly detector from <code>configs/pipeline/rx_statistical.yaml</code></li> <li>Trained model - Statistical initialization on training data</li> <li>Saved results - Pipeline, weights, and metrics to <code>outputs/</code></li> </ol>"},{"location":"user-guide/quickstart/#use-your-trained-model","title":"Use Your Trained Model","text":"<p>After training, restore and use your model for inference:</p> <pre><code># Restore trained pipeline\nuv run restore-pipeline --pipeline-path outputs/base_trainrun/trained_models/RX_Statistical.yaml --cu3s-file-path data/Lentils/Lentils_000.cu3s\n</code></pre> <p>The pipeline will load your trained weights and run inference on new data.</p>"},{"location":"user-guide/quickstart/#next-steps","title":"Next Steps","text":"<p>Learn the fundamentals:</p> <ul> <li>Core Concepts Overview - Understand the architecture</li> <li>Configuration Basics - Master Hydra composition</li> </ul> <p>Follow comprehensive tutorials:</p> <ul> <li>RX Statistical Tutorial - Statistical anomaly detection</li> <li>Channel Selector Tutorial - Learnable band selection</li> <li>Deep SVDD Tutorial - Deep learning approach</li> </ul> <p>Explore how-to guides:</p> <ul> <li>Build Pipelines in Python</li> <li>Build Pipelines in YAML</li> <li>Restore Trained Models</li> </ul>"}]}